{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ENGR 1330 Example Course A framework for a semester long course in Computational Thinking and Data Science at Texas Tech University For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#engr-1330-example-course","text":"A framework for a semester long course in Computational Thinking and Data Science at Texas Tech University For full documentation visit mkdocs.org .","title":"ENGR 1330 Example Course"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/","text":"%%html <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Course Description: Introducion to Python programming, its relevant modules and libraries, and computational thinking for solving problems in Data Science. Data science approaches for importing, manipulating, and analyzing data. Modeling and visualizing real-world data sets in various science and engineering disciplines. 3 credit hours comprising of lectures and hands-on lab sessions. This course provides a hands-on learning experience in programming and data science using iPython and JupyterLab. iPython is the interactive python kernel implemented in JupyterLab. Prerequisites: Prior programming background is NOT required. The course is intended for first-year WCOE students (aka engineering foundational) COVID-19 Important Guidelines: If Texas Tech University campus operations are required to change because of health concerns related to the COVID-19 pandemic, it is possible that this course will move to a fully online delivery format. Should that be necessary, students will be advised of technical and/or equipment requirements, including remote proctoring software. Policy on absences resulting from illness: We anticipate that some students may have extended absences. To avoid students feeling compelled to attend in-person class periods when having symptoms or feeling unwell, a standard policy is provided that holds students harmless for illness-related absences (see Section A below). A. Illness-Based Absence Policy (Face-to-Face Classes) If at any time during the semester you are ill, in the interest of your own health and safety as well as the health and safety of your instructors and classmates, you are encouraged not to attend face-to-face class meetings or events. Please review the steps outlined below that you should follow to ensure your absence for illness will be excused. These steps also apply to not participating in synchronous online class meetings if you feel too ill to do so and missing specified assignment due dates in asynchronous online classes because of illness. If you are ill and think the symptoms might be COVID-19-related: Call Student Health Services at 806.743.2848 or your health care provider. During after-hours and on weekends, contact TTU COVID-19 Helpline at TBD. Self-report as soon as possible using the Dean of Students COVID-19 webpage. This website has specific directions about how to upload documentation from a medical provider and what will happen if your illness renders you unable to participate in classes for more than one week. If your illness is determined to be COVID-19-related, all remaining documentation and communication will be handled through the Office of the Dean of Students, including notification of your instructors of the time you may be absent from and may return to classes. If your illness is determined not to be COVID-19-related, please follow steps 2.a-d below. If you are ill and can attribute your symptoms to something other than COVID-19: If your illness renders you unable to attend face-to-face classes, participate in synchronous online classes, or miss specified assignment due dates in asynchronous online classes, you are encouraged to contact either Student Health Services at 806.743.2848 or your health care provider. Note that Student Health Services and your own and other health care providers may arrange virtual visits. During the health provider visit, request a \u201creturn to school\u201d note. E-mail the instructor a picture of that note. Return to class by the next class period after the date indicated on your note. Following the steps outlined above helps to keep your instructors informed about your absences and ensures your absence or missing an assignment due date because of illness will be marked excused. You will still be responsible to complete within a week of returning to class any assignments, quizzes, or exams you miss because of illness. B. Illness-Based Absence Policy (Telepresence/On-Line Classes) Same as above with respect potential to infect others; go to a health care provider if you are ill. Telepresence courses are recorded and will be available on TTU MediaSite and/or YouTube (unlisted). Exercises, Quizzes, and Examinations are all administered by a Learning Management System (Blackboard) and students need to allow enough time to complete and upload their work. Due date adjustments/late submits on case-by-case basis; documentation required as in subsection A above. Course Sections Lesson time, days, and location: Section D04; CRN 64436; 1000-1120 T, TH ; Telepresence Lab Section D66; CRN 64441; 1130-1250 T, TH Section D01; CRN 63306; 1000-1120 T, TH ; Telepresence Lab Section D61; CRN 63744; 1130-1250 T, TH Course Instructor: Instructor: Theodore G. Cleveland, Ph.D., P.E., M. ASCE, F. EWRI Email: theodore.cleveland@ttu.edu (put ENGR 1330 in subject line for email related to this class) Office location: Telepresence ( Zoom ) Office hours: 1000-1100 M, 1600-1700 W or by appointment (meetings will be by Zoom call) Teaching Assistant: Teaching Assistant: Farhang Forghanparast, MSCE Email : Farhang.Forghanparast@ttu.edu Office location: Telepresence ( Zoom ) Office hours: 0900-1000 M; 1700-1800 W Textbook: Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0). Link: https://www.inferentialthinking.com/chapters/intro. Course Contents: Computational thinking for problem-solving: Logical problem solving, decomposition, pattern recognition, abstraction, representation, algorithm design, and generalization. Python Programming: Variables, constants, data types, data structures, strings, math operators boolean operators, expressions, program constructs, functions, looping, I/O files, modules, and database. Data science fundamentals: Experimental setup: Importing and formatting data sets, Displaying data, Data pre-processing. Introductory statistical analysis with Python: Elementary statistics, randomness, sampling, probability distributions, Confidence intervals, hypothesis testing, and A/B testing. Basic data analysis, visualization, and machine learning: Data pre-processing, Supervised/unsupervised learning, Performance evaluation metrics. Learning Outcomes: On completion of the course, students will have * Created Python programs employing computational thinking concepts to * Employed Python libraries relevant to data science. * Downloaded data files from selected public sources and analyzed content. * Created scripts to perform fundamental data analytics and basic visualization. ABET Student Outcomes Engineering: An ability to identify, formulate, and solve complex engineering problems by applying principles of engineering, science, and mathematics. An ability to acquire and apply new knowledge as needed, using appropriate learning strategies. Computer Science: Analyze a complex computing problem and to apply principles of computing and other relevant disciplines to identify solutions. Design, implement, and evaluate a computing-based solution to meet a given set of computing requirements in the context of the program\u2019s discipline. Resources/Tools Platforms for Python Programming (for your own computers) Anaconda platform (https://www.anaconda.com/): Anaconda distribution is an open-source Data Science Distribution Development Platform. It includes Python 3 with over 1,500 data science packages making it easy to manage libraries and dependencies. Available in Linux, Windows, and Mac OS X. Jupyter (https://jupyter.org/): JupyterLab is a web-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: Configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. note Anaconda for MacOS includes a JupyterLab instance, so a separate install is not required. Additional Modules for Python Programming Math module (https://docs.python.org/3/library/math.html): Gives access to the mathematical functions defined by the C standard e.g. factorial, gcd, exponential, logarithm. Operator module (https://docs.python.org/3/library/operator.html): Helps in exporting a set of efficient functions corresponding to the intrinsic operators of Python. For example, the operator add(x,y) is equivalent to the expression x+y. Python Modules for Data Science Scipy module (https://www.scipy.org/): A Python-based ecosystem of open-source software for mathematics, science, and engineering. Some of the core packages are: Numpy: Provides n-dimensional array package Scipy: Fundamental for scientific computing (e.g. linear algorithm, optimization) Matplotlib: Visualizations/2D plotting IPython: Enhanced interactive console <<= this is the kernel used in JupyterLab Pandas: Data structures and data analysis Scikit-learn module (https://scikit-learn.org/stable/): A library for machine learning in Python. It is a simple and efficient tool for predictive data analysis. It is built on NumPy, SciPy, and matplotlib modules. On-Line Options AWS Lightsail Instance (use Windows Server 2000 template; lowest resource provision tier; AWS RDP client, or download and install own RDP client) Then install Anaconda onto the AWS Instance Hardware Requirements Minimal, in fact this syllabus was created using a JupyterLab notebook (as a markdown processor) on a Raspberry Pi 4B, which technically cannot support a JupyterHub, but does. Your current laptop should be fine, or if you only have a chromebook, build an AWS instance. The college of engineering has specific laptop requirements for your other courses that are listed at https://www.depts.ttu.edu/coe/dean/engineeringitservices/buyingtherightcomputer.php Content Server Blackboard is used as the learning management system (LMS) for this class, and it uses web links to a content server at https://3.137.111.182/engr-1330-webroot/ The Blackboard links will generally go directly to a section in the webroot, but feel free to explore by going in the front door! Course Schedule Item Lesson Lab JupyterLab(Python Kernel) and Programming 21Jan2021 Lesson 0 Introduction to Computational Thinking with Data Science: - Computational thinking concepts - Python as a programming environment - Data science and practices - CCMR Approach Computing Environment set up: - Installing Anaconda (Win/MacOS/AWS) \u2013 Jupyter notebooks - Simple Examples 26Jan2021 Lesson 1 Programming Fundamentals: - Data types (int, float, string, bool) - Variables, operators, expressions, basic I/O - String functions and operations Introduction to Python - Data types (e.g. int, float, string, bool) - Expressions 28Jan2021 Lesson 2 Programming Fundamentals: - Data structures: Array, list, tuple, set, dictionary - Conditional statements Introduction to Python - Data structures - Conditional statements 2Feb2021 Lesson 3 Programming Fundamentals: - Loops - Flowcharts Introduction to Python - Loops 4Feb2021 Lesson 4 Programming Fundamentals: - Functions - Variable scope Introduction to Python - Functions - Variable scope 9Feb2021 Lesson 5 Programming Fundamentals: - Class and objects - File handling Introduction to Python - Class and objects - File handling Data Science External Modules 11Feb2021 Lesson 6 Data Representation and Operations: Python library: NumPy - Data representation: Arrays, vectors, matrices - Data operations: Indexing, math functions Exercises on NumPy 16Feb2021 Lesson 7 Data Query and Manipulation: Python Library: Pandas - Data frames: - Create, index, summarize statistics - fill and drop values - read/write to file Exercises on Pandas 18Feb2021 Lesson 8 Data Display: Python Libraries: Matplotlib - Graphing Conventions - Data Display for line charts, bar charts, box plot, scatter plot, and histograms Exercises on data display 23Feb2021 Lesson 9 Data Modeling: - Establishing causality - Randomness - Models as Preciction Machines Exercises on causality and simulation 25Feb2021 Lesson 10 Randomness and Probabilities: - Sampling - Empirical distributions Exercises on probabilities 2Mar2021 Lesson 11 Descriptive Statistics - Location/Center (mean, median,mode) - Dispersion/Spread (variance, standard deviation) - Asymmetry/Skew (Coefficient of Skewness) Descriptive Statistics 4Mar2021 Review for Exam-1 (Lessons 0-9) Exam-1 - LMS administered Data Modeling: Statistical Approaches 9Mar2021 Lesson 12 Distributions: - Normal, LogNormal - Gamma, Weibull - Extreme Value (Gumbell) Exercises on sampling 11Mar2021 Lesson 13 Probability Estimation Modeling - Ranking, order, plotting position - Distribution Fitting ; Method Of Moments; Maximum Likelihood Estimation Exercises 16Mar2021 Lesson 14 Hypothesis testing: - General concept - Assessing data models. Exercises on hypothesis testing 18Mar2021 Lesson 15 Hypothesis testing: -Comparing proportions - Type1 & Type2 errors - Attained significance (p-value) Exercises on hypothesis testing 23Mar2021 Lesson 16 Comparing two samples: A/B Testing Exercises on A/B testing 25Mar2021 17. Interval Estimates - Confidence interval concept - Bootstrap simulation Exercises 30Mar2021 Review for Exam-2 (Lessons 10-16) Exam-2 - LMS administered Data Modeling: Regression (Model Fitting) Approaches 1Apr2021 18. Data Modeling: Regression Approach - Linear algebra of equation fitting Exercises 6Apr2021 19. Estimation Modeling by Regression: - Ordinary least squares (OLS) regression - Weighted least squares - Explanitory variables (features) - Response variable(s) exre 8Apr2021 20. Estimation Modeling by Regression: - Residuals - Performance metrics: Accuracy, error - Inference exercises 13Apr2021 21. Estimation Modeling by Regression: - Logistic Regression (a type of classification) Exercises on sample means Data Modeling: Machine Learning Approaches 15Apr2021 22. Data Modeling : The Machine Learning Approach: - Correlation - Training (a model fitting analog) - Confusion matrix, precision, recall, accuracy, F-score. - Making decisions Final projects selection - Project choices - Delivery schedule 20Apr2021 Lesson 23 Evaluation and Making Decisions: - Confusion matrix, precision, recall, accuracy, F-score. - Making decisions KNN with evaluation 22Apr2021 Lesson 24 Classification: - K Nearest Neighbor (KNN) More KNN Demonstration/Examples 27Apr2021 Review for Exam-3 (Lessons 17-24) Exam 3 - LMS administered 29Apr2021 Lesson 25 Classification: - Support Vector Machines (SVM) SVM Demonstration/Examples 4May2021 Lesson 26 Classification: - Artifical Neural Networks (ANN) ANN Demonstration 11May2021 Final Project Report and Link to Video Course Assessment and Grading Criteria: There will be three exams and one comprehensive final project for this course. An AI-supervised online community using the Packback Questions (https://www.packback.co) platform will be used for online discussion about class topics. The platform is the place to pose questions and answers and share codes and problems. Face-to-face sections do not use packback, and use a different scoring distribution In addition, lab notebooks, quizzes, and homework assignments also contribute to the final grade. Late assignments will not be scored. Grades will be based on the following components; weighting is approximate: Assessment Instrument Weight(%) Exam-1 10 Exam-2 10 Exam-3 15 Lab Notebooks & Homework 30 Quizzes 10 Packback 15 Final project 10 Overall total 100 Letter grades will be assigned using the following proportions: Normalized Score Range Letter Grade \u2265 90 A 80-89 B 70-79 C 55-69 D < 55 F Packback Questions Environment Participation is a requirement for this course, and the Packback Questions platform will be used for online discussion about class topics. Packback Questions is an online community where you can be fearlessly curious and ask open-ended questions to build on top of what we are covering in class and relate topics to real-world applications. Packback Requirements: Your participation on Packback will count toward 15% of your overall course grade. There will be a Weekly Sunday at 10:00PM CST deadline for submissions. In order to receive your points per week, you should submit the following per each deadline period: 1 open-ended Question per week with a minimum Curiosity Score of 30, each worth 33.33% of each assignment grade 2 Responses per week with a minimum Curiosity Score of 30, each worth 66.67% of each assignment grade Half credit will be provided for questions and responses that do not meet the minimum curiosity score. How to Register on Packback: An email invitation will be sent to you from help@packback.co prompting you to finish registration. If you don\u2019t receive an email (be sure to check your spam), you may register by following the instructions below: Create an account by navigating to https://questions.packback.co and clicking \u201cSign up for an Account\u201d Note: If you already have an account on Packback you can log in with your credentials. Then enter our class community\u2019s lookup key into the \u201cLooking to join a community you don't see here?\u201d section in Packback at the bottom of the homepage. Community Lookup Key: 1e3bb85a-ddb2-456a-a8fc-178ead55206d Follow the instructions on your screen to finish your registration. Packback will require a paid subscription (~$25). Refer to www.packback.co/product/pricing for more information. How to Get Help from the Packback Team: If you have any questions or concerns about Packback throughout the semester, please read their FAQ at help.packback.co. If you need more help, contact their customer support team directly at help@packback.co. For a brief introduction to Packback Questions and why we are using it in class, watch this video: vimeo.com/packback/Welcome-to-Packback-Questions Classroom Policy: The following activities are not allowed in the classroom: Texting or talking on the cellphone or other electronic devices, and reading non-course related materials. Telepresence (On-line) Courses Obviously electronic devices are vital; disrupting the conference is prohibited, please mute your microphone unless you have a question - consider typing your question into the chat window as well. Be aware of bandwidth issues and remember most lessons and laboratory sessions are recorded and posted on youtube. Recording, editing, and rendering takes awhile, so expect 24-36 hour delay before video is available. ADA Statement: Any student who, because of a disability, may require special arrangements in order to meet the course requirements should contact the instructor as soon as possible to make necessary arrangements. Students must present appropriate verification from Student Disability Services during the instructor's office hours. Please note that instructors are not allowed to provide classroom accommodation to a student until appropriate verification from Student Disability Services has been provided. For additional information, please contact Student Disability Services office in 335 West Hall or call 806.742.2405. Academic Integrity Statement: Academic integrity is taking responsibility for one\u2019s own class and/or course work, being individually accountable, and demonstrating intellectual honesty and ethical behavior. Academic integrity is a personal choice to abide by the standards of intellectual honesty and responsibility. Because education is a shared effort to achieve learning through the exchange of ideas, students, faculty, and staff have the collective responsibility to build mutual trust and respect. Ethical behavior and independent thought are essential for the highest level of academic achievement, which then must be measured. Academic achievement includes scholarship, teaching, and learning, all of which are shared endeavors. Grades are a device used to quantify the successful accumulation of knowledge through learning. Adhering to the standards of academic integrity ensures grades are earned honestly. Academic integrity is the foundation upon which students, faculty, and staff build their educational and professional careers. [Texas Tech University (\u201cUniversity\u201d) Quality Enhancement Plan, Academic Integrity Task Force, 2010]. Religious Holy Day Statement: \u201cReligious holy day\u201d means a holy day observed by a religion whose places of worship are exempt from property taxation under Texas Tax Code \u00a711.20. A student who intends to observe a religious holy day should make that intention known to the instructor prior to the absence. A student who is absent from classes for the observance of a religious holy day shall be allowed to take an examination or complete an assignment scheduled for that day within a reasonable time after the absence. A student who is excused may not be penalized for the absence; however, the instructor may respond appropriately if the student fails to complete the assignment satisfactorily. Ethical Conduct Policy: Cheating is prohibited, and the representation of the work of another person as your own will be grounds for receiving a failing grade in the course. DISCRIMINATION, HARASSMENT, AND SEXUAL VIOLENCE STATEMENT: Texas Tech University is committed to providing and strengthening an educational, working, and living environment where students, faculty, staff, and visitors are free from gender and/or sex discrimination of any kind. Sexual assault, discrimination, harassment, and other Title IX violations are not tolerated by the University. Report any incidents to the Office for Student Rights & Resolution, (806)-742-SAFE (7233) or file a report online at titleix.ttu.edu/students. Faculty and staff members at TTU are committed to connecting you to resources on campus. Some of these available resources are: TTU Student Counseling Center, 806- 742-3674, https://www.depts.ttu.edu/scc/(Provides confidential support on campus.) TTU 24-hour Crisis Helpline, 806-742-5555, (Assists students who are experiencing a mental health or interpersonal violence crisis. If you call the helpline, you will speak with a mental health counselor.) Voice of Hope Lubbock Rape Crisis Center, 806-763-7273, voiceofhopelubbock.org (24-hour hotline that provides support for survivors of sexual violence.) The Risk, Intervention, Safety and Education (RISE) Office, 806-742-2110, https://www.depts.ttu.edu/rise/ (Provides a range of resources and support options focused on prevention education and student wellness.) Texas Tech Police Department, 806-742- 3931,http://www.depts.ttu.edu/ttpd/ (To report criminal activity that occurs on or near Texas Tech campus.) CIVILITY IN THE CLASSROOM STATEMENT: Texas Tech University is a community of faculty, students, and staff that enjoys an expectation of cooperation, professionalism, and civility during the conduct of all forms of university business, including the conduct of student\u2013student and student\u2013faculty interactions in and out of the classroom. Further, the classroom is a setting in which an exchange of ideas and creative thinking should be encouraged and where intellectual growth and development are fostered. Students who disrupt this classroom mission by rude, sarcastic, threatening, abusive or obscene language and/or behavior will be subject to appropriate sanctions according to university policy. Likewise, faculty members are expected to maintain the highest standards of professionalism in all interactions with all constituents of the university. To ensure that you are fully engaged in class discussions and account team meetings during class time, you are expected to do the following: - Maintain the same level of civility and professionalism that would be expected in a face-to-face classroom setting. - Attend all classes regularly. - Log into the video conference on time and remain logged in for the duration of the class period. - Activate your camera so that you are visible to the instructor and other students in the class. If you have concerns about leaving your camera on (such as childcare obligations, privacy issues, or a particular circumstance during a class period), please talk to the instructor. - Refrain from engaging in non-class related activities during class time that create a distraction for other students in the class and/or limit your ability to engage in the course. Failure to meet these expectations may result in the following consequences: 1. Being counted as absent for the class meeting. 2. Not receiving credit for class participation for that class period. 3. Other consequences as stipulated in the syllabus, Texas Tech Code of Student Conduct, or other university policy. Repeated failure to meet expectations (e.g., attendance, participation in class, etc.), in addition to the above consequences, may result in the one or more of the following consequences: 1. Referral to the appropriate Associate Dean. 2. Academic penalty, ranging from a warning to failure of the course. (www.depts.ttu.edu/ethics/matadorchallenge/ethicalprinciples.php). LGBTQIA SUPPORT STATEMENT: I identify as an ally to the lesbian, gay, bisexual, transgender, queer, intersex, and asexual (LGBTQIA) community, and I am available to listen and support you in an affirming manner. I can assist in connecting you with resources on campus to address problems you may face pertaining to sexual orientation and/or gender identity that could interfere with your success at Texas Tech. Please note that additional resources are available through the Office of LGBTQIA within the Center for Campus Life, Student Union Building Room 201, www.lgbtqia.ttu.edu, 806.742.5433.\u201d Office of LGBTQIA, Student Union Building Room 201, www.lgbtqia.ttu.edu, 806.742.5433 Within the Center for Campus Life, the Office serves the Texas Tech community through facilitation and leadership of programming and advocacy efforts. This work is aimed at strengthening the lesbian, gay, bisexual, transgender, queer, intersex, and asexual (LGBTQIA) community and sustaining an inclusive campus that welcomes people of all sexual orientations, gender identities, and gender expressions.","title":"Syllabus"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#engr-1330-computational-thinking-with-data-science","text":"","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#course-description","text":"Introducion to Python programming, its relevant modules and libraries, and computational thinking for solving problems in Data Science. Data science approaches for importing, manipulating, and analyzing data. Modeling and visualizing real-world data sets in various science and engineering disciplines. 3 credit hours comprising of lectures and hands-on lab sessions. This course provides a hands-on learning experience in programming and data science using iPython and JupyterLab. iPython is the interactive python kernel implemented in JupyterLab.","title":"Course Description:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#prerequisites","text":"Prior programming background is NOT required. The course is intended for first-year WCOE students (aka engineering foundational)","title":"Prerequisites:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#covid-19-important-guidelines","text":"If Texas Tech University campus operations are required to change because of health concerns related to the COVID-19 pandemic, it is possible that this course will move to a fully online delivery format. Should that be necessary, students will be advised of technical and/or equipment requirements, including remote proctoring software. Policy on absences resulting from illness: We anticipate that some students may have extended absences. To avoid students feeling compelled to attend in-person class periods when having symptoms or feeling unwell, a standard policy is provided that holds students harmless for illness-related absences (see Section A below).","title":"COVID-19 Important Guidelines:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#a-illness-based-absence-policy-face-to-face-classes","text":"If at any time during the semester you are ill, in the interest of your own health and safety as well as the health and safety of your instructors and classmates, you are encouraged not to attend face-to-face class meetings or events. Please review the steps outlined below that you should follow to ensure your absence for illness will be excused. These steps also apply to not participating in synchronous online class meetings if you feel too ill to do so and missing specified assignment due dates in asynchronous online classes because of illness. If you are ill and think the symptoms might be COVID-19-related: Call Student Health Services at 806.743.2848 or your health care provider. During after-hours and on weekends, contact TTU COVID-19 Helpline at TBD. Self-report as soon as possible using the Dean of Students COVID-19 webpage. This website has specific directions about how to upload documentation from a medical provider and what will happen if your illness renders you unable to participate in classes for more than one week. If your illness is determined to be COVID-19-related, all remaining documentation and communication will be handled through the Office of the Dean of Students, including notification of your instructors of the time you may be absent from and may return to classes. If your illness is determined not to be COVID-19-related, please follow steps 2.a-d below. If you are ill and can attribute your symptoms to something other than COVID-19: If your illness renders you unable to attend face-to-face classes, participate in synchronous online classes, or miss specified assignment due dates in asynchronous online classes, you are encouraged to contact either Student Health Services at 806.743.2848 or your health care provider. Note that Student Health Services and your own and other health care providers may arrange virtual visits. During the health provider visit, request a \u201creturn to school\u201d note. E-mail the instructor a picture of that note. Return to class by the next class period after the date indicated on your note. Following the steps outlined above helps to keep your instructors informed about your absences and ensures your absence or missing an assignment due date because of illness will be marked excused. You will still be responsible to complete within a week of returning to class any assignments, quizzes, or exams you miss because of illness.","title":"A. Illness-Based Absence Policy (Face-to-Face Classes)"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#b-illness-based-absence-policy-telepresenceon-line-classes","text":"Same as above with respect potential to infect others; go to a health care provider if you are ill. Telepresence courses are recorded and will be available on TTU MediaSite and/or YouTube (unlisted). Exercises, Quizzes, and Examinations are all administered by a Learning Management System (Blackboard) and students need to allow enough time to complete and upload their work. Due date adjustments/late submits on case-by-case basis; documentation required as in subsection A above.","title":"B. Illness-Based Absence Policy (Telepresence/On-Line Classes)"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#course-sections","text":"Lesson time, days, and location: Section D04; CRN 64436; 1000-1120 T, TH ; Telepresence Lab Section D66; CRN 64441; 1130-1250 T, TH Section D01; CRN 63306; 1000-1120 T, TH ; Telepresence Lab Section D61; CRN 63744; 1130-1250 T, TH","title":"Course Sections"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#course-instructor","text":"Instructor: Theodore G. Cleveland, Ph.D., P.E., M. ASCE, F. EWRI Email: theodore.cleveland@ttu.edu (put ENGR 1330 in subject line for email related to this class) Office location: Telepresence ( Zoom ) Office hours: 1000-1100 M, 1600-1700 W or by appointment (meetings will be by Zoom call)","title":"Course Instructor:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#teaching-assistant","text":"Teaching Assistant: Farhang Forghanparast, MSCE Email : Farhang.Forghanparast@ttu.edu Office location: Telepresence ( Zoom ) Office hours: 0900-1000 M; 1700-1800 W","title":"Teaching Assistant:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#textbook","text":"Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0). Link: https://www.inferentialthinking.com/chapters/intro.","title":"Textbook:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#course-contents","text":"Computational thinking for problem-solving: Logical problem solving, decomposition, pattern recognition, abstraction, representation, algorithm design, and generalization. Python Programming: Variables, constants, data types, data structures, strings, math operators boolean operators, expressions, program constructs, functions, looping, I/O files, modules, and database. Data science fundamentals: Experimental setup: Importing and formatting data sets, Displaying data, Data pre-processing. Introductory statistical analysis with Python: Elementary statistics, randomness, sampling, probability distributions, Confidence intervals, hypothesis testing, and A/B testing. Basic data analysis, visualization, and machine learning: Data pre-processing, Supervised/unsupervised learning, Performance evaluation metrics.","title":"Course Contents:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#learning-outcomes","text":"On completion of the course, students will have * Created Python programs employing computational thinking concepts to * Employed Python libraries relevant to data science. * Downloaded data files from selected public sources and analyzed content. * Created scripts to perform fundamental data analytics and basic visualization.","title":"Learning Outcomes:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#abet-student-outcomes","text":"Engineering: An ability to identify, formulate, and solve complex engineering problems by applying principles of engineering, science, and mathematics. An ability to acquire and apply new knowledge as needed, using appropriate learning strategies. Computer Science: Analyze a complex computing problem and to apply principles of computing and other relevant disciplines to identify solutions. Design, implement, and evaluate a computing-based solution to meet a given set of computing requirements in the context of the program\u2019s discipline.","title":"ABET Student Outcomes"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#resourcestools","text":"","title":"Resources/Tools"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#platforms-for-python-programming-for-your-own-computers","text":"Anaconda platform (https://www.anaconda.com/): Anaconda distribution is an open-source Data Science Distribution Development Platform. It includes Python 3 with over 1,500 data science packages making it easy to manage libraries and dependencies. Available in Linux, Windows, and Mac OS X. Jupyter (https://jupyter.org/): JupyterLab is a web-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: Configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. note Anaconda for MacOS includes a JupyterLab instance, so a separate install is not required.","title":"Platforms for Python Programming (for your own computers)"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#additional-modules-for-python-programming","text":"Math module (https://docs.python.org/3/library/math.html): Gives access to the mathematical functions defined by the C standard e.g. factorial, gcd, exponential, logarithm. Operator module (https://docs.python.org/3/library/operator.html): Helps in exporting a set of efficient functions corresponding to the intrinsic operators of Python. For example, the operator add(x,y) is equivalent to the expression x+y.","title":"Additional Modules for Python Programming"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#python-modules-for-data-science","text":"Scipy module (https://www.scipy.org/): A Python-based ecosystem of open-source software for mathematics, science, and engineering. Some of the core packages are: Numpy: Provides n-dimensional array package Scipy: Fundamental for scientific computing (e.g. linear algorithm, optimization) Matplotlib: Visualizations/2D plotting IPython: Enhanced interactive console <<= this is the kernel used in JupyterLab Pandas: Data structures and data analysis Scikit-learn module (https://scikit-learn.org/stable/): A library for machine learning in Python. It is a simple and efficient tool for predictive data analysis. It is built on NumPy, SciPy, and matplotlib modules.","title":"Python Modules for Data Science"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#on-line-options","text":"AWS Lightsail Instance (use Windows Server 2000 template; lowest resource provision tier; AWS RDP client, or download and install own RDP client) Then install Anaconda onto the AWS Instance","title":"On-Line Options"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#hardware-requirements","text":"Minimal, in fact this syllabus was created using a JupyterLab notebook (as a markdown processor) on a Raspberry Pi 4B, which technically cannot support a JupyterHub, but does. Your current laptop should be fine, or if you only have a chromebook, build an AWS instance. The college of engineering has specific laptop requirements for your other courses that are listed at https://www.depts.ttu.edu/coe/dean/engineeringitservices/buyingtherightcomputer.php","title":"Hardware Requirements"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#content-server","text":"Blackboard is used as the learning management system (LMS) for this class, and it uses web links to a content server at https://3.137.111.182/engr-1330-webroot/ The Blackboard links will generally go directly to a section in the webroot, but feel free to explore by going in the front door!","title":"Content Server"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#course-schedule","text":"Item Lesson Lab JupyterLab(Python Kernel) and Programming 21Jan2021 Lesson 0 Introduction to Computational Thinking with Data Science: - Computational thinking concepts - Python as a programming environment - Data science and practices - CCMR Approach Computing Environment set up: - Installing Anaconda (Win/MacOS/AWS) \u2013 Jupyter notebooks - Simple Examples 26Jan2021 Lesson 1 Programming Fundamentals: - Data types (int, float, string, bool) - Variables, operators, expressions, basic I/O - String functions and operations Introduction to Python - Data types (e.g. int, float, string, bool) - Expressions 28Jan2021 Lesson 2 Programming Fundamentals: - Data structures: Array, list, tuple, set, dictionary - Conditional statements Introduction to Python - Data structures - Conditional statements 2Feb2021 Lesson 3 Programming Fundamentals: - Loops - Flowcharts Introduction to Python - Loops 4Feb2021 Lesson 4 Programming Fundamentals: - Functions - Variable scope Introduction to Python - Functions - Variable scope 9Feb2021 Lesson 5 Programming Fundamentals: - Class and objects - File handling Introduction to Python - Class and objects - File handling Data Science External Modules 11Feb2021 Lesson 6 Data Representation and Operations: Python library: NumPy - Data representation: Arrays, vectors, matrices - Data operations: Indexing, math functions Exercises on NumPy 16Feb2021 Lesson 7 Data Query and Manipulation: Python Library: Pandas - Data frames: - Create, index, summarize statistics - fill and drop values - read/write to file Exercises on Pandas 18Feb2021 Lesson 8 Data Display: Python Libraries: Matplotlib - Graphing Conventions - Data Display for line charts, bar charts, box plot, scatter plot, and histograms Exercises on data display 23Feb2021 Lesson 9 Data Modeling: - Establishing causality - Randomness - Models as Preciction Machines Exercises on causality and simulation 25Feb2021 Lesson 10 Randomness and Probabilities: - Sampling - Empirical distributions Exercises on probabilities 2Mar2021 Lesson 11 Descriptive Statistics - Location/Center (mean, median,mode) - Dispersion/Spread (variance, standard deviation) - Asymmetry/Skew (Coefficient of Skewness) Descriptive Statistics 4Mar2021 Review for Exam-1 (Lessons 0-9) Exam-1 - LMS administered Data Modeling: Statistical Approaches 9Mar2021 Lesson 12 Distributions: - Normal, LogNormal - Gamma, Weibull - Extreme Value (Gumbell) Exercises on sampling 11Mar2021 Lesson 13 Probability Estimation Modeling - Ranking, order, plotting position - Distribution Fitting ; Method Of Moments; Maximum Likelihood Estimation Exercises 16Mar2021 Lesson 14 Hypothesis testing: - General concept - Assessing data models. Exercises on hypothesis testing 18Mar2021 Lesson 15 Hypothesis testing: -Comparing proportions - Type1 & Type2 errors - Attained significance (p-value) Exercises on hypothesis testing 23Mar2021 Lesson 16 Comparing two samples: A/B Testing Exercises on A/B testing 25Mar2021 17. Interval Estimates - Confidence interval concept - Bootstrap simulation Exercises 30Mar2021 Review for Exam-2 (Lessons 10-16) Exam-2 - LMS administered Data Modeling: Regression (Model Fitting) Approaches 1Apr2021 18. Data Modeling: Regression Approach - Linear algebra of equation fitting Exercises 6Apr2021 19. Estimation Modeling by Regression: - Ordinary least squares (OLS) regression - Weighted least squares - Explanitory variables (features) - Response variable(s) exre 8Apr2021 20. Estimation Modeling by Regression: - Residuals - Performance metrics: Accuracy, error - Inference exercises 13Apr2021 21. Estimation Modeling by Regression: - Logistic Regression (a type of classification) Exercises on sample means Data Modeling: Machine Learning Approaches 15Apr2021 22. Data Modeling : The Machine Learning Approach: - Correlation - Training (a model fitting analog) - Confusion matrix, precision, recall, accuracy, F-score. - Making decisions Final projects selection - Project choices - Delivery schedule 20Apr2021 Lesson 23 Evaluation and Making Decisions: - Confusion matrix, precision, recall, accuracy, F-score. - Making decisions KNN with evaluation 22Apr2021 Lesson 24 Classification: - K Nearest Neighbor (KNN) More KNN Demonstration/Examples 27Apr2021 Review for Exam-3 (Lessons 17-24) Exam 3 - LMS administered 29Apr2021 Lesson 25 Classification: - Support Vector Machines (SVM) SVM Demonstration/Examples 4May2021 Lesson 26 Classification: - Artifical Neural Networks (ANN) ANN Demonstration 11May2021 Final Project Report and Link to Video","title":"Course Schedule"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#course-assessment-and-grading-criteria","text":"There will be three exams and one comprehensive final project for this course. An AI-supervised online community using the Packback Questions (https://www.packback.co) platform will be used for online discussion about class topics. The platform is the place to pose questions and answers and share codes and problems. Face-to-face sections do not use packback, and use a different scoring distribution In addition, lab notebooks, quizzes, and homework assignments also contribute to the final grade. Late assignments will not be scored. Grades will be based on the following components; weighting is approximate: Assessment Instrument Weight(%) Exam-1 10 Exam-2 10 Exam-3 15 Lab Notebooks & Homework 30 Quizzes 10 Packback 15 Final project 10 Overall total 100 Letter grades will be assigned using the following proportions: Normalized Score Range Letter Grade \u2265 90 A 80-89 B 70-79 C 55-69 D < 55 F","title":"Course Assessment and Grading Criteria:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#packback-questions-environment","text":"Participation is a requirement for this course, and the Packback Questions platform will be used for online discussion about class topics. Packback Questions is an online community where you can be fearlessly curious and ask open-ended questions to build on top of what we are covering in class and relate topics to real-world applications.","title":"Packback Questions Environment"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#packback-requirements","text":"Your participation on Packback will count toward 15% of your overall course grade. There will be a Weekly Sunday at 10:00PM CST deadline for submissions. In order to receive your points per week, you should submit the following per each deadline period: 1 open-ended Question per week with a minimum Curiosity Score of 30, each worth 33.33% of each assignment grade 2 Responses per week with a minimum Curiosity Score of 30, each worth 66.67% of each assignment grade Half credit will be provided for questions and responses that do not meet the minimum curiosity score.","title":"Packback Requirements:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#how-to-register-on-packback","text":"An email invitation will be sent to you from help@packback.co prompting you to finish registration. If you don\u2019t receive an email (be sure to check your spam), you may register by following the instructions below: Create an account by navigating to https://questions.packback.co and clicking \u201cSign up for an Account\u201d Note: If you already have an account on Packback you can log in with your credentials. Then enter our class community\u2019s lookup key into the \u201cLooking to join a community you don't see here?\u201d section in Packback at the bottom of the homepage. Community Lookup Key: 1e3bb85a-ddb2-456a-a8fc-178ead55206d Follow the instructions on your screen to finish your registration. Packback will require a paid subscription (~$25). Refer to www.packback.co/product/pricing for more information. How to Get Help from the Packback Team: If you have any questions or concerns about Packback throughout the semester, please read their FAQ at help.packback.co. If you need more help, contact their customer support team directly at help@packback.co. For a brief introduction to Packback Questions and why we are using it in class, watch this video: vimeo.com/packback/Welcome-to-Packback-Questions","title":"How to Register on Packback:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#classroom-policy","text":"The following activities are not allowed in the classroom: Texting or talking on the cellphone or other electronic devices, and reading non-course related materials.","title":"Classroom Policy:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#telepresence-on-line-courses","text":"Obviously electronic devices are vital; disrupting the conference is prohibited, please mute your microphone unless you have a question - consider typing your question into the chat window as well. Be aware of bandwidth issues and remember most lessons and laboratory sessions are recorded and posted on youtube. Recording, editing, and rendering takes awhile, so expect 24-36 hour delay before video is available.","title":"Telepresence (On-line) Courses"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#ada-statement","text":"Any student who, because of a disability, may require special arrangements in order to meet the course requirements should contact the instructor as soon as possible to make necessary arrangements. Students must present appropriate verification from Student Disability Services during the instructor's office hours. Please note that instructors are not allowed to provide classroom accommodation to a student until appropriate verification from Student Disability Services has been provided. For additional information, please contact Student Disability Services office in 335 West Hall or call 806.742.2405.","title":"ADA Statement:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#academic-integrity-statement","text":"Academic integrity is taking responsibility for one\u2019s own class and/or course work, being individually accountable, and demonstrating intellectual honesty and ethical behavior. Academic integrity is a personal choice to abide by the standards of intellectual honesty and responsibility. Because education is a shared effort to achieve learning through the exchange of ideas, students, faculty, and staff have the collective responsibility to build mutual trust and respect. Ethical behavior and independent thought are essential for the highest level of academic achievement, which then must be measured. Academic achievement includes scholarship, teaching, and learning, all of which are shared endeavors. Grades are a device used to quantify the successful accumulation of knowledge through learning. Adhering to the standards of academic integrity ensures grades are earned honestly. Academic integrity is the foundation upon which students, faculty, and staff build their educational and professional careers. [Texas Tech University (\u201cUniversity\u201d) Quality Enhancement Plan, Academic Integrity Task Force, 2010].","title":"Academic Integrity Statement:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#religious-holy-day-statement","text":"\u201cReligious holy day\u201d means a holy day observed by a religion whose places of worship are exempt from property taxation under Texas Tax Code \u00a711.20. A student who intends to observe a religious holy day should make that intention known to the instructor prior to the absence. A student who is absent from classes for the observance of a religious holy day shall be allowed to take an examination or complete an assignment scheduled for that day within a reasonable time after the absence. A student who is excused may not be penalized for the absence; however, the instructor may respond appropriately if the student fails to complete the assignment satisfactorily.","title":"Religious Holy Day Statement:"},{"location":"0-Syllabus/ENGR-1330-2021-1-Syllabus/#ethical-conduct-policy","text":"Cheating is prohibited, and the representation of the work of another person as your own will be grounds for receiving a failing grade in the course. DISCRIMINATION, HARASSMENT, AND SEXUAL VIOLENCE STATEMENT: Texas Tech University is committed to providing and strengthening an educational, working, and living environment where students, faculty, staff, and visitors are free from gender and/or sex discrimination of any kind. Sexual assault, discrimination, harassment, and other Title IX violations are not tolerated by the University. Report any incidents to the Office for Student Rights & Resolution, (806)-742-SAFE (7233) or file a report online at titleix.ttu.edu/students. Faculty and staff members at TTU are committed to connecting you to resources on campus. Some of these available resources are: TTU Student Counseling Center, 806- 742-3674, https://www.depts.ttu.edu/scc/(Provides confidential support on campus.) TTU 24-hour Crisis Helpline, 806-742-5555, (Assists students who are experiencing a mental health or interpersonal violence crisis. If you call the helpline, you will speak with a mental health counselor.) Voice of Hope Lubbock Rape Crisis Center, 806-763-7273, voiceofhopelubbock.org (24-hour hotline that provides support for survivors of sexual violence.) The Risk, Intervention, Safety and Education (RISE) Office, 806-742-2110, https://www.depts.ttu.edu/rise/ (Provides a range of resources and support options focused on prevention education and student wellness.) Texas Tech Police Department, 806-742- 3931,http://www.depts.ttu.edu/ttpd/ (To report criminal activity that occurs on or near Texas Tech campus.) CIVILITY IN THE CLASSROOM STATEMENT: Texas Tech University is a community of faculty, students, and staff that enjoys an expectation of cooperation, professionalism, and civility during the conduct of all forms of university business, including the conduct of student\u2013student and student\u2013faculty interactions in and out of the classroom. Further, the classroom is a setting in which an exchange of ideas and creative thinking should be encouraged and where intellectual growth and development are fostered. Students who disrupt this classroom mission by rude, sarcastic, threatening, abusive or obscene language and/or behavior will be subject to appropriate sanctions according to university policy. Likewise, faculty members are expected to maintain the highest standards of professionalism in all interactions with all constituents of the university. To ensure that you are fully engaged in class discussions and account team meetings during class time, you are expected to do the following: - Maintain the same level of civility and professionalism that would be expected in a face-to-face classroom setting. - Attend all classes regularly. - Log into the video conference on time and remain logged in for the duration of the class period. - Activate your camera so that you are visible to the instructor and other students in the class. If you have concerns about leaving your camera on (such as childcare obligations, privacy issues, or a particular circumstance during a class period), please talk to the instructor. - Refrain from engaging in non-class related activities during class time that create a distraction for other students in the class and/or limit your ability to engage in the course. Failure to meet these expectations may result in the following consequences: 1. Being counted as absent for the class meeting. 2. Not receiving credit for class participation for that class period. 3. Other consequences as stipulated in the syllabus, Texas Tech Code of Student Conduct, or other university policy. Repeated failure to meet expectations (e.g., attendance, participation in class, etc.), in addition to the above consequences, may result in the one or more of the following consequences: 1. Referral to the appropriate Associate Dean. 2. Academic penalty, ranging from a warning to failure of the course. (www.depts.ttu.edu/ethics/matadorchallenge/ethicalprinciples.php). LGBTQIA SUPPORT STATEMENT: I identify as an ally to the lesbian, gay, bisexual, transgender, queer, intersex, and asexual (LGBTQIA) community, and I am available to listen and support you in an affirming manner. I can assist in connecting you with resources on campus to address problems you may face pertaining to sexual orientation and/or gender identity that could interfere with your success at Texas Tech. Please note that additional resources are available through the Office of LGBTQIA within the Center for Campus Life, Student Union Building Room 201, www.lgbtqia.ttu.edu, 806.742.5433.\u201d Office of LGBTQIA, Student Union Building Room 201, www.lgbtqia.ttu.edu, 806.742.5433 Within the Center for Campus Life, the Office serves the Texas Tech community through facilitation and leadership of programming and advocacy efforts. This work is aimed at strengthening the lesbian, gay, bisexual, transgender, queer, intersex, and asexual (LGBTQIA) community and sustaining an inclusive campus that welcomes people of all sexual orientations, gender identities, and gender expressions.","title":"Ethical Conduct Policy:"},{"location":"1-Lessons/Lesson00/lesson0/","text":"table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 13 January 2021 Lesson 0 Introduction to Computational Thinking with Data Science: Computational thinking concepts Data science and practices JupyterLab (iPython) as a programming environment Programming as a problem solving process CCMR Approach Special Script Blocks In the lesson notebooks there will usually be two script blocks, identical to the ones below. The first block identifies the particular computer, the user, and the python kernel in use. The second block sets markdown tables to left edge when rendering. I usually put both blocks at the top of the notebook, just after some kind of title block, as done here. Computational Thinking Concepts Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. Much of what follows is borrowed from (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2696102/) . Computational thinking is taking an approach to solving problems, designing systems and understanding human behaviour that draws on concepts fundamental to computing (http://www.cs.cmu.edu/~15110-s13/Wing06-ct.pdf) . Computational thinking is a kind of analytical thinking: It shares with mathematical thinking in the general ways in which we might approach solving a problem. It shares with engineering thinking in the general ways in which we might approach designing and evaluating a large, complex system that operates within the constraints of the real world. - It shares with scientific thinking in the general ways in which we might approach understanding computability, intelligence, the mind and human behaviour. The essence of computational thinking is abstraction and automation . In computing, we abstract notions beyond the physical dimensions of time and space. Our abstractions are extremely general because they are symbolic, where numeric abstractions are just a special case. CT Foundations CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation) Decomposition Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Examples include: - Writing a paper: - Introduction - Body - Conclusion Wide-viewed (Panorama) image: Taking multiple overlapped photos Stitch them Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it also will require assembly (system integration) to produce a desired solution. Abstraction Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve. Books in an online bookstore Important NOT important title Cover color ISBN Author\u2019s hometown Authors ... ... ... Algorithms Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. Image from https://www.newyorker.com/magazine/2021/01/18/whats-wrong-with-the-way-we-work?utm_source=pocket-newtab An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation. Algorithms are unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, can incorporate random input. System Integration (implementation) System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Data Science and Practice Data science is leveraging existing data sources, to create new ones as needed in order to extract meaningful information and actionable insights through business domain expertise, effective communication and results interpretation. Data science uses relevant statistical techniques, programming languages, software packages and libraries, and data infrastructure; The insights are used to drive business decisions and take actions intended to achieve business goals. Why is this important for engineers? Because engineering is a business! A list of typical skills (https://elitedatascience.com/data-science-resources): Foundational Skills Programming and Data Manipulation Statistics and Probability Technical Skills Data Collection SQL Data Visualization Applied Machine Learning Business Skills Communication Creativity and Innovation Operations and Strategy Business Analytics Supplementary Skills Natural Language Processing Recommendation Systems Time Series Analysis Practice Projects Competitions Problem Solving Challenges JupyterLab (iPython) Environment The tools: JupyterLab (https://jupyter.org/) is a web-based interactive development environment for Jupyter notebooks, code, and data. Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data organizing and transformation, numerical simulation, statistical modeling, visualization, machine learning, and other similar types of uses. JupyterHub (https://github.com/jupyterhub/jupyterhub) is a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server. All these tools allow use of various coding languages; Python is the choice for ENGR 1330. Installing JupyterLab on your own computer is relatively straightforward if it is an Intel-based Linux, Macintosh, or Windows machine - simply use Anaconda (https://www.anaconda.com/) as the installer. Installing onto an ARM-based machine is more difficult, but possible (this notebook was created on a Raspberry Pi). With both Apple and Microsoft abandoning Intel you can expect Anaconda builds for aarch64 (ARM) in the future. This course: You will create and use Jupyter Notebooks that use the ipython kernel, the notebook files will look like filename.ipynb ; these are ASCII files that the JupyterLab interprets and runs. Python The programming language we will use is Python (actually iPython). Python is an example of a high-level language; other high-level languages include C, C++, PHP, FORTRAN, ADA, Pascal, Go, Java, etc (there are a lot). As you might infer from the name high-level language, there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous. First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with few or no modifications. Low-level programs can run on only one kind of computer and have to be rewritten to run on another. Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications, and for device drivers. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. Interpreted Program. Image from (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) A compiler reads the program and translates it completely before the program starts running. In this case, the high-level program is called the source code, and the translated program is called the object code or the executable. Once a program is compiled, you can execute it repeatedly without further translation. Compiled Prorgam. Image from: (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) Many modern languages use both processes. They are first compiled into a lower level language, called byte code, and then interpreted by a program called a virtual machine. Python uses both processes, but because of the way programmers interact with it, it is usually considered an interpreted language. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Formal languages are languages that are designed by people for specific applications. For example, the notation that mathematicians use is a formal language that is particularly good at denoting relationships among numbers and symbols. Chemists use a formal language to represent the chemical structure of molecules. Programming languages are formal languages that have been designed to express computations. Formal languages have strict rules about syntax. For example, 3+3=6 is a syntactically correct mathematical statement, but 3=+6& is not. Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. One of the problems with 3=+6& is that & is not a legal token in mathematics (at least as far as we know). The second type of syntax rule pertains to the structure of a statement\u2014 that is, the way the tokens are arranged. The statement 3=+6& is structurally illegal (in mathematics) because you don\u2019t place a plus sign immediately after an equal sign (of course we will in python!). When you read a sentence in English or a statement in a formal language, you have to figure out what the structure of the sentence is; This process is called parsing . For example, when you hear the sentence, \u201cThe other shoe fell\u201d, you understand that the other shoe is the subject and fell is the verb. Once you have parsed a sentence, you can figure out what it means, or the semantics of the sentence. Assuming that you know what a shoe is and what it means to fall, you will understand the general implication of this sentence. Good Resources: Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science) Programming as a problem solving process The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method (https://en.wikipedia.org/wiki/Scientific_method) is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Generate and evaluate potential solutions Refine and implement a solution Verify and test the solution. For actual computational methods the protocol becomes: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4). Example 1 Problem Solving Process Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background. CCMR Approach A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /home/sensei/1330-textbook-webroot/docs/lesson0 /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Computational Thinking and Data Science"},{"location":"1-Lessons/Lesson00/lesson0/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 13 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson00/lesson0/#lesson-0-introduction-to-computational-thinking-with-data-science","text":"Computational thinking concepts Data science and practices JupyterLab (iPython) as a programming environment Programming as a problem solving process CCMR Approach","title":"Lesson 0 Introduction to Computational Thinking with Data Science:"},{"location":"1-Lessons/Lesson00/lesson0/#special-script-blocks","text":"In the lesson notebooks there will usually be two script blocks, identical to the ones below. The first block identifies the particular computer, the user, and the python kernel in use. The second block sets markdown tables to left edge when rendering. I usually put both blocks at the top of the notebook, just after some kind of title block, as done here.","title":"Special Script Blocks"},{"location":"1-Lessons/Lesson00/lesson0/#computational-thinking-concepts","text":"Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. Much of what follows is borrowed from (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2696102/) . Computational thinking is taking an approach to solving problems, designing systems and understanding human behaviour that draws on concepts fundamental to computing (http://www.cs.cmu.edu/~15110-s13/Wing06-ct.pdf) . Computational thinking is a kind of analytical thinking: It shares with mathematical thinking in the general ways in which we might approach solving a problem. It shares with engineering thinking in the general ways in which we might approach designing and evaluating a large, complex system that operates within the constraints of the real world. - It shares with scientific thinking in the general ways in which we might approach understanding computability, intelligence, the mind and human behaviour. The essence of computational thinking is abstraction and automation . In computing, we abstract notions beyond the physical dimensions of time and space. Our abstractions are extremely general because they are symbolic, where numeric abstractions are just a special case.","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson00/lesson0/#ct-foundations","text":"CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation)","title":"CT Foundations"},{"location":"1-Lessons/Lesson00/lesson0/#decomposition","text":"Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Examples include: - Writing a paper: - Introduction - Body - Conclusion Wide-viewed (Panorama) image: Taking multiple overlapped photos Stitch them Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution.","title":"Decomposition"},{"location":"1-Lessons/Lesson00/lesson0/#pattern-recognition","text":"Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it also will require assembly (system integration) to produce a desired solution.","title":"Pattern Recognition"},{"location":"1-Lessons/Lesson00/lesson0/#abstraction","text":"Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve. Books in an online bookstore Important NOT important title Cover color ISBN Author\u2019s hometown Authors ... ... ...","title":"Abstraction"},{"location":"1-Lessons/Lesson00/lesson0/#algorithms","text":"Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. Image from https://www.newyorker.com/magazine/2021/01/18/whats-wrong-with-the-way-we-work?utm_source=pocket-newtab An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation. Algorithms are unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, can incorporate random input.","title":"Algorithms"},{"location":"1-Lessons/Lesson00/lesson0/#system-integration-implementation","text":"System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand.","title":"System Integration (implementation)"},{"location":"1-Lessons/Lesson00/lesson0/#data-science-and-practice","text":"Data science is leveraging existing data sources, to create new ones as needed in order to extract meaningful information and actionable insights through business domain expertise, effective communication and results interpretation. Data science uses relevant statistical techniques, programming languages, software packages and libraries, and data infrastructure; The insights are used to drive business decisions and take actions intended to achieve business goals. Why is this important for engineers? Because engineering is a business! A list of typical skills (https://elitedatascience.com/data-science-resources): Foundational Skills Programming and Data Manipulation Statistics and Probability Technical Skills Data Collection SQL Data Visualization Applied Machine Learning Business Skills Communication Creativity and Innovation Operations and Strategy Business Analytics Supplementary Skills Natural Language Processing Recommendation Systems Time Series Analysis Practice Projects Competitions Problem Solving Challenges","title":"Data Science and Practice"},{"location":"1-Lessons/Lesson00/lesson0/#jupyterlab-ipython-environment","text":"","title":"JupyterLab (iPython) Environment"},{"location":"1-Lessons/Lesson00/lesson0/#the-tools","text":"JupyterLab (https://jupyter.org/) is a web-based interactive development environment for Jupyter notebooks, code, and data. Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data organizing and transformation, numerical simulation, statistical modeling, visualization, machine learning, and other similar types of uses. JupyterHub (https://github.com/jupyterhub/jupyterhub) is a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server. All these tools allow use of various coding languages; Python is the choice for ENGR 1330. Installing JupyterLab on your own computer is relatively straightforward if it is an Intel-based Linux, Macintosh, or Windows machine - simply use Anaconda (https://www.anaconda.com/) as the installer. Installing onto an ARM-based machine is more difficult, but possible (this notebook was created on a Raspberry Pi). With both Apple and Microsoft abandoning Intel you can expect Anaconda builds for aarch64 (ARM) in the future.","title":"The tools:"},{"location":"1-Lessons/Lesson00/lesson0/#this-course","text":"You will create and use Jupyter Notebooks that use the ipython kernel, the notebook files will look like filename.ipynb ; these are ASCII files that the JupyterLab interprets and runs.","title":"This course:"},{"location":"1-Lessons/Lesson00/lesson0/#python","text":"The programming language we will use is Python (actually iPython). Python is an example of a high-level language; other high-level languages include C, C++, PHP, FORTRAN, ADA, Pascal, Go, Java, etc (there are a lot). As you might infer from the name high-level language, there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous. First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with few or no modifications. Low-level programs can run on only one kind of computer and have to be rewritten to run on another. Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications, and for device drivers. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. Interpreted Program. Image from (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) A compiler reads the program and translates it completely before the program starts running. In this case, the high-level program is called the source code, and the translated program is called the object code or the executable. Once a program is compiled, you can execute it repeatedly without further translation. Compiled Prorgam. Image from: (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) Many modern languages use both processes. They are first compiled into a lower level language, called byte code, and then interpreted by a program called a virtual machine. Python uses both processes, but because of the way programmers interact with it, it is usually considered an interpreted language. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Formal languages are languages that are designed by people for specific applications. For example, the notation that mathematicians use is a formal language that is particularly good at denoting relationships among numbers and symbols. Chemists use a formal language to represent the chemical structure of molecules. Programming languages are formal languages that have been designed to express computations. Formal languages have strict rules about syntax. For example, 3+3=6 is a syntactically correct mathematical statement, but 3=+6& is not. Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. One of the problems with 3=+6& is that & is not a legal token in mathematics (at least as far as we know). The second type of syntax rule pertains to the structure of a statement\u2014 that is, the way the tokens are arranged. The statement 3=+6& is structurally illegal (in mathematics) because you don\u2019t place a plus sign immediately after an equal sign (of course we will in python!). When you read a sentence in English or a statement in a formal language, you have to figure out what the structure of the sentence is; This process is called parsing . For example, when you hear the sentence, \u201cThe other shoe fell\u201d, you understand that the other shoe is the subject and fell is the verb. Once you have parsed a sentence, you can figure out what it means, or the semantics of the sentence. Assuming that you know what a shoe is and what it means to fall, you will understand the general implication of this sentence.","title":"Python"},{"location":"1-Lessons/Lesson00/lesson0/#good-resources","text":"Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science)","title":"Good Resources:"},{"location":"1-Lessons/Lesson00/lesson0/#programming-as-a-problem-solving-process","text":"The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method (https://en.wikipedia.org/wiki/Scientific_method) is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Generate and evaluate potential solutions Refine and implement a solution Verify and test the solution. For actual computational methods the protocol becomes: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4).","title":"Programming as a problem solving process"},{"location":"1-Lessons/Lesson00/lesson0/#example-1-problem-solving-process","text":"Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background.","title":"Example 1 Problem Solving Process"},{"location":"1-Lessons/Lesson00/lesson0/#ccmr-approach","text":"A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity.","title":"CCMR Approach"},{"location":"1-Lessons/Lesson00/lesson0/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /home/sensei/1330-textbook-webroot/docs/lesson0 /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Readings"},{"location":"1-Lessons/Lesson01/lesson1/","text":"%%html <!-- Script Block to set tables to left alignment --> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 13 January 2021 Lesson 1 Problem Solving and Computational Thinking: CT concepts Programming as a problem solving process CCMR Approach Special Script Blocks In the lesson notebooks there will usually be two script blocks, identical to the ones below. The first block identifies the particular computer, the user, and the python kernel in use. The second block sets markdown tables to left edge when rendering. I usually put both blocks at the top of the notebook, just after some kind of title block, as done here. CT Foundations CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation) Decomposition Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Decomposition leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it also will require assembly (system integration) to produce a desired solution. Abstraction Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve. Algorithms Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation. System Integration (implementation) System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Programming as a problem solving process The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method (https://en.wikipedia.org/wiki/Scientific_method) is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Input information Governing equations or principles, and The required output information. Generate and evaluate potential solutions Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4). Example 1 Problem Solving Process Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background. CCMR Approach A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We can give this process a simple acronym CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much doing original programming as we are just scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /home/sensei/1330-textbook-webroot/docs/lesson0 /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Problem Solving with Computational Thinking"},{"location":"1-Lessons/Lesson01/lesson1/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 13 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson01/lesson1/#lesson-1-problem-solving-and-computational-thinking","text":"CT concepts Programming as a problem solving process CCMR Approach","title":"Lesson 1 Problem Solving and Computational Thinking:"},{"location":"1-Lessons/Lesson01/lesson1/#special-script-blocks","text":"In the lesson notebooks there will usually be two script blocks, identical to the ones below. The first block identifies the particular computer, the user, and the python kernel in use. The second block sets markdown tables to left edge when rendering. I usually put both blocks at the top of the notebook, just after some kind of title block, as done here.","title":"Special Script Blocks"},{"location":"1-Lessons/Lesson01/lesson1/#ct-foundations","text":"CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation)","title":"CT Foundations"},{"location":"1-Lessons/Lesson01/lesson1/#decomposition","text":"Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Decomposition leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution.","title":"Decomposition"},{"location":"1-Lessons/Lesson01/lesson1/#pattern-recognition","text":"Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it also will require assembly (system integration) to produce a desired solution.","title":"Pattern Recognition"},{"location":"1-Lessons/Lesson01/lesson1/#abstraction","text":"Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve.","title":"Abstraction"},{"location":"1-Lessons/Lesson01/lesson1/#algorithms","text":"Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation.","title":"Algorithms"},{"location":"1-Lessons/Lesson01/lesson1/#system-integration-implementation","text":"System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand.","title":"System Integration (implementation)"},{"location":"1-Lessons/Lesson01/lesson1/#programming-as-a-problem-solving-process","text":"The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method (https://en.wikipedia.org/wiki/Scientific_method) is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Input information Governing equations or principles, and The required output information. Generate and evaluate potential solutions Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4).","title":"Programming as a problem solving process"},{"location":"1-Lessons/Lesson01/lesson1/#example-1-problem-solving-process","text":"Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background.","title":"Example 1 Problem Solving Process"},{"location":"1-Lessons/Lesson01/lesson1/#ccmr-approach","text":"A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We can give this process a simple acronym CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much doing original programming as we are just scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity.","title":"CCMR Approach"},{"location":"1-Lessons/Lesson01/lesson1/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /home/sensei/1330-textbook-webroot/docs/lesson0 /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Readings"},{"location":"1-Lessons/Lesson02/lesson1/","text":"%%html <!-- Script Block to set tables to left alignment --> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 19 January 2021 Simple Arithmetic Computations: iPython, tokens, and structure Data types (int, float, string, bool) Variables, operators, expressions, basic I/O String functions and operations Programming Fundamentals Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. CT is a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Recall the 5 fundamental CT concepts are: Decomposition: the process of taking a complex problem and breaking it into more manageable sub-problems. Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition: finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method ( automation ) for each occurrence of the pattern. Abstraction : Determine important characteristics of the problem and use these characteristics to create a representation of the problem. Algorithms : Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. System Integration: the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Programming is (generally) writing code in a specific programming language to address a certain problem. In the above list it is largely contained within the algorithms concept. iPython The programming language we will use is Python (actually iPython). Python is an example of a high-level language; there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous: First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with just a few modifications. Low-level programs can run on only one kind of computer (chipset-specific for sure, in some cases hardware specific) and have to be rewritten to run on other processors. (e.g. x86-64 vs. arm7 vs. aarch64 vs. PowerPC ...) Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. The second type of syntax rule pertains to the structure of a statement specifically in the way the tokens are arranged. Tokens and Structure Consider the relativistic equation relating energy, mass, and the speed of light e = m \\cdot c^2 In this equation the tokens are e , m , c , = , \\cdot , and the structure is parsed from left to right as into the token named e place the result of the product of the contents of the tokens m and c^2 . Given that the speed of light is some universal constant, the only things that can change are the contents of m and the resulting change in e . In the above discourse, the tokens e , m , c are names for things that can have values -- we will call these variables (or constants as appropriate). The tokens = , \\cdot , and ~^2 are symbols for various arithmetic operations -- we will call these operators. The structure of the equation is specific -- we will call it a statement. When we attempt to write and execute python scripts - we will make various mistakes; these will generate warnings and errors, which we will repair to make a working program. Consider our equation: #clear all variables# Example Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-4-1c1f1fa5363a> in <module> 1 #clear all variables# Example ----> 2 Energy = Mass * SpeedOfLight**2 NameError: name 'Mass' is not defined Notice how the interpreter tells us that Mass is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-5-a4a52966e6df> in <module> 1 # Example 2 Mass = 1000000 ----> 3 Energy = Mass * SpeedOfLight**2 NameError: name 'SpeedOfLight' is not defined Notice how the interpreter now tells us that SpeedOfLight is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 Now the script ran without any reported errors, but we have not instructed the program on how to produce output. To keep the example simple we will just add a generic print statement. # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 print(\"Energy is:\", Energy, \"Newton meters\") Energy is: 89875517873681764000000 Newton meters Now lets examine our program. Identify the tokens that have values, Identify the tokens that are symbols of operations, identify the structure. Variables Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float). Naming Rules Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print , input , if , while , and for . There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables. Operators The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10. Arithmetic Operators In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x y Raises value in x by value in y. ( e.g. x y) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0 Data Type In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary Integer Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309 Real (Float) A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427 String(Alphanumeric) A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting. Changing Types A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens! 234 876.543 What is your name? Integer as float 234.0 Float as integer 876 Integer as string 234 Integer as hexadecimal 0xea Integer Type <class 'int'> Expressions Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15 Summary So far consider our story - a tool to help with problem solving is CT leading to an algorithm. The tool to implement the algorithm is the program and in our case JupyterLab running iPython interpreter for us. As a formal language we introduced: - tokens - structure From these two constructs we further introduced variables (a kind of token), data types (an abstraction, and arguably a decomposition), and expressions (a structure). We created simple scripts (with errors), examined the errors, corrected our script, and eventually got an answer. So we are well on our way in CT as it applies in Engineering. Programming as a problem solving process Recall the entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). Recall our suggested problem solving protocol: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Refine general solution for deployment (frequent use) Another protocol with the same goal is at https://3.137.111.182/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint/HowToBuildAProgram.html Notice the similarity! Example 2 Problem Solving Process Consider an engineering material problem where we wish to classify whether a material in loaded in the elastic or inelastic region as determined the stress (solid pressure) in a rod for some applied load. The yield stress is the classifier, and once the material yields (begins to fail) it will not carry any additional load (until ultimate failure, when it carries no load). Step 1. Compute the material stress under an applied load; determine if value exceedes yield stress, and report the loading condition Step 2. - Inputs: applied load, cross sectional area, yield stress - Governing equation: \\sigma = \\frac{P}{A} when \\frac{P}{A} is less than the yield stress, and is equal to the yield stress otherwise. - Outputs: The material stress \\sigma , and the classification elastic or inelastic. Step 3. Work a sample problem by-hand for testing the general solution. Assuming the yield stress is 1 million psi (units matter in an actual problem - kind of glossed over here) Applied Load (lbf) Cross Section Area (sq.in.) Stress (psi) Classification 10,000 1.0 10,000 Elastic 10,000 0.1 100,000 Elastic 100,000 0.1 1,000,000 Inelastic The stress requires us to read in the load value, read in the cross sectional area, divide the load by the area, and compare the result to the yield stress. If it exceeds the yield stress, then the actual stress is the yield stress, and the loading is inelastic, otherwise elastic \\sigma = \\frac{P}{A} If \\sigma >= \\text{Yield Stress Report Inelastic} Step 4. Develop a general solution (code) In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. We have not yet learned prompts to get input we simply direct assign values as below (and the conditional execution is the subject of a later lesson) In a simple JupyterLab script # Example 2 Problem Solving Process yield_stress = 1e6 applied_load = 1e5 cross_section = 0.1 computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",computed_stress) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",yield_stress) Inelastic Region: Stress = 1000000.0 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the inputs by user entry,and tidy the output by rounding to only two decimal places. A little CCMR from https://www.geeksforgeeks.org/taking-input-in-python/ gives us a way to deal with the inputs and typecasting. Some more CCMR from https://www.programiz.com/python-programming/methods/built-in/round gets us rounded out! # Example 2 Problem Solving Process yield_stress = float(input('Yield Stress (psi)')) applied_load = float(input('Applied Load (lbf)')) cross_section = float(input('Cross Section Area (sq.in.)')) computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",round(computed_stress,2)) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",round(yield_stress,2)) Yield Stress (psi) 1000000 Applied Load (lbf) 100000 Cross Section Area (sq.in.) 1 Elastic Region: Stress = 100000.0 So the simple task of computing the stress, is a bit more complex when decomposed, that it first appears, but illustrates a five step process (with a refinement step). CCMR Approach A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 3 https://www.inferentialthinking.com/chapters/03/programming-in-python.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python in One Day and Learn It Well. Python for Beginners with Hands-on Project. (Learn Coding Fast with Hands-On Project Book -- Kindle Edition by LCF Publishing (Author), Jamie Chan https://www.amazon.com/Python-2nd-Beginners-Hands-Project-ebook/dp/B071Z2Q6TQ/ref=sr_1_3?dchild=1&keywords=learn+python+in+a+day&qid=1611108340&sr=8-3 Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science) # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Arithmetic"},{"location":"1-Lessons/Lesson02/lesson1/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 19 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson02/lesson1/#simple-arithmetic-computations","text":"iPython, tokens, and structure Data types (int, float, string, bool) Variables, operators, expressions, basic I/O String functions and operations","title":"Simple Arithmetic Computations:"},{"location":"1-Lessons/Lesson02/lesson1/#programming-fundamentals","text":"Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. CT is a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Recall the 5 fundamental CT concepts are: Decomposition: the process of taking a complex problem and breaking it into more manageable sub-problems. Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition: finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method ( automation ) for each occurrence of the pattern. Abstraction : Determine important characteristics of the problem and use these characteristics to create a representation of the problem. Algorithms : Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. System Integration: the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Programming is (generally) writing code in a specific programming language to address a certain problem. In the above list it is largely contained within the algorithms concept.","title":"Programming Fundamentals"},{"location":"1-Lessons/Lesson02/lesson1/#ipython","text":"The programming language we will use is Python (actually iPython). Python is an example of a high-level language; there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous: First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with just a few modifications. Low-level programs can run on only one kind of computer (chipset-specific for sure, in some cases hardware specific) and have to be rewritten to run on other processors. (e.g. x86-64 vs. arm7 vs. aarch64 vs. PowerPC ...) Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. The second type of syntax rule pertains to the structure of a statement specifically in the way the tokens are arranged.","title":"iPython"},{"location":"1-Lessons/Lesson02/lesson1/#tokens-and-structure","text":"Consider the relativistic equation relating energy, mass, and the speed of light e = m \\cdot c^2 In this equation the tokens are e , m , c , = , \\cdot , and the structure is parsed from left to right as into the token named e place the result of the product of the contents of the tokens m and c^2 . Given that the speed of light is some universal constant, the only things that can change are the contents of m and the resulting change in e . In the above discourse, the tokens e , m , c are names for things that can have values -- we will call these variables (or constants as appropriate). The tokens = , \\cdot , and ~^2 are symbols for various arithmetic operations -- we will call these operators. The structure of the equation is specific -- we will call it a statement. When we attempt to write and execute python scripts - we will make various mistakes; these will generate warnings and errors, which we will repair to make a working program. Consider our equation: #clear all variables# Example Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-4-1c1f1fa5363a> in <module> 1 #clear all variables# Example ----> 2 Energy = Mass * SpeedOfLight**2 NameError: name 'Mass' is not defined Notice how the interpreter tells us that Mass is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-5-a4a52966e6df> in <module> 1 # Example 2 Mass = 1000000 ----> 3 Energy = Mass * SpeedOfLight**2 NameError: name 'SpeedOfLight' is not defined Notice how the interpreter now tells us that SpeedOfLight is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 Now the script ran without any reported errors, but we have not instructed the program on how to produce output. To keep the example simple we will just add a generic print statement. # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 print(\"Energy is:\", Energy, \"Newton meters\") Energy is: 89875517873681764000000 Newton meters Now lets examine our program. Identify the tokens that have values, Identify the tokens that are symbols of operations, identify the structure.","title":"Tokens and Structure"},{"location":"1-Lessons/Lesson02/lesson1/#variables","text":"Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float).","title":"Variables"},{"location":"1-Lessons/Lesson02/lesson1/#naming-rules","text":"Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print , input , if , while , and for . There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables.","title":"Naming Rules"},{"location":"1-Lessons/Lesson02/lesson1/#operators","text":"The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10.","title":"Operators"},{"location":"1-Lessons/Lesson02/lesson1/#arithmetic-operators","text":"In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x y Raises value in x by value in y. ( e.g. x y) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0","title":"Arithmetic Operators"},{"location":"1-Lessons/Lesson02/lesson1/#data-type","text":"In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary","title":"Data Type"},{"location":"1-Lessons/Lesson02/lesson1/#integer","text":"Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309","title":"Integer"},{"location":"1-Lessons/Lesson02/lesson1/#real-float","text":"A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427","title":"Real (Float)"},{"location":"1-Lessons/Lesson02/lesson1/#stringalphanumeric","text":"A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting.","title":"String(Alphanumeric)"},{"location":"1-Lessons/Lesson02/lesson1/#changing-types","text":"A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens! 234 876.543 What is your name? Integer as float 234.0 Float as integer 876 Integer as string 234 Integer as hexadecimal 0xea Integer Type <class 'int'>","title":"Changing Types"},{"location":"1-Lessons/Lesson02/lesson1/#expressions","text":"Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15","title":"Expressions"},{"location":"1-Lessons/Lesson02/lesson1/#summary","text":"So far consider our story - a tool to help with problem solving is CT leading to an algorithm. The tool to implement the algorithm is the program and in our case JupyterLab running iPython interpreter for us. As a formal language we introduced: - tokens - structure From these two constructs we further introduced variables (a kind of token), data types (an abstraction, and arguably a decomposition), and expressions (a structure). We created simple scripts (with errors), examined the errors, corrected our script, and eventually got an answer. So we are well on our way in CT as it applies in Engineering.","title":"Summary"},{"location":"1-Lessons/Lesson02/lesson1/#programming-as-a-problem-solving-process","text":"Recall the entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). Recall our suggested problem solving protocol: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Refine general solution for deployment (frequent use) Another protocol with the same goal is at https://3.137.111.182/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint/HowToBuildAProgram.html Notice the similarity!","title":"Programming as a problem solving process"},{"location":"1-Lessons/Lesson02/lesson1/#example-2-problem-solving-process","text":"Consider an engineering material problem where we wish to classify whether a material in loaded in the elastic or inelastic region as determined the stress (solid pressure) in a rod for some applied load. The yield stress is the classifier, and once the material yields (begins to fail) it will not carry any additional load (until ultimate failure, when it carries no load). Step 1. Compute the material stress under an applied load; determine if value exceedes yield stress, and report the loading condition Step 2. - Inputs: applied load, cross sectional area, yield stress - Governing equation: \\sigma = \\frac{P}{A} when \\frac{P}{A} is less than the yield stress, and is equal to the yield stress otherwise. - Outputs: The material stress \\sigma , and the classification elastic or inelastic. Step 3. Work a sample problem by-hand for testing the general solution. Assuming the yield stress is 1 million psi (units matter in an actual problem - kind of glossed over here) Applied Load (lbf) Cross Section Area (sq.in.) Stress (psi) Classification 10,000 1.0 10,000 Elastic 10,000 0.1 100,000 Elastic 100,000 0.1 1,000,000 Inelastic The stress requires us to read in the load value, read in the cross sectional area, divide the load by the area, and compare the result to the yield stress. If it exceeds the yield stress, then the actual stress is the yield stress, and the loading is inelastic, otherwise elastic \\sigma = \\frac{P}{A} If \\sigma >= \\text{Yield Stress Report Inelastic} Step 4. Develop a general solution (code) In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. We have not yet learned prompts to get input we simply direct assign values as below (and the conditional execution is the subject of a later lesson) In a simple JupyterLab script # Example 2 Problem Solving Process yield_stress = 1e6 applied_load = 1e5 cross_section = 0.1 computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",computed_stress) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",yield_stress) Inelastic Region: Stress = 1000000.0 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the inputs by user entry,and tidy the output by rounding to only two decimal places. A little CCMR from https://www.geeksforgeeks.org/taking-input-in-python/ gives us a way to deal with the inputs and typecasting. Some more CCMR from https://www.programiz.com/python-programming/methods/built-in/round gets us rounded out! # Example 2 Problem Solving Process yield_stress = float(input('Yield Stress (psi)')) applied_load = float(input('Applied Load (lbf)')) cross_section = float(input('Cross Section Area (sq.in.)')) computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",round(computed_stress,2)) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",round(yield_stress,2)) Yield Stress (psi) 1000000 Applied Load (lbf) 100000 Cross Section Area (sq.in.) 1 Elastic Region: Stress = 100000.0 So the simple task of computing the stress, is a bit more complex when decomposed, that it first appears, but illustrates a five step process (with a refinement step).","title":"Example 2 Problem Solving Process"},{"location":"1-Lessons/Lesson02/lesson1/#ccmr-approach","text":"A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity.","title":"CCMR Approach"},{"location":"1-Lessons/Lesson02/lesson1/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 3 https://www.inferentialthinking.com/chapters/03/programming-in-python.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python in One Day and Learn It Well. Python for Beginners with Hands-on Project. (Learn Coding Fast with Hands-On Project Book -- Kindle Edition by LCF Publishing (Author), Jamie Chan https://www.amazon.com/Python-2nd-Beginners-Hands-Project-ebook/dp/B071Z2Q6TQ/ref=sr_1_3?dchild=1&keywords=learn+python+in+a+day&qid=1611108340&sr=8-3 Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science) # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Readings"},{"location":"1-Lessons/Lesson03/lesson2/","text":"%%html <!-- Script Block to set tables to left alignment --> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 25 January 2021 Lesson 2 Data Structures and Conditional Statements: Data structures; lists, arrays, tuples, sets, dictionaries Name, index, contents; keys Conditional structures; logical compares, block and in-line if Objectives 1) Develop awareness of data structures available in Python to store and manipulate data - Implement arrays (lists), dictionaries, and tuples - Address contents of lists , dictionaries, and tuples 2) Develop awareness of decision making in Python - Implement decision making in Python using using if-then ... conditional statements Data Structures and Conditional Statements Computational thinking (CT) concepts involved are: Decomposition : Data interpretation, manipulation, and analysis of NumPy arrays Abstraction : Data structures; Arrays, lists, tuples, sets, and dictionaries Algorithms : Conditional statements What is a data structure? Data Structures are a specialized means of organizing and storing data in computers in such a way that we can perform operations on the stored data more efficiently. In our iPython world the structures are illustrated in the figure below Lists A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\\\ x_1= 11 \\\\ x_2= 5 \\\\ x_3= 9 \\\\ x_4= 13 \\\\ \\dots \\\\ x_N= 223 \\\\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO . A lot of other languages start at ONE. It's just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9. Arrays Arrays are lists that are used to store only elements of a specific data type - Ordered: Elements in an array can be indexed - Mutable: Elements in an array can be altered Data type that an array must hold is specified using the type code when it is created - \u2018f\u2019 for float - \u2018d\u2019 for double - \u2018i\u2019 for signed int - \u2018I\u2019 for unsigned int More types are listed below Type Code C Data Type Python Data Type Minimum Size in Bytes 'b' signed char int 1 'B' unsigned char int 1 'h' signed short int 2 'H' unsigned short int 2 'i' signed int int 2 'I' unsigned int int 2 'l' signed long int 4 'L' unsigned long int 4 'q' signed long long int 8 'Q' unsigned long long int 8 'f' float float 4 'd' double float 8 To use arrays, a library named \u2018array\u2019 must be imported import array Creating an array that contains signed integer numbers myarray = array.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 import array as arr #import using an alias so the calls dont look so funny myarray = arr.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 Lists: Can store elements of different data types; like arrays they are (arrays are lists, but lists are not quite arrays!) - Ordered: Elements in a list can be indexed - Mutable: Elements in a list can be altered - Mathematical operations must be applied to each element of the list Tuple - A special list A tuple is a special kind of list where the values cannot be changed after the list is created. Such a property is called immutable It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") Tuples are often created as output from packages and functions. Dictionary - A special list A dictionary is a special kind of list where the items are related data PAIRS . It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Dictionary properties - Unordered: Elements in a dictionary cannot be - Mutable elements: Elements in a dictionary can be altered - Immutable keys: Keys in a dictionary cannot be altered Sets - A special list Sets: Are used to store elements of different data types - Unordered: Elements in a set cannot be indexed - Mutable: Elements in a set can be altered - Non-repetition: Elements in a set are unique Elements of a set are enclosed in curly brackets { } - Creating sets that contains different data types - Sets cannot be nested What's the difference between a set and dictionary? From https://stackoverflow.com/questions/34370599/difference-between-dict-and-set-python \"Well, a set is like a dict with keys but no values, and they're both implemented using a hash table. But yes, it's a little annoying that the {} notation denotes an empty dict rather than an empty set , but that's a historical artifact.\" Conditional Statements Decision making via conditional statements is an important step in algorithm design; they control the flow of execution of a program. Conditional statements in Python include: if statement if....else statements if....elif....else statements Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Expressed in a flowchart a block if statement looks like: As psuedo code: if(condition is true): do stuff Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, but more recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs. Comparison The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal. Block if statement The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter. Inline if statement An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 Subpart 3 https://www.inferentialthinking.com/chapters/04/3/Comparison.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson03/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Data Structures"},{"location":"1-Lessons/Lesson03/lesson2/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 25 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson03/lesson2/#lesson-2-data-structures-and-conditional-statements","text":"Data structures; lists, arrays, tuples, sets, dictionaries Name, index, contents; keys Conditional structures; logical compares, block and in-line if","title":"Lesson 2 Data Structures and Conditional Statements:"},{"location":"1-Lessons/Lesson03/lesson2/#objectives","text":"1) Develop awareness of data structures available in Python to store and manipulate data - Implement arrays (lists), dictionaries, and tuples - Address contents of lists , dictionaries, and tuples 2) Develop awareness of decision making in Python - Implement decision making in Python using using if-then ... conditional statements","title":"Objectives"},{"location":"1-Lessons/Lesson03/lesson2/#data-structures-and-conditional-statements","text":"Computational thinking (CT) concepts involved are: Decomposition : Data interpretation, manipulation, and analysis of NumPy arrays Abstraction : Data structures; Arrays, lists, tuples, sets, and dictionaries Algorithms : Conditional statements","title":"Data Structures and Conditional Statements"},{"location":"1-Lessons/Lesson03/lesson2/#what-is-a-data-structure","text":"Data Structures are a specialized means of organizing and storing data in computers in such a way that we can perform operations on the stored data more efficiently. In our iPython world the structures are illustrated in the figure below","title":"What is a data structure?"},{"location":"1-Lessons/Lesson03/lesson2/#lists","text":"A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\\\ x_1= 11 \\\\ x_2= 5 \\\\ x_3= 9 \\\\ x_4= 13 \\\\ \\dots \\\\ x_N= 223 \\\\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO . A lot of other languages start at ONE. It's just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9.","title":"Lists"},{"location":"1-Lessons/Lesson03/lesson2/#arrays","text":"Arrays are lists that are used to store only elements of a specific data type - Ordered: Elements in an array can be indexed - Mutable: Elements in an array can be altered Data type that an array must hold is specified using the type code when it is created - \u2018f\u2019 for float - \u2018d\u2019 for double - \u2018i\u2019 for signed int - \u2018I\u2019 for unsigned int More types are listed below Type Code C Data Type Python Data Type Minimum Size in Bytes 'b' signed char int 1 'B' unsigned char int 1 'h' signed short int 2 'H' unsigned short int 2 'i' signed int int 2 'I' unsigned int int 2 'l' signed long int 4 'L' unsigned long int 4 'q' signed long long int 8 'Q' unsigned long long int 8 'f' float float 4 'd' double float 8 To use arrays, a library named \u2018array\u2019 must be imported import array Creating an array that contains signed integer numbers myarray = array.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 import array as arr #import using an alias so the calls dont look so funny myarray = arr.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 Lists: Can store elements of different data types; like arrays they are (arrays are lists, but lists are not quite arrays!) - Ordered: Elements in a list can be indexed - Mutable: Elements in a list can be altered - Mathematical operations must be applied to each element of the list","title":"Arrays"},{"location":"1-Lessons/Lesson03/lesson2/#tuple-a-special-list","text":"A tuple is a special kind of list where the values cannot be changed after the list is created. Such a property is called immutable It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") Tuples are often created as output from packages and functions.","title":"Tuple - A special list"},{"location":"1-Lessons/Lesson03/lesson2/#dictionary-a-special-list","text":"A dictionary is a special kind of list where the items are related data PAIRS . It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Dictionary properties - Unordered: Elements in a dictionary cannot be - Mutable elements: Elements in a dictionary can be altered - Immutable keys: Keys in a dictionary cannot be altered","title":"Dictionary - A special list"},{"location":"1-Lessons/Lesson03/lesson2/#sets-a-special-list","text":"Sets: Are used to store elements of different data types - Unordered: Elements in a set cannot be indexed - Mutable: Elements in a set can be altered - Non-repetition: Elements in a set are unique Elements of a set are enclosed in curly brackets { } - Creating sets that contains different data types - Sets cannot be nested","title":"Sets - A special list"},{"location":"1-Lessons/Lesson03/lesson2/#whats-the-difference-between-a-set-and-dictionary","text":"From https://stackoverflow.com/questions/34370599/difference-between-dict-and-set-python \"Well, a set is like a dict with keys but no values, and they're both implemented using a hash table. But yes, it's a little annoying that the {} notation denotes an empty dict rather than an empty set , but that's a historical artifact.\"","title":"What's the difference between a set and dictionary?"},{"location":"1-Lessons/Lesson03/lesson2/#conditional-statements","text":"Decision making via conditional statements is an important step in algorithm design; they control the flow of execution of a program. Conditional statements in Python include: if statement if....else statements if....elif....else statements Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Expressed in a flowchart a block if statement looks like: As psuedo code: if(condition is true): do stuff Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, but more recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs.","title":"Conditional Statements"},{"location":"1-Lessons/Lesson03/lesson2/#comparison","text":"The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal.","title":"Comparison"},{"location":"1-Lessons/Lesson03/lesson2/#block-if-statement","text":"The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter.","title":"Block if statement"},{"location":"1-Lessons/Lesson03/lesson2/#inline-if-statement","text":"An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops.","title":"Inline if statement"},{"location":"1-Lessons/Lesson03/lesson2/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 Subpart 3 https://www.inferentialthinking.com/chapters/04/3/Comparison.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson03/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Readings"},{"location":"1-Lessons/Lesson07/lesson6/","text":"ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 31 January 2021 Lesson 6 Classes, Objects, and File Handling: Classes and Objects Files Create (new), Open (existing) Read from .... Write to ... Close (save) Delete Special Script Blocks %%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} Objectives To understand the use of classes and objects to do effective coding in Python To understand the basic idea of how to manipulate the data in a file using file handling options in Python Classes and Objects In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated. When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class. Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state. Class definitions, like function definitions (def statements) must be executed before they have any effect. (You could conceivably place a class definition in a branch of an if statement, or inside a function.) In practice, the statements inside a class definition will usually be function definitions, but other statements are allowed, and sometimes useful \u2014 we\u2019ll come back to this later. The function definitions inside a class normally have a peculiar form of argument list, dictated by the calling conventions for methods \u2014 again, this is explained later. When a class definition is entered, a new namespace is created, and used as the local scope \u2014 thus, all assignments to local variables go into this new namespace. In particular, function definitions bind the name of the new function here. When a class definition is left normally (via the end), a class object is created. This is basically a wrapper around the contents of the namespace created by the class definition; we\u2019ll learn more about class objects in the next section. The original local scope (the one in effect just before the class definition was entered) is reinstated, and the class object is bound here to the class name given in the class definition header (ClassName in the example). What is an object? An object is simply a collection of data (variables) and methods (functions) that act on those data. Similarly, a class is a blueprint for that object. We can think of class as a sketch (prototype) of a house. It contains all the details about the floors, doors, windows etc. Based on these descriptions we build the house. House is the object. As many houses can be made from a house's blueprint, we can create many objects from a class. An object is also called an instance of a class and the process of creating this object is called instantiation Learn more at 1. https://docs.python.org/3/tutorial/classes.html 2. https://en.wikipedia.org/wiki/Class_(computer_programming) An Example: Write a class named ' Tax ' to calculate the state tax (in dollars) of Employees at Texas Tech University based on their annual salary. The state tax is 16% if the annual salary is below 80,000 dollars and 22% if the salary is more than 80,000 dollars. Employee Annual salary (dollars) Bob 150,000 Mary 78,000 John 55,000 Danny 175,000 Notes: Use docstrings to describe the purpose of the class. Use if....else conditional statements within the method of the class to choose the relevant tax % based on the annual salary. Create an object for employee and display the output as shown below. Bob's tax amount (in dollars): AMOUNT Mary's tax amount (in dollars): AMOUNT John's tax amount (in dollars): AMOUNT Danny's tax amount (in dollars): AMOUNT class Tax: \"\"\"This class calculates the tax amount based on the annual salary and the state tax %\"\"\" def __init__(self, salary): # here is the instantiation constructor self.salary = salary def taxamount(self): # here is a method (function) that can operate on the class once created if self.salary < 80000: return self.salary*(16/100) else: return self.salary*(22/100) bob = Tax(150000) dir(bob) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] bob = Tax(150000) # objects constructed using Tax class mary = Tax(78000) john = Tax(55000) danny = Tax(175000) dir(Tax) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'taxamount'] dir(mary) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] print(\"bobz salary \", bob.salary ) print(\"Bob's tax amount (in dollars):\", bob.taxamount() ) print(\"Mary's tax amount (in dollars):\", mary.taxamount()) print(\"John's tax amount (in dollars):\", john.taxamount()) print(\"Danny's tax amount (in dollars):\", danny.taxamount()) bobz salary 150000 Bob's tax amount (in dollars): 33000.0 Mary's tax amount (in dollars): 12480.0 John's tax amount (in dollars): 8800.0 Danny's tax amount (in dollars): 38500.0 Numbers, strings, lists, and dictionaries are all objects that are instances of a parent class print(type(0)) <class 'int'> print(type(\"\")) <class 'str'> print(type([1, 2, 3, 4])) <class 'list'> To get more information about the built-in classes and objects, use dir( ) and help( ) functions print(dir(int)) print(help(\"\")) User-defined classes: Defining docstrings class Dog: \"\"\"This class enables the dog to say its name and age in dog years\"\"\" def __init__(self, name, years): \"\"\"This function contains all the necessary attributes\"\"\" self.name = name self.years = years self.dog_age = years*9 def sound(self): \"\"\"This function enables the dog to speak\"\"\" print(\"woof! I am {} and I am {} dog years old! woof!\".format(self.name, self.dog_age)) fudge = Dog(\"Fudge\", 2) maple = Dog(\"Maple\", 1.5) fudge.sound() maple.sound() woof! I am Fudge and I am 18 dog years old! woof! woof! I am Maple and I am 13.5 dog years old! woof! help(Dog) Help on class Dog in module __main__: class Dog(builtins.object) | Dog(name, years) | | This class enables the dog to say its name and age in dog years | | Methods defined here: | | __init__(self, name, years) | This function contains all the necessary attributes | | sound(self) | This function enables the dog to speak | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) Files and Filesystems Background A computer file is a computer resource for recording data discretely (not in the secretive context, but specifically somewhere on a piece of hardware) in a computer storage device. Just as words can be written to paper, so can information be written to a computer file. Files can be edited and transferred through the internet on that particular computer system. There are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once. By using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times. Typically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access. File system In computing, a file system or filesystem, controls how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stops and the next begins. By separating the data into pieces and giving each piece a name, the data is isolated and identified. Taking its name from the way paper-based data management system is named, each group of data is called a \u201cfile\u201d. The structure and logic rules used to manage the groups of data and their names is called a \u201cfile system\u201d. Path A path, the general form of the name of a file or directory, specifies a unique location in a file system. A path points to a file system location by following the directory tree hierarchy expressed in a string of characters in which path components, separated by a delimiting character, represent each directory. The delimiting character is most commonly the slash (\u201d/\u201d), the backslash character (\u201d\\\u201d), or colon (\u201d:\u201d), though some operating systems may use a different delimiter. Paths are used extensively in computer science to represent the directory/file relationships common in modern operating systems, and are essential in the construction of Uniform Resource Locators (URLs). Resources can be represented by either absolute or relative paths. As an example consider the following two files: /Users/theodore/MyGit/@atomickitty/hurri-sensors/.git/Guest.conf /etc/apache2/users/Guest.conf They both have the same file name, but are located on different paths. Failure to provide the path when addressing the file can be a problem. Another way to interpret is that the two unique files actually have different names, and only part of those names is common (Guest.conf) The two names above (including the path) are called fully qualified filenames (or absolute names), a relative path (usually relative to the file or program of interest depends on where in the directory structure the file lives. If we are currently in the .git directory (the first file) the path to the file is just the filename. We have experienced path issues with dependencies on .png files - in general your JupyterLab notebooks on CoCalc can only look at the local directory which is why we have to copy files into the directory for things to work. File Types Text Files. Text files are regular files that contain information readable by the user. This information is stored in ASCII. You can display and print these files. The lines of a text file must not contain NULL characters, and none can exceed a prescribed (by architecture) length, including the new-line character. The term text file does not prevent the inclusion of control or other nonprintable characters (other than NUL). Therefore, standard utilities that list text files as inputs or outputs are either able to process the special characters gracefully or they explicitly describe their limitations within their individual sections. Binary Files. Binary files are regular files that contain information readable by the computer. Binary files may be executable files that instruct the system to accomplish a job. Commands and programs are stored in executable, binary files. Special compiling programs translate ASCII text into binary code. The only difference between text and binary files is that text files have lines of less than some length, with no NULL characters, each terminated by a new-line character. Directory Files. Directory files contain information the system needs to access all types of files, but they do not contain the actual file data. As a result, directories occupy less space than a regular file and give the file system structure flexibility and depth. Each directory entry represents either a file or a subdirectory. Each entry contains the name of the file and the file's index node reference number (i-node). The i-node points to the unique index node assigned to the file. The i-node describes the location of the data associated with the file. Directories are created and controlled by a separate set of commands. File Manipulation For this lesson we examine just a handfull of file manipulations which are quite useful. Files can be \"created\",\"read\",\"updated\", or \"deleted\" (CRUD). Example: Create a file, write to it. Below is an example of creating a file that does not yet exist. The script is a bit pendandic on purpose. First will use some system commands to view the contents of the local directory import sys # on a Mac/Linux ! rm -rf myfirstfile.txt # delete file if it exists ! pwd # list name of working directory, note it includes path, so it is an absolute path # on Winderz #! del myfirstfile.txt # delete file if it exists #! %pwd # list name of working directory, note it includes path, so it is an absolute path /home/sensei/1330-textbook-webroot/docs/lesson6 # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 752 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt # create file example externalfile = open(\"myfirstfile.txt\",'w') # create connection to file, set to write (w), file does not need to exist mymessage = 'message in a bottle' #some object to write, in this case a string externalfile.write(mymessage)# write the contents of mymessage to the file externalfile.close() # close the file connection At this point our new file should exist, lets list the directory and see if that is so # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 756 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 19 Feb 17 17:35 myfirstfile.txt -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt Sure enough, its there, we will use a bash command cat to look at the contents of the file. ! cat myfirstfile.txt # Mac/Linux # ! type myfirstfile.txt # Winderz message in a bottle Example: Read from an existing file. We will continue using the file we just made, and read from it the example is below # read file example externalfile = open(\"myfirstfile.txt\",'r') # create connection to file, set to read (r), file must exist silly_string = externalfile.read() # read the contents externalfile.close() # close the file connection print(silly_string) message in a bottle Example: Update a file. This example continues with our same file, but we will now add contents without destroying existing contents. The keyword is append externalfile = open(\"myfirstfile.txt\",'a') # create connection to file, set to append (a), file does not need to exist externalfile.write('\\n') # adds a newline character what_to_add = 'I love rock-and-roll, put another dime in the jukebox baby ... \\n' externalfile.write(what_to_add) # add a string including the linefeed what_to_add = '... the waiting is the hardest part \\n' externalfile.write(what_to_add) # add a string including the linefeed mylist = [1,2,3,4,5] # a list of numbers what_to_add = ','.join(map(repr, mylist)) + \"\\n\" # one way to write the list externalfile.write(what_to_add) what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" # another way to write the list externalfile.write(what_to_add) externalfile.close() As before we can examine the contents using a shell command sent from the notebook. ! cat myfirstfile.txt # ! type myfirstfile.txt # Winderz message in a bottle I love rock-and-roll, put another dime in the jukebox baby ... ... the waiting is the hardest part 1,2,3,4,5 1,2,3,4,5 Example: Delete a file Delete can be done by a system call as we did above to clear the local directory In a JupyterLab notebook, we can either use import sys ! rm -rf myfirstfile.txt # delete file if it exists or import os os.remove(\"myfirstfile.txt\") they both have same effect, both equally dangerous to your filesystem. Learn more about CRUD with text files at https://www.guru99.com/reading-and-writing-files-in-python.html Learn more about file delete at https://www.dummies.com/programming/python/how-to-delete-a-file-in-python/ import os file2kill = \"myfirstfile.txt\" try: os.remove(file2kill) # file must exist or will generate an exception except: pass # example of using pass to improve readability print(file2kill, \" missing or deleted !\") myfirstfile.txt missing or deleted ! A little discussion on the part where we wrote numbers what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" Here are descriptions of the two functions map and repr map(function, iterable, ...) Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list. repr(object) Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse quotes). It is sometimes useful to be able to access this operation as an ordinary function. For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() , otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. A class can control what this function returns for its instances by defining a repr() method. What they do in this script is important. The statement: what_to_add = \u2019,\u2019.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" is building a string that will be comprised of elements of mylist[0:len(mylist)]. The repr() function gets these elements as they are represented in the computer, the delimiter a comma is added using the join method in Python, and because everything is now a string the ... + \"\\n\" puts a linefeed character at the end of the string so the output will start a new line the next time something is written. Example create a text file, name it \"MyFavoriteQuotation\" . Write your favorite quotation in the file. Read the file. Add this string to it in a new line : \"And that's something I wish I had said...\" Show the final outcome. # create the \"My Favorite Quotation\" file: externalfile = open(\"MyFavoriteQuotation.txt\",'w') # create connection to file, set to write (w) myquotation = 'The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.' #My choice: quotation from Pulp Fiction externalfile.write(myquotation)# write the contents of mymessage to the file externalfile.close() # close the file connection #Let's read the file ! cat MyFavoriteQuotation.txt # Let's add the string externalfile = open(\"MyFavoriteQuotation.txt\",'a') #create connection to file, set to append (a) externalfile.write('\\n') # adds a newline character what_to_add = \"And that's something I wish I had said ... \\n\" externalfile.write(what_to_add) externalfile.close() #Let's read the file one last time ! cat MyFavoriteQuotation.txt # ! type MyFavoriteQuotation # Winderz The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you. And that's something I wish I had said ... References Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty compthink /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Data Structures using NUMPY"},{"location":"1-Lessons/Lesson07/lesson6/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 31 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson07/lesson6/#lesson-6-classes-objects-and-file-handling","text":"Classes and Objects Files Create (new), Open (existing) Read from .... Write to ... Close (save) Delete","title":"Lesson 6 Classes, Objects, and File Handling:"},{"location":"1-Lessons/Lesson07/lesson6/#special-script-blocks","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;}","title":"Special Script Blocks"},{"location":"1-Lessons/Lesson07/lesson6/#objectives","text":"To understand the use of classes and objects to do effective coding in Python To understand the basic idea of how to manipulate the data in a file using file handling options in Python","title":"Objectives"},{"location":"1-Lessons/Lesson07/lesson6/#classes-and-objects","text":"In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated. When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class. Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state. Class definitions, like function definitions (def statements) must be executed before they have any effect. (You could conceivably place a class definition in a branch of an if statement, or inside a function.) In practice, the statements inside a class definition will usually be function definitions, but other statements are allowed, and sometimes useful \u2014 we\u2019ll come back to this later. The function definitions inside a class normally have a peculiar form of argument list, dictated by the calling conventions for methods \u2014 again, this is explained later. When a class definition is entered, a new namespace is created, and used as the local scope \u2014 thus, all assignments to local variables go into this new namespace. In particular, function definitions bind the name of the new function here. When a class definition is left normally (via the end), a class object is created. This is basically a wrapper around the contents of the namespace created by the class definition; we\u2019ll learn more about class objects in the next section. The original local scope (the one in effect just before the class definition was entered) is reinstated, and the class object is bound here to the class name given in the class definition header (ClassName in the example).","title":"Classes and Objects"},{"location":"1-Lessons/Lesson07/lesson6/#what-is-an-object","text":"An object is simply a collection of data (variables) and methods (functions) that act on those data. Similarly, a class is a blueprint for that object. We can think of class as a sketch (prototype) of a house. It contains all the details about the floors, doors, windows etc. Based on these descriptions we build the house. House is the object. As many houses can be made from a house's blueprint, we can create many objects from a class. An object is also called an instance of a class and the process of creating this object is called instantiation Learn more at 1. https://docs.python.org/3/tutorial/classes.html 2. https://en.wikipedia.org/wiki/Class_(computer_programming)","title":"What is an object?"},{"location":"1-Lessons/Lesson07/lesson6/#an-example","text":"Write a class named ' Tax ' to calculate the state tax (in dollars) of Employees at Texas Tech University based on their annual salary. The state tax is 16% if the annual salary is below 80,000 dollars and 22% if the salary is more than 80,000 dollars. Employee Annual salary (dollars) Bob 150,000 Mary 78,000 John 55,000 Danny 175,000","title":"An Example:"},{"location":"1-Lessons/Lesson07/lesson6/#notes","text":"Use docstrings to describe the purpose of the class. Use if....else conditional statements within the method of the class to choose the relevant tax % based on the annual salary. Create an object for employee and display the output as shown below. Bob's tax amount (in dollars): AMOUNT Mary's tax amount (in dollars): AMOUNT John's tax amount (in dollars): AMOUNT Danny's tax amount (in dollars): AMOUNT class Tax: \"\"\"This class calculates the tax amount based on the annual salary and the state tax %\"\"\" def __init__(self, salary): # here is the instantiation constructor self.salary = salary def taxamount(self): # here is a method (function) that can operate on the class once created if self.salary < 80000: return self.salary*(16/100) else: return self.salary*(22/100) bob = Tax(150000) dir(bob) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] bob = Tax(150000) # objects constructed using Tax class mary = Tax(78000) john = Tax(55000) danny = Tax(175000) dir(Tax) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'taxamount'] dir(mary) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] print(\"bobz salary \", bob.salary ) print(\"Bob's tax amount (in dollars):\", bob.taxamount() ) print(\"Mary's tax amount (in dollars):\", mary.taxamount()) print(\"John's tax amount (in dollars):\", john.taxamount()) print(\"Danny's tax amount (in dollars):\", danny.taxamount()) bobz salary 150000 Bob's tax amount (in dollars): 33000.0 Mary's tax amount (in dollars): 12480.0 John's tax amount (in dollars): 8800.0 Danny's tax amount (in dollars): 38500.0 Numbers, strings, lists, and dictionaries are all objects that are instances of a parent class print(type(0)) <class 'int'> print(type(\"\")) <class 'str'> print(type([1, 2, 3, 4])) <class 'list'> To get more information about the built-in classes and objects, use dir( ) and help( ) functions print(dir(int)) print(help(\"\")) User-defined classes: Defining docstrings class Dog: \"\"\"This class enables the dog to say its name and age in dog years\"\"\" def __init__(self, name, years): \"\"\"This function contains all the necessary attributes\"\"\" self.name = name self.years = years self.dog_age = years*9 def sound(self): \"\"\"This function enables the dog to speak\"\"\" print(\"woof! I am {} and I am {} dog years old! woof!\".format(self.name, self.dog_age)) fudge = Dog(\"Fudge\", 2) maple = Dog(\"Maple\", 1.5) fudge.sound() maple.sound() woof! I am Fudge and I am 18 dog years old! woof! woof! I am Maple and I am 13.5 dog years old! woof! help(Dog) Help on class Dog in module __main__: class Dog(builtins.object) | Dog(name, years) | | This class enables the dog to say its name and age in dog years | | Methods defined here: | | __init__(self, name, years) | This function contains all the necessary attributes | | sound(self) | This function enables the dog to speak | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined)","title":"Notes:"},{"location":"1-Lessons/Lesson07/lesson6/#files-and-filesystems","text":"","title":"Files and Filesystems"},{"location":"1-Lessons/Lesson07/lesson6/#background","text":"A computer file is a computer resource for recording data discretely (not in the secretive context, but specifically somewhere on a piece of hardware) in a computer storage device. Just as words can be written to paper, so can information be written to a computer file. Files can be edited and transferred through the internet on that particular computer system. There are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once. By using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times. Typically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access.","title":"Background"},{"location":"1-Lessons/Lesson07/lesson6/#file-system","text":"In computing, a file system or filesystem, controls how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stops and the next begins. By separating the data into pieces and giving each piece a name, the data is isolated and identified. Taking its name from the way paper-based data management system is named, each group of data is called a \u201cfile\u201d. The structure and logic rules used to manage the groups of data and their names is called a \u201cfile system\u201d.","title":"File system"},{"location":"1-Lessons/Lesson07/lesson6/#path","text":"A path, the general form of the name of a file or directory, specifies a unique location in a file system. A path points to a file system location by following the directory tree hierarchy expressed in a string of characters in which path components, separated by a delimiting character, represent each directory. The delimiting character is most commonly the slash (\u201d/\u201d), the backslash character (\u201d\\\u201d), or colon (\u201d:\u201d), though some operating systems may use a different delimiter. Paths are used extensively in computer science to represent the directory/file relationships common in modern operating systems, and are essential in the construction of Uniform Resource Locators (URLs). Resources can be represented by either absolute or relative paths. As an example consider the following two files: /Users/theodore/MyGit/@atomickitty/hurri-sensors/.git/Guest.conf /etc/apache2/users/Guest.conf They both have the same file name, but are located on different paths. Failure to provide the path when addressing the file can be a problem. Another way to interpret is that the two unique files actually have different names, and only part of those names is common (Guest.conf) The two names above (including the path) are called fully qualified filenames (or absolute names), a relative path (usually relative to the file or program of interest depends on where in the directory structure the file lives. If we are currently in the .git directory (the first file) the path to the file is just the filename. We have experienced path issues with dependencies on .png files - in general your JupyterLab notebooks on CoCalc can only look at the local directory which is why we have to copy files into the directory for things to work.","title":"Path"},{"location":"1-Lessons/Lesson07/lesson6/#file-types","text":"Text Files. Text files are regular files that contain information readable by the user. This information is stored in ASCII. You can display and print these files. The lines of a text file must not contain NULL characters, and none can exceed a prescribed (by architecture) length, including the new-line character. The term text file does not prevent the inclusion of control or other nonprintable characters (other than NUL). Therefore, standard utilities that list text files as inputs or outputs are either able to process the special characters gracefully or they explicitly describe their limitations within their individual sections. Binary Files. Binary files are regular files that contain information readable by the computer. Binary files may be executable files that instruct the system to accomplish a job. Commands and programs are stored in executable, binary files. Special compiling programs translate ASCII text into binary code. The only difference between text and binary files is that text files have lines of less than some length, with no NULL characters, each terminated by a new-line character. Directory Files. Directory files contain information the system needs to access all types of files, but they do not contain the actual file data. As a result, directories occupy less space than a regular file and give the file system structure flexibility and depth. Each directory entry represents either a file or a subdirectory. Each entry contains the name of the file and the file's index node reference number (i-node). The i-node points to the unique index node assigned to the file. The i-node describes the location of the data associated with the file. Directories are created and controlled by a separate set of commands.","title":"File Types"},{"location":"1-Lessons/Lesson07/lesson6/#file-manipulation","text":"For this lesson we examine just a handfull of file manipulations which are quite useful. Files can be \"created\",\"read\",\"updated\", or \"deleted\" (CRUD).","title":"File Manipulation"},{"location":"1-Lessons/Lesson07/lesson6/#example-create-a-file-write-to-it","text":"Below is an example of creating a file that does not yet exist. The script is a bit pendandic on purpose. First will use some system commands to view the contents of the local directory import sys # on a Mac/Linux ! rm -rf myfirstfile.txt # delete file if it exists ! pwd # list name of working directory, note it includes path, so it is an absolute path # on Winderz #! del myfirstfile.txt # delete file if it exists #! %pwd # list name of working directory, note it includes path, so it is an absolute path /home/sensei/1330-textbook-webroot/docs/lesson6 # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 752 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt # create file example externalfile = open(\"myfirstfile.txt\",'w') # create connection to file, set to write (w), file does not need to exist mymessage = 'message in a bottle' #some object to write, in this case a string externalfile.write(mymessage)# write the contents of mymessage to the file externalfile.close() # close the file connection At this point our new file should exist, lets list the directory and see if that is so # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 756 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 19 Feb 17 17:35 myfirstfile.txt -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt Sure enough, its there, we will use a bash command cat to look at the contents of the file. ! cat myfirstfile.txt # Mac/Linux # ! type myfirstfile.txt # Winderz message in a bottle","title":"Example: Create a file, write to it."},{"location":"1-Lessons/Lesson07/lesson6/#example-read-from-an-existing-file","text":"We will continue using the file we just made, and read from it the example is below # read file example externalfile = open(\"myfirstfile.txt\",'r') # create connection to file, set to read (r), file must exist silly_string = externalfile.read() # read the contents externalfile.close() # close the file connection print(silly_string) message in a bottle","title":"Example: Read from an existing file."},{"location":"1-Lessons/Lesson07/lesson6/#example-update-a-file","text":"This example continues with our same file, but we will now add contents without destroying existing contents. The keyword is append externalfile = open(\"myfirstfile.txt\",'a') # create connection to file, set to append (a), file does not need to exist externalfile.write('\\n') # adds a newline character what_to_add = 'I love rock-and-roll, put another dime in the jukebox baby ... \\n' externalfile.write(what_to_add) # add a string including the linefeed what_to_add = '... the waiting is the hardest part \\n' externalfile.write(what_to_add) # add a string including the linefeed mylist = [1,2,3,4,5] # a list of numbers what_to_add = ','.join(map(repr, mylist)) + \"\\n\" # one way to write the list externalfile.write(what_to_add) what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" # another way to write the list externalfile.write(what_to_add) externalfile.close() As before we can examine the contents using a shell command sent from the notebook. ! cat myfirstfile.txt # ! type myfirstfile.txt # Winderz message in a bottle I love rock-and-roll, put another dime in the jukebox baby ... ... the waiting is the hardest part 1,2,3,4,5 1,2,3,4,5","title":"Example: Update a file."},{"location":"1-Lessons/Lesson07/lesson6/#example-delete-a-file","text":"Delete can be done by a system call as we did above to clear the local directory In a JupyterLab notebook, we can either use import sys ! rm -rf myfirstfile.txt # delete file if it exists or import os os.remove(\"myfirstfile.txt\") they both have same effect, both equally dangerous to your filesystem. Learn more about CRUD with text files at https://www.guru99.com/reading-and-writing-files-in-python.html Learn more about file delete at https://www.dummies.com/programming/python/how-to-delete-a-file-in-python/ import os file2kill = \"myfirstfile.txt\" try: os.remove(file2kill) # file must exist or will generate an exception except: pass # example of using pass to improve readability print(file2kill, \" missing or deleted !\") myfirstfile.txt missing or deleted ! A little discussion on the part where we wrote numbers what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" Here are descriptions of the two functions map and repr map(function, iterable, ...) Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list. repr(object) Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse quotes). It is sometimes useful to be able to access this operation as an ordinary function. For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() , otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. A class can control what this function returns for its instances by defining a repr() method. What they do in this script is important. The statement: what_to_add = \u2019,\u2019.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" is building a string that will be comprised of elements of mylist[0:len(mylist)]. The repr() function gets these elements as they are represented in the computer, the delimiter a comma is added using the join method in Python, and because everything is now a string the ... + \"\\n\" puts a linefeed character at the end of the string so the output will start a new line the next time something is written.","title":"Example: Delete a file"},{"location":"1-Lessons/Lesson07/lesson6/#example","text":"create a text file, name it \"MyFavoriteQuotation\" . Write your favorite quotation in the file. Read the file. Add this string to it in a new line : \"And that's something I wish I had said...\" Show the final outcome. # create the \"My Favorite Quotation\" file: externalfile = open(\"MyFavoriteQuotation.txt\",'w') # create connection to file, set to write (w) myquotation = 'The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.' #My choice: quotation from Pulp Fiction externalfile.write(myquotation)# write the contents of mymessage to the file externalfile.close() # close the file connection #Let's read the file ! cat MyFavoriteQuotation.txt # Let's add the string externalfile = open(\"MyFavoriteQuotation.txt\",'a') #create connection to file, set to append (a) externalfile.write('\\n') # adds a newline character what_to_add = \"And that's something I wish I had said ... \\n\" externalfile.write(what_to_add) externalfile.close() #Let's read the file one last time ! cat MyFavoriteQuotation.txt # ! type MyFavoriteQuotation # Winderz The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you. And that's something I wish I had said ...","title":"Example"},{"location":"1-Lessons/Lesson07/lesson6/#references","text":"Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty compthink /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"References"},{"location":"1-Lessons/Lesson08/lesson7/","text":"ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 14 February 2021 Lesson 7 The Pandas module About Pandas How to install Anaconda JupyterHub/Lab (on Linux) JupyterHub/Lab (on MacOS) JupyterHub/Lab (on Windoze) The Dataframe Primatives Using Pandas Create, Modify, Delete datagrames Slice Dataframes Conditional Selection Synthetic Programming (Symbolic Function Application) Files Access Files from a remote Web Server Get file contents Get the actual file Adaptations for encrypted servers (future semester) Special Script Blocks %%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} Objectives To understand the dataframe abstraction as implemented in the Pandas library(module). To be able to access and manipulate data within a dataframe To be able to obtain basic statistical measures of data within a dataframe Read/Write from/to files MS Excel-type files (.xls,.xlsx,.csv) (LibreOffice files use the MS .xml standard) Ordinary ASCII (.txt) files Access files directly from a URL (advanced concept) Using a wget-type function Using a curl-type function Using API keys (future versions) Pandas: Pandas is the core library for dataframe manipulation in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is derived from the term \u2018Panel Data\u2019. If you are curious about Pandas, this cheat sheet is recommended: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf Data Structure The Primary data structure is called a dataframe. It is an abstraction where data are represented as a 2-dimensional mutable and heterogenous tabular data structure; much like a Worksheet in MS Excel. The structure itself is popular among statisticians and data scientists and business executives. According to the marketing department \"Pandas Provides rich data structures and functions designed to make working with data fast, easy, and expressive. It is useful in data manipulation, cleaning, and analysis; Pandas excels in performance and productivity \" The Dataframe A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. Like MS Excel we can query the dataframe to find the contents of a particular cell using its row name and column name , or operate on entire rows and columns To use pandas, we need to import the module. Computational Thinking Concepts The CT concepts expressed within Pandas include: Decomposition : Data interpretation, manipulation, and analysis of Pandas dataframes is an act of decomposition -- although the dataframes can be quite complex. Abstraction : The dataframe is a data representation abstraction that allows for placeholder operations, later substituted with specific contents for a problem; enhances reuse and readability. We leverage the principle of algebraic replacement using these abstractions. Algorithms : Data interpretation, manipulation, and analysis of dataframes are generally implemented as part of a supervisory algorithm. Module Set-Up In principle, Pandas should be available in a default Anaconda install - You should not have to do any extra installation steps to install the library in Python - You do have to import the library in your scripts How to check - Simply open a code cell and run import pandas if the notebook does not protest (i.e. pink block of error), the youis good to go. import pandas If you do get an error, that means that you will have to install using conda or pip ; you are on-your-own here! On the content server the process is: Open a new terminal from the launcher Change to root user su then enter the root password sudo -H /opt/jupyterhib/bin/python3 -m pip install pandas Wait until the install is complete; for security, user compthink is not in the sudo group Verify the install by trying to execute import pandas as above. The process above will be similar on a Macintosh, or Windows if you did not use an Anaconda distribution. Best is to have a sucessful anaconda install, or go to the GoodJobUntilMyOrgansGetHarvested . If you have to do this kind of install, you will have to do some reading, some references I find useful are: 1. https://jupyterlab.readthedocs.io/en/stable/user/extensions.html 2. https://www.pugetsystems.com/labs/hpc/Note-How-To-Install-JupyterHub-on-a-Local-Server-1673/#InstallJupyterHub 3. https://jupyterhub.readthedocs.io/en/stable/installation-guide-hard.html (This is the approach on the content server which has a functioning JupyterHub) Dataframe-type Structure using primative python First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. import numpy mytabular = numpy.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 19, 21, 22, 81] ['B', 94, 75, 66, 44] ['C', 70, 56, 47, 63] ['D', 56, 80, 39, 39] ['E', 66, 78, 39, 15] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 70, 56, 47, 63] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 21 75 56 80 78 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 47 Now we shall create a proper dataframe We will now do the same using pandas mydf = pandas.DataFrame(numpy.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 We can also turn our table into a dataframe, notice how the constructor adds header row and index column mydf1 = pandas.DataFrame(mytable) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 19 21 22 81 2 B 94 75 66 44 3 C 70 56 47 63 4 D 56 80 39 39 5 E 66 78 39 15 To get proper behavior, we can just reuse our original objects mydf2 = pandas.DataFrame(mytabular,myrowname,mycolname) mydf2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 19 21 22 81 B 94 75 66 44 C 70 56 47 63 D 56 80 39 39 E 66 78 39 15 Why are mydf and mydf2 different? Getting the shape of dataframes The shape method, which is available after the dataframe is constructed, will return the row and column rank (count) of a dataframe. mydf.shape (5, 4) mydf1.shape (6, 5) mydf2.shape (5, 4) Appending new columns To append a column simply assign a value to a new column name to the dataframe mydf['new']= 'NA' mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA Appending new rows This is sometimes a bit trickier but here is one way: - create a copy of a row, give it a new name. - concatenate it back into the dataframe. newrow = mydf.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pandas.concat([mydf,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA X 80 67 4 24 NA Removing Rows and Columns To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 # or just use drop with axis specify newtable.drop('X', axis=0, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 Indexing We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 20 B 8 D 83 E 67 Name: X, dtype: int64 newtable[['X','W']] #Selecing a multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 20 97 B 8 74 D 83 92 E 67 80 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 80 X 67 Y 4 Z 24 Name: E, dtype: int64 newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 newtable.loc[['E','D','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 80 67 4 24 D 92 83 90 28 B 74 8 7 99 newtable.loc[['B','E','D'],['X','Y']] #Selecting elements via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 8 7 E 67 4 D 83 90 Conditional Selection mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? mydf[mydf['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? mydf[mydf['col2']==mydf['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object Descriptor Functions #Creating a dataframe from a dictionary mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach head method Returns the first few rows, useful to infer structure #Returns only the first five rows mydf.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit info method Returns the data model (data column count, names, data types) #Info about the dataframe mydf.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes describe method Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe mydf.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000 Counting and Sum methods There are also methods for counts and sums by specific columns mydf['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) mydf['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values mydf['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) mydf['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64 Using functions in dataframes - symbolic apply The power of Pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. This employs principles of pattern matching , abstraction , and algorithm development ; a holy trinity of Computational Thinning. It's somewhat complicated but quite handy, best shown by an example: def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(mydf) print('Apply the times2 function to col2') mydf['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64 Sorts mydf.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit mydf.sort_values('col3', ascending = True) #Lexiographic sort .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 1 2 555 apple 6 7 222 banana 2 3 666 grape 4 5 666 jackfruit 3 4 444 mango 0 1 444 orange 7 8 222 peach 5 6 111 watermelon Aggregating (Grouping Values) dataframe contents #Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } mydf1 = pandas.DataFrame(data) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' mydf1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' mydf1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27 Filtering out missing values Filtering and cleaning are often used to describe the process where data that does not support a narrative is removed ;typically for maintenance of profit applications, if the data are actually missing that is common situation where cleaning is justified. #Creating a dataframe from a dictionary df = pandas.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach Reading a File into a Dataframe Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) # xlsx reads deprecated here is a hack using openpyxl readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 Writing a dataframe to file #Creating and writing to a .csv file readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pandas.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pandas.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', index = False, engine='openpyxl') readfileexcel = pandas.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 Downloading files from websites (optional) This section shows how to get files from a remote computer. There are several ways to get the files, most importantly you need the FQDN to the file. Method 1: Get data from a file on a remote server (unencrypted) This section shows how to obtain data files from public URLs. Prerequesites: You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://... Web Developer Notes If you want to distribute files (web developers) the files need to be in the server webroot, but can be deep into the heirarchial structure. Here we will do an example with a file that contains topographic data in XYZ format, without header information. The first few lines of the remote file look like: 74.90959724 93.21251922 0 75.17907367 64.40278759 0 94.9935575 93.07951286 0 95.26234119 64.60091165 0 54.04976655 64.21159095 0 54.52914363 35.06934342 0 75.44993558 34.93079513 0 75.09317373 5.462959114 0 74.87357468 10.43130083 0 74.86249082 15.72938748 0 And importantly it is tab delimited. The module to manipulate url in python is called urllib Google search to learn more, here we are using only a small component without exception trapping. #Step 1: import needed modules to interact with the internet from urllib.request import urlopen # import a method that will connect to a url and read file contents import pandas #import pandas This next code fragment sets a string called remote_url ; it is just a variable, name can be anything that honors python naming rules. Then the urllib function urlopen with read and decode methods is employed, the result is stored in an object named elevationXYZ #Step 2: make the connection to the remote file (actually its implementing \"bash curl -O http://fqdn/path ...\") remote_url = 'http://www.rtfmps.com/share_files/pip-corner-sumps.txt' # elevationXYZ = urlopen(remote_url).read().decode().split() # Gets the file contents as a single vector, comma delimited, file is not retained locally At this point the object exists as a single vector with hundreds of elements. We now need to structure the content. Here using python primatives, and knowing how the data are supposed to look, we prepare variables to recieve the structured results #Step 3 Python primatives to structure the data, or use fancy modules (probably easy in numpy) howmany = len(elevationXYZ) # how long is the vector? nrow = int(howmany/3) xyz = [[0 for j in range(3)] for j in range(nrow)] # null space to receive data define columnX Now that everything is ready, we can extract from the object the values we want into xyz #Step4 Now will build xyz as a matrix with 3 columns index = 0 for irow in range(0,nrow): xyz[irow][0]=float(elevationXYZ[index]) xyz[irow][1]=float(elevationXYZ[index+1]) xyz[irow][2]=float(elevationXYZ[index+2]) index += 3 #increment the index xyz is now a 3-column float array and can now probably be treated as a data frame. Here we use a pandas method to build the dataframe. df = pandas.DataFrame(xyz) Get some info, yep three columns (ordered triples to be precise!) df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 774 entries, 0 to 773 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 774 non-null float64 1 1 774 non-null float64 2 2 774 non-null float64 dtypes: float64(3) memory usage: 18.3 KB And some summary statistics (meaningless for these data), but now have taken data from the internet and prepared it for analysis. df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 count 774.000000 774.000000 774.000000 mean 52.064621 48.770060 2.364341 std 30.883400 32.886277 1.497413 min -2.113554 -11.360960 0.000000 25% 25.640786 21.809579 2.000000 50% 55.795821 49.059950 2.000000 75% 76.752290 75.015933 4.000000 max 111.726727 115.123931 4.000000 And lets look at the first few rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 74.909597 93.212519 0.0 1 75.179074 64.402788 0.0 2 94.993557 93.079513 0.0 3 95.262341 64.600912 0.0 4 54.049767 64.211591 0.0 Method 2: Get the actual file from a remote web server (unencrypted) You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://... We will need a module to interface with the remote server, in this example lets use something different than urllib . Here we will use requests , so first we load the module import requests # Module to process http/https requests Now we will generate a GET request to the remote http server. I chose to do so using a variable to store the remote URL so I can reuse code in future projects. The GET request (an http/https method) is generated with the requests method get and assigned to an object named rget -- the name is arbitrary. Next we extract the file from the rget object and write it to a local file with the name of the remote file - esentially automating the download process. Then we import the pandas module. remote_url=\"http://54.243.252.9/engr-1330-psuedo-course/MyJupyterNotebooks/42-DataScience-EvaporationAnalysis/all_quads_gross_evaporation.csv\" # set the url rget = requests.get(remote_url, allow_redirects=True) # get the remote resource, follow imbedded links open('all_quads_gross_evaporation.csv','wb').write(rget.content) # extract from the remote the contents, assign to a local file same name import pandas as pd # Module to process dataframes (not absolutely needed but somewhat easier than using primatives, and gives graphing tools) # verify file exists ! pwd ! ls -la /home/sensei/1330-textbook-webroot/docs/lesson7 total 1412 drwxrwxr-x 3 sensei sensei 4096 Feb 16 20:57 . drwxr-xr-x 10 sensei sensei 4096 Feb 16 20:30 .. drwxrwxr-x 2 sensei sensei 4096 Feb 16 20:53 .ipynb_checkpoints -rw-rw-r-- 1 sensei sensei 21150 Feb 15 15:58 01-table-dataframe.png -rw-rw-r-- 1 sensei sensei 51 Feb 15 15:58 CSV_ReadingFile.csv -rw-rw-r-- 1 sensei sensei 55 Feb 16 20:59 CSV_WritingFile1.csv -rw-rw-r-- 1 sensei sensei 46 Feb 16 20:59 CSV_WritingFile2.csv -rw-rw-r-- 1 sensei sensei 693687 Feb 15 15:58 ENGR-1330-Lesson8-Dev.html -rw-rw-r-- 1 sensei sensei 166938 Feb 15 15:58 ENGR-1330-Lesson8-Dev.ipynb -rw-rw-r-- 1 sensei sensei 5508 Feb 15 15:58 Excel_ReadingFile.xlsx -rw-rw-r-- 1 sensei sensei 5041 Feb 16 20:59 Excel_WritingFile.xlsx -rw-rw-r-- 1 sensei sensei 363498 Feb 16 20:59 all_quads_gross_evaporation.csv -rw-rw-r-- 1 sensei sensei 108222 Feb 16 20:57 lesson7.ipynb -rw-rw-r-- 1 sensei sensei 40566 Feb 15 15:58 output_126_1.png Now we can read the file contents and check its structure, before proceeding. evapdf = pd.read_csv(\"all_quads_gross_evaporation.csv\",parse_dates=[\"YYYY-MM\"]) # Read the file as a .CSV assign to a dataframe evapdf evapdf.head() # check structure .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } YYYY-MM 104 105 106 107 108 204 205 206 207 ... 911 912 1008 1009 1010 1011 1108 1109 1110 1210 0 1954-01-01 1.80 1.80 2.02 2.24 2.24 2.34 1.89 1.80 1.99 ... 1.42 1.30 2.50 2.42 1.94 1.29 2.59 2.49 2.22 2.27 1 1954-02-01 4.27 4.27 4.13 3.98 3.90 4.18 4.26 4.27 4.26 ... 2.59 2.51 4.71 4.30 3.84 2.50 5.07 4.62 4.05 4.18 2 1954-03-01 4.98 4.98 4.62 4.25 4.20 5.01 4.98 4.98 4.68 ... 3.21 3.21 6.21 6.06 5.02 3.21 6.32 6.20 5.68 5.70 3 1954-04-01 6.09 5.94 5.94 6.07 5.27 6.31 5.98 5.89 5.72 ... 3.83 3.54 6.45 6.25 4.92 3.54 6.59 6.44 5.88 5.95 4 1954-05-01 5.41 5.09 5.14 4.40 3.61 5.57 4.56 4.47 4.18 ... 3.48 3.97 7.92 8.13 6.31 3.99 7.75 7.98 7.40 7.40 5 rows \u00d7 93 columns Structure looks like a spreadsheet as expected; lets plot the time series for cell '911' evapdf.plot.line(x='YYYY-MM',y='911') # Plot quadrant 911 evaporation time series <AxesSubplot:xlabel='YYYY-MM'> Method 3: Get the actual file from an encrypted server This section is saved for future semesters References Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Data Frames using PANDAS"},{"location":"1-Lessons/Lesson08/lesson7/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 14 February 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson08/lesson7/#lesson-7-the-pandas-module","text":"About Pandas How to install Anaconda JupyterHub/Lab (on Linux) JupyterHub/Lab (on MacOS) JupyterHub/Lab (on Windoze) The Dataframe Primatives Using Pandas Create, Modify, Delete datagrames Slice Dataframes Conditional Selection Synthetic Programming (Symbolic Function Application) Files Access Files from a remote Web Server Get file contents Get the actual file Adaptations for encrypted servers (future semester)","title":"Lesson 7 The Pandas module"},{"location":"1-Lessons/Lesson08/lesson7/#special-script-blocks","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;}","title":"Special Script Blocks"},{"location":"1-Lessons/Lesson08/lesson7/#objectives","text":"To understand the dataframe abstraction as implemented in the Pandas library(module). To be able to access and manipulate data within a dataframe To be able to obtain basic statistical measures of data within a dataframe Read/Write from/to files MS Excel-type files (.xls,.xlsx,.csv) (LibreOffice files use the MS .xml standard) Ordinary ASCII (.txt) files Access files directly from a URL (advanced concept) Using a wget-type function Using a curl-type function Using API keys (future versions)","title":"Objectives"},{"location":"1-Lessons/Lesson08/lesson7/#pandas","text":"Pandas is the core library for dataframe manipulation in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is derived from the term \u2018Panel Data\u2019. If you are curious about Pandas, this cheat sheet is recommended: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf","title":"Pandas:"},{"location":"1-Lessons/Lesson08/lesson7/#data-structure","text":"The Primary data structure is called a dataframe. It is an abstraction where data are represented as a 2-dimensional mutable and heterogenous tabular data structure; much like a Worksheet in MS Excel. The structure itself is popular among statisticians and data scientists and business executives. According to the marketing department \"Pandas Provides rich data structures and functions designed to make working with data fast, easy, and expressive. It is useful in data manipulation, cleaning, and analysis; Pandas excels in performance and productivity \"","title":"Data Structure"},{"location":"1-Lessons/Lesson08/lesson7/#the-dataframe","text":"A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. Like MS Excel we can query the dataframe to find the contents of a particular cell using its row name and column name , or operate on entire rows and columns To use pandas, we need to import the module.","title":"The Dataframe"},{"location":"1-Lessons/Lesson08/lesson7/#computational-thinking-concepts","text":"The CT concepts expressed within Pandas include: Decomposition : Data interpretation, manipulation, and analysis of Pandas dataframes is an act of decomposition -- although the dataframes can be quite complex. Abstraction : The dataframe is a data representation abstraction that allows for placeholder operations, later substituted with specific contents for a problem; enhances reuse and readability. We leverage the principle of algebraic replacement using these abstractions. Algorithms : Data interpretation, manipulation, and analysis of dataframes are generally implemented as part of a supervisory algorithm.","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson08/lesson7/#module-set-up","text":"In principle, Pandas should be available in a default Anaconda install - You should not have to do any extra installation steps to install the library in Python - You do have to import the library in your scripts How to check - Simply open a code cell and run import pandas if the notebook does not protest (i.e. pink block of error), the youis good to go. import pandas If you do get an error, that means that you will have to install using conda or pip ; you are on-your-own here! On the content server the process is: Open a new terminal from the launcher Change to root user su then enter the root password sudo -H /opt/jupyterhib/bin/python3 -m pip install pandas Wait until the install is complete; for security, user compthink is not in the sudo group Verify the install by trying to execute import pandas as above. The process above will be similar on a Macintosh, or Windows if you did not use an Anaconda distribution. Best is to have a sucessful anaconda install, or go to the GoodJobUntilMyOrgansGetHarvested . If you have to do this kind of install, you will have to do some reading, some references I find useful are: 1. https://jupyterlab.readthedocs.io/en/stable/user/extensions.html 2. https://www.pugetsystems.com/labs/hpc/Note-How-To-Install-JupyterHub-on-a-Local-Server-1673/#InstallJupyterHub 3. https://jupyterhub.readthedocs.io/en/stable/installation-guide-hard.html (This is the approach on the content server which has a functioning JupyterHub)","title":"Module Set-Up"},{"location":"1-Lessons/Lesson08/lesson7/#dataframe-type-structure-using-primative-python","text":"First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. import numpy mytabular = numpy.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 19, 21, 22, 81] ['B', 94, 75, 66, 44] ['C', 70, 56, 47, 63] ['D', 56, 80, 39, 39] ['E', 66, 78, 39, 15] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 70, 56, 47, 63] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 21 75 56 80 78 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 47","title":"Dataframe-type Structure using primative python"},{"location":"1-Lessons/Lesson08/lesson7/#now-we-shall-create-a-proper-dataframe","text":"We will now do the same using pandas mydf = pandas.DataFrame(numpy.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 We can also turn our table into a dataframe, notice how the constructor adds header row and index column mydf1 = pandas.DataFrame(mytable) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 19 21 22 81 2 B 94 75 66 44 3 C 70 56 47 63 4 D 56 80 39 39 5 E 66 78 39 15 To get proper behavior, we can just reuse our original objects mydf2 = pandas.DataFrame(mytabular,myrowname,mycolname) mydf2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 19 21 22 81 B 94 75 66 44 C 70 56 47 63 D 56 80 39 39 E 66 78 39 15 Why are mydf and mydf2 different?","title":"Now we shall create a proper dataframe"},{"location":"1-Lessons/Lesson08/lesson7/#getting-the-shape-of-dataframes","text":"The shape method, which is available after the dataframe is constructed, will return the row and column rank (count) of a dataframe. mydf.shape (5, 4) mydf1.shape (6, 5) mydf2.shape (5, 4)","title":"Getting the shape of dataframes"},{"location":"1-Lessons/Lesson08/lesson7/#appending-new-columns","text":"To append a column simply assign a value to a new column name to the dataframe mydf['new']= 'NA' mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA","title":"Appending new columns"},{"location":"1-Lessons/Lesson08/lesson7/#appending-new-rows","text":"This is sometimes a bit trickier but here is one way: - create a copy of a row, give it a new name. - concatenate it back into the dataframe. newrow = mydf.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pandas.concat([mydf,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA X 80 67 4 24 NA","title":"Appending new rows"},{"location":"1-Lessons/Lesson08/lesson7/#removing-rows-and-columns","text":"To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 # or just use drop with axis specify newtable.drop('X', axis=0, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24","title":"Removing Rows and Columns"},{"location":"1-Lessons/Lesson08/lesson7/#indexing","text":"We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 20 B 8 D 83 E 67 Name: X, dtype: int64 newtable[['X','W']] #Selecing a multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 20 97 B 8 74 D 83 92 E 67 80 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 80 X 67 Y 4 Z 24 Name: E, dtype: int64 newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 newtable.loc[['E','D','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 80 67 4 24 D 92 83 90 28 B 74 8 7 99 newtable.loc[['B','E','D'],['X','Y']] #Selecting elements via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 8 7 E 67 4 D 83 90","title":"Indexing"},{"location":"1-Lessons/Lesson08/lesson7/#conditional-selection","text":"mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? mydf[mydf['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? mydf[mydf['col2']==mydf['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object","title":"Conditional Selection"},{"location":"1-Lessons/Lesson08/lesson7/#descriptor-functions","text":"#Creating a dataframe from a dictionary mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach","title":"Descriptor Functions"},{"location":"1-Lessons/Lesson08/lesson7/#head-method","text":"Returns the first few rows, useful to infer structure #Returns only the first five rows mydf.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit","title":"head method"},{"location":"1-Lessons/Lesson08/lesson7/#info-method","text":"Returns the data model (data column count, names, data types) #Info about the dataframe mydf.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes","title":"info method"},{"location":"1-Lessons/Lesson08/lesson7/#describe-method","text":"Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe mydf.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000","title":"describe method"},{"location":"1-Lessons/Lesson08/lesson7/#counting-and-sum-methods","text":"There are also methods for counts and sums by specific columns mydf['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) mydf['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values mydf['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) mydf['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64","title":"Counting and Sum methods"},{"location":"1-Lessons/Lesson08/lesson7/#using-functions-in-dataframes-symbolic-apply","text":"The power of Pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. This employs principles of pattern matching , abstraction , and algorithm development ; a holy trinity of Computational Thinning. It's somewhat complicated but quite handy, best shown by an example: def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(mydf) print('Apply the times2 function to col2') mydf['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64","title":"Using functions in dataframes - symbolic apply"},{"location":"1-Lessons/Lesson08/lesson7/#sorts","text":"mydf.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit mydf.sort_values('col3', ascending = True) #Lexiographic sort .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 1 2 555 apple 6 7 222 banana 2 3 666 grape 4 5 666 jackfruit 3 4 444 mango 0 1 444 orange 7 8 222 peach 5 6 111 watermelon","title":"Sorts"},{"location":"1-Lessons/Lesson08/lesson7/#aggregating-grouping-values-dataframe-contents","text":"#Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } mydf1 = pandas.DataFrame(data) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' mydf1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' mydf1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27","title":"Aggregating (Grouping Values) dataframe contents"},{"location":"1-Lessons/Lesson08/lesson7/#filtering-out-missing-values","text":"Filtering and cleaning are often used to describe the process where data that does not support a narrative is removed ;typically for maintenance of profit applications, if the data are actually missing that is common situation where cleaning is justified. #Creating a dataframe from a dictionary df = pandas.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach","title":"Filtering out missing values"},{"location":"1-Lessons/Lesson08/lesson7/#reading-a-file-into-a-dataframe","text":"Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) # xlsx reads deprecated here is a hack using openpyxl readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15","title":"Reading a File into a Dataframe"},{"location":"1-Lessons/Lesson08/lesson7/#writing-a-dataframe-to-file","text":"#Creating and writing to a .csv file readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pandas.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pandas.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', index = False, engine='openpyxl') readfileexcel = pandas.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15","title":"Writing a dataframe to file"},{"location":"1-Lessons/Lesson08/lesson7/#downloading-files-from-websites-optional","text":"This section shows how to get files from a remote computer. There are several ways to get the files, most importantly you need the FQDN to the file.","title":"Downloading files from websites (optional)"},{"location":"1-Lessons/Lesson08/lesson7/#method-1-get-data-from-a-file-on-a-remote-server-unencrypted","text":"This section shows how to obtain data files from public URLs. Prerequesites: You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://...","title":"Method 1: Get data from a file on a remote server (unencrypted)"},{"location":"1-Lessons/Lesson08/lesson7/#web-developer-notes","text":"If you want to distribute files (web developers) the files need to be in the server webroot, but can be deep into the heirarchial structure. Here we will do an example with a file that contains topographic data in XYZ format, without header information. The first few lines of the remote file look like: 74.90959724 93.21251922 0 75.17907367 64.40278759 0 94.9935575 93.07951286 0 95.26234119 64.60091165 0 54.04976655 64.21159095 0 54.52914363 35.06934342 0 75.44993558 34.93079513 0 75.09317373 5.462959114 0 74.87357468 10.43130083 0 74.86249082 15.72938748 0 And importantly it is tab delimited. The module to manipulate url in python is called urllib Google search to learn more, here we are using only a small component without exception trapping. #Step 1: import needed modules to interact with the internet from urllib.request import urlopen # import a method that will connect to a url and read file contents import pandas #import pandas This next code fragment sets a string called remote_url ; it is just a variable, name can be anything that honors python naming rules. Then the urllib function urlopen with read and decode methods is employed, the result is stored in an object named elevationXYZ #Step 2: make the connection to the remote file (actually its implementing \"bash curl -O http://fqdn/path ...\") remote_url = 'http://www.rtfmps.com/share_files/pip-corner-sumps.txt' # elevationXYZ = urlopen(remote_url).read().decode().split() # Gets the file contents as a single vector, comma delimited, file is not retained locally At this point the object exists as a single vector with hundreds of elements. We now need to structure the content. Here using python primatives, and knowing how the data are supposed to look, we prepare variables to recieve the structured results #Step 3 Python primatives to structure the data, or use fancy modules (probably easy in numpy) howmany = len(elevationXYZ) # how long is the vector? nrow = int(howmany/3) xyz = [[0 for j in range(3)] for j in range(nrow)] # null space to receive data define columnX Now that everything is ready, we can extract from the object the values we want into xyz #Step4 Now will build xyz as a matrix with 3 columns index = 0 for irow in range(0,nrow): xyz[irow][0]=float(elevationXYZ[index]) xyz[irow][1]=float(elevationXYZ[index+1]) xyz[irow][2]=float(elevationXYZ[index+2]) index += 3 #increment the index xyz is now a 3-column float array and can now probably be treated as a data frame. Here we use a pandas method to build the dataframe. df = pandas.DataFrame(xyz) Get some info, yep three columns (ordered triples to be precise!) df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 774 entries, 0 to 773 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 774 non-null float64 1 1 774 non-null float64 2 2 774 non-null float64 dtypes: float64(3) memory usage: 18.3 KB And some summary statistics (meaningless for these data), but now have taken data from the internet and prepared it for analysis. df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 count 774.000000 774.000000 774.000000 mean 52.064621 48.770060 2.364341 std 30.883400 32.886277 1.497413 min -2.113554 -11.360960 0.000000 25% 25.640786 21.809579 2.000000 50% 55.795821 49.059950 2.000000 75% 76.752290 75.015933 4.000000 max 111.726727 115.123931 4.000000 And lets look at the first few rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 74.909597 93.212519 0.0 1 75.179074 64.402788 0.0 2 94.993557 93.079513 0.0 3 95.262341 64.600912 0.0 4 54.049767 64.211591 0.0","title":"Web Developer Notes"},{"location":"1-Lessons/Lesson08/lesson7/#method-2-get-the-actual-file-from-a-remote-web-server-unencrypted","text":"You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://... We will need a module to interface with the remote server, in this example lets use something different than urllib . Here we will use requests , so first we load the module import requests # Module to process http/https requests Now we will generate a GET request to the remote http server. I chose to do so using a variable to store the remote URL so I can reuse code in future projects. The GET request (an http/https method) is generated with the requests method get and assigned to an object named rget -- the name is arbitrary. Next we extract the file from the rget object and write it to a local file with the name of the remote file - esentially automating the download process. Then we import the pandas module. remote_url=\"http://54.243.252.9/engr-1330-psuedo-course/MyJupyterNotebooks/42-DataScience-EvaporationAnalysis/all_quads_gross_evaporation.csv\" # set the url rget = requests.get(remote_url, allow_redirects=True) # get the remote resource, follow imbedded links open('all_quads_gross_evaporation.csv','wb').write(rget.content) # extract from the remote the contents, assign to a local file same name import pandas as pd # Module to process dataframes (not absolutely needed but somewhat easier than using primatives, and gives graphing tools) # verify file exists ! pwd ! ls -la /home/sensei/1330-textbook-webroot/docs/lesson7 total 1412 drwxrwxr-x 3 sensei sensei 4096 Feb 16 20:57 . drwxr-xr-x 10 sensei sensei 4096 Feb 16 20:30 .. drwxrwxr-x 2 sensei sensei 4096 Feb 16 20:53 .ipynb_checkpoints -rw-rw-r-- 1 sensei sensei 21150 Feb 15 15:58 01-table-dataframe.png -rw-rw-r-- 1 sensei sensei 51 Feb 15 15:58 CSV_ReadingFile.csv -rw-rw-r-- 1 sensei sensei 55 Feb 16 20:59 CSV_WritingFile1.csv -rw-rw-r-- 1 sensei sensei 46 Feb 16 20:59 CSV_WritingFile2.csv -rw-rw-r-- 1 sensei sensei 693687 Feb 15 15:58 ENGR-1330-Lesson8-Dev.html -rw-rw-r-- 1 sensei sensei 166938 Feb 15 15:58 ENGR-1330-Lesson8-Dev.ipynb -rw-rw-r-- 1 sensei sensei 5508 Feb 15 15:58 Excel_ReadingFile.xlsx -rw-rw-r-- 1 sensei sensei 5041 Feb 16 20:59 Excel_WritingFile.xlsx -rw-rw-r-- 1 sensei sensei 363498 Feb 16 20:59 all_quads_gross_evaporation.csv -rw-rw-r-- 1 sensei sensei 108222 Feb 16 20:57 lesson7.ipynb -rw-rw-r-- 1 sensei sensei 40566 Feb 15 15:58 output_126_1.png Now we can read the file contents and check its structure, before proceeding. evapdf = pd.read_csv(\"all_quads_gross_evaporation.csv\",parse_dates=[\"YYYY-MM\"]) # Read the file as a .CSV assign to a dataframe evapdf evapdf.head() # check structure .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } YYYY-MM 104 105 106 107 108 204 205 206 207 ... 911 912 1008 1009 1010 1011 1108 1109 1110 1210 0 1954-01-01 1.80 1.80 2.02 2.24 2.24 2.34 1.89 1.80 1.99 ... 1.42 1.30 2.50 2.42 1.94 1.29 2.59 2.49 2.22 2.27 1 1954-02-01 4.27 4.27 4.13 3.98 3.90 4.18 4.26 4.27 4.26 ... 2.59 2.51 4.71 4.30 3.84 2.50 5.07 4.62 4.05 4.18 2 1954-03-01 4.98 4.98 4.62 4.25 4.20 5.01 4.98 4.98 4.68 ... 3.21 3.21 6.21 6.06 5.02 3.21 6.32 6.20 5.68 5.70 3 1954-04-01 6.09 5.94 5.94 6.07 5.27 6.31 5.98 5.89 5.72 ... 3.83 3.54 6.45 6.25 4.92 3.54 6.59 6.44 5.88 5.95 4 1954-05-01 5.41 5.09 5.14 4.40 3.61 5.57 4.56 4.47 4.18 ... 3.48 3.97 7.92 8.13 6.31 3.99 7.75 7.98 7.40 7.40 5 rows \u00d7 93 columns Structure looks like a spreadsheet as expected; lets plot the time series for cell '911' evapdf.plot.line(x='YYYY-MM',y='911') # Plot quadrant 911 evaporation time series <AxesSubplot:xlabel='YYYY-MM'>","title":"Method 2: Get the actual file from a remote web server (unencrypted)"},{"location":"1-Lessons/Lesson08/lesson7/#method-3-get-the-actual-file-from-an-encrypted-server","text":"This section is saved for future semesters","title":"Method 3: Get the actual file from an encrypted server"},{"location":"1-Lessons/Lesson08/lesson7/#references","text":"Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"References"},{"location":"1-Lessons/Lesson09/lesson8/","text":"ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 18 February 2021 Lesson 8 Visual Display of Data This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, box plot, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib Graphic Standards for Plots Parts of a Plot Building Plots using matplotlib external package Objectives Define the ordinate, abscissa, independent and dependent variables Identify the parts of a proper plot Define how to plot experimental data and theoretical data About matplotlib Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis). Computational thinking (CT) concepts involved are: Decomposition : Break a problem down into smaller pieces; separating plotting from other parts of analysis simplifies maintenace of scripts Abstraction : Pulling out specific differences to make one solution work for multiple problems; wrappers around generic plot calls enhances reuse Algorithms : A list of steps that you can follow to finish a task; Often the last step and most important to make professional graphics to justify the expense (of paying you to do engineering) to the client. Graphics Conventions for Plots Terminology: Ordinate, Abscissa, Dependent and Independent Variables A few terms are used in describing plots: - Abscissa \u2013 the horizontal axis on a plot (the left-right axis) - Ordinate \u2013 the vertical axis on a plot (the up-down axis) A few terms in describing data models - Independent Variable (Explainatory, Predictor, Feature, ...) \u2013 a variable that can be controlled/manipulated in an experiment or theoretical analysis - Dependent Variable (Response, Prediction, ...) \u2013 the variable that measured/observed as a function of the independent variable Plotting convention in most cases assigns explainatory variables to the horizontal axis (e.g. Independent variable is plotted on the Abscissa) and the response variable(s) to the vertical axis (e.g. Dependent Variable is plotted on the Ordinate) Conventions for Proper Plots Include a title OR a caption with a brief description of the plot Label both axes clearly Include the variable name, the variable, and the unit in each label If possible, select increments for both the x and y axes that provide for easy interpolation Include gridlines Show experimental measurements as symbols Show model (theoretical) relationships as lines Use portrait orientation when making your plot Make the plot large enough to be easily read If more than one experimental dataset is plotted Use different shapes for each dataset Use different colors for each dataset Include a legend defining the datasets Background Data are not always numerical. Data can be music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) and other types. Categorical data is a type where you can place individual components into a category: For example visualize a freezer where a business stores ice cream, the product is categorized by flavor, each carton is a component. - The individual components are cartons of ice-cream, and the category is the flavor in the carton Bar Graphs Bar charts (graphs) are useful tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! type(flavors) list myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio <Figure size 720x360 with 0 Axes> # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='magenta', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='green', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now lets deconstruct the script a bit: ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! This part of the code creates a dictionary object, keys are the flavors, values are the carton counts (not the best way, but good for our learning needs). Next we import the python plotting library from matplotlib and name it plt to keep the script a bit easier to read. Next we use the list method to create two lists from the dictionary, flavors and cartons . Keep this in mind plotting is usually done on lists, so we need to prepare the structures properly. The next statement myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio Uses the figure class in pyplot from matplotlib to make a figure object named myfigure, the plot is built into this object. Every call to a method in plt adds content to myfigure until we send the instruction to render the plot ( plt.show() ) The next portion of the script builds the plot: plt.bar(flavors, cartons, color ='orange', width = 0.4) # Build a bar chart, plot series flavor on x-axis, plot series carton on y-axis. Make the bars orange, set bar width (units unspecified) plt.xlabel(\"Flavors\") # Label the x-axis as Flavors plt.ylabel(\"No. of Cartons in Stock\") # Label the x-axis as Flavors plt.title(\"Current Ice Cream in Storage\") # Title for the whole plot This last statement renders the plot to the graphics device (probably localhost in the web browser) plt.show() Now lets add another set of categories to the plot and see what happens ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beastcount = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='gray', width = 0.4) plt.bar(animals, beastcount, color ='brown', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now suppose we want horizontal bars we can search pyplot for such a thing. If one types horizontal bar chart into the pyplot search engine there is a link that leads to: Which has the right look! If we examine the script there is a method called barh so lets try that. ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beasts = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.barh(flavors, cartons, color ='orange') plt.barh(animals, beasts, color ='green') plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.barh(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:ylabel='Flavor'> import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show() Line Charts A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application. Example Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() time = [0,1.0,4.0,5.0,6.0,2.0,3.0] speed = [0,3,20,30,45.6,7,12] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='green', marker='o',linewidth=1) # basic line plot plt.show() # Estimate acceleration (naive) dvdt = (max(speed) - min(speed))/(max(time)-min(time)) plottitle = 'Average acceleration %.3f' % (dvdt) + ' m/sec/sec' seriesnames = ['Data','Model'] modely = [min(speed),max(speed)] modelx = [min(time),max(time)] mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=0) # basic line plot plt.plot(modelx, modely, c='blue',linewidth=1) # basic line plot plt.xlabel('Time (sec)') plt.ylabel('Speed (m/sec)') plt.legend(seriesnames) plt.title(plottitle) plt.show() Scatter Plots A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot # Example 1. A data file containing heights of fathers, mothers, and sons is to be examined df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists daddy = df['father'] ; mommy = df['mother'] ; baby = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red' , label='Father') # one plot series plt.scatter(baby, mommy, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\") df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> Histograms Quilting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson. To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\" import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Gross Gross (Adjusted) Year count 2.000000e+02 2.000000e+02 200.000000 mean 2.216196e+08 5.041983e+08 1986.620000 std 1.441574e+08 2.159814e+08 20.493548 min 9.183673e+06 3.222619e+08 1921.000000 25% 1.087824e+08 3.677804e+08 1973.000000 50% 2.001273e+08 4.388570e+08 1990.000000 75% 3.069535e+08 5.512131e+08 2003.250000 max 9.067234e+08 1.757788e+09 2015.000000 References Constructing Horizontal Bar Charts (matplotlib.org documentation) https://matplotlib.org/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html?highlight=horizontal%20bar%20chart How to make a bar chart https://www.geeksforgeeks.org/bar-plot-in-matplotlib/","title":"Data Display using MATPLOTLIB"},{"location":"1-Lessons/Lesson09/lesson8/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 18 February 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson09/lesson8/#lesson-8-visual-display-of-data","text":"This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, box plot, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib Graphic Standards for Plots Parts of a Plot Building Plots using matplotlib external package","title":"Lesson 8 Visual Display of Data"},{"location":"1-Lessons/Lesson09/lesson8/#objectives","text":"Define the ordinate, abscissa, independent and dependent variables Identify the parts of a proper plot Define how to plot experimental data and theoretical data","title":"Objectives"},{"location":"1-Lessons/Lesson09/lesson8/#about-matplotlib","text":"Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis). Computational thinking (CT) concepts involved are: Decomposition : Break a problem down into smaller pieces; separating plotting from other parts of analysis simplifies maintenace of scripts Abstraction : Pulling out specific differences to make one solution work for multiple problems; wrappers around generic plot calls enhances reuse Algorithms : A list of steps that you can follow to finish a task; Often the last step and most important to make professional graphics to justify the expense (of paying you to do engineering) to the client.","title":"About matplotlib"},{"location":"1-Lessons/Lesson09/lesson8/#graphics-conventions-for-plots","text":"","title":"Graphics Conventions for Plots"},{"location":"1-Lessons/Lesson09/lesson8/#terminology-ordinate-abscissa-dependent-and-independent-variables","text":"A few terms are used in describing plots: - Abscissa \u2013 the horizontal axis on a plot (the left-right axis) - Ordinate \u2013 the vertical axis on a plot (the up-down axis) A few terms in describing data models - Independent Variable (Explainatory, Predictor, Feature, ...) \u2013 a variable that can be controlled/manipulated in an experiment or theoretical analysis - Dependent Variable (Response, Prediction, ...) \u2013 the variable that measured/observed as a function of the independent variable Plotting convention in most cases assigns explainatory variables to the horizontal axis (e.g. Independent variable is plotted on the Abscissa) and the response variable(s) to the vertical axis (e.g. Dependent Variable is plotted on the Ordinate)","title":"Terminology: Ordinate, Abscissa, Dependent and Independent Variables"},{"location":"1-Lessons/Lesson09/lesson8/#conventions-for-proper-plots","text":"Include a title OR a caption with a brief description of the plot Label both axes clearly Include the variable name, the variable, and the unit in each label If possible, select increments for both the x and y axes that provide for easy interpolation Include gridlines Show experimental measurements as symbols Show model (theoretical) relationships as lines Use portrait orientation when making your plot Make the plot large enough to be easily read If more than one experimental dataset is plotted Use different shapes for each dataset Use different colors for each dataset Include a legend defining the datasets","title":"Conventions for Proper Plots"},{"location":"1-Lessons/Lesson09/lesson8/#background","text":"Data are not always numerical. Data can be music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) and other types. Categorical data is a type where you can place individual components into a category: For example visualize a freezer where a business stores ice cream, the product is categorized by flavor, each carton is a component. - The individual components are cartons of ice-cream, and the category is the flavor in the carton","title":"Background"},{"location":"1-Lessons/Lesson09/lesson8/#bar-graphs","text":"Bar charts (graphs) are useful tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! type(flavors) list myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio <Figure size 720x360 with 0 Axes> # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='magenta', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='green', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now lets deconstruct the script a bit: ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! This part of the code creates a dictionary object, keys are the flavors, values are the carton counts (not the best way, but good for our learning needs). Next we import the python plotting library from matplotlib and name it plt to keep the script a bit easier to read. Next we use the list method to create two lists from the dictionary, flavors and cartons . Keep this in mind plotting is usually done on lists, so we need to prepare the structures properly. The next statement myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio Uses the figure class in pyplot from matplotlib to make a figure object named myfigure, the plot is built into this object. Every call to a method in plt adds content to myfigure until we send the instruction to render the plot ( plt.show() ) The next portion of the script builds the plot: plt.bar(flavors, cartons, color ='orange', width = 0.4) # Build a bar chart, plot series flavor on x-axis, plot series carton on y-axis. Make the bars orange, set bar width (units unspecified) plt.xlabel(\"Flavors\") # Label the x-axis as Flavors plt.ylabel(\"No. of Cartons in Stock\") # Label the x-axis as Flavors plt.title(\"Current Ice Cream in Storage\") # Title for the whole plot This last statement renders the plot to the graphics device (probably localhost in the web browser) plt.show() Now lets add another set of categories to the plot and see what happens ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beastcount = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='gray', width = 0.4) plt.bar(animals, beastcount, color ='brown', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now suppose we want horizontal bars we can search pyplot for such a thing. If one types horizontal bar chart into the pyplot search engine there is a link that leads to: Which has the right look! If we examine the script there is a method called barh so lets try that. ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beasts = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.barh(flavors, cartons, color ='orange') plt.barh(animals, beasts, color ='green') plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.barh(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:ylabel='Flavor'> import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show()","title":"Bar Graphs"},{"location":"1-Lessons/Lesson09/lesson8/#line-charts","text":"A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application. Example Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() time = [0,1.0,4.0,5.0,6.0,2.0,3.0] speed = [0,3,20,30,45.6,7,12] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='green', marker='o',linewidth=1) # basic line plot plt.show() # Estimate acceleration (naive) dvdt = (max(speed) - min(speed))/(max(time)-min(time)) plottitle = 'Average acceleration %.3f' % (dvdt) + ' m/sec/sec' seriesnames = ['Data','Model'] modely = [min(speed),max(speed)] modelx = [min(time),max(time)] mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=0) # basic line plot plt.plot(modelx, modely, c='blue',linewidth=1) # basic line plot plt.xlabel('Time (sec)') plt.ylabel('Speed (m/sec)') plt.legend(seriesnames) plt.title(plottitle) plt.show()","title":"Line Charts"},{"location":"1-Lessons/Lesson09/lesson8/#scatter-plots","text":"A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot # Example 1. A data file containing heights of fathers, mothers, and sons is to be examined df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists daddy = df['father'] ; mommy = df['mother'] ; baby = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red' , label='Father') # one plot series plt.scatter(baby, mommy, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\") df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'>","title":"Scatter Plots"},{"location":"1-Lessons/Lesson09/lesson8/#histograms","text":"Quilting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson. To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\" import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Gross Gross (Adjusted) Year count 2.000000e+02 2.000000e+02 200.000000 mean 2.216196e+08 5.041983e+08 1986.620000 std 1.441574e+08 2.159814e+08 20.493548 min 9.183673e+06 3.222619e+08 1921.000000 25% 1.087824e+08 3.677804e+08 1973.000000 50% 2.001273e+08 4.388570e+08 1990.000000 75% 3.069535e+08 5.512131e+08 2003.250000 max 9.067234e+08 1.757788e+09 2015.000000","title":"Histograms"},{"location":"1-Lessons/Lesson09/lesson8/#references","text":"Constructing Horizontal Bar Charts (matplotlib.org documentation) https://matplotlib.org/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html?highlight=horizontal%20bar%20chart How to make a bar chart https://www.geeksforgeeks.org/bar-plot-in-matplotlib/","title":"References"},{"location":"1-Lessons/Lesson10/lesson9/","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 18 February 2021 Lesson 9 Data Modeling: Statistical Approach This lesson covers concepts related to modeling data - it is the start of several lessons on the subject. The ultimate goal is to explain observed behavior with a model (like \\textbf{F} = m\\textbf{a} ) so that the model can be used to predict behavior. If we are predicting between existing observations, that's interpolation and is relatively straightforward. If we are predicting beyond existing observations, that's called extrapolation and is less straightforward. To get started we will examine the concepts of causality (cause => effect) and correlation, and the use of simulation to generate probability estimates. Objectives To understand the fundamental concepts involved in causality; and the difference between cause and correlation. To understand the fundamental concepts involved in iteration. To understand the fundamental concepts involved in simulation Computational Thinking Concepts The CT concepts include: Algorithm Design => Causality, Iteration, Simulation System Integration => Iteration, Simulation Correlation and Causality What is causality? (A long winded psuedo definition!) Causality is the relationship between causes and effects. The notion of causality does not have a uniform definition in the sciences, and is studied using philosophy and statistics. From the perspective of physics, it is generally believed that causality cannot occur between an effect and an event that is not in the back (past) light cone of said effect. Similarly, a cause could not have an effect outside its front (future) light cone. Here are some recent articles regarding Closed Time Loops, that explains causal consistency. The second paper is by an undergraduate student! https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.125.040605 https://iopscience.iop.org/article/10.1088/1361-6382/aba4bc Both to some extent theoretically support our popular notion of time travel (aka Dr. Who) without pesky paradoxes; someone with creative writing juices, could have a good science fiction career using these papers as a starting thesis! In classical physics, an effect cannot occur before its cause. In Einstein's theory of special relativity, causality means that an effect can not occur from a cause that is not in the back (past) light cone of that event. Similarly, a cause cannot have an effect outside its front (future) light cone. These restrictions are consistent with the assumption that causal influences cannot travel faster than the speed of light and/or backwards in time. In quantum field theory, observables of events with a spacelike relationship, \"elsewhere\", have to commute, so the order of observations or measurements of such observables do not impact each other. Causality in this context should not be confused with Newton's second law, which is related to the conservation of momentum, and is a consequence of the spatial homogeneity of physical laws. The word causality in this context means that all effects must have specific causes. Another requirement, at least valid at the level of human experience, is that cause and effect be mediated across space and time (requirement of contiguity). This requirement has been very influential in the past, in the first place as a result of direct observation of causal processes (like pushing a cart), in the second place as a problematic aspect of Newton's theory of gravitation (attraction of the earth by the sun by means of action at a distance) replacing mechanistic proposals like Descartes' vortex theory; in the third place as an incentive to develop dynamic field theories (e.g., Maxwell's electrodynamics and Einstein's general theory of relativity) restoring contiguity in the transmission of influences in a more successful way than in Descartes' theory. Yada yada bla bla bla ... Correlation (Causality's mimic!) The literary (as in writing!) formulation of causality is a \"why?, because ...\" structure (sort of like if=>then) The answer to a because question, should be the \"cause.\" Many authors use \"since\" to imply cause, but it is incorrect grammar - since answers the question of when? Think \"CAUSE\" => \"EFFECT\" Correlation doesn\u2019t mean cause (although it is a really good predictor of the crap we all buy - its why Amazon is sucessfull) Consider the chart below The correlation between money spent on pets and the number of lawyers is quite good (nearly perfect), so does having pets cause lawyers? Of course not, the general social economic conditions that improve general wealth, and create sufficient disposable income to have pets (here we mean companion animals, not food on the hoof) also creates conditions for laywers to proliferate, hence a good correlation. Nice video : Correlation and Causation https://www.youtube.com/watch?v=1Sa2v7kVEc0 Quoting from http://water.usgs.gov/pubs/twri/twri4a3/ Concentrations of atrazine and nitrate in shallow groundwaters are measured in wells over a several county area. For each sample, the concentration of one is plotted versus the concentration of the other. As atrazine concentrations increase, so do nitrate. How might the strength of this association be measured and summarized? Streams draining the Sierra Nevada mountains in California usually receive less precipitation in November than in other months. Has the amount of November precipitation significantly changed over the last 70 years, showing a gradual change in the climate of the area? How might this be tested? The above situations require a measure of the strength of association between two continuous variables, such as between two chemical concentrations, or between amount of precipitation and time. How do they co-vary? One class of measures are called correlation coefficients. Also important is how the significance of that association can be tested for, to determine whether the observed pattern differs from what is expected due entirely to chance. Whenever a correlation coefficient is calculated, the data should be plotted on a scatterplot. No single numerical measure can substitute for the visual insight gained from a plot. Many different patterns can produce the same correlation coefficient, and similar strengths of relationships can produce differing coefficients, depending on the curvature of the relationship. Implications Most research questions attempt to explain cause and effect. - In experimental research, the relationship is constructed and the experiment is somewhat of a failure if none of the presumed causal (causal == explainatory) variables influence the response (response == effect) - In a data science experimental context, causality may be impossible to establish, however correlations can be established and exploited. In data science, many studies involve observations on a group of individuals, a factor of interest called a treatment (explainatory variable, predictor variable, predictor feature ...), and an outcome (response, effect, state, predicted value ...) measured on each individual. The presumptive establishment of causality takes place in two stages. First, an association is observed. Any relation between the treatment and the outcome is called an association (we can measure the strength of the association using correlation coefficients!). Second, A more careful analysis is used to establish causality. a. One approach would be to control all variables other than the suspected (explainatory) variables, which for any meaningful process is essentially impossible. b. Another approach is to establish randomized control studies: 1. Start with a sample from a population (e.g. volunteers to test Covid 19 vaccines) 2. Randomly assign members to either a. Control group b. Treatment group 3. Expose the two groups identically, except the control group recieves a false (null) treatment 4. Compare the responses of the two groups, if they are same, there exists no evidence that the treatment variable CAUSES a response These concepts can be extended with some ingenuity to engineered systems and natural systems. Consider Data Science Questions: - Does going to school cause flu? - Does flu cause school attendance? - Does going to school contribute to the spread of flu? - Does the spread of flu contribute to the school attendance? - Are there other variables that affects both? a. These are called \u201cconfounding factors\u201d or \u201clurking variables\u201d. b. Cold weather?, more indoor time?, more interaction? Confounding Factors An underlying difference between the two groups (other than the treatment) is called a confounding factor, because it might confound you (that is, mess you up) when you try to reach a conclusion. For example, Cold weather in the previous example. Confounding also occurs when explainatory variables are correlated to another, for instance flood flows are well correlated to drainage area, main channel length, mean annual precipitation, main channel slope, and elevation. However main channel length is itself strongly correlated to drainage area, so much so as to be nearly useless as an explainatory variable when drainage area is retained in a data model. It would be a \"confounding variable\" in this context. Randomization To establish presumptive causality in our data science experiments, we need randomization tools. We can use Python to make psuedo-random choices. There are built-in functions in numpy library under random submodule. The choice function randomly picks one item from an array. The syntax is np.random.choice(array_name) , where array_name is the name of the array from which to make the choice.\u200b #Making Random Choice from an Array (or list) import numpy as np two_groups = np.array(['treatment', 'control']) np.random.choice(two_groups,1) # mylist = ['treatment', 'control'] # this works too # np.random.choice(mylist) array(['treatment'], dtype='<U9') The difference of this function from others that we learned so far, is that it doesn\u2019t give the same result every time. We can roll a dice using this function by randomly selecting from an array from 1 to 6. my_die = np.array(['one', 'two','three', 'four','five', 'six']) np.random.choice(my_die) 'six' # now a bunch of rolls print('roll #1 ',np.random.choice(my_die) ) print('roll #2 ',np.random.choice(my_die) ) print('roll #3 ',np.random.choice(my_die) ) print('roll #4 ',np.random.choice(my_die) ) print('roll #5 ',np.random.choice(my_die) ) print('roll #6 ',np.random.choice(my_die) ) roll #1 four roll #2 four roll #3 four roll #4 five roll #5 six roll #6 one # or multiple rolls, single call myDiceRolls = np.random.choice(my_die,6) print(myDiceRolls) ['six' 'two' 'two' 'six' 'one' 'five'] 'six' We might need to repeat a process multiple times to reach better results or cover more results. Let\u2019s create a game with following rules: If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. my_wallet = 1 # start with 1 dollars def place_a_bet(wallet): print(\"Place your bet!\") if wallet == 0: print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Single play print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) print(\"Amount in my account =:\",my_wallet) Amount in my account =: 1 Place your bet! Roll the die! You Lose, Bummer! Amount in my account =: 0 A more automated solution is to use a for statement to loop over the contents of a sequence. Each result is called iteration. Here we use a for statement in a more realistic way: we print the results of betting five times on the die as described earlier. This process is called simulating the results of five bets. We use the word simulating to remind ourselves that we are not physically rolling dice and exchanging money but using Python to mimic the process. # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' my_wallet = 10 how_many_throws = 1 for i in range(how_many_throws): print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) print(\"After \",i+1,\" plays\") print(CRED + \"Amount in my account =:\",my_wallet,CEND) print(\"_______________________\") Amount in my account =: 10 Place your bet! Roll the die! You win a dollar! After 1 plays \u001b[91mAmount in my account =: 11 \u001b[0m _______________________ Simulation of multiple gamblers/multiple visits to the Casino https://www.inferentialthinking.com/chapters/09/3/Simulation.html outcomes = np.array([]) #null array to store outcomes # redefine functions to suppress output def place_a_bet(wallet): # print(\"Place your bet!\") if wallet == 0: # print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" # print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: #print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: #print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: #print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' how_many_simulations = 100000 for j in range(how_many_simulations): my_wallet = 1 how_many_throws = 30 for i in range(how_many_throws): # print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) # print(\"After \",i+1,\" plays\") # print(CRED + \"Amount in my account =:\",my_wallet,CEND) # print(\"_______________________\") outcomes = np.append(outcomes,my_wallet) # build a histogram chart - outcomes is an array import matplotlib.pyplot as plt from scipy.stats import gamma #ax.hist(r, density=True, histtype='stepfilled', alpha=0.2) plt.hist(outcomes, density=True, bins = 20) plt.xlabel(\"Dollars in Gamer's Wallet\") plt.ylabel('Relative Frequency') #### just a data model, gamma distribution ############## # code below adapted from https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html a = 5 # bit of trial and error x = np.linspace(gamma.ppf(0.001, a),gamma.ppf(0.999, a), 1000) plt.plot(x, gamma.pdf(x, a, loc=-1.25, scale=1),'r-', lw=5, alpha=1.0, label='gamma pdf') ######################################################### # Render the plot plt.show() #print(\"Expected value of wallet (mean) =: \",outcomes.mean()) import pandas as pd df = pd.DataFrame(outcomes) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 count 100000.000000 mean 1.632990 std 1.651133 min 0.000000 25% 0.000000 50% 1.000000 75% 2.000000 max 14.000000 Simulation Simulation is the process of using a computer to mimic a real experiment or process. In this class, those experiments will almost invariably involve chance. To summarize from: https://www.inferentialthinking.com/chapters/09/3/Simulation.html Step 1: What to Simulate: Specify the quantity you want to simulate. For example, you might decide that you want to simulate the outcomes of tosses of a coin. Step 2: Simulating One Value: Figure out how to simulate one value of the quantity you specified in Step 1. (usually turn into a function for readability) Step 3: Number of Repetitions: Decide how many times you want to simulate the quantity. You will have to repeat Step 2 that many times. Step 4: Coding the Simulation: Put it all together in code. Step 5: Interpret the results (plots, Simulation Example Should I change my choice? Based on Monty Hall example from https://youtu.be/Xp6V_lO1ZKA But we already have a small car! (Also watch https://www.youtube.com/watch?v=6Ewq_ytHA7g to learn significance of the small car!) Consider The gist of the game is that a contestent chooses a door, the host reveals one of the unselected doors and offers the contestant a chance to change their choice. Should the contestant stick with her initial choice, or switch to the other door? That is the Monty Hall problem. Using classical probability theory it is straightforward to show that: The chance that the car is behind the originally chosen door is 1/3. After Monty opens the door with the goat, the chance distribution changes. If the contestant switches the decision, he/she doubles the chance. Suppose we have harder situations, can we use this simple problem to learn how to ask complex questions? import numpy as np import pandas as pd import matplotlib.pyplot as plt def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 1', 'Goat 2', 'Car'] Goat 1 Goat 2 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining how_many_games = 10000 for i in np.arange(how_many_games): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Goat 2 Goat 1 Car 1 Goat 1 Goat 2 Car 2 Goat 1 Goat 2 Car 3 Goat 2 Goat 1 Car 4 Goat 2 Goat 1 Car ... ... ... ... 9995 Car Goat 2 Goat 1 9996 Car Goat 1 Goat 2 9997 Car Goat 2 Goat 1 9998 Car Goat 2 Goat 1 9999 Goat 1 Goat 2 Car 10000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show() Interpret Results According to the plot, it is beneficial for the players to switch doors because the initial chance for being correct is only 1/3 Does changing doors have a CAUSAL effect on outcome? ## Various Examples Defect Chances A sample of four electronic components is taken from the output of a production line. The probabilities of the various outcomes are calculated to be: Pr [0 defectives] = 0.6561, Pr [1 defective] = 0.2916, Pr [2 defectives] = 0.0486, Pr [3 defectives] = 0.0036, Pr [4 defectives] = 0.0001. What is the probability of at least one defective? #Method-1 pr_atleast1 = 1-0.6561 print(pr_atleast1) 0.3439 #Method-2 pr_atleast1 = 0.2916+0.0483+0.0036+0.0001 print(pr_atleast1) 0.3436 Common is a Birthday? A class of engineering students consists of 45 people. What is the probability that no two students have birthdays on the same day, not considering the year of birth? To simplify the calculation, assume that there are 365 days in the year and that births are equally likely on all of them. Then what is the probability that some members of the class have birthdays on the same day? Also, vary the number of students in the class from 2 to 200 to see its effect on the probability values. #A student in the class states his birthday. So the probability that he/she has the birthday on that date is 1 pr_first = 1 print(pr_first) 1 #Probability that the second student has different birthday than the first student is 364/365 pr_second = 364/365 print(pr_second) 0.9972602739726028 #Probability that the third student has different birthday than the first and the second students is 363/365 pr_third = 363/365 print(pr_third) 0.9945205479452055 #Probability that the fourth student has different birthday than the first, the second, and the third students is 362/365 pr_fourth = 362/365 print(pr_fourth) 0.9917808219178083 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-1: Looping over a list student_ids = list(range(2,46,1)) pr_nosame = 1 for i in student_ids: pr_nosame = pr_nosame*((365-i+1)/365) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) 0.05902410053422507 0.940975899465775 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop student_ids = np.arange(2,46,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame = np.prod(pr_eachstudent) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-19-e397c0f6a5ec> in <module> 7 #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop 8 ----> 9 student_ids = np.arange(2,46,1) 10 11 pr_eachstudent = ((365-student_ids+1)/365) NameError: name 'np' is not defined #Simulation: Getting the probability for different numbers of total students in the class total_students = np.arange(2,201,1) pr_nosame = [] pr_same = [] for i in total_students: student_ids = np.arange(2,i,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame_total = np.prod(pr_eachstudent) pr_nosame.append(pr_nosame_total) pr_same.append(1 - pr_nosame_total) #Creating a dataframe with columns - number of students and probability import pandas as pd final_data = {'Number of students': total_students, 'Probability': pr_same} df = pd.DataFrame(final_data) print(df) #Creating a scatter plot between number of students and probability that at least a pair of students have the same birthday import matplotlib.pyplot as plt plt.scatter(total_students, pr_same, color = 'blue') plt.xlabel('No. of students in the class') plt.ylabel('P [same birthday]') plt.title('Effect of sample size on the chance of success') Making Hole (and money!) An oil company is bidding for the rights to drill a well in field A and a well in field B. The probability it will drill a well in field A is 40%. If it does, the probability the well will be successful is 45%. The probability it will drill a well in field B is 30%. If it does, the probability the well will be successful is 55%. Calculate each of the following probabilities: a) What is the probability of a successful well in field A? pr_successA = 0.40*0.45 pr_successA 0.18000000000000002 b) What is the probability of a successful well in field B? pr_successB = 0.30*0.55 pr_successB 0.165 c) What is the probability of both a successful well in field A and a successful well in field B? pr_successAB = pr_successA*pr_successB pr_successAB 0.029700000000000004 d) What is the probability of at least one successful well in the two fields together? pr_onesuccess = pr_successA + pr_successB - pr_successAB pr_onesuccess 0.3153 e) What is the probability of no successful well in field A? pr_nosuccessA = (1-0.4)+(0.4*0.55) pr_nosuccessA 0.8200000000000001 f) What is the probability of no successful well in field B? pr_nosuccessB = (1-0.3)+(0.3*0.45) pr_nosuccessB 0.835 g) What is the probability of no successful well in the two fields together? pr_nosuccessAB = 1 - pr_onesuccess pr_nosuccessAB 0.6847 h) What is the probability of exactly one successful well in the two fields together? pr_exactonesuccess = (0.18*0.835)+(0.165*0.82) pr_exactonesuccess 0.28559999999999997 References Ford, Martin. 2009 The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future (p. 107). Acculant Publishing. Kindle Edition. Computational and Inferential Thinking: The Foundations of Data Science. By Ani Adhikari and John DeNero, with Contributions by David Wagner and Henry Milner. Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0). https://www.inferentialthinking.com/chapters/09/Randomness.html # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) ! pwd atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) /home/sensei/1330-textbook-webroot/docs/lesson9","title":"Lesson9"},{"location":"1-Lessons/Lesson10/lesson9/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 18 February 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson10/lesson9/#lesson-9-data-modeling-statistical-approach","text":"This lesson covers concepts related to modeling data - it is the start of several lessons on the subject. The ultimate goal is to explain observed behavior with a model (like \\textbf{F} = m\\textbf{a} ) so that the model can be used to predict behavior. If we are predicting between existing observations, that's interpolation and is relatively straightforward. If we are predicting beyond existing observations, that's called extrapolation and is less straightforward. To get started we will examine the concepts of causality (cause => effect) and correlation, and the use of simulation to generate probability estimates.","title":"Lesson 9 Data Modeling: Statistical Approach"},{"location":"1-Lessons/Lesson10/lesson9/#objectives","text":"To understand the fundamental concepts involved in causality; and the difference between cause and correlation. To understand the fundamental concepts involved in iteration. To understand the fundamental concepts involved in simulation","title":"Objectives"},{"location":"1-Lessons/Lesson10/lesson9/#computational-thinking-concepts","text":"The CT concepts include: Algorithm Design => Causality, Iteration, Simulation System Integration => Iteration, Simulation","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson10/lesson9/#correlation-and-causality","text":"","title":"Correlation and Causality"},{"location":"1-Lessons/Lesson10/lesson9/#what-is-causality-a-long-winded-psuedo-definition","text":"Causality is the relationship between causes and effects. The notion of causality does not have a uniform definition in the sciences, and is studied using philosophy and statistics. From the perspective of physics, it is generally believed that causality cannot occur between an effect and an event that is not in the back (past) light cone of said effect. Similarly, a cause could not have an effect outside its front (future) light cone. Here are some recent articles regarding Closed Time Loops, that explains causal consistency. The second paper is by an undergraduate student! https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.125.040605 https://iopscience.iop.org/article/10.1088/1361-6382/aba4bc Both to some extent theoretically support our popular notion of time travel (aka Dr. Who) without pesky paradoxes; someone with creative writing juices, could have a good science fiction career using these papers as a starting thesis! In classical physics, an effect cannot occur before its cause. In Einstein's theory of special relativity, causality means that an effect can not occur from a cause that is not in the back (past) light cone of that event. Similarly, a cause cannot have an effect outside its front (future) light cone. These restrictions are consistent with the assumption that causal influences cannot travel faster than the speed of light and/or backwards in time. In quantum field theory, observables of events with a spacelike relationship, \"elsewhere\", have to commute, so the order of observations or measurements of such observables do not impact each other. Causality in this context should not be confused with Newton's second law, which is related to the conservation of momentum, and is a consequence of the spatial homogeneity of physical laws. The word causality in this context means that all effects must have specific causes. Another requirement, at least valid at the level of human experience, is that cause and effect be mediated across space and time (requirement of contiguity). This requirement has been very influential in the past, in the first place as a result of direct observation of causal processes (like pushing a cart), in the second place as a problematic aspect of Newton's theory of gravitation (attraction of the earth by the sun by means of action at a distance) replacing mechanistic proposals like Descartes' vortex theory; in the third place as an incentive to develop dynamic field theories (e.g., Maxwell's electrodynamics and Einstein's general theory of relativity) restoring contiguity in the transmission of influences in a more successful way than in Descartes' theory. Yada yada bla bla bla ...","title":"What is causality? (A long winded psuedo definition!)"},{"location":"1-Lessons/Lesson10/lesson9/#correlation-causalitys-mimic","text":"The literary (as in writing!) formulation of causality is a \"why?, because ...\" structure (sort of like if=>then) The answer to a because question, should be the \"cause.\" Many authors use \"since\" to imply cause, but it is incorrect grammar - since answers the question of when? Think \"CAUSE\" => \"EFFECT\" Correlation doesn\u2019t mean cause (although it is a really good predictor of the crap we all buy - its why Amazon is sucessfull) Consider the chart below The correlation between money spent on pets and the number of lawyers is quite good (nearly perfect), so does having pets cause lawyers? Of course not, the general social economic conditions that improve general wealth, and create sufficient disposable income to have pets (here we mean companion animals, not food on the hoof) also creates conditions for laywers to proliferate, hence a good correlation. Nice video : Correlation and Causation https://www.youtube.com/watch?v=1Sa2v7kVEc0 Quoting from http://water.usgs.gov/pubs/twri/twri4a3/ Concentrations of atrazine and nitrate in shallow groundwaters are measured in wells over a several county area. For each sample, the concentration of one is plotted versus the concentration of the other. As atrazine concentrations increase, so do nitrate. How might the strength of this association be measured and summarized? Streams draining the Sierra Nevada mountains in California usually receive less precipitation in November than in other months. Has the amount of November precipitation significantly changed over the last 70 years, showing a gradual change in the climate of the area? How might this be tested? The above situations require a measure of the strength of association between two continuous variables, such as between two chemical concentrations, or between amount of precipitation and time. How do they co-vary? One class of measures are called correlation coefficients. Also important is how the significance of that association can be tested for, to determine whether the observed pattern differs from what is expected due entirely to chance. Whenever a correlation coefficient is calculated, the data should be plotted on a scatterplot. No single numerical measure can substitute for the visual insight gained from a plot. Many different patterns can produce the same correlation coefficient, and similar strengths of relationships can produce differing coefficients, depending on the curvature of the relationship.","title":"Correlation (Causality's mimic!)"},{"location":"1-Lessons/Lesson10/lesson9/#implications","text":"Most research questions attempt to explain cause and effect. - In experimental research, the relationship is constructed and the experiment is somewhat of a failure if none of the presumed causal (causal == explainatory) variables influence the response (response == effect) - In a data science experimental context, causality may be impossible to establish, however correlations can be established and exploited. In data science, many studies involve observations on a group of individuals, a factor of interest called a treatment (explainatory variable, predictor variable, predictor feature ...), and an outcome (response, effect, state, predicted value ...) measured on each individual. The presumptive establishment of causality takes place in two stages. First, an association is observed. Any relation between the treatment and the outcome is called an association (we can measure the strength of the association using correlation coefficients!). Second, A more careful analysis is used to establish causality. a. One approach would be to control all variables other than the suspected (explainatory) variables, which for any meaningful process is essentially impossible. b. Another approach is to establish randomized control studies: 1. Start with a sample from a population (e.g. volunteers to test Covid 19 vaccines) 2. Randomly assign members to either a. Control group b. Treatment group 3. Expose the two groups identically, except the control group recieves a false (null) treatment 4. Compare the responses of the two groups, if they are same, there exists no evidence that the treatment variable CAUSES a response These concepts can be extended with some ingenuity to engineered systems and natural systems. Consider Data Science Questions: - Does going to school cause flu? - Does flu cause school attendance? - Does going to school contribute to the spread of flu? - Does the spread of flu contribute to the school attendance? - Are there other variables that affects both? a. These are called \u201cconfounding factors\u201d or \u201clurking variables\u201d. b. Cold weather?, more indoor time?, more interaction?","title":"Implications"},{"location":"1-Lessons/Lesson10/lesson9/#confounding-factors","text":"An underlying difference between the two groups (other than the treatment) is called a confounding factor, because it might confound you (that is, mess you up) when you try to reach a conclusion. For example, Cold weather in the previous example. Confounding also occurs when explainatory variables are correlated to another, for instance flood flows are well correlated to drainage area, main channel length, mean annual precipitation, main channel slope, and elevation. However main channel length is itself strongly correlated to drainage area, so much so as to be nearly useless as an explainatory variable when drainage area is retained in a data model. It would be a \"confounding variable\" in this context.","title":"Confounding Factors"},{"location":"1-Lessons/Lesson10/lesson9/#randomization","text":"To establish presumptive causality in our data science experiments, we need randomization tools. We can use Python to make psuedo-random choices. There are built-in functions in numpy library under random submodule. The choice function randomly picks one item from an array. The syntax is np.random.choice(array_name) , where array_name is the name of the array from which to make the choice.\u200b #Making Random Choice from an Array (or list) import numpy as np two_groups = np.array(['treatment', 'control']) np.random.choice(two_groups,1) # mylist = ['treatment', 'control'] # this works too # np.random.choice(mylist) array(['treatment'], dtype='<U9') The difference of this function from others that we learned so far, is that it doesn\u2019t give the same result every time. We can roll a dice using this function by randomly selecting from an array from 1 to 6. my_die = np.array(['one', 'two','three', 'four','five', 'six']) np.random.choice(my_die) 'six' # now a bunch of rolls print('roll #1 ',np.random.choice(my_die) ) print('roll #2 ',np.random.choice(my_die) ) print('roll #3 ',np.random.choice(my_die) ) print('roll #4 ',np.random.choice(my_die) ) print('roll #5 ',np.random.choice(my_die) ) print('roll #6 ',np.random.choice(my_die) ) roll #1 four roll #2 four roll #3 four roll #4 five roll #5 six roll #6 one # or multiple rolls, single call myDiceRolls = np.random.choice(my_die,6) print(myDiceRolls) ['six' 'two' 'two' 'six' 'one' 'five'] 'six' We might need to repeat a process multiple times to reach better results or cover more results. Let\u2019s create a game with following rules: If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. my_wallet = 1 # start with 1 dollars def place_a_bet(wallet): print(\"Place your bet!\") if wallet == 0: print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Single play print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) print(\"Amount in my account =:\",my_wallet) Amount in my account =: 1 Place your bet! Roll the die! You Lose, Bummer! Amount in my account =: 0 A more automated solution is to use a for statement to loop over the contents of a sequence. Each result is called iteration. Here we use a for statement in a more realistic way: we print the results of betting five times on the die as described earlier. This process is called simulating the results of five bets. We use the word simulating to remind ourselves that we are not physically rolling dice and exchanging money but using Python to mimic the process. # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' my_wallet = 10 how_many_throws = 1 for i in range(how_many_throws): print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) print(\"After \",i+1,\" plays\") print(CRED + \"Amount in my account =:\",my_wallet,CEND) print(\"_______________________\") Amount in my account =: 10 Place your bet! Roll the die! You win a dollar! After 1 plays \u001b[91mAmount in my account =: 11 \u001b[0m _______________________","title":"Randomization"},{"location":"1-Lessons/Lesson10/lesson9/#simulation-of-multiple-gamblersmultiple-visits-to-the-casino","text":"https://www.inferentialthinking.com/chapters/09/3/Simulation.html outcomes = np.array([]) #null array to store outcomes # redefine functions to suppress output def place_a_bet(wallet): # print(\"Place your bet!\") if wallet == 0: # print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" # print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: #print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: #print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: #print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' how_many_simulations = 100000 for j in range(how_many_simulations): my_wallet = 1 how_many_throws = 30 for i in range(how_many_throws): # print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) # print(\"After \",i+1,\" plays\") # print(CRED + \"Amount in my account =:\",my_wallet,CEND) # print(\"_______________________\") outcomes = np.append(outcomes,my_wallet) # build a histogram chart - outcomes is an array import matplotlib.pyplot as plt from scipy.stats import gamma #ax.hist(r, density=True, histtype='stepfilled', alpha=0.2) plt.hist(outcomes, density=True, bins = 20) plt.xlabel(\"Dollars in Gamer's Wallet\") plt.ylabel('Relative Frequency') #### just a data model, gamma distribution ############## # code below adapted from https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html a = 5 # bit of trial and error x = np.linspace(gamma.ppf(0.001, a),gamma.ppf(0.999, a), 1000) plt.plot(x, gamma.pdf(x, a, loc=-1.25, scale=1),'r-', lw=5, alpha=1.0, label='gamma pdf') ######################################################### # Render the plot plt.show() #print(\"Expected value of wallet (mean) =: \",outcomes.mean()) import pandas as pd df = pd.DataFrame(outcomes) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 count 100000.000000 mean 1.632990 std 1.651133 min 0.000000 25% 0.000000 50% 1.000000 75% 2.000000 max 14.000000","title":"Simulation of multiple gamblers/multiple visits to the Casino"},{"location":"1-Lessons/Lesson10/lesson9/#simulation","text":"Simulation is the process of using a computer to mimic a real experiment or process. In this class, those experiments will almost invariably involve chance. To summarize from: https://www.inferentialthinking.com/chapters/09/3/Simulation.html Step 1: What to Simulate: Specify the quantity you want to simulate. For example, you might decide that you want to simulate the outcomes of tosses of a coin. Step 2: Simulating One Value: Figure out how to simulate one value of the quantity you specified in Step 1. (usually turn into a function for readability) Step 3: Number of Repetitions: Decide how many times you want to simulate the quantity. You will have to repeat Step 2 that many times. Step 4: Coding the Simulation: Put it all together in code. Step 5: Interpret the results (plots,","title":"Simulation"},{"location":"1-Lessons/Lesson10/lesson9/#simulation-example","text":"Should I change my choice? Based on Monty Hall example from https://youtu.be/Xp6V_lO1ZKA But we already have a small car! (Also watch https://www.youtube.com/watch?v=6Ewq_ytHA7g to learn significance of the small car!) Consider The gist of the game is that a contestent chooses a door, the host reveals one of the unselected doors and offers the contestant a chance to change their choice. Should the contestant stick with her initial choice, or switch to the other door? That is the Monty Hall problem. Using classical probability theory it is straightforward to show that: The chance that the car is behind the originally chosen door is 1/3. After Monty opens the door with the goat, the chance distribution changes. If the contestant switches the decision, he/she doubles the chance. Suppose we have harder situations, can we use this simple problem to learn how to ask complex questions? import numpy as np import pandas as pd import matplotlib.pyplot as plt def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 1', 'Goat 2', 'Car'] Goat 1 Goat 2 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining how_many_games = 10000 for i in np.arange(how_many_games): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Goat 2 Goat 1 Car 1 Goat 1 Goat 2 Car 2 Goat 1 Goat 2 Car 3 Goat 2 Goat 1 Car 4 Goat 2 Goat 1 Car ... ... ... ... 9995 Car Goat 2 Goat 1 9996 Car Goat 1 Goat 2 9997 Car Goat 2 Goat 1 9998 Car Goat 2 Goat 1 9999 Goat 1 Goat 2 Car 10000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show()","title":"Simulation Example"},{"location":"1-Lessons/Lesson10/lesson9/#interpret-results","text":"According to the plot, it is beneficial for the players to switch doors because the initial chance for being correct is only 1/3 Does changing doors have a CAUSAL effect on outcome? ## Various Examples","title":"Interpret Results"},{"location":"1-Lessons/Lesson10/lesson9/#defect-chances","text":"A sample of four electronic components is taken from the output of a production line. The probabilities of the various outcomes are calculated to be: Pr [0 defectives] = 0.6561, Pr [1 defective] = 0.2916, Pr [2 defectives] = 0.0486, Pr [3 defectives] = 0.0036, Pr [4 defectives] = 0.0001. What is the probability of at least one defective? #Method-1 pr_atleast1 = 1-0.6561 print(pr_atleast1) 0.3439 #Method-2 pr_atleast1 = 0.2916+0.0483+0.0036+0.0001 print(pr_atleast1) 0.3436","title":"Defect Chances"},{"location":"1-Lessons/Lesson10/lesson9/#common-is-a-birthday","text":"A class of engineering students consists of 45 people. What is the probability that no two students have birthdays on the same day, not considering the year of birth? To simplify the calculation, assume that there are 365 days in the year and that births are equally likely on all of them. Then what is the probability that some members of the class have birthdays on the same day? Also, vary the number of students in the class from 2 to 200 to see its effect on the probability values. #A student in the class states his birthday. So the probability that he/she has the birthday on that date is 1 pr_first = 1 print(pr_first) 1 #Probability that the second student has different birthday than the first student is 364/365 pr_second = 364/365 print(pr_second) 0.9972602739726028 #Probability that the third student has different birthday than the first and the second students is 363/365 pr_third = 363/365 print(pr_third) 0.9945205479452055 #Probability that the fourth student has different birthday than the first, the second, and the third students is 362/365 pr_fourth = 362/365 print(pr_fourth) 0.9917808219178083 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-1: Looping over a list student_ids = list(range(2,46,1)) pr_nosame = 1 for i in student_ids: pr_nosame = pr_nosame*((365-i+1)/365) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) 0.05902410053422507 0.940975899465775 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop student_ids = np.arange(2,46,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame = np.prod(pr_eachstudent) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-19-e397c0f6a5ec> in <module> 7 #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop 8 ----> 9 student_ids = np.arange(2,46,1) 10 11 pr_eachstudent = ((365-student_ids+1)/365) NameError: name 'np' is not defined #Simulation: Getting the probability for different numbers of total students in the class total_students = np.arange(2,201,1) pr_nosame = [] pr_same = [] for i in total_students: student_ids = np.arange(2,i,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame_total = np.prod(pr_eachstudent) pr_nosame.append(pr_nosame_total) pr_same.append(1 - pr_nosame_total) #Creating a dataframe with columns - number of students and probability import pandas as pd final_data = {'Number of students': total_students, 'Probability': pr_same} df = pd.DataFrame(final_data) print(df) #Creating a scatter plot between number of students and probability that at least a pair of students have the same birthday import matplotlib.pyplot as plt plt.scatter(total_students, pr_same, color = 'blue') plt.xlabel('No. of students in the class') plt.ylabel('P [same birthday]') plt.title('Effect of sample size on the chance of success')","title":"Common is a Birthday?"},{"location":"1-Lessons/Lesson10/lesson9/#making-hole-and-money","text":"An oil company is bidding for the rights to drill a well in field A and a well in field B. The probability it will drill a well in field A is 40%. If it does, the probability the well will be successful is 45%. The probability it will drill a well in field B is 30%. If it does, the probability the well will be successful is 55%. Calculate each of the following probabilities: a) What is the probability of a successful well in field A? pr_successA = 0.40*0.45 pr_successA 0.18000000000000002 b) What is the probability of a successful well in field B? pr_successB = 0.30*0.55 pr_successB 0.165 c) What is the probability of both a successful well in field A and a successful well in field B? pr_successAB = pr_successA*pr_successB pr_successAB 0.029700000000000004 d) What is the probability of at least one successful well in the two fields together? pr_onesuccess = pr_successA + pr_successB - pr_successAB pr_onesuccess 0.3153 e) What is the probability of no successful well in field A? pr_nosuccessA = (1-0.4)+(0.4*0.55) pr_nosuccessA 0.8200000000000001 f) What is the probability of no successful well in field B? pr_nosuccessB = (1-0.3)+(0.3*0.45) pr_nosuccessB 0.835 g) What is the probability of no successful well in the two fields together? pr_nosuccessAB = 1 - pr_onesuccess pr_nosuccessAB 0.6847 h) What is the probability of exactly one successful well in the two fields together? pr_exactonesuccess = (0.18*0.835)+(0.165*0.82) pr_exactonesuccess 0.28559999999999997","title":"Making Hole (and money!)"},{"location":"1-Lessons/Lesson10/lesson9/#references","text":"Ford, Martin. 2009 The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future (p. 107). Acculant Publishing. Kindle Edition. Computational and Inferential Thinking: The Foundations of Data Science. By Ani Adhikari and John DeNero, with Contributions by David Wagner and Henry Milner. Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0). https://www.inferentialthinking.com/chapters/09/Randomness.html # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) ! pwd atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) /home/sensei/1330-textbook-webroot/docs/lesson9","title":"References"},{"location":"1-Lessons/Lesson11/lesson10/","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 25 February 2021 Lesson 10 Data Modeling: Randomness and Probability This lesson introduces probability as the chance of occurance of some event. Concepts of sampling and empirical distributions are introduced. Objectives To be able to find probabilities of enumerable (discrete) events. To be able to approximate probabilities of enumerable and/or continuous events. Explain the concepts of sample, population, and probabilities Computing probability: single events, both events, at least event. Computational Thinking Concepts The CT concepts include: Decomposition => Reduce complex observations into concept of an event Abstraction => Outcome == an event, and its likelihood Pattern Matching => Fliping coins, rolling die == map to an event space; apply gambling principles System Integration => Iteration, Simulation Randomness and Probabilities The textbook discusses randomness at: https://www.inferentialthinking.com/chapters/09/Randomness.html Section 9.5 of that link elaborates on probabilities \"Over the centuries, there has been considerable philosophical debate about what probabilities are. Some people think that probabilities are relative frequencies; others think they are long run relative frequencies; still others think that probabilities are a subjective measure of their own personal degree of uncertainty.\" As a practical matter, most probabilities are relative frequencies. If you are a Bayesian statistician, its just conditioned relative frequency. By convention, probabilities are numbers between 0 and 1, or, equivalently, 0% and 100%. Impossible events have probability 0. Events that are certain have probability 1. As a silly example, the probability that a Great White shark will swim up your sewer pipe and bite you on the bottom, is zero. Unless the sewer pipe is pretty big, the shark cannot physically get to you - hence impossible. Now if you are swimming in a freshwater river, lets say the Columbia river on the Oregon border, that probability of sharkbite increases a bit, perhaps 1 in 100 million, or 0.000001% chance of a Great White shark (a pelagic species adapted to salt water), swimming upriver in freshwater, past a couple of fish ladders, still hungry enough bite your bottom. It would be a rare bite indeed; but not physically impossible. At the other end of the scale, \"sure things\" have a probability close to 1. If you run and jump off Glacier point in Yosemite Valley, its almost guarenteed that you will have a 1000 foot plunge until you hit the apron of the cliff and make a big red smear - there could be a gust of wind pushing you away into the trees, but pretty unlikely. So without a squirrel suit and a parachute you are pretty much going to expire with probability 100% chance. Math is the main tool for finding probabilities exactly, though computers are useful for this purpose too. Simulation can provide excellent approximations. In this section, we will informally develop a few simple rules that govern the calculation of probabilities. In subsequent sections we will return to simulations to approximate probabilities of complex events. We will use the standard notation \ud835\udc43(event) to denote the probability that \"event\" happens, and we will use the words \"chance\" and \"probability\" interchangeably. Simple Exclusion If the chance that event happens is 40%, then the chance that it doesn't happen is 60%. This natural calculation can be described in general as follows: \ud835\udc43(an event doesn't happen) = 1\u2212\ud835\udc43(the event happens) The result is correct if the entireity of possibilities are enumerated, that is the entire population is described. Complete Enumeration If you are rolling an ordinary die, a natural assumption is that all six faces are equally likely. Then probabilities of how one roll comes out can be easily calculated as a ratio. For example, the chance that the die shows an even number is \\frac{number~of~even~faces}{number~of~all~faces} = \\frac{\\#{2,4,6}}{\\#{1,2,3,4,5,6}} = \\frac{3}{6} Similarly, \ud835\udc43(die~shows~a~multiple~of~3) = \\frac{\\#{3,6}}{\\#{1,2,3,4,5,6}} = \\frac{2}{6} In general, \ud835\udc43(an event happens) = \\frac{outcomes that make the event happen}{all outcomes} Provided all the outcomes are equally likely. As above, this presumes the entireity of possibilities are enumerated. In the case of a single die, there are six outcomes - these comprise the entire population of outcomes. If we roll two die there are 12 outcomes, three die 18 and so on. Not all random phenomena are as simple as one roll of a die. The two main rules of probability, developed below, allow mathematicians to find probabilities even in complex situations. Conditioning (Two events must happen) Suppose you have a box that contains three tickets: one red, one blue, and one green. Suppose you draw two tickets at random without replacement; that is, you shuffle the three tickets, draw one, shuffle the remaining two, and draw another from those two. What is the chance you get the green ticket first, followed by the red one? There are six possible pairs of colors: RB, BR, RG, GR, BG, GB (we've abbreviated the names of each color to just its first letter). All of these are equally likely by the sampling scheme, and only one of them (GR) makes the event happen. So \ud835\udc43(green~first,~then~red) = \\frac{GR}{RB, BR, RG, GR, BG, GB} = \\frac{1}{6} But there is another way of arriving at the answer, by thinking about the event in two stages. First, the green ticket has to be drawn. That has chance 1/3, which means that the green ticket is drawn first in about 1/3 of all repetitions of the experiment. But that doesn't complete the event. Among the 1/3 of repetitions when green is drawn first, the red ticket has to be drawn next. That happens in about 1/2 of those repetitions, and so: \ud835\udc43(green~first,~then~red) = \\frac{1}{2} of \\frac{1}{3} = \\frac{1}{6} This calculation is usually written \"in chronological order,\" as follows. \ud835\udc43(green~first,~then~red) = \\frac{1}{3} of \\frac{1}{2} = \\frac{1}{6} The factor of \\frac{1}{2} is called \" the conditional chance that the red ticket appears second, given that the green ticket appeared first.\" In general, we have the multiplication rule: \ud835\udc43(two~events~both~happen) = \ud835\udc43(one~event~happens)\\times \ud835\udc43(the~other~event~happens, given~that~the~first~one~happened) Thus, when there are two conditions \u2013 one event must happen, as well as another \u2013 the chance is a fraction of a fraction, which is smaller than either of the two component fractions. The more conditions that have to be satisfied, the less likely they are to all be satisfied. Partitioning (When sequence doesn't matter) - A kind of enumeration! Suppose instead we want the chance that one of the two tickets is green and the other red. This event doesn't specify the order in which the colors must appear. So they can appear in either order. A good way to tackle problems like this is to partition the event so that it can happen in exactly one of several different ways. The natural partition of \"one green and one red\" is: GR, RG. Each of GR and RG has chance 1/6 by the calculation above. So you can calculate the chance of \"one green and one red\" by adding them up. \ud835\udc43(one~green~and~one~red) = \ud835\udc43(GR)+\ud835\udc43(RG) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6} In general, we have the addition rule: \ud835\udc43(an~event~happens) = \ud835\udc43(first~way~it~can~happen)+\ud835\udc43(second~way~it~can~happen) provided the event happens in exactly one of the two ways. Thus, when an event can happen in one of two different ways, the chance that it happens is a sum of chances, and hence bigger than the chance of either of the individual ways. The multiplication rule has a natural extension to more than two events, as we will see below. So also the addition rule has a natural extension to events that can happen in one of several different ways. Learn more at: https://ocw.mit.edu/courses/mathematics/18-440-probability-and-random-variables-spring-2014/lecture-notes/MIT18_440S14_Lecture3.pdf At Least One Success (A kind of exclusion/partition) Data scientists work with random samples from populations. A question that sometimes arises is about the likelihood that a particular individual in the population is selected to be in the sample. To work out the chance, that individual is called a \"success,\" and the problem is to find the chance that the sample contains a success. To see how such chances might be calculated, we start with a simpler setting: tossing a coin two times. If you toss a coin twice, there are four equally likely outcomes: HH, HT, TH, and TT. We have abbreviated \"Heads\" to H and \"Tails\" to T. The chance of getting at least one head in two tosses is therefore 3/4. Another way of coming up with this answer is to work out what happens if you don't get at least one head: both the tosses have to land tails. So \ud835\udc43(at~least~one~head~in~two~tosses) = 1\u2212\ud835\udc43(both~tails) = 1\u2212\\frac{1}{4} = \\frac{3}{4} Notice also that \ud835\udc43(both~tails) = \\frac{1}{4} = \\frac{1}{2} \\times \\frac{1}{2} = (\\frac{1}{2})^2 by the multiplication rule. These two observations allow us to find the chance of at least one head in any given number of tosses. For example, \ud835\udc43(at~least~one~head~in~17~tosses) = 1\u2212\ud835\udc43(all~17~are~tails) = 1\u2212(\\frac{1}{2})^{17} And now we are in a position to find the chance that the face with six spots comes up at least once in rolls of a die. For example, \ud835\udc43(a~single~roll~is~not~6) = \ud835\udc43(1)+\ud835\udc43(2)+\ud835\udc43(3)+\ud835\udc43(4)+\ud835\udc43(5) = \\frac{5}{6} Therefore, \ud835\udc43(at~least~one~6~in~two~rolls) = 1\u2212\ud835\udc43(both~rolls~are~not~6) = 1\u2212(\\frac{5}{6})^2 and \ud835\udc43(at~least~one~6~in~17~rolls) = 1\u2212(\\frac{5}{6})^{17} The table below shows these probabilities as the number of rolls increases from 1 to 50. import pandas as pd HowManyRollsToTake = 50 numRolls = [] probabilities = [] for i in range(HowManyRollsToTake+1): numRolls.append(i) probabilities.append(1-(5/6)**i) rolls = { \"NumRolls\": numRolls, \"Prob at least one 6\": probabilities } df = pd.DataFrame(rolls) df.plot.scatter(x=\"NumRolls\", y=\"Prob at least one 6\") <AxesSubplot:xlabel='NumRolls', ylabel='Prob at least one 6'> df.describe() Why Should anyone buy Flood Protection? Lets apply these ideas to insurance. Suppose you have a house that is located in the 100-year ARI (Annual Recurrance Interval) regulatory flood plain; and you are in a community with a good engineer, who got the probability correct, that is the chance in any year of a total loss is 1 in 100 or 0.01. Thus the chance of no loss in any year is 99 in 100 or 0.99 (pretty good odds)! So what is the chance during a 30-year loan, of no loss? We can just apply the multiplication rule on the no loss probability P(No~Loss) = 0.99^{30} But lets simulate - literally adapting the prior script. import pandas as pd HowManyYears = 600 numYears = [] nolossprobabilities = [] lossprobabilities = [] for i in range(HowManyYears+1): numYears.append(i) # How many years in the sequence nolossprobabilities.append((1-(1/100))**i) #Probability of No Loss after i-years lossprobabilities.append(1 - (1-(1/100))**i) #Probability of Loss after i-years years = { \"Years from Start of Loan\": numYears, \"Probability of No Loss\": nolossprobabilities, \"Probability of Loss\": lossprobabilities } df = pd.DataFrame(years) df.plot.line(x=\"Years from Start of Loan\", y=\"Probability of Loss\") # df.plot.line(x=\"Years from Start of Loan\", y=\"Probability of No Loss\") <AxesSubplot:xlabel='Years from Start of Loan'> df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Years from Start of Loan Probability of No Loss Probability of Loss 0 0 1.000000 0.000000 1 1 0.990000 0.010000 2 2 0.980100 0.019900 3 3 0.970299 0.029701 4 4 0.960596 0.039404 df[\"Probability of Loss\"].loc[30] 0.2602996266117198","title":"Lesson10"},{"location":"1-Lessons/Lesson11/lesson10/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 25 February 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson11/lesson10/#lesson-10-data-modeling-randomness-and-probability","text":"This lesson introduces probability as the chance of occurance of some event. Concepts of sampling and empirical distributions are introduced.","title":"Lesson 10 Data Modeling: Randomness and Probability"},{"location":"1-Lessons/Lesson11/lesson10/#objectives","text":"To be able to find probabilities of enumerable (discrete) events. To be able to approximate probabilities of enumerable and/or continuous events. Explain the concepts of sample, population, and probabilities Computing probability: single events, both events, at least event.","title":"Objectives"},{"location":"1-Lessons/Lesson11/lesson10/#computational-thinking-concepts","text":"The CT concepts include: Decomposition => Reduce complex observations into concept of an event Abstraction => Outcome == an event, and its likelihood Pattern Matching => Fliping coins, rolling die == map to an event space; apply gambling principles System Integration => Iteration, Simulation","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson11/lesson10/#randomness-and-probabilities","text":"The textbook discusses randomness at: https://www.inferentialthinking.com/chapters/09/Randomness.html Section 9.5 of that link elaborates on probabilities \"Over the centuries, there has been considerable philosophical debate about what probabilities are. Some people think that probabilities are relative frequencies; others think they are long run relative frequencies; still others think that probabilities are a subjective measure of their own personal degree of uncertainty.\" As a practical matter, most probabilities are relative frequencies. If you are a Bayesian statistician, its just conditioned relative frequency. By convention, probabilities are numbers between 0 and 1, or, equivalently, 0% and 100%. Impossible events have probability 0. Events that are certain have probability 1. As a silly example, the probability that a Great White shark will swim up your sewer pipe and bite you on the bottom, is zero. Unless the sewer pipe is pretty big, the shark cannot physically get to you - hence impossible. Now if you are swimming in a freshwater river, lets say the Columbia river on the Oregon border, that probability of sharkbite increases a bit, perhaps 1 in 100 million, or 0.000001% chance of a Great White shark (a pelagic species adapted to salt water), swimming upriver in freshwater, past a couple of fish ladders, still hungry enough bite your bottom. It would be a rare bite indeed; but not physically impossible. At the other end of the scale, \"sure things\" have a probability close to 1. If you run and jump off Glacier point in Yosemite Valley, its almost guarenteed that you will have a 1000 foot plunge until you hit the apron of the cliff and make a big red smear - there could be a gust of wind pushing you away into the trees, but pretty unlikely. So without a squirrel suit and a parachute you are pretty much going to expire with probability 100% chance. Math is the main tool for finding probabilities exactly, though computers are useful for this purpose too. Simulation can provide excellent approximations. In this section, we will informally develop a few simple rules that govern the calculation of probabilities. In subsequent sections we will return to simulations to approximate probabilities of complex events. We will use the standard notation \ud835\udc43(event) to denote the probability that \"event\" happens, and we will use the words \"chance\" and \"probability\" interchangeably.","title":"Randomness and Probabilities"},{"location":"1-Lessons/Lesson11/lesson10/#simple-exclusion","text":"If the chance that event happens is 40%, then the chance that it doesn't happen is 60%. This natural calculation can be described in general as follows: \ud835\udc43(an event doesn't happen) = 1\u2212\ud835\udc43(the event happens) The result is correct if the entireity of possibilities are enumerated, that is the entire population is described.","title":"Simple Exclusion"},{"location":"1-Lessons/Lesson11/lesson10/#complete-enumeration","text":"If you are rolling an ordinary die, a natural assumption is that all six faces are equally likely. Then probabilities of how one roll comes out can be easily calculated as a ratio. For example, the chance that the die shows an even number is \\frac{number~of~even~faces}{number~of~all~faces} = \\frac{\\#{2,4,6}}{\\#{1,2,3,4,5,6}} = \\frac{3}{6} Similarly, \ud835\udc43(die~shows~a~multiple~of~3) = \\frac{\\#{3,6}}{\\#{1,2,3,4,5,6}} = \\frac{2}{6} In general, \ud835\udc43(an event happens) = \\frac{outcomes that make the event happen}{all outcomes} Provided all the outcomes are equally likely. As above, this presumes the entireity of possibilities are enumerated. In the case of a single die, there are six outcomes - these comprise the entire population of outcomes. If we roll two die there are 12 outcomes, three die 18 and so on. Not all random phenomena are as simple as one roll of a die. The two main rules of probability, developed below, allow mathematicians to find probabilities even in complex situations.","title":"Complete Enumeration"},{"location":"1-Lessons/Lesson11/lesson10/#conditioning-two-events-must-happen","text":"Suppose you have a box that contains three tickets: one red, one blue, and one green. Suppose you draw two tickets at random without replacement; that is, you shuffle the three tickets, draw one, shuffle the remaining two, and draw another from those two. What is the chance you get the green ticket first, followed by the red one? There are six possible pairs of colors: RB, BR, RG, GR, BG, GB (we've abbreviated the names of each color to just its first letter). All of these are equally likely by the sampling scheme, and only one of them (GR) makes the event happen. So \ud835\udc43(green~first,~then~red) = \\frac{GR}{RB, BR, RG, GR, BG, GB} = \\frac{1}{6} But there is another way of arriving at the answer, by thinking about the event in two stages. First, the green ticket has to be drawn. That has chance 1/3, which means that the green ticket is drawn first in about 1/3 of all repetitions of the experiment. But that doesn't complete the event. Among the 1/3 of repetitions when green is drawn first, the red ticket has to be drawn next. That happens in about 1/2 of those repetitions, and so: \ud835\udc43(green~first,~then~red) = \\frac{1}{2} of \\frac{1}{3} = \\frac{1}{6} This calculation is usually written \"in chronological order,\" as follows. \ud835\udc43(green~first,~then~red) = \\frac{1}{3} of \\frac{1}{2} = \\frac{1}{6} The factor of \\frac{1}{2} is called \" the conditional chance that the red ticket appears second, given that the green ticket appeared first.\" In general, we have the multiplication rule: \ud835\udc43(two~events~both~happen) = \ud835\udc43(one~event~happens)\\times \ud835\udc43(the~other~event~happens, given~that~the~first~one~happened) Thus, when there are two conditions \u2013 one event must happen, as well as another \u2013 the chance is a fraction of a fraction, which is smaller than either of the two component fractions. The more conditions that have to be satisfied, the less likely they are to all be satisfied.","title":"Conditioning (Two events must happen)"},{"location":"1-Lessons/Lesson11/lesson10/#partitioning-when-sequence-doesnt-matter-a-kind-of-enumeration","text":"Suppose instead we want the chance that one of the two tickets is green and the other red. This event doesn't specify the order in which the colors must appear. So they can appear in either order. A good way to tackle problems like this is to partition the event so that it can happen in exactly one of several different ways. The natural partition of \"one green and one red\" is: GR, RG. Each of GR and RG has chance 1/6 by the calculation above. So you can calculate the chance of \"one green and one red\" by adding them up. \ud835\udc43(one~green~and~one~red) = \ud835\udc43(GR)+\ud835\udc43(RG) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6} In general, we have the addition rule: \ud835\udc43(an~event~happens) = \ud835\udc43(first~way~it~can~happen)+\ud835\udc43(second~way~it~can~happen) provided the event happens in exactly one of the two ways. Thus, when an event can happen in one of two different ways, the chance that it happens is a sum of chances, and hence bigger than the chance of either of the individual ways. The multiplication rule has a natural extension to more than two events, as we will see below. So also the addition rule has a natural extension to events that can happen in one of several different ways. Learn more at: https://ocw.mit.edu/courses/mathematics/18-440-probability-and-random-variables-spring-2014/lecture-notes/MIT18_440S14_Lecture3.pdf","title":"Partitioning (When sequence doesn't matter) - A kind of enumeration!"},{"location":"1-Lessons/Lesson11/lesson10/#at-least-one-success-a-kind-of-exclusionpartition","text":"Data scientists work with random samples from populations. A question that sometimes arises is about the likelihood that a particular individual in the population is selected to be in the sample. To work out the chance, that individual is called a \"success,\" and the problem is to find the chance that the sample contains a success. To see how such chances might be calculated, we start with a simpler setting: tossing a coin two times. If you toss a coin twice, there are four equally likely outcomes: HH, HT, TH, and TT. We have abbreviated \"Heads\" to H and \"Tails\" to T. The chance of getting at least one head in two tosses is therefore 3/4. Another way of coming up with this answer is to work out what happens if you don't get at least one head: both the tosses have to land tails. So \ud835\udc43(at~least~one~head~in~two~tosses) = 1\u2212\ud835\udc43(both~tails) = 1\u2212\\frac{1}{4} = \\frac{3}{4} Notice also that \ud835\udc43(both~tails) = \\frac{1}{4} = \\frac{1}{2} \\times \\frac{1}{2} = (\\frac{1}{2})^2 by the multiplication rule. These two observations allow us to find the chance of at least one head in any given number of tosses. For example, \ud835\udc43(at~least~one~head~in~17~tosses) = 1\u2212\ud835\udc43(all~17~are~tails) = 1\u2212(\\frac{1}{2})^{17} And now we are in a position to find the chance that the face with six spots comes up at least once in rolls of a die. For example, \ud835\udc43(a~single~roll~is~not~6) = \ud835\udc43(1)+\ud835\udc43(2)+\ud835\udc43(3)+\ud835\udc43(4)+\ud835\udc43(5) = \\frac{5}{6} Therefore, \ud835\udc43(at~least~one~6~in~two~rolls) = 1\u2212\ud835\udc43(both~rolls~are~not~6) = 1\u2212(\\frac{5}{6})^2 and \ud835\udc43(at~least~one~6~in~17~rolls) = 1\u2212(\\frac{5}{6})^{17} The table below shows these probabilities as the number of rolls increases from 1 to 50. import pandas as pd HowManyRollsToTake = 50 numRolls = [] probabilities = [] for i in range(HowManyRollsToTake+1): numRolls.append(i) probabilities.append(1-(5/6)**i) rolls = { \"NumRolls\": numRolls, \"Prob at least one 6\": probabilities } df = pd.DataFrame(rolls) df.plot.scatter(x=\"NumRolls\", y=\"Prob at least one 6\") <AxesSubplot:xlabel='NumRolls', ylabel='Prob at least one 6'> df.describe()","title":"At Least One Success (A kind of exclusion/partition)"},{"location":"1-Lessons/Lesson11/lesson10/#why-should-anyone-buy-flood-protection","text":"Lets apply these ideas to insurance. Suppose you have a house that is located in the 100-year ARI (Annual Recurrance Interval) regulatory flood plain; and you are in a community with a good engineer, who got the probability correct, that is the chance in any year of a total loss is 1 in 100 or 0.01. Thus the chance of no loss in any year is 99 in 100 or 0.99 (pretty good odds)! So what is the chance during a 30-year loan, of no loss? We can just apply the multiplication rule on the no loss probability P(No~Loss) = 0.99^{30} But lets simulate - literally adapting the prior script. import pandas as pd HowManyYears = 600 numYears = [] nolossprobabilities = [] lossprobabilities = [] for i in range(HowManyYears+1): numYears.append(i) # How many years in the sequence nolossprobabilities.append((1-(1/100))**i) #Probability of No Loss after i-years lossprobabilities.append(1 - (1-(1/100))**i) #Probability of Loss after i-years years = { \"Years from Start of Loan\": numYears, \"Probability of No Loss\": nolossprobabilities, \"Probability of Loss\": lossprobabilities } df = pd.DataFrame(years) df.plot.line(x=\"Years from Start of Loan\", y=\"Probability of Loss\") # df.plot.line(x=\"Years from Start of Loan\", y=\"Probability of No Loss\") <AxesSubplot:xlabel='Years from Start of Loan'> df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Years from Start of Loan Probability of No Loss Probability of Loss 0 0 1.000000 0.000000 1 1 0.990000 0.010000 2 2 0.980100 0.019900 3 3 0.970299 0.029701 4 4 0.960596 0.039404 df[\"Probability of Loss\"].loc[30] 0.2602996266117198","title":"Why Should anyone buy Flood Protection?"},{"location":"1-Lessons/Lesson12/lesson11/","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 2 Mar 2021 Lesson 11 Descriptive Statistics A fundamental part of working with data is describing it. Descriptive statistics help simplify and summarize large amounts of data in a sensible manner. The ultimate goal is to be able to explain observed behavior with a model (like \\textbf{F} = m\\textbf{a} ) so that the model can be used to predict behavior. Objectives To understand the fundamental concepts involved in measurements of a data collection; Central tendency Dispersion Assymmetry Comparison of two sets of data, are they the same? Computational Thinking Concepts The CT concepts include: Abstraction => Represent indvidual behavior with a generalization (mean, median, deviation, \\dots ) Algorithm Design => Simulation Descriptive Statistics with Python In this lecture, we will discuss descriptive statistics and cover a variety of methods for summarizing, describing, and representing datasets in Python. The contents of this notebook are inspired by various online resources including the following links: - \"Descriptive statistics with Python-NumPy\" by Rashmi Jain , available @ https://www.hackerearth.com/blog/developers/descriptive-statistics-python-numpy/. \"Python Statistics Fundamentals: How to Describe Your Data\" by Mirko Stojiljkovi\u0107 , available @ https://realpython.com/python-statistics/. \"A Quick Guide on Descriptive Statistics using Pandas and Seaborn\" by Bee Guan Teo , available @ https://towardsdatascience.com/a-quick-guide-on-descriptive-statistics-using-pandas-and-seaborn-2aadc7395f32. \"Tutorial: Basic Statistics in Python \u2014 Descriptive Statistics\" , available @ https://www.dataquest.io/blog/basic-statistics-with-python-descriptive-statistics/. First lets start with fabricated data. Suppose we made 10,000 observations of some real variable which we named series1 . Then later we made another 10,000 observations on the same variable named series2 . Are the two series similar? How can we quickly quantify? Below is a script, that simulates this situation - parts are left unexplained for now. The script produces the two series, and makes a histogram of the two series. # Example import math import matplotlib.pyplot # the python plotting library import numpy as np import pandas as pd series1 = np.random.normal(0,1.9,10000)# syntax is func(mean,variance,how_many_things) series2 = np.random.normal(6,1,10000)# syntax is func(mean,variance,how_many_things) mydata={'s1':series1,'s2':series2} mydata=pd.DataFrame.from_dict(mydata) mydata.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } s1 s2 count 10000.000000 10000.000000 mean 0.017907 5.984835 std 1.902299 0.994961 min -9.011004 2.463720 25% -1.274411 5.314034 50% 0.012283 6.001274 75% 1.293006 6.649160 max 8.410946 9.768979 Now lets plot the two collections: myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.hist(series1, color ='blue', bins = 100) matplotlib.pyplot.hist(series2, color ='red', bins = 100) matplotlib.pyplot.xlabel(\"Value of RV\") matplotlib.pyplot.ylabel(\"Relative Frequency or Count\") matplotlib.pyplot.title(\"Example Data\") matplotlib.pyplot.show() We will use the \"HighestGrossingMovies.csv\" dataset as an illustrative example. Let's have a look at it first. #Import the necessary external packages import numpy as np import pandas as pd Movies = pd.read_csv(\"HighestGrossingMovies.csv\") #Dataset of the Top10 highest-grossing films as of 2019 (adjusted for inflation) #5 columns (Movie, Director, Year, Budget, Gross) and 10 rows Movies .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Movie Director Year Budget_million$ Gross_million$ 0 Gone with the Wind Victor Fleming 1939 3.9 3706 1 Avatar James Cameron 2009 237.0 3257 2 Titanic James Cameron 1997 200.0 3081 3 Star Wars George Lucas 1977 11.0 3043 4 Avengers: Endgame Joe & Anthony Russo 2019 356.0 2798 5 The Sound of Music Robert Wise 1965 8.2 2549 6 E.T. the Extra-Terrestrial Steven Spielberg 1982 10.5 2489 7 The Ten Commandments Cecil B. DeMille 1956 13.0 2356 8 Doctor Zhivago David Lean 1965 11.0 2233 9 Star Wars: The Force Awakens J.J. Abrams 2015 306.0 2202 Here is an overall look at some but not all of measures we will be discussing today: Measures of Central Tendency Centrality measures give us an estimate of the center of a distribution and a sense of a typical value we would expect to see. The three major measures of center include the mean, median, and mode . Mean Mean aka arithmetic mean aka average is the sum of all the values, divided by the number of values. Mean represents the typical value that acts as a yardstick for all observations. Let's calculate the average budget of the Top10 highest-grossing films. Budget = Movies['Budget_million$'] Budget 0 3.9 1 237.0 2 200.0 3 11.0 4 356.0 5 8.2 6 10.5 7 13.0 8 11.0 9 306.0 Name: Budget_million$, dtype: float64 We can use primitive python to calculate the mean of set of numbers: # Create a list of all the numbers: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] mean1 = sum(budget) / len(budget) print(\"The average budget of the Top10 highest-grossing films is \",mean1,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD We can also utilize a variety of external libraries. (You may find some of them familiar!) # The usual suspects! import numpy as np import pandas as pd # Also, these two libraries offer useful functions for descriptive statistics import statistics import scipy.stats # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the Pandas library mean2 = Budget.mean() print(\"The average budget of the Top10 highest-grossing films is \",mean2,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the Numpy library mean3 = np.mean(Budget) print(\"The average budget of the Top10 highest-grossing films is \",mean3,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the statistics library mean4 = statistics.mean(Budget) print(\"The average budget of the Top10 highest-grossing films is \",mean4,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD Harmonic Mean The harmonic mean is the reciprocal of the mean of the reciprocals of all items in the dataset. Let's calculate the harmonic mean for the same set of numbers: # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] hmean1 = len(budget) / sum(1 / item for item in budget) hmean1 = round(hmean1,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] hmean2 = statistics.harmonic_mean(Budget) hmean2 = round(hmean2,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean2,\"million USD\") # via the scipy.stats library: Budget = Movies['Budget_million$'] hmean3 = scipy.stats.hmean(Budget) hmean3 = round(hmean3,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean3,\"million USD\") The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD Geometric Mean The geometric mean is the \ud835\udc5b-th root of the product of all \ud835\udc5b elements \ud835\udc65\u1d62 in a dataset. Let's calculate the geometric mean for the same set of numbers: # Primitive Python: -it is getting more lengthy and labour-intensive budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] gmean1 = 1 for item in budget: gmean1 *= item gmean1 **= 1 / len(budget) gmean1 = round(gmean1,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] gmean2 = statistics.geometric_mean(Budget) gmean2 = round(gmean2,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean2,\"million USD\") # via the scipy.stats library: Budget = Movies['Budget_million$'] gmean3 = scipy.stats.gmean(Budget) gmean3 = round(gmean3,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean3,\"million USD\") The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD Arithmetic or Geometric or Harmonic?- How to be Mean! If values have the same units: Use the arithmetic mean. If values have differing units: Use the geometric mean. | Also, commonly used for growth rates, like population growth or interest rates. If values are rates: Use the harmonic mean. If you are interested in knowing more about these 3 and their differences, you may find these interesting: - \"Arithmetic, Geometric, and Harmonic Means for Machine Learning Arithmetic, Geometric, and Harmonic Means for Machine Learning\" by Jason Brownlee , available @ https://machinelearningmastery.com/arithmetic-geometric-and-harmonic-means-for-machine-learning/#:~:text=The%20arithmetic%20mean%20is%20appropriate,with%20different%20measures%2C%20called%20rates. \"On Average, You\u2019re Using the Wrong Average: Geometric & Harmonic Means in Data Analysis\" by Daniel McNichol , available @ https://towardsdatascience.com/on-average-youre-using-the-wrong-average-geometric-harmonic-means-in-data-analysis-2a703e21ea0 Median Median is the middle element of a sorted dataset. The value where the upper half of the data lies above it and lower half lies below it. In other words, it is the middle value of a data set. To calculate the median, arrange the data points in the increasing (or decreasing) order and the middle value is the median. If the number of elements \ud835\udc5b of the dataset is odd, then the median is the value at the middle position: 0.5(\ud835\udc5b + 1). If \ud835\udc5b is even, then the median is the arithmetic mean of the two values in the middle, that is, the items at the positions 0.5\ud835\udc5b and 0.5\ud835\udc5b + 1. Let's find the median of the gross of the Top10 highest-grossing films: Gross = Movies['Gross_million$'] Gross 0 3706 1 3257 2 3081 3 3043 4 2798 5 2549 6 2489 7 2356 8 2233 9 2202 Name: Gross_million$, dtype: int64 We can use primitive python to calculate the median of a set of numbers: # Create a list of all the numbers: gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202] n = len(gross) if n % 2: median1 = sorted(gross)[round(0.5*(n-1))] else: gross_ord, index = sorted(gross), round(0.5 * n) median1 = 0.5 * (gross_ord[index-1] + gross_ord[index]) print(\"The median of gross of the Top10 highest-grossing films is \",median1,\"million USD\") The median of gross of the Top10 highest-grossing films is 2673.5 million USD We can use also use external libraries: #via the Pandas library: Gross = Movies['Gross_million$'] median2 = Gross.median() print(\"The median of gross of the Top10 highest-grossing films is \",median2,\"million USD\") #via the Numpy library: Gross = Movies['Gross_million$'] median3 = np.median(Gross) print(\"The median of gross of the Top10 highest-grossing films is \",median3,\"million USD\") #via the Statistics library: Gross = Movies['Gross_million$'] median4 = statistics.median(Gross) print(\"The median of gross of the Top10 highest-grossing films is \",median4,\"million USD\") #2 more functions from the same library- For even number of cases: print(\"low median :\",statistics.median_low(Gross)) print(\"high median :\",statistics.median_high(Gross)) The median of gross of the Top10 highest-grossing films is 2673.5 million USD The median of gross of the Top10 highest-grossing films is 2673.5 million USD The median of gross of the Top10 highest-grossing films is 2673.5 million USD low median : 2549 high median : 2798 The main difference between the behavior of the mean and median is related to dataset outliers or extremes. The mean is heavily affected by outliers, but the median only depends on outliers either slightly or not at all. You can compare the mean and median as one way to detect outliers and asymmetry in your data. Whether the mean value or the median value is more useful to you depends on the context of your particular problem. The mean is a better choice when there are no extreme values that can affect it. It is a better summary because the information from every observation is included rather than median, which is just the middle value. However, in the presence of outliers, median is considered a better alternative. Check this out: newgross = [99999,3257,3081,3043,2798,2549,2489,2356,2233,2202] #We have replaced 3706 with 99999- an extremely high number (an outlier) newmean = np.mean(newgross) newmedian = np.median(newgross) print(newmean) #A huge change from the previous value (115.66) - Mean is very sensitive to outliers and extreme values print(newmedian) #No Change- the median only depends on outliers either slightly or not at all. 12400.7 2673.5 To read more about the differences of mean and median, check these out: - \"Stuck in the middle \u2013 mean vs. median\" , available @ https://www.clinfo.eu/mean-median/ \"Mean vs Median: When to Use Which Measure?\" , available @ https://www.datascienceblog.net/post/basic-statistics/mean_vs_median/ \"Mean vs. Median\" by AnswerMiner , available @ https://www.answerminer.com/blog/mean-vs-median Mode The value that occurs the most number of times in our data set. Closely tied to the concept of frequency, mode provides information on the most recurrent elements in a dataset. When the mode is not unique, we say that the data set is bimodal, while a data set with more than two modes is multimodal. Let's find the mode in the gross of the Top10 highest-grossing films: # In primitive Python: # Create a list of all the numbers: gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202] mode1 = max((gross.count(item), item) for item in gross)[1] print(mode1) #Since each item is repeated only once, only the first element is printed- This is a multimodal set. #via the Pandas library: Gross = Movies['Gross_million$'] mode2 = Gross.mode() print(mode2) #Returns all modal values- This is a multimodal set. #via the Statistics library: Gross = Movies['Gross_million$'] mode3 = statistics.mode(Gross) print(mode3) #Return a single value mode4 = statistics.multimode(Gross) print(mode4) #Returns a list of all modes #via the scipy.stats library: Gross = Movies['Gross_million$'] mode5 = scipy.stats.mode(Gross) print(mode5) #Returns the object with the modal value and the number of times it occurs- If multimodal: only the smallest value 3706 0 2202 1 2233 2 2356 3 2489 4 2549 5 2798 6 3043 7 3081 8 3257 9 3706 dtype: int64 3706 [3706, 3257, 3081, 3043, 2798, 2549, 2489, 2356, 2233, 2202] ModeResult(mode=array([2202]), count=array([1])) Mode is not useful when our distribution is flat; i.e., the frequencies of all groups are similar. Mode makes sense when we do not have a numeric-valued data set which is required in case of the mean and the median. For instance: Director = Movies['Director'] # via statistics: mode6 = statistics.mode(Director) print(mode6) #\"James Cameron\" with two films (x2 repeats) is the mode # via pandas: mode7 = Director.mode() print(mode7) #\"James Cameron\" with two films (x2 repeats) is the mode James Cameron 0 James Cameron dtype: object To read more about mode, check these out: - \"Mode: A statistical measure of central tendency\" , available @ https://corporatefinanceinstitute.com/resources/knowledge/other/mode/ \"When to use each measure of Central Tendency\" , available @ https://courses.lumenlearning.com/introstats1/chapter/when-to-use-each-measure-of-central-tendency/ \"Mean, Median, Mode: What They Are, How to Find Them\" , available @ https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/ Measures of Dispersion Measures of dispersion are values that describe how the data varies. It gives us a sense of how much the data tends to diverge from the typical value. Aka measures of variability, they quantify the spread of data points.The major measures of dispersion include range, percentiles, inter-quentile range, variance, standard deviation, skeness and kurtosis . Range The range gives a quick sense of the spread of the distribution to those who require only a rough indication of the data. There are some disadvantages of using the range as a measure of spread. One being it does not give any information of the data in between maximum and minimum. Also, the range is very sensitive to extreme values. Let's calculate the range for the budget of the Top10 highest-grossing films: # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] range1 = max(budget)-min(budget) print(\"The range of the budget of the Top10 highest-grossing films is \",range1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] range2 = np.ptp(Budget) #ptp stands for Peak To Peak print(\"The range of the budget of the Top10 highest-grossing films is \",range2,\"million USD\") The range of the budget of the Top10 highest-grossing films is 352.1 million USD The range of the budget of the Top10 highest-grossing films is 352.1 million USD Percentiles and Quartiles A measure which indicates the value below which a given percentage of points in a dataset fall. The sample \ud835\udc5d percentile is the element in the dataset such that \ud835\udc5d% of the elements in the dataset are less than or equal to that value. Also, (100 \u2212 \ud835\udc5d)% of the elements are greater than or equal to that value. For example, median represents the 50th percentile. Similarly, we can have 0th percentile representing the minimum and 100th percentile representing the maximum of all data points. Percentile gives the relative position of a particular value within the dataset. It also helps in comparing the data sets which have different means and deviations. Each dataset has three quartiles, which are the percentiles that divide the dataset into four parts: The first quartile (Q1) is the sample 25th percentile. It divides roughly 25% of the smallest items from the rest of the dataset. The second quartile Q2) is the sample 50th percentile or the median. Approximately 25% of the items lie between the first and second quartiles and another 25% between the second and third quartiles. The third quartile (Q3) is the sample 75th percentile. It divides roughly 25% of the largest items from the rest of the dataset. Budget = Movies['Budget_million$'] #via Numpy: p10 = np.percentile(Budget, 10) #returns the 10th percentile print(\"The 10th percentile of the budget of the Top10 highest-grossing films is \",p10) p4070 = np.percentile(Budget, [40,70]) #returns the 40th and 70th percentile print(\"The 40th and 70th percentile of the budget of the Top10 highest-grossing films are \",p4070) #via Pandas: p10n = Budget.quantile(0.10) #returns the 10th percentile - notice the difference from Numpy print(\"The 10th percentile of the budget of the Top10 highest-grossing films is \",p10n) #via Statistics: Qs = statistics.quantiles(Budget, n=4, method='inclusive') #The parameter n defines the number of resulting equal-probability percentiles: #n=4 returns the quartiles | n=2 returns the median print(\"The quartiles of the budget of the Top10 highest-grossing films is \",Qs) The 10th percentile of the budget of the Top10 highest-grossing films is 7.77 The 40th and 70th percentile of the budget of the Top10 highest-grossing films are [ 11. 211.1] The 10th percentile of the budget of the Top10 highest-grossing films is 7.77 The quartiles of the budget of the Top10 highest-grossing films is [10.625, 12.0, 227.75] InterQuartile Range (IQR) IQR is the difference between the third quartile and the first quartile (Q3-Q1). The interquartile range is a better option than range because it is not affected by outliers. It removes the outliers by just focusing on the distance within the middle 50% of the data. Budget = Movies['Budget_million$'] #via Numpy: IQR1 = np.percentile(Budget, 75) -np.percentile(Budget, 25) #returns the IQR = Q3-Q1 = P75-P25 print(\"The IQR of the budget of the Top10 highest-grossing films is \",IQR1) #via scipy.stats: IQR2 = scipy.stats.iqr(Budget) #returns the IQR- Can be used for other percentile differences as well >> iqr(object, rng=(p1, p2)) print(\"The IQR of the budget of the Top10 highest-grossing films is \",IQR2) The IQR of the budget of the Top10 highest-grossing films is 217.125 The IQR of the budget of the Top10 highest-grossing films is 217.125 The Five-number Summary A five-number summary is especially useful in descriptive analyses or during the preliminary investigation of a large data set. A summary consists of five values: the most extreme values in the data set (the maximum and minimum values), the lower and upper quartiles, and the median. Five-number summary can be used to describe any data distribution. Boxplots are extremely useful graphical representation of the 5-number summary that we will discuss later. Budget = Movies['Budget_million$'] Budget.describe() #Remember this jewel from Pandas? -It directly return the 5-number summary AND MORE! count 10.000000 mean 115.660000 std 142.739991 min 3.900000 25% 10.625000 50% 12.000000 75% 227.750000 max 356.000000 Name: Budget_million$, dtype: float64 Boxplots are extremely useful graphical representation of the 5-number summary. It can show the range, interquartile range, median, mode, outliers, and all quartiles. import matplotlib.pyplot as plt #Required for the plot gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202,5000] #same data + an outlier: 5000 fig = plt.figure(figsize =(7, 5)) plt.boxplot(gross,medianprops={'linewidth': 1, 'color': 'purple'}) plt.show() To read more about the 5-number summary, check these out: - \"Find a Five-Number Summary in Statistics: Easy Steps\" , available @ https://www.statisticshowto.com/how-to-find-a-five-number-summary-in-statistics/ \"The Five-Number Summary\" , available @ https://www.purplemath.com/modules/boxwhisk2.htm \"What Is the 5 Number Summary?\" by Courtney Taylor , available @ https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/ Variance The sample variance quantifies the spread of the data. It shows numerically how far the data points are from the mean. The observations may or may not be meaningful if observations in data sets are highly spread. Let's calculate the variance for budget of the Top10 highest-grossing films. Note that if we are working with the entire population (and not the sample), the denominator should be \"n\" instead of \"n-1\". Note that if we are working with the entire population (and not the sample), the denominator should be \"n\" instead of \"n-1\". # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var1 = sum((item - mean)**2 for item in budget) / (n - 1) print(\"The variance of the budget of the Top10 highest-grossing films is \",var1) # via the Statistics library: Budget = Movies['Budget_million$'] var2 = statistics.variance(Budget) print(\"The variance of the budget of the Top10 highest-grossing films is \",var2) The variance of the budget of the Top10 highest-grossing films is 20374.70488888889 The variance of the budget of the Top10 highest-grossing films is 20374.70488888889 Standard Deviation The sample standard deviation is another measure of data spread. It\u2019s connected to the sample variance, as standard deviation, \ud835\udc60, is the positive square root of the sample variance. The standard deviation is often more convenient than the variance because it has the same unit as the data points. # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var = sum((item - mean)**2 for item in budget) / (n - 1) sd1 = var**0.5 print(\"The standard deviation of the budget of the Top10 highest-grossing films is \",sd1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] sd2 = statistics.stdev(Budget) print(\"The standard deviation of the budget of the Top10 highest-grossing films is \",sd2,\"million USD\") The standard deviation of the budget of the Top10 highest-grossing films is 142.73999050332353 million USD The standard deviation of the budget of the Top10 highest-grossing films is 142.73999050332353 million USD Skewness The sample skewness measures the asymmetry of a data sample. There are several mathematical definitions of skewness. The Fisher-Pearson standardized moment coefficient is calculated by using mean, median and standard deviation of the data. Usually, negative skewness values indicate that there\u2019s a dominant tail on the left side. Positive skewness values correspond to a longer or fatter tail on the right side. If the skewness is close to 0 (for example, between \u22120.5 and 0.5), then the dataset is considered quite symmetrical. # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var = sum((item - mean)**2 for item in budget) / (n - 1) std = var**0.5 skew1 = (sum((item - mean)**3 for item in budget) * n / ((n - 1) * (n - 2) * std**3)) print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew1) # via the scipy.stats library: Budget = Movies['Budget_million$'] skew2 = scipy.stats.skew(Budget, bias=False) print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew2) # via the Pandas library: Budget = Movies['Budget_million$'] skew3 = Budget.skew() print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew3) The skewness of the budget of the Top10 highest-grossing films is 0.7636547490528159 The skewness of the budget of the Top10 highest-grossing films is 0.763654749052816 The skewness of the budget of the Top10 highest-grossing films is 0.763654749052816 Kurtosis Kurtosis describes the peakedness of the distribution. In other words, Kurtosis identifies whether the tails of a given distribution contain extreme values. While Skewness essentially measures the symmetry of the distribution, kurtosis determines the heaviness of the distribution tails. If the distribution is tall and thin it is called a leptokurtic distribution. Values in a leptokurtic distribution are near the mean or at the extremes. A flat distribution where the values are moderately spread out (i.e., unlike leptokurtic) is called platykurtic distribution. A distribution whose shape is in between a leptokurtic distribution and a platykurtic distribution is called a mesokurtic distribution. # via the scipy.stats library: Budget = Movies['Budget_million$'] Kurt = scipy.stats.kurtosis(Budget) print(\"The kurtosis of the budget of the Top10 highest-grossing films is \",Kurt) #a platykurtic distribution | the tails are heavy The kurtosis of the budget of the Top10 highest-grossing films is -1.3110307923262225 To read more about skewness and kurtosis, check these out: - \"Measures of Skewness and Kurtosis\" , available @ https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm#:~:text=Skewness%20is%20a%20measure%20of,relative%20to%20a%20normal%20distribution. \"Are the Skewness and Kurtosis Useful Statistics?\" , available @ https://www.spcforexcel.com/knowledge/basic-statistics/are-skewness-and-kurtosis-useful-statistics \"Skew and Kurtosis: 2 Important Statistics terms you need to know in Data Science\" by Diva Dugar , available @ https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa \"Measures of Shape: Skewness and Kurtosis\" by Stan Brown , available @ https://brownmath.com/stat/shape.htm","title":"Linear Equation Systems"},{"location":"1-Lessons/Lesson12/lesson11/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 2 Mar 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson12/lesson11/#lesson-11-descriptive-statistics","text":"A fundamental part of working with data is describing it. Descriptive statistics help simplify and summarize large amounts of data in a sensible manner. The ultimate goal is to be able to explain observed behavior with a model (like \\textbf{F} = m\\textbf{a} ) so that the model can be used to predict behavior.","title":"Lesson 11 Descriptive Statistics"},{"location":"1-Lessons/Lesson12/lesson11/#objectives","text":"To understand the fundamental concepts involved in measurements of a data collection; Central tendency Dispersion Assymmetry Comparison of two sets of data, are they the same?","title":"Objectives"},{"location":"1-Lessons/Lesson12/lesson11/#computational-thinking-concepts","text":"The CT concepts include: Abstraction => Represent indvidual behavior with a generalization (mean, median, deviation, \\dots ) Algorithm Design => Simulation","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson12/lesson11/#descriptive-statistics-with-python","text":"In this lecture, we will discuss descriptive statistics and cover a variety of methods for summarizing, describing, and representing datasets in Python. The contents of this notebook are inspired by various online resources including the following links: - \"Descriptive statistics with Python-NumPy\" by Rashmi Jain , available @ https://www.hackerearth.com/blog/developers/descriptive-statistics-python-numpy/. \"Python Statistics Fundamentals: How to Describe Your Data\" by Mirko Stojiljkovi\u0107 , available @ https://realpython.com/python-statistics/. \"A Quick Guide on Descriptive Statistics using Pandas and Seaborn\" by Bee Guan Teo , available @ https://towardsdatascience.com/a-quick-guide-on-descriptive-statistics-using-pandas-and-seaborn-2aadc7395f32. \"Tutorial: Basic Statistics in Python \u2014 Descriptive Statistics\" , available @ https://www.dataquest.io/blog/basic-statistics-with-python-descriptive-statistics/. First lets start with fabricated data. Suppose we made 10,000 observations of some real variable which we named series1 . Then later we made another 10,000 observations on the same variable named series2 . Are the two series similar? How can we quickly quantify? Below is a script, that simulates this situation - parts are left unexplained for now. The script produces the two series, and makes a histogram of the two series. # Example import math import matplotlib.pyplot # the python plotting library import numpy as np import pandas as pd series1 = np.random.normal(0,1.9,10000)# syntax is func(mean,variance,how_many_things) series2 = np.random.normal(6,1,10000)# syntax is func(mean,variance,how_many_things) mydata={'s1':series1,'s2':series2} mydata=pd.DataFrame.from_dict(mydata) mydata.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } s1 s2 count 10000.000000 10000.000000 mean 0.017907 5.984835 std 1.902299 0.994961 min -9.011004 2.463720 25% -1.274411 5.314034 50% 0.012283 6.001274 75% 1.293006 6.649160 max 8.410946 9.768979 Now lets plot the two collections: myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.hist(series1, color ='blue', bins = 100) matplotlib.pyplot.hist(series2, color ='red', bins = 100) matplotlib.pyplot.xlabel(\"Value of RV\") matplotlib.pyplot.ylabel(\"Relative Frequency or Count\") matplotlib.pyplot.title(\"Example Data\") matplotlib.pyplot.show() We will use the \"HighestGrossingMovies.csv\" dataset as an illustrative example. Let's have a look at it first. #Import the necessary external packages import numpy as np import pandas as pd Movies = pd.read_csv(\"HighestGrossingMovies.csv\") #Dataset of the Top10 highest-grossing films as of 2019 (adjusted for inflation) #5 columns (Movie, Director, Year, Budget, Gross) and 10 rows Movies .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Movie Director Year Budget_million$ Gross_million$ 0 Gone with the Wind Victor Fleming 1939 3.9 3706 1 Avatar James Cameron 2009 237.0 3257 2 Titanic James Cameron 1997 200.0 3081 3 Star Wars George Lucas 1977 11.0 3043 4 Avengers: Endgame Joe & Anthony Russo 2019 356.0 2798 5 The Sound of Music Robert Wise 1965 8.2 2549 6 E.T. the Extra-Terrestrial Steven Spielberg 1982 10.5 2489 7 The Ten Commandments Cecil B. DeMille 1956 13.0 2356 8 Doctor Zhivago David Lean 1965 11.0 2233 9 Star Wars: The Force Awakens J.J. Abrams 2015 306.0 2202 Here is an overall look at some but not all of measures we will be discussing today:","title":"Descriptive Statistics with Python"},{"location":"1-Lessons/Lesson12/lesson11/#measures-of-central-tendency","text":"Centrality measures give us an estimate of the center of a distribution and a sense of a typical value we would expect to see. The three major measures of center include the mean, median, and mode .","title":"Measures of Central Tendency"},{"location":"1-Lessons/Lesson12/lesson11/#mean","text":"Mean aka arithmetic mean aka average is the sum of all the values, divided by the number of values. Mean represents the typical value that acts as a yardstick for all observations. Let's calculate the average budget of the Top10 highest-grossing films. Budget = Movies['Budget_million$'] Budget 0 3.9 1 237.0 2 200.0 3 11.0 4 356.0 5 8.2 6 10.5 7 13.0 8 11.0 9 306.0 Name: Budget_million$, dtype: float64 We can use primitive python to calculate the mean of set of numbers: # Create a list of all the numbers: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] mean1 = sum(budget) / len(budget) print(\"The average budget of the Top10 highest-grossing films is \",mean1,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD We can also utilize a variety of external libraries. (You may find some of them familiar!) # The usual suspects! import numpy as np import pandas as pd # Also, these two libraries offer useful functions for descriptive statistics import statistics import scipy.stats # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the Pandas library mean2 = Budget.mean() print(\"The average budget of the Top10 highest-grossing films is \",mean2,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the Numpy library mean3 = np.mean(Budget) print(\"The average budget of the Top10 highest-grossing films is \",mean3,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the statistics library mean4 = statistics.mean(Budget) print(\"The average budget of the Top10 highest-grossing films is \",mean4,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD","title":"Mean"},{"location":"1-Lessons/Lesson12/lesson11/#harmonic-mean","text":"The harmonic mean is the reciprocal of the mean of the reciprocals of all items in the dataset. Let's calculate the harmonic mean for the same set of numbers: # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] hmean1 = len(budget) / sum(1 / item for item in budget) hmean1 = round(hmean1,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] hmean2 = statistics.harmonic_mean(Budget) hmean2 = round(hmean2,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean2,\"million USD\") # via the scipy.stats library: Budget = Movies['Budget_million$'] hmean3 = scipy.stats.hmean(Budget) hmean3 = round(hmean3,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean3,\"million USD\") The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD","title":"Harmonic Mean"},{"location":"1-Lessons/Lesson12/lesson11/#geometric-mean","text":"The geometric mean is the \ud835\udc5b-th root of the product of all \ud835\udc5b elements \ud835\udc65\u1d62 in a dataset. Let's calculate the geometric mean for the same set of numbers: # Primitive Python: -it is getting more lengthy and labour-intensive budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] gmean1 = 1 for item in budget: gmean1 *= item gmean1 **= 1 / len(budget) gmean1 = round(gmean1,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] gmean2 = statistics.geometric_mean(Budget) gmean2 = round(gmean2,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean2,\"million USD\") # via the scipy.stats library: Budget = Movies['Budget_million$'] gmean3 = scipy.stats.gmean(Budget) gmean3 = round(gmean3,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean3,\"million USD\") The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD","title":"Geometric Mean"},{"location":"1-Lessons/Lesson12/lesson11/#arithmetic-or-geometric-or-harmonic-how-to-be-mean","text":"If values have the same units: Use the arithmetic mean. If values have differing units: Use the geometric mean. | Also, commonly used for growth rates, like population growth or interest rates. If values are rates: Use the harmonic mean. If you are interested in knowing more about these 3 and their differences, you may find these interesting: - \"Arithmetic, Geometric, and Harmonic Means for Machine Learning Arithmetic, Geometric, and Harmonic Means for Machine Learning\" by Jason Brownlee , available @ https://machinelearningmastery.com/arithmetic-geometric-and-harmonic-means-for-machine-learning/#:~:text=The%20arithmetic%20mean%20is%20appropriate,with%20different%20measures%2C%20called%20rates. \"On Average, You\u2019re Using the Wrong Average: Geometric & Harmonic Means in Data Analysis\" by Daniel McNichol , available @ https://towardsdatascience.com/on-average-youre-using-the-wrong-average-geometric-harmonic-means-in-data-analysis-2a703e21ea0","title":"Arithmetic or Geometric or Harmonic?- How to be Mean!"},{"location":"1-Lessons/Lesson12/lesson11/#median","text":"Median is the middle element of a sorted dataset. The value where the upper half of the data lies above it and lower half lies below it. In other words, it is the middle value of a data set. To calculate the median, arrange the data points in the increasing (or decreasing) order and the middle value is the median. If the number of elements \ud835\udc5b of the dataset is odd, then the median is the value at the middle position: 0.5(\ud835\udc5b + 1). If \ud835\udc5b is even, then the median is the arithmetic mean of the two values in the middle, that is, the items at the positions 0.5\ud835\udc5b and 0.5\ud835\udc5b + 1. Let's find the median of the gross of the Top10 highest-grossing films: Gross = Movies['Gross_million$'] Gross 0 3706 1 3257 2 3081 3 3043 4 2798 5 2549 6 2489 7 2356 8 2233 9 2202 Name: Gross_million$, dtype: int64 We can use primitive python to calculate the median of a set of numbers: # Create a list of all the numbers: gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202] n = len(gross) if n % 2: median1 = sorted(gross)[round(0.5*(n-1))] else: gross_ord, index = sorted(gross), round(0.5 * n) median1 = 0.5 * (gross_ord[index-1] + gross_ord[index]) print(\"The median of gross of the Top10 highest-grossing films is \",median1,\"million USD\") The median of gross of the Top10 highest-grossing films is 2673.5 million USD We can use also use external libraries: #via the Pandas library: Gross = Movies['Gross_million$'] median2 = Gross.median() print(\"The median of gross of the Top10 highest-grossing films is \",median2,\"million USD\") #via the Numpy library: Gross = Movies['Gross_million$'] median3 = np.median(Gross) print(\"The median of gross of the Top10 highest-grossing films is \",median3,\"million USD\") #via the Statistics library: Gross = Movies['Gross_million$'] median4 = statistics.median(Gross) print(\"The median of gross of the Top10 highest-grossing films is \",median4,\"million USD\") #2 more functions from the same library- For even number of cases: print(\"low median :\",statistics.median_low(Gross)) print(\"high median :\",statistics.median_high(Gross)) The median of gross of the Top10 highest-grossing films is 2673.5 million USD The median of gross of the Top10 highest-grossing films is 2673.5 million USD The median of gross of the Top10 highest-grossing films is 2673.5 million USD low median : 2549 high median : 2798 The main difference between the behavior of the mean and median is related to dataset outliers or extremes. The mean is heavily affected by outliers, but the median only depends on outliers either slightly or not at all. You can compare the mean and median as one way to detect outliers and asymmetry in your data. Whether the mean value or the median value is more useful to you depends on the context of your particular problem. The mean is a better choice when there are no extreme values that can affect it. It is a better summary because the information from every observation is included rather than median, which is just the middle value. However, in the presence of outliers, median is considered a better alternative. Check this out: newgross = [99999,3257,3081,3043,2798,2549,2489,2356,2233,2202] #We have replaced 3706 with 99999- an extremely high number (an outlier) newmean = np.mean(newgross) newmedian = np.median(newgross) print(newmean) #A huge change from the previous value (115.66) - Mean is very sensitive to outliers and extreme values print(newmedian) #No Change- the median only depends on outliers either slightly or not at all. 12400.7 2673.5 To read more about the differences of mean and median, check these out: - \"Stuck in the middle \u2013 mean vs. median\" , available @ https://www.clinfo.eu/mean-median/ \"Mean vs Median: When to Use Which Measure?\" , available @ https://www.datascienceblog.net/post/basic-statistics/mean_vs_median/ \"Mean vs. Median\" by AnswerMiner , available @ https://www.answerminer.com/blog/mean-vs-median","title":"Median"},{"location":"1-Lessons/Lesson12/lesson11/#mode","text":"The value that occurs the most number of times in our data set. Closely tied to the concept of frequency, mode provides information on the most recurrent elements in a dataset. When the mode is not unique, we say that the data set is bimodal, while a data set with more than two modes is multimodal. Let's find the mode in the gross of the Top10 highest-grossing films: # In primitive Python: # Create a list of all the numbers: gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202] mode1 = max((gross.count(item), item) for item in gross)[1] print(mode1) #Since each item is repeated only once, only the first element is printed- This is a multimodal set. #via the Pandas library: Gross = Movies['Gross_million$'] mode2 = Gross.mode() print(mode2) #Returns all modal values- This is a multimodal set. #via the Statistics library: Gross = Movies['Gross_million$'] mode3 = statistics.mode(Gross) print(mode3) #Return a single value mode4 = statistics.multimode(Gross) print(mode4) #Returns a list of all modes #via the scipy.stats library: Gross = Movies['Gross_million$'] mode5 = scipy.stats.mode(Gross) print(mode5) #Returns the object with the modal value and the number of times it occurs- If multimodal: only the smallest value 3706 0 2202 1 2233 2 2356 3 2489 4 2549 5 2798 6 3043 7 3081 8 3257 9 3706 dtype: int64 3706 [3706, 3257, 3081, 3043, 2798, 2549, 2489, 2356, 2233, 2202] ModeResult(mode=array([2202]), count=array([1])) Mode is not useful when our distribution is flat; i.e., the frequencies of all groups are similar. Mode makes sense when we do not have a numeric-valued data set which is required in case of the mean and the median. For instance: Director = Movies['Director'] # via statistics: mode6 = statistics.mode(Director) print(mode6) #\"James Cameron\" with two films (x2 repeats) is the mode # via pandas: mode7 = Director.mode() print(mode7) #\"James Cameron\" with two films (x2 repeats) is the mode James Cameron 0 James Cameron dtype: object To read more about mode, check these out: - \"Mode: A statistical measure of central tendency\" , available @ https://corporatefinanceinstitute.com/resources/knowledge/other/mode/ \"When to use each measure of Central Tendency\" , available @ https://courses.lumenlearning.com/introstats1/chapter/when-to-use-each-measure-of-central-tendency/ \"Mean, Median, Mode: What They Are, How to Find Them\" , available @ https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/","title":"Mode"},{"location":"1-Lessons/Lesson12/lesson11/#measures-of-dispersion","text":"Measures of dispersion are values that describe how the data varies. It gives us a sense of how much the data tends to diverge from the typical value. Aka measures of variability, they quantify the spread of data points.The major measures of dispersion include range, percentiles, inter-quentile range, variance, standard deviation, skeness and kurtosis .","title":"Measures of Dispersion"},{"location":"1-Lessons/Lesson12/lesson11/#range","text":"The range gives a quick sense of the spread of the distribution to those who require only a rough indication of the data. There are some disadvantages of using the range as a measure of spread. One being it does not give any information of the data in between maximum and minimum. Also, the range is very sensitive to extreme values. Let's calculate the range for the budget of the Top10 highest-grossing films: # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] range1 = max(budget)-min(budget) print(\"The range of the budget of the Top10 highest-grossing films is \",range1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] range2 = np.ptp(Budget) #ptp stands for Peak To Peak print(\"The range of the budget of the Top10 highest-grossing films is \",range2,\"million USD\") The range of the budget of the Top10 highest-grossing films is 352.1 million USD The range of the budget of the Top10 highest-grossing films is 352.1 million USD","title":"Range"},{"location":"1-Lessons/Lesson12/lesson11/#percentiles-and-quartiles","text":"A measure which indicates the value below which a given percentage of points in a dataset fall. The sample \ud835\udc5d percentile is the element in the dataset such that \ud835\udc5d% of the elements in the dataset are less than or equal to that value. Also, (100 \u2212 \ud835\udc5d)% of the elements are greater than or equal to that value. For example, median represents the 50th percentile. Similarly, we can have 0th percentile representing the minimum and 100th percentile representing the maximum of all data points. Percentile gives the relative position of a particular value within the dataset. It also helps in comparing the data sets which have different means and deviations. Each dataset has three quartiles, which are the percentiles that divide the dataset into four parts: The first quartile (Q1) is the sample 25th percentile. It divides roughly 25% of the smallest items from the rest of the dataset. The second quartile Q2) is the sample 50th percentile or the median. Approximately 25% of the items lie between the first and second quartiles and another 25% between the second and third quartiles. The third quartile (Q3) is the sample 75th percentile. It divides roughly 25% of the largest items from the rest of the dataset. Budget = Movies['Budget_million$'] #via Numpy: p10 = np.percentile(Budget, 10) #returns the 10th percentile print(\"The 10th percentile of the budget of the Top10 highest-grossing films is \",p10) p4070 = np.percentile(Budget, [40,70]) #returns the 40th and 70th percentile print(\"The 40th and 70th percentile of the budget of the Top10 highest-grossing films are \",p4070) #via Pandas: p10n = Budget.quantile(0.10) #returns the 10th percentile - notice the difference from Numpy print(\"The 10th percentile of the budget of the Top10 highest-grossing films is \",p10n) #via Statistics: Qs = statistics.quantiles(Budget, n=4, method='inclusive') #The parameter n defines the number of resulting equal-probability percentiles: #n=4 returns the quartiles | n=2 returns the median print(\"The quartiles of the budget of the Top10 highest-grossing films is \",Qs) The 10th percentile of the budget of the Top10 highest-grossing films is 7.77 The 40th and 70th percentile of the budget of the Top10 highest-grossing films are [ 11. 211.1] The 10th percentile of the budget of the Top10 highest-grossing films is 7.77 The quartiles of the budget of the Top10 highest-grossing films is [10.625, 12.0, 227.75]","title":"Percentiles and Quartiles"},{"location":"1-Lessons/Lesson12/lesson11/#interquartile-range-iqr","text":"IQR is the difference between the third quartile and the first quartile (Q3-Q1). The interquartile range is a better option than range because it is not affected by outliers. It removes the outliers by just focusing on the distance within the middle 50% of the data. Budget = Movies['Budget_million$'] #via Numpy: IQR1 = np.percentile(Budget, 75) -np.percentile(Budget, 25) #returns the IQR = Q3-Q1 = P75-P25 print(\"The IQR of the budget of the Top10 highest-grossing films is \",IQR1) #via scipy.stats: IQR2 = scipy.stats.iqr(Budget) #returns the IQR- Can be used for other percentile differences as well >> iqr(object, rng=(p1, p2)) print(\"The IQR of the budget of the Top10 highest-grossing films is \",IQR2) The IQR of the budget of the Top10 highest-grossing films is 217.125 The IQR of the budget of the Top10 highest-grossing films is 217.125","title":"InterQuartile Range (IQR)"},{"location":"1-Lessons/Lesson12/lesson11/#the-five-number-summary","text":"A five-number summary is especially useful in descriptive analyses or during the preliminary investigation of a large data set. A summary consists of five values: the most extreme values in the data set (the maximum and minimum values), the lower and upper quartiles, and the median. Five-number summary can be used to describe any data distribution. Boxplots are extremely useful graphical representation of the 5-number summary that we will discuss later. Budget = Movies['Budget_million$'] Budget.describe() #Remember this jewel from Pandas? -It directly return the 5-number summary AND MORE! count 10.000000 mean 115.660000 std 142.739991 min 3.900000 25% 10.625000 50% 12.000000 75% 227.750000 max 356.000000 Name: Budget_million$, dtype: float64 Boxplots are extremely useful graphical representation of the 5-number summary. It can show the range, interquartile range, median, mode, outliers, and all quartiles. import matplotlib.pyplot as plt #Required for the plot gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202,5000] #same data + an outlier: 5000 fig = plt.figure(figsize =(7, 5)) plt.boxplot(gross,medianprops={'linewidth': 1, 'color': 'purple'}) plt.show() To read more about the 5-number summary, check these out: - \"Find a Five-Number Summary in Statistics: Easy Steps\" , available @ https://www.statisticshowto.com/how-to-find-a-five-number-summary-in-statistics/ \"The Five-Number Summary\" , available @ https://www.purplemath.com/modules/boxwhisk2.htm \"What Is the 5 Number Summary?\" by Courtney Taylor , available @ https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/","title":"The Five-number Summary"},{"location":"1-Lessons/Lesson12/lesson11/#variance","text":"The sample variance quantifies the spread of the data. It shows numerically how far the data points are from the mean. The observations may or may not be meaningful if observations in data sets are highly spread. Let's calculate the variance for budget of the Top10 highest-grossing films. Note that if we are working with the entire population (and not the sample), the denominator should be \"n\" instead of \"n-1\". Note that if we are working with the entire population (and not the sample), the denominator should be \"n\" instead of \"n-1\". # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var1 = sum((item - mean)**2 for item in budget) / (n - 1) print(\"The variance of the budget of the Top10 highest-grossing films is \",var1) # via the Statistics library: Budget = Movies['Budget_million$'] var2 = statistics.variance(Budget) print(\"The variance of the budget of the Top10 highest-grossing films is \",var2) The variance of the budget of the Top10 highest-grossing films is 20374.70488888889 The variance of the budget of the Top10 highest-grossing films is 20374.70488888889","title":"Variance"},{"location":"1-Lessons/Lesson12/lesson11/#standard-deviation","text":"The sample standard deviation is another measure of data spread. It\u2019s connected to the sample variance, as standard deviation, \ud835\udc60, is the positive square root of the sample variance. The standard deviation is often more convenient than the variance because it has the same unit as the data points. # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var = sum((item - mean)**2 for item in budget) / (n - 1) sd1 = var**0.5 print(\"The standard deviation of the budget of the Top10 highest-grossing films is \",sd1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] sd2 = statistics.stdev(Budget) print(\"The standard deviation of the budget of the Top10 highest-grossing films is \",sd2,\"million USD\") The standard deviation of the budget of the Top10 highest-grossing films is 142.73999050332353 million USD The standard deviation of the budget of the Top10 highest-grossing films is 142.73999050332353 million USD","title":"Standard Deviation"},{"location":"1-Lessons/Lesson12/lesson11/#skewness","text":"The sample skewness measures the asymmetry of a data sample. There are several mathematical definitions of skewness. The Fisher-Pearson standardized moment coefficient is calculated by using mean, median and standard deviation of the data. Usually, negative skewness values indicate that there\u2019s a dominant tail on the left side. Positive skewness values correspond to a longer or fatter tail on the right side. If the skewness is close to 0 (for example, between \u22120.5 and 0.5), then the dataset is considered quite symmetrical. # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var = sum((item - mean)**2 for item in budget) / (n - 1) std = var**0.5 skew1 = (sum((item - mean)**3 for item in budget) * n / ((n - 1) * (n - 2) * std**3)) print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew1) # via the scipy.stats library: Budget = Movies['Budget_million$'] skew2 = scipy.stats.skew(Budget, bias=False) print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew2) # via the Pandas library: Budget = Movies['Budget_million$'] skew3 = Budget.skew() print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew3) The skewness of the budget of the Top10 highest-grossing films is 0.7636547490528159 The skewness of the budget of the Top10 highest-grossing films is 0.763654749052816 The skewness of the budget of the Top10 highest-grossing films is 0.763654749052816","title":"Skewness"},{"location":"1-Lessons/Lesson12/lesson11/#kurtosis","text":"Kurtosis describes the peakedness of the distribution. In other words, Kurtosis identifies whether the tails of a given distribution contain extreme values. While Skewness essentially measures the symmetry of the distribution, kurtosis determines the heaviness of the distribution tails. If the distribution is tall and thin it is called a leptokurtic distribution. Values in a leptokurtic distribution are near the mean or at the extremes. A flat distribution where the values are moderately spread out (i.e., unlike leptokurtic) is called platykurtic distribution. A distribution whose shape is in between a leptokurtic distribution and a platykurtic distribution is called a mesokurtic distribution. # via the scipy.stats library: Budget = Movies['Budget_million$'] Kurt = scipy.stats.kurtosis(Budget) print(\"The kurtosis of the budget of the Top10 highest-grossing films is \",Kurt) #a platykurtic distribution | the tails are heavy The kurtosis of the budget of the Top10 highest-grossing films is -1.3110307923262225 To read more about skewness and kurtosis, check these out: - \"Measures of Skewness and Kurtosis\" , available @ https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm#:~:text=Skewness%20is%20a%20measure%20of,relative%20to%20a%20normal%20distribution. \"Are the Skewness and Kurtosis Useful Statistics?\" , available @ https://www.spcforexcel.com/knowledge/basic-statistics/are-skewness-and-kurtosis-useful-statistics \"Skew and Kurtosis: 2 Important Statistics terms you need to know in Data Science\" by Diva Dugar , available @ https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa \"Measures of Shape: Skewness and Kurtosis\" by Stan Brown , available @ https://brownmath.com/stat/shape.htm","title":"Kurtosis"},{"location":"1-Lessons/Lesson13/lesson12/","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 2 Mar 2021 Lesson 13 : Data Modeling with Special Functions (Probability Distributions) Objectives To understand the fundamental concepts involved in representing a data collection; Interpolation Extrapolation Concept of a fitting function Introduce select special functions Normal distribution function Gamma distribution function Extreme value distribution function Pearson Type 3 distribution function Computational Thinking Concepts The CT concepts include: Decomposition => Assert data are drawn from some process that is functionally explainable Abstraction => Represent data behavior with a function Algorithm Design => Use the function to predict \"new\" values of observations Explaining Data Recall our speed and time example, repeated below. # Our data time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Our model def poly1(b0,b1,x): # return y = b0 + b1*x poly1=b0+b1*x return(poly1) # Our plotting function import matplotlib.pyplot as plt def make2plot(listx1,listy1,listx2,listy2,strlablx,strlably,strtitle): mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(listx1,listy1, c='red', marker='v',linewidth=0) # basic data plot plt.plot(listx2,listy2, c='blue',linewidth=1) # basic model plot plt.xlabel(strlablx) plt.ylabel(strlably) plt.legend(['Data','Model'])# modify for argument insertion plt.title(strtitle) plt.show() # Our \"fitting\" process intercept = 5.0 slope = 3.0 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly1(intercept,slope,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') We are limited in the ability to fit the data because our representation function is limited to a straight line, now lets make a quadratic a possible model option. # Our new model def poly2(b0,b1,b2,x): # return y = b0 + b1*x poly2=b0+(b1+b2*x)*x # faster than b0 + b1*x + b2*x**2 return(poly2) Now try fitting and plotting using our new model and should get indetical result, then we can explore using the new parameter b2 # Our \"fitting\" process intercept = 5.0 # set to 0.0 slope = 3.0 # adjust to 2.0 curvature = 0.0 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') # Our \"fitting\" process intercept = 0.0 # set to 0.0 slope = 2.0 # adjust to 2.0 curvature = 0.9 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') Now which \"model\" is more useful for these data? Explain your reasoning. Lets take a look over the process we just implemented Prepare our data series Select a function type as the data model (in this case polynomials of order 1 and 2) Use a plotting tool to plot observed data (red) and our model (blue) Adjust model parameters (b0,b1,b2, ...) to get the blue model to pass through the red dots as best we can. That's it, later we will explore ways to quantify the fit, which will help us choose a data model when multiple models appear good. ## Now lets apply our tools to different data, first we will read data from a file amatrix = [] xvalue = [] yvalue = [] rowNumA = 0 file1 = open(\"MyFile.txt\", \"r\") # get the data for line in file1: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 file1.close() # Disconnect the file for i in range(len(amatrix)): # deconstruct the list, rename each column xvalue.append(amatrix[i][0]) yvalue.append(amatrix[i][1]) make2plot(xvalue,yvalue,[],[],'x-value','y-value','EDA Plot of model and observations') # Our \"fitting\" process intercept = 0.0 # 0.0 slope = 0.01 # 0.018 curvature = 1e-09 # -0.0001 modelY = [] # empty list for i in range(len(xvalue)): modelY.append(poly2(intercept,slope,curvature,xvalue[i])) # Plotting results make2plot(xvalue,yvalue,xvalue,modelY,'x-value','y-value','EDA Plot of model and observations') Lets build a different type of data model, here we will use a special function called the normal distribution function. A useful notation using the Normal density function as an example is: \\text{pdf(x)} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\times exp (-\\frac{(x-\\mu)^2}{2 \\sigma^2}) In the function, x is the random variable, \\mu is the population mean and \\sigma^2 is the population variance. These parameters ( \\mu , and \\sigma^2 ) play the same role that b_0,b_1,b_2, \\dots play in our polynomial model - they simply adjust shape of the model. Often we don't actually know the population values so we estimate them from the collection of observations, in this context these are called the sample mean and variance. Computation of the sample values is done using methods described in the lesson on descriptive statistics. The integral of the \\text{pdf(x)} from -\\infty~to ~ X , produces a result called the cumulative distribution function. The value X is not a random variable, but the integral value of the probability of the random variable x being less than or equal to X . A useful notation using the Normal distribution as an example is: F(X) = \\int_{-\\infty}^X{\\frac{1}{\\sigma \\sqrt{2\\pi}} \\times exp (-\\frac{(x-\\mu)^2}{2 \\sigma^2}) dx} For the Normal distribution the integral is a special function called the Error function and can be written as: F(X) = \\frac{1}{2} \\cdot (1+erf(\\frac{(X-\\mu)}{\\sqrt{2} \\sigma})) We will use these concepts to build an alternative to poly1 and poly2 as data models. Normal Distribution Model (Using Math Package) Here we will build a normal distribution model, essentially the functions for the above equations, and then will plot them. Then we will sample from a list of numbers from 1 to 100 and see if the data model is representative of the sample. import math def normdensity(x,mu,sigma): weight = 1.0 /(sigma * math.sqrt(2.0*math.pi)) argument = ((x - mu)**2)/(2.0*sigma**2) normdensity = weight*math.exp(-1.0*argument) return normdensity def normdist(x,mu,sigma): argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist # Our \"fitting\" process mu = 50.0 # 50.0 sigma = 10 # 850.01**0.5 modelY = [] # empty list for i in range(len(xvalue)): modelY.append(normdist(mu,sigma,xvalue[i])) # Plotting results make2plot(xvalue,yvalue,xvalue,modelY,'x-value','y-value','EDA Plot of model and observations') --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-1-ac71c289fea2> in <module> 3 sigma = 10 # 850.01**0.5 4 modelY = [] # empty list ----> 5 for i in range(len(xvalue)): 6 modelY.append(normdist(mu,sigma,xvalue[i])) 7 # Plotting results NameError: name 'xvalue' is not defined Interpolation Lets return to our time/speed model and estimate the speed at 4.5 seconds # Our \"fitting\" process intercept = 0.0 # set to 0.0 slope = 2.0 # adjust to 2.0 curvature = 0.9 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') print('Speed estimate at time ',4.5,'is ',poly2(intercept,slope,curvature,4.5)) Speed estimate at time 4.5 is 27.224999999999998 Extrapolation print('Speed estimate at time ',9.5,'is ',poly2(intercept,slope,curvature,9.5)) Speed estimate at time 9.5 is 100.22500000000001 Probability Estimation Modeling Probability estimation modeling is the use of probability distributions (population data models) to model or explain behavior in observed (sample data) values. Once a particular distribution is selected, then the concept of risk (probability) can be explored for events of varying magnitudes. Two important \u201cextremes\u201d in engineering: Uncommon (rare) events (floods, nuclear plant explosions, etc.) Common, almost predictable events (routine discharges, traffic accidents at a dangerous intersection, network failure on a due date, etc.) The probability distribution is just a model of the data, like a trend line for deterministic behavior; different distributions have different shapes, and domains and can explain certain types of observations better than others. Some Useful Distributions (data models) include: Normal LogNormal Gamma Weibull Extreme Value (Gumbell) Beta There are many more; they all have the common property that they integrate to unity on the domain -\\infty~to ~ \\infty . The probability distributions (models) are often expressed as a density function or a cumulative distribution function. # Standard Normal mu = 0 sigma = 1 x = [] ypdf = [] ycdf = [] xlow = -10 xhigh = 10 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdensity(xlow + i*xstep,mu,sigma) ypdf.append(yvalue) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) #x #ypdf #ycdf Make the plot below, nothing too special just yet. Plots of the density (in blue) and cumulative density (probability) in red. import matplotlib.pyplot # the python plotting library myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.plot(x, ypdf, color ='blue') matplotlib.pyplot.plot(x, ycdf, color ='red') matplotlib.pyplot.xlabel(\"Value of RV\") matplotlib.pyplot.ylabel(\"Density or Quantile Value\") matplotlib.pyplot.title(\"Normal Distribution Data Model\") matplotlib.pyplot.show() Exceedence Probability The purpose of distributions is to model data and allow us to estimate an answer to the question, what is the probability that we will observe a value of the random variable less than or equal to some sentinel value. A common way to plot the quantile function is with accumulated probability on the horizontal axis, and random variable value on the vertical axis. Consider the figure below; The RV Value is about 50,000 indicated by the horizontal magenta line. The blue curve is some data model, for instance one of our distributions below. The accumulated probability value at 50,000 is 0.1 or roughly 10% chance, but we also have to stipulate whether we are interested in less than or greater than. In the figure shown, P(x <= 50,000)~ =~1.00~-~0.1~= 0.9~or~90\\% and is a non-exceedence probability. In words we would state \"The probability of observing a value less than or equal to 50,000 is 90%\" the other side of the vertical line is the exceedence probability; in the figure P(x > 50,000)~=~0.1~or~10\\% . In words we would state \"The probability of observing a value equal to or greater than 50,000 is 10%.\" In risk analysis the sense of the probability is easily confusing, so when you can - make a plot. Another way to look at the situation is to simply realize that the blue curve is the quantile function F(X) with X plotted on the vertical axis, and F(X) plotted on the horizontal axis. Now lets put these ideas to use. We will sample from the population of integers from 0 to 100, with replacement. Any single pull from the population is equally likely. Lets take 25 samples (about 1/4 of the total population - usually we dont know the size of the population). import numpy population = [] for i in range(0,101,1): population.append(i) sample = numpy.random.choice(population,25) # lets get some statistics sample_mean = sample.mean() sample_variance = sample.std()**2 # sort the sample in place! sample.sort() # built a relative frequency approximation to probability, assume each pick is equally likely weibull_pp = [] for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Density or Quantile Value\") matplotlib.pyplot.title(\"Normal Distribution Data Model\") matplotlib.pyplot.show() What a horrible plot, but lets now use the sample statistics to \"fit\" the data model (red) to the observations (blue). Notice we have already rotated the axes so this plot and ones that follow are structured like the \"Exceedence\" plot above. # Fitted Model mu = sample_mean sigma = math.sqrt(sample_variance) x = [] ycdf = [] xlow = 0 xhigh = 100 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Quantile Value\") mytitle = \"Normal Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() popmean = numpy.array(population).mean() popvar = numpy.array(population).std()**2 # Fitted Model mu = popmean sigma = math.sqrt(popvar) x = [] ycdf = [] xlow = 0 xhigh = 100 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Quantile Value\") mytitle = \"Normal Distribution Data Model Population mean = : \" + str(popmean)+ \" Population variance =:\" + str(popvar) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Some observations are in order: The population is a uniformly distributed collection. By random sampling, and keeping the sample size small, the sample distribution appears approximately normal. Real things of engineering interest are not always bounded as shown here, the choice of the Weibull plotting position is not arbitrary. The blue dot scatterplot in practice is called the empirical distribution function, or empirical quantile function. Now we will apply these ideas to some realistic data. Beargrass Creek The file beargrass.txt contains annual peak flows for Beargrass Creek. The year is a water year, so the peaks occur on different days in each year; thus it is not a time series. Let's examine the data and see how well a Normal distribution data model fits, then estimate from the distribution the peak magnitude with exceedence probability 0.01 (1%-chance that will observe a value equal to or greater). import pandas beargrass = pandas.read_csv('beargrass.txt') #Reading a .csv file beargrass.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Peak 0 1945 1810 1 1946 791 2 1947 839 3 1948 1750 4 1949 898 # beargrass.plot() Now we will just copy code (the miracle of cut-n-paste!) sample = beargrass['Peak'].tolist() # put the peaks into a list sample_mean = numpy.array(sample).mean() sample_variance = numpy.array(sample).std()**2 sample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(sample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Normal Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() beargrass['Peak'].describe() count 31.000000 mean 1599.258065 std 1006.239500 min 707.000000 25% 908.000000 50% 1250.000000 75% 1945.000000 max 5200.000000 Name: Peak, dtype: float64 A 1% chance exceedence is on the right side of the chart, it is the compliment of 99% non-exceedence, in terms of our quantile function we want to find the value X that returns a quantile of 0.99. myguess = 6000 print(mu,sigma) print(normdist(myguess,mu,sigma)) 1599.258064516129 989.8767915427474 0.9999956206542673 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): mu = 1599.258064516129 sigma = 989.8767915427474 quantile = 0.99999 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) 5820.974479887303 So a peak discharge of 4000 or so is expected to be observed with 1% chance, notice we took the value from the fitted distribution, not the empirical set. As an observation, the Normal model is not a very good data model for these observations. Log-Normal Another data model we can try is log-normal, where we stipulate that the logarithms of the observations are normal. The scripts are practically the same, but there is an inverse transformation required to recover original value scale. Again we will use Beargrass creek. def loggit(x): # A prototype function to log transform x return(math.log(x)) logsample = beargrass['Peak'].apply(loggit).tolist() # put the peaks into a list sample_mean = numpy.array(logsample).mean() sample_variance = numpy.array(logsample).std()**2 logsample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model in Log Space sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(logsample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, logsample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Normal Data Model log sample mean = : \" + str(sample_mean)+ \" log sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() The plot doesn't look too bad, but we are in log-space, which is hard to interpret, so we will transform back to arithmetic space def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) sample = beargrass['Peak'].tolist() # pull original list sample.sort() # sort in place ################ mu = sample_mean # Fitted Model in Log Space sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(logsample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(antiloggit(xlow + i*xstep)) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Normal Data Model sample log mean = : \" + str((sample_mean))+ \" sample log variance =:\" + str((sample_variance)) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Visually a better data model, now lets determine the 1% chance value. myguess = 4440 print(mu,sigma) print(normdist(loggit(myguess),mu,sigma)) # mu, sigma already in log space - convert myguess 7.23730905616488 0.4984855728993489 0.9900772507418303 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): mu = 7.23730905616488 sigma = 0.4984855728993489 quantile = 0.99 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) 4433.567789173268 Now we have a decent method, we should put stuff into functions to keep code concise, lets examine a couple more data models Gumbell (Double Exponential) Distribution The Gumbell is also called the Extreme-Value Type I distribution, the density and quantile function are: \\text{pdf(x)} = \\frac{1}{\\beta} \\cdot exp [-\\frac{(x-\\alpha)}{\\beta} - exp (-\\frac{(x-\\alpha)}{\\beta}) ] F(X) = \\int_{-\\infty}^X{\\frac{1}{\\beta} \\cdot exp [-\\frac{(x-\\alpha)}{\\beta} - exp (-\\frac{(x-\\alpha)}{\\beta}) ] dx} = exp [- exp (-\\frac{(X-\\alpha)}{\\beta})] The distribution has two parameters, \\alpha and \\beta , which in some sense play the same role as mean and variance. Lets modify our scripts further to see how this data model performs on the Bearcreek data. Of course we need a way to estimate the parameters, a good approximation can be obtained using: \\alpha = \\mu \\cdot \\frac{\\sqrt{6}}{\\pi} and \\beta = 0.45 \\cdot \\sigma where \\mu and \\sigma^2 are the sample mean and variance. def ev1dist(x,alpha,beta): argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist Now literally substitute into our prior code! sample = beargrass['Peak'].tolist() # put the peaks into a list sample_mean = numpy.array(sample).mean() sample_variance = numpy.array(sample).std()**2 alpha_mom = sample_mean*math.sqrt(6)/math.pi beta_mom = math.sqrt(sample_variance)*0.45 sample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(sample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = ev1dist(xlow + i*xstep,alpha_mom,beta_mom) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Extreme Value Type 1 Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Again a so-so visual fit. To find the 1% chance value myguess = 3300 print(alpha_mom,beta_mom) print(ev1dist(myguess,alpha_mom,beta_mom)) # 1246.9363972503857 445.4445561942363 0.990087892543188 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): alpha = 1246.9363972503857 beta = 445.4445561942363 quantile = 0.99 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) 3296.0478279991366 Gamma Distribution (as Pearson Type 3) One last data model to consider is one that is specifically stipulated for use by federal agencies for probability estimation of extreme hydrologic events. The data model ia called the Log-Pearson Type III distribution, its actually a specific case of a Gamma distrubution. This example we will dispense with tyring to build it in python primative, and just use a package - the density function is not all that hard, but the quantile function is elaborate. Learn more at http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/3-Readings/NumericalRecipesinF77.pdf (in particular around Page 276) As usual, lets let Google do some work for us, using the search term \"gamma quantile function; scipy\" we get to this nice blog entry https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html which is a good start. A Pearson Type III data model has the following density function: f(x|\\tau,\\alpha,\\beta) = \\frac{(\\frac{x-\\tau}{\\beta})^{\\alpha -1}\\cdot exp( - \\frac{x-\\tau}{\\beta})}{|\\beta| \\Gamma(\\alpha)} If we make some substitutions: \\lambda = \\frac{1}{\\beta} ; \\hat{x} = x -\\tau then the density function is f(\\hat{x}) = \\frac{ 1}{\\Gamma(\\alpha)} (\\lambda \\hat{x})^{\\alpha -1}\\cdot exp( - \\lambda \\hat{x} ) which is now a one parameter Gamma density function just like the example in the link. Reading a little from http://atomickitty.ddns.net/documents/university-courses/ce-5361-swhydrology/1-Lessons.src/Lesson22/AdditionalReading/Bulletin17C-tm4b5-draft-ACWI-17Jan2018.pdf we can relate the transformations to descriptive statistics (shown below without explaination) as: \\mu = \\text{sample mean} , \\sigma = \\text{sample standard deviation} , \\gamma = \\text{sample skew coefficient} = (\\frac{n}{\\sigma^3(n-1)(n-2)})\\sum_{i=1}^n(x_i - \\mu)^3 \\alpha = \\frac{4}{\\gamma^2} \\beta = sign(\\gamma)\\sqrt{\\frac{\\sigma^2}{\\alpha}} \\tau = \\mu - \\alpha \\cdot \\beta So we have a bit of work to do. The name of the functions in scipy we are interested in are gamma.pdf(x,a) and gamma.cdf(x,a) So lets build a tool to generate a Log-Pearson Type III data model, then apply it to Beargrass Creek. We will use a lot of glue here. First load in dependencies, and define support functions we will need import scipy.stats # import scipy stats package import math # import math package import numpy # import numpy package # log and antilog def loggit(x): # A prototype function to log transform x return(math.log(x)) def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) def weibull_pp(sample): # plotting position function # returns a list of plotting positions; sample must be a numeric list weibull_pp = [] # null list to return after fill sample.sort() # sort the sample list in place for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) return weibull_pp Then the gamma distribution from scipy, modified for our type of inputs. def gammacdf(x,tau,alpha,beta): # Gamma Cumulative Density function - with three parameter to one parameter convert xhat = x-tau lamda = 1.0/beta gammacdf = scipy.stats.gamma.cdf(lamda*xhat, alpha) return gammacdf Then load in the data from the data frame, log transform and generate descriptive statistics. #sample = beargrass['Peak'].tolist() # put the peaks into a list sample = beargrass['Peak'].apply(loggit).tolist() # put the log peaks into a list sample_mean = numpy.array(sample).mean() sample_stdev = numpy.array(sample).std() sample_skew = 3.0 # scipy.stats.skew(sample) sample_alpha = 4.0/(sample_skew**2) sample_beta = numpy.sign(sample_skew)*math.sqrt(sample_stdev**2/sample_alpha) sample_tau = sample_mean - sample_alpha*sample_beta Now generate plotting positions for the sample observations plotting = weibull_pp(sample) Now generate values for the data model (for plotting our red line \"fit\"), set limits to be a little beyond the sample range. x = []; ycdf = [] xlow = (0.9*min(sample)); xhigh = (1.1*max(sample)) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = gammacdf(xlow + i*xstep,sample_tau,sample_alpha,sample_beta) ycdf.append(yvalue) Now reverse transform back to native scale, and plot the sample values vs plotting position in blue, and the data model in red # reverse transform the peaks, and the data model peaks for i in range(len(sample)): sample[i] = antiloggit(sample[i]) for i in range(len(x)): x[i] = antiloggit(x[i]) myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(plotting, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Pearson Type III Distribution Data Model\\n \" mytitle += \"Mean = \" + str(antiloggit(sample_mean)) + \"\\n\" mytitle += \"SD = \" + str(antiloggit(sample_stdev)) + \"\\n\" mytitle += \"Skew = \" + str(antiloggit(sample_skew)) + \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() And as before lets find the value that retruns the 99% quantile - we will just use the newton method above. First recover the required model parameters. Then we will paste these into the f(x) function for the Newton's method. print(sample_tau) print(sample_alpha) print(sample_beta) 4.714701566527543 25.609091660104536 0.0985043719284747 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): sample_tau = 5.976005311346212 sample_alpha = 6.402272915026134 sample_beta = 0.1970087438569494 quantile = 0.9900 argument = loggit(x) gammavalue = gammacdf(argument,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile myguess = 5000 print(newton(f, myguess)) 5856.10913158364 Trust, but verify! round(gammacdf(loggit(5856.109),sample_tau,sample_alpha,sample_beta),4) 0.9943 Now lets summarize our efforts regarding Beargrass Creek annual peaks and probabilities anticipated. Data Model 99% Peak Flow Remarks Normal 3902 so-so visual fit Log-Normal 4433 better visual fit Gumbell 3296 better visual fit Log-Pearson III 5856 best (of the set) visual fit At this point, now we have to choose our model and then can investigate different questions. So using LP3 as our favorite, lets now determine anticipated flow values for different probabilities (from the data model) - easy enought to just change the quantile value and rerun the newtons optimizer, for example: Exceedence Probability Flow Value Remarks 25% 968 First Quartile Divider 50% 1302 Median, and Second Quartile Divider 75% 1860 3rd Quartile Divider 90% 2706 10% chance of greater value 99% 5856 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% 9420 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% 11455 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): sample_tau = 5.976005311346212 sample_alpha = 6.402272915026134 sample_beta = 0.1970087438569494 quantile = 0.50 argument = loggit(x) gammavalue = gammacdf(argument,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile myguess = 1000 print(newton(f, myguess)) 1302.814639184079 References: Jamie Chan (2014) Learn Python in One Day and Learn It Well. LCF Publishing. Kindle Edition. http://www.learncodingfast.com/python Grus, Joel. Data Science from Scratch: First Principles with Python. O'Reilly Media. Kindle Edition. (http://safaribooksonline.com) Christian, B, and Griffiths Tom (2016) Algorithms to live by: The computer science of human decisions. Henry Holt and Company, ISBN 9781627790369 (hardcover)|ISBN 9781627790376 (electronic book) https://www.amazon.com/Distributional-Statistics-Environment-Statistical-Computing/dp/1463508417 England, J.F. Jr., Cohn, T.A., Faber, B.A., Stedinger, J.R., Thomas Jr., W.O., Veilleux, A.G., Kiang, J.E., and Mason, R.R.Jr., 2018, Guidelines for Determining Flood Flow Frequency\u2014Bulletin 17C: U.S. Geological Survey Techniques andMethods, book 4, chap. B5, 146 p., https://doi.org/10.3133/tm4B5 https://www.astroml.org/book_figures/chapter3/fig_gamma_distribution.html https://www.inferentialthinking.com/chapters/10/Sampling_and_Empirical_Distributions.html https://www.inferentialthinking.com/chapters/15/Prediction.html","title":"Non-Linear Equation System"},{"location":"1-Lessons/Lesson13/lesson12/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 2 Mar 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson13/lesson12/#lesson-13-data-modeling-with-special-functions-probability-distributions","text":"","title":"Lesson 13 : Data Modeling with Special Functions (Probability Distributions)"},{"location":"1-Lessons/Lesson13/lesson12/#objectives","text":"To understand the fundamental concepts involved in representing a data collection; Interpolation Extrapolation Concept of a fitting function Introduce select special functions Normal distribution function Gamma distribution function Extreme value distribution function Pearson Type 3 distribution function","title":"Objectives"},{"location":"1-Lessons/Lesson13/lesson12/#computational-thinking-concepts","text":"The CT concepts include: Decomposition => Assert data are drawn from some process that is functionally explainable Abstraction => Represent data behavior with a function Algorithm Design => Use the function to predict \"new\" values of observations","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson13/lesson12/#explaining-data","text":"Recall our speed and time example, repeated below. # Our data time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Our model def poly1(b0,b1,x): # return y = b0 + b1*x poly1=b0+b1*x return(poly1) # Our plotting function import matplotlib.pyplot as plt def make2plot(listx1,listy1,listx2,listy2,strlablx,strlably,strtitle): mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(listx1,listy1, c='red', marker='v',linewidth=0) # basic data plot plt.plot(listx2,listy2, c='blue',linewidth=1) # basic model plot plt.xlabel(strlablx) plt.ylabel(strlably) plt.legend(['Data','Model'])# modify for argument insertion plt.title(strtitle) plt.show() # Our \"fitting\" process intercept = 5.0 slope = 3.0 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly1(intercept,slope,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') We are limited in the ability to fit the data because our representation function is limited to a straight line, now lets make a quadratic a possible model option. # Our new model def poly2(b0,b1,b2,x): # return y = b0 + b1*x poly2=b0+(b1+b2*x)*x # faster than b0 + b1*x + b2*x**2 return(poly2) Now try fitting and plotting using our new model and should get indetical result, then we can explore using the new parameter b2 # Our \"fitting\" process intercept = 5.0 # set to 0.0 slope = 3.0 # adjust to 2.0 curvature = 0.0 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') # Our \"fitting\" process intercept = 0.0 # set to 0.0 slope = 2.0 # adjust to 2.0 curvature = 0.9 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') Now which \"model\" is more useful for these data? Explain your reasoning. Lets take a look over the process we just implemented Prepare our data series Select a function type as the data model (in this case polynomials of order 1 and 2) Use a plotting tool to plot observed data (red) and our model (blue) Adjust model parameters (b0,b1,b2, ...) to get the blue model to pass through the red dots as best we can. That's it, later we will explore ways to quantify the fit, which will help us choose a data model when multiple models appear good. ## Now lets apply our tools to different data, first we will read data from a file amatrix = [] xvalue = [] yvalue = [] rowNumA = 0 file1 = open(\"MyFile.txt\", \"r\") # get the data for line in file1: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 file1.close() # Disconnect the file for i in range(len(amatrix)): # deconstruct the list, rename each column xvalue.append(amatrix[i][0]) yvalue.append(amatrix[i][1]) make2plot(xvalue,yvalue,[],[],'x-value','y-value','EDA Plot of model and observations') # Our \"fitting\" process intercept = 0.0 # 0.0 slope = 0.01 # 0.018 curvature = 1e-09 # -0.0001 modelY = [] # empty list for i in range(len(xvalue)): modelY.append(poly2(intercept,slope,curvature,xvalue[i])) # Plotting results make2plot(xvalue,yvalue,xvalue,modelY,'x-value','y-value','EDA Plot of model and observations') Lets build a different type of data model, here we will use a special function called the normal distribution function. A useful notation using the Normal density function as an example is: \\text{pdf(x)} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\times exp (-\\frac{(x-\\mu)^2}{2 \\sigma^2}) In the function, x is the random variable, \\mu is the population mean and \\sigma^2 is the population variance. These parameters ( \\mu , and \\sigma^2 ) play the same role that b_0,b_1,b_2, \\dots play in our polynomial model - they simply adjust shape of the model. Often we don't actually know the population values so we estimate them from the collection of observations, in this context these are called the sample mean and variance. Computation of the sample values is done using methods described in the lesson on descriptive statistics. The integral of the \\text{pdf(x)} from -\\infty~to ~ X , produces a result called the cumulative distribution function. The value X is not a random variable, but the integral value of the probability of the random variable x being less than or equal to X . A useful notation using the Normal distribution as an example is: F(X) = \\int_{-\\infty}^X{\\frac{1}{\\sigma \\sqrt{2\\pi}} \\times exp (-\\frac{(x-\\mu)^2}{2 \\sigma^2}) dx} For the Normal distribution the integral is a special function called the Error function and can be written as: F(X) = \\frac{1}{2} \\cdot (1+erf(\\frac{(X-\\mu)}{\\sqrt{2} \\sigma})) We will use these concepts to build an alternative to poly1 and poly2 as data models.","title":"Explaining Data"},{"location":"1-Lessons/Lesson13/lesson12/#normal-distribution-model-using-math-package","text":"Here we will build a normal distribution model, essentially the functions for the above equations, and then will plot them. Then we will sample from a list of numbers from 1 to 100 and see if the data model is representative of the sample. import math def normdensity(x,mu,sigma): weight = 1.0 /(sigma * math.sqrt(2.0*math.pi)) argument = ((x - mu)**2)/(2.0*sigma**2) normdensity = weight*math.exp(-1.0*argument) return normdensity def normdist(x,mu,sigma): argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist # Our \"fitting\" process mu = 50.0 # 50.0 sigma = 10 # 850.01**0.5 modelY = [] # empty list for i in range(len(xvalue)): modelY.append(normdist(mu,sigma,xvalue[i])) # Plotting results make2plot(xvalue,yvalue,xvalue,modelY,'x-value','y-value','EDA Plot of model and observations') --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-1-ac71c289fea2> in <module> 3 sigma = 10 # 850.01**0.5 4 modelY = [] # empty list ----> 5 for i in range(len(xvalue)): 6 modelY.append(normdist(mu,sigma,xvalue[i])) 7 # Plotting results NameError: name 'xvalue' is not defined","title":"Normal Distribution Model (Using Math Package)"},{"location":"1-Lessons/Lesson13/lesson12/#interpolation","text":"Lets return to our time/speed model and estimate the speed at 4.5 seconds # Our \"fitting\" process intercept = 0.0 # set to 0.0 slope = 2.0 # adjust to 2.0 curvature = 0.9 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') print('Speed estimate at time ',4.5,'is ',poly2(intercept,slope,curvature,4.5)) Speed estimate at time 4.5 is 27.224999999999998","title":"Interpolation"},{"location":"1-Lessons/Lesson13/lesson12/#extrapolation","text":"print('Speed estimate at time ',9.5,'is ',poly2(intercept,slope,curvature,9.5)) Speed estimate at time 9.5 is 100.22500000000001","title":"Extrapolation"},{"location":"1-Lessons/Lesson13/lesson12/#probability-estimation-modeling","text":"Probability estimation modeling is the use of probability distributions (population data models) to model or explain behavior in observed (sample data) values. Once a particular distribution is selected, then the concept of risk (probability) can be explored for events of varying magnitudes. Two important \u201cextremes\u201d in engineering: Uncommon (rare) events (floods, nuclear plant explosions, etc.) Common, almost predictable events (routine discharges, traffic accidents at a dangerous intersection, network failure on a due date, etc.) The probability distribution is just a model of the data, like a trend line for deterministic behavior; different distributions have different shapes, and domains and can explain certain types of observations better than others. Some Useful Distributions (data models) include: Normal LogNormal Gamma Weibull Extreme Value (Gumbell) Beta There are many more; they all have the common property that they integrate to unity on the domain -\\infty~to ~ \\infty . The probability distributions (models) are often expressed as a density function or a cumulative distribution function. # Standard Normal mu = 0 sigma = 1 x = [] ypdf = [] ycdf = [] xlow = -10 xhigh = 10 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdensity(xlow + i*xstep,mu,sigma) ypdf.append(yvalue) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) #x #ypdf #ycdf Make the plot below, nothing too special just yet. Plots of the density (in blue) and cumulative density (probability) in red. import matplotlib.pyplot # the python plotting library myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.plot(x, ypdf, color ='blue') matplotlib.pyplot.plot(x, ycdf, color ='red') matplotlib.pyplot.xlabel(\"Value of RV\") matplotlib.pyplot.ylabel(\"Density or Quantile Value\") matplotlib.pyplot.title(\"Normal Distribution Data Model\") matplotlib.pyplot.show()","title":"Probability Estimation Modeling"},{"location":"1-Lessons/Lesson13/lesson12/#exceedence-probability","text":"The purpose of distributions is to model data and allow us to estimate an answer to the question, what is the probability that we will observe a value of the random variable less than or equal to some sentinel value. A common way to plot the quantile function is with accumulated probability on the horizontal axis, and random variable value on the vertical axis. Consider the figure below; The RV Value is about 50,000 indicated by the horizontal magenta line. The blue curve is some data model, for instance one of our distributions below. The accumulated probability value at 50,000 is 0.1 or roughly 10% chance, but we also have to stipulate whether we are interested in less than or greater than. In the figure shown, P(x <= 50,000)~ =~1.00~-~0.1~= 0.9~or~90\\% and is a non-exceedence probability. In words we would state \"The probability of observing a value less than or equal to 50,000 is 90%\" the other side of the vertical line is the exceedence probability; in the figure P(x > 50,000)~=~0.1~or~10\\% . In words we would state \"The probability of observing a value equal to or greater than 50,000 is 10%.\" In risk analysis the sense of the probability is easily confusing, so when you can - make a plot. Another way to look at the situation is to simply realize that the blue curve is the quantile function F(X) with X plotted on the vertical axis, and F(X) plotted on the horizontal axis. Now lets put these ideas to use. We will sample from the population of integers from 0 to 100, with replacement. Any single pull from the population is equally likely. Lets take 25 samples (about 1/4 of the total population - usually we dont know the size of the population). import numpy population = [] for i in range(0,101,1): population.append(i) sample = numpy.random.choice(population,25) # lets get some statistics sample_mean = sample.mean() sample_variance = sample.std()**2 # sort the sample in place! sample.sort() # built a relative frequency approximation to probability, assume each pick is equally likely weibull_pp = [] for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Density or Quantile Value\") matplotlib.pyplot.title(\"Normal Distribution Data Model\") matplotlib.pyplot.show() What a horrible plot, but lets now use the sample statistics to \"fit\" the data model (red) to the observations (blue). Notice we have already rotated the axes so this plot and ones that follow are structured like the \"Exceedence\" plot above. # Fitted Model mu = sample_mean sigma = math.sqrt(sample_variance) x = [] ycdf = [] xlow = 0 xhigh = 100 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Quantile Value\") mytitle = \"Normal Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() popmean = numpy.array(population).mean() popvar = numpy.array(population).std()**2 # Fitted Model mu = popmean sigma = math.sqrt(popvar) x = [] ycdf = [] xlow = 0 xhigh = 100 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Quantile Value\") mytitle = \"Normal Distribution Data Model Population mean = : \" + str(popmean)+ \" Population variance =:\" + str(popvar) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Some observations are in order: The population is a uniformly distributed collection. By random sampling, and keeping the sample size small, the sample distribution appears approximately normal. Real things of engineering interest are not always bounded as shown here, the choice of the Weibull plotting position is not arbitrary. The blue dot scatterplot in practice is called the empirical distribution function, or empirical quantile function. Now we will apply these ideas to some realistic data.","title":"Exceedence Probability"},{"location":"1-Lessons/Lesson13/lesson12/#beargrass-creek","text":"The file beargrass.txt contains annual peak flows for Beargrass Creek. The year is a water year, so the peaks occur on different days in each year; thus it is not a time series. Let's examine the data and see how well a Normal distribution data model fits, then estimate from the distribution the peak magnitude with exceedence probability 0.01 (1%-chance that will observe a value equal to or greater). import pandas beargrass = pandas.read_csv('beargrass.txt') #Reading a .csv file beargrass.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Peak 0 1945 1810 1 1946 791 2 1947 839 3 1948 1750 4 1949 898 # beargrass.plot() Now we will just copy code (the miracle of cut-n-paste!) sample = beargrass['Peak'].tolist() # put the peaks into a list sample_mean = numpy.array(sample).mean() sample_variance = numpy.array(sample).std()**2 sample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(sample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Normal Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() beargrass['Peak'].describe() count 31.000000 mean 1599.258065 std 1006.239500 min 707.000000 25% 908.000000 50% 1250.000000 75% 1945.000000 max 5200.000000 Name: Peak, dtype: float64 A 1% chance exceedence is on the right side of the chart, it is the compliment of 99% non-exceedence, in terms of our quantile function we want to find the value X that returns a quantile of 0.99. myguess = 6000 print(mu,sigma) print(normdist(myguess,mu,sigma)) 1599.258064516129 989.8767915427474 0.9999956206542673 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): mu = 1599.258064516129 sigma = 989.8767915427474 quantile = 0.99999 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) 5820.974479887303 So a peak discharge of 4000 or so is expected to be observed with 1% chance, notice we took the value from the fitted distribution, not the empirical set. As an observation, the Normal model is not a very good data model for these observations.","title":"Beargrass Creek"},{"location":"1-Lessons/Lesson13/lesson12/#log-normal","text":"Another data model we can try is log-normal, where we stipulate that the logarithms of the observations are normal. The scripts are practically the same, but there is an inverse transformation required to recover original value scale. Again we will use Beargrass creek. def loggit(x): # A prototype function to log transform x return(math.log(x)) logsample = beargrass['Peak'].apply(loggit).tolist() # put the peaks into a list sample_mean = numpy.array(logsample).mean() sample_variance = numpy.array(logsample).std()**2 logsample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model in Log Space sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(logsample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, logsample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Normal Data Model log sample mean = : \" + str(sample_mean)+ \" log sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() The plot doesn't look too bad, but we are in log-space, which is hard to interpret, so we will transform back to arithmetic space def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) sample = beargrass['Peak'].tolist() # pull original list sample.sort() # sort in place ################ mu = sample_mean # Fitted Model in Log Space sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(logsample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(antiloggit(xlow + i*xstep)) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Normal Data Model sample log mean = : \" + str((sample_mean))+ \" sample log variance =:\" + str((sample_variance)) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Visually a better data model, now lets determine the 1% chance value. myguess = 4440 print(mu,sigma) print(normdist(loggit(myguess),mu,sigma)) # mu, sigma already in log space - convert myguess 7.23730905616488 0.4984855728993489 0.9900772507418303 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): mu = 7.23730905616488 sigma = 0.4984855728993489 quantile = 0.99 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) 4433.567789173268 Now we have a decent method, we should put stuff into functions to keep code concise, lets examine a couple more data models","title":"Log-Normal"},{"location":"1-Lessons/Lesson13/lesson12/#gumbell-double-exponential-distribution","text":"The Gumbell is also called the Extreme-Value Type I distribution, the density and quantile function are: \\text{pdf(x)} = \\frac{1}{\\beta} \\cdot exp [-\\frac{(x-\\alpha)}{\\beta} - exp (-\\frac{(x-\\alpha)}{\\beta}) ] F(X) = \\int_{-\\infty}^X{\\frac{1}{\\beta} \\cdot exp [-\\frac{(x-\\alpha)}{\\beta} - exp (-\\frac{(x-\\alpha)}{\\beta}) ] dx} = exp [- exp (-\\frac{(X-\\alpha)}{\\beta})] The distribution has two parameters, \\alpha and \\beta , which in some sense play the same role as mean and variance. Lets modify our scripts further to see how this data model performs on the Bearcreek data. Of course we need a way to estimate the parameters, a good approximation can be obtained using: \\alpha = \\mu \\cdot \\frac{\\sqrt{6}}{\\pi} and \\beta = 0.45 \\cdot \\sigma where \\mu and \\sigma^2 are the sample mean and variance. def ev1dist(x,alpha,beta): argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist Now literally substitute into our prior code! sample = beargrass['Peak'].tolist() # put the peaks into a list sample_mean = numpy.array(sample).mean() sample_variance = numpy.array(sample).std()**2 alpha_mom = sample_mean*math.sqrt(6)/math.pi beta_mom = math.sqrt(sample_variance)*0.45 sample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(sample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = ev1dist(xlow + i*xstep,alpha_mom,beta_mom) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Extreme Value Type 1 Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Again a so-so visual fit. To find the 1% chance value myguess = 3300 print(alpha_mom,beta_mom) print(ev1dist(myguess,alpha_mom,beta_mom)) # 1246.9363972503857 445.4445561942363 0.990087892543188 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): alpha = 1246.9363972503857 beta = 445.4445561942363 quantile = 0.99 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) 3296.0478279991366","title":"Gumbell (Double Exponential) Distribution"},{"location":"1-Lessons/Lesson13/lesson12/#gamma-distribution-as-pearson-type-3","text":"One last data model to consider is one that is specifically stipulated for use by federal agencies for probability estimation of extreme hydrologic events. The data model ia called the Log-Pearson Type III distribution, its actually a specific case of a Gamma distrubution. This example we will dispense with tyring to build it in python primative, and just use a package - the density function is not all that hard, but the quantile function is elaborate. Learn more at http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/3-Readings/NumericalRecipesinF77.pdf (in particular around Page 276) As usual, lets let Google do some work for us, using the search term \"gamma quantile function; scipy\" we get to this nice blog entry https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html which is a good start. A Pearson Type III data model has the following density function: f(x|\\tau,\\alpha,\\beta) = \\frac{(\\frac{x-\\tau}{\\beta})^{\\alpha -1}\\cdot exp( - \\frac{x-\\tau}{\\beta})}{|\\beta| \\Gamma(\\alpha)} If we make some substitutions: \\lambda = \\frac{1}{\\beta} ; \\hat{x} = x -\\tau then the density function is f(\\hat{x}) = \\frac{ 1}{\\Gamma(\\alpha)} (\\lambda \\hat{x})^{\\alpha -1}\\cdot exp( - \\lambda \\hat{x} ) which is now a one parameter Gamma density function just like the example in the link. Reading a little from http://atomickitty.ddns.net/documents/university-courses/ce-5361-swhydrology/1-Lessons.src/Lesson22/AdditionalReading/Bulletin17C-tm4b5-draft-ACWI-17Jan2018.pdf we can relate the transformations to descriptive statistics (shown below without explaination) as: \\mu = \\text{sample mean} , \\sigma = \\text{sample standard deviation} , \\gamma = \\text{sample skew coefficient} = (\\frac{n}{\\sigma^3(n-1)(n-2)})\\sum_{i=1}^n(x_i - \\mu)^3 \\alpha = \\frac{4}{\\gamma^2} \\beta = sign(\\gamma)\\sqrt{\\frac{\\sigma^2}{\\alpha}} \\tau = \\mu - \\alpha \\cdot \\beta So we have a bit of work to do. The name of the functions in scipy we are interested in are gamma.pdf(x,a) and gamma.cdf(x,a) So lets build a tool to generate a Log-Pearson Type III data model, then apply it to Beargrass Creek. We will use a lot of glue here. First load in dependencies, and define support functions we will need import scipy.stats # import scipy stats package import math # import math package import numpy # import numpy package # log and antilog def loggit(x): # A prototype function to log transform x return(math.log(x)) def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) def weibull_pp(sample): # plotting position function # returns a list of plotting positions; sample must be a numeric list weibull_pp = [] # null list to return after fill sample.sort() # sort the sample list in place for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) return weibull_pp Then the gamma distribution from scipy, modified for our type of inputs. def gammacdf(x,tau,alpha,beta): # Gamma Cumulative Density function - with three parameter to one parameter convert xhat = x-tau lamda = 1.0/beta gammacdf = scipy.stats.gamma.cdf(lamda*xhat, alpha) return gammacdf Then load in the data from the data frame, log transform and generate descriptive statistics. #sample = beargrass['Peak'].tolist() # put the peaks into a list sample = beargrass['Peak'].apply(loggit).tolist() # put the log peaks into a list sample_mean = numpy.array(sample).mean() sample_stdev = numpy.array(sample).std() sample_skew = 3.0 # scipy.stats.skew(sample) sample_alpha = 4.0/(sample_skew**2) sample_beta = numpy.sign(sample_skew)*math.sqrt(sample_stdev**2/sample_alpha) sample_tau = sample_mean - sample_alpha*sample_beta Now generate plotting positions for the sample observations plotting = weibull_pp(sample) Now generate values for the data model (for plotting our red line \"fit\"), set limits to be a little beyond the sample range. x = []; ycdf = [] xlow = (0.9*min(sample)); xhigh = (1.1*max(sample)) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = gammacdf(xlow + i*xstep,sample_tau,sample_alpha,sample_beta) ycdf.append(yvalue) Now reverse transform back to native scale, and plot the sample values vs plotting position in blue, and the data model in red # reverse transform the peaks, and the data model peaks for i in range(len(sample)): sample[i] = antiloggit(sample[i]) for i in range(len(x)): x[i] = antiloggit(x[i]) myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(plotting, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Pearson Type III Distribution Data Model\\n \" mytitle += \"Mean = \" + str(antiloggit(sample_mean)) + \"\\n\" mytitle += \"SD = \" + str(antiloggit(sample_stdev)) + \"\\n\" mytitle += \"Skew = \" + str(antiloggit(sample_skew)) + \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() And as before lets find the value that retruns the 99% quantile - we will just use the newton method above. First recover the required model parameters. Then we will paste these into the f(x) function for the Newton's method. print(sample_tau) print(sample_alpha) print(sample_beta) 4.714701566527543 25.609091660104536 0.0985043719284747 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): sample_tau = 5.976005311346212 sample_alpha = 6.402272915026134 sample_beta = 0.1970087438569494 quantile = 0.9900 argument = loggit(x) gammavalue = gammacdf(argument,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile myguess = 5000 print(newton(f, myguess)) 5856.10913158364 Trust, but verify! round(gammacdf(loggit(5856.109),sample_tau,sample_alpha,sample_beta),4) 0.9943 Now lets summarize our efforts regarding Beargrass Creek annual peaks and probabilities anticipated. Data Model 99% Peak Flow Remarks Normal 3902 so-so visual fit Log-Normal 4433 better visual fit Gumbell 3296 better visual fit Log-Pearson III 5856 best (of the set) visual fit At this point, now we have to choose our model and then can investigate different questions. So using LP3 as our favorite, lets now determine anticipated flow values for different probabilities (from the data model) - easy enought to just change the quantile value and rerun the newtons optimizer, for example: Exceedence Probability Flow Value Remarks 25% 968 First Quartile Divider 50% 1302 Median, and Second Quartile Divider 75% 1860 3rd Quartile Divider 90% 2706 10% chance of greater value 99% 5856 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% 9420 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% 11455 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): sample_tau = 5.976005311346212 sample_alpha = 6.402272915026134 sample_beta = 0.1970087438569494 quantile = 0.50 argument = loggit(x) gammavalue = gammacdf(argument,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile myguess = 1000 print(newton(f, myguess)) 1302.814639184079","title":"Gamma Distribution (as Pearson Type 3)"},{"location":"1-Lessons/Lesson13/lesson12/#references","text":"Jamie Chan (2014) Learn Python in One Day and Learn It Well. LCF Publishing. Kindle Edition. http://www.learncodingfast.com/python Grus, Joel. Data Science from Scratch: First Principles with Python. O'Reilly Media. Kindle Edition. (http://safaribooksonline.com) Christian, B, and Griffiths Tom (2016) Algorithms to live by: The computer science of human decisions. Henry Holt and Company, ISBN 9781627790369 (hardcover)|ISBN 9781627790376 (electronic book) https://www.amazon.com/Distributional-Statistics-Environment-Statistical-Computing/dp/1463508417 England, J.F. Jr., Cohn, T.A., Faber, B.A., Stedinger, J.R., Thomas Jr., W.O., Veilleux, A.G., Kiang, J.E., and Mason, R.R.Jr., 2018, Guidelines for Determining Flood Flow Frequency\u2014Bulletin 17C: U.S. Geological Survey Techniques andMethods, book 4, chap. B5, 146 p., https://doi.org/10.3133/tm4B5 https://www.astroml.org/book_figures/chapter3/fig_gamma_distribution.html https://www.inferentialthinking.com/chapters/10/Sampling_and_Empirical_Distributions.html https://www.inferentialthinking.com/chapters/15/Prediction.html","title":"References:"},{"location":"2-Homework/ES1/es1-deploy/","text":"Week 1 Assignment Full name: R#: Title of the notebook: Date: Question1: Playing with a Markdown cell Use the empty Markdown cell below, set it up so that it meets all the mentioned requirements: \"O Captain! my Captain! our fearful trip is done, The ship has weather\u2019d every rack, the prize we sought is won, The port is near, the bells I hear, the people all exulting, While follow eyes the steady keel, the vessel grim and daring;\" [Have this verse in blue and italic] But O heart! heart! heart! O the bleeding drops of red, Where on the deck my Captain lies, Fallen cold and dead. [Have this verse as indented quoting] O Captain! my Captain! rise up and hear the bells; Rise up\u2014for you the flag is flung\u2014for you the bugle trills, For you bouquets and ribbon\u2019d wreaths\u2014for you the shores a-crowding, For you they call, the swaying mass, their eager faces turning; [Have this verse in pink and bold] Here Captain! dear father! This arm beneath your head! It is some dream that on the deck, You\u2019ve fallen cold and dead. [Have this verse as indented quoting] My Captain does not answer, his lips are pale and still, My father does not feel my arm, he has no pulse nor will, The ship is anchor\u2019d safe and sound, its voyage closed and done, From fearful trip the victor ship comes in with object won; [Have this verse in orange, italic and bold] Exult O shores, and ring O bells! But I with mournful tread, Walk the deck my Captain lies, Fallen cold and dead.\" [Have this verse as indented quoting] [Attach an image of WALT WHITMAN from an online resource here.] EDIT THIS MARKDOWN CELL Question2: To quote or not to quote! Change the cell below to a code cell and run the script: print(3 + 7 ) print('3 + 7') MyNumber = 3+7 MyName = 'Dusty' print(MyName, MyNumber) print('MyName', 'MyNumber') Answer the following questions based on the output What is the difference between print( 3 + 7 ) and print( '3 + 7') ? What is the difference between print( MyName, MyNumber) and print('MyName', 'MyNumber') Change MyNumber = 3+2 to MyNumber = 3+2.0 , and re-run the script, what happens? Why? Write your answers below: Question 1 Answer Question 2 Answer Question 3 Answer Question3: Arithmetic and Expressions Calculate the expressions below by hand taking care to keep track of result type (integer or float): x1 = 7 + 3 * 6 / 2 - 1 x2 = 2 % 2 + 2 * 2 - 2 / 2 x3 = ( 3 * 9 * ( 3 + ( 9 * 3 / ( 3 ) ) ) ) Write your results below x1 (by hand) = x2 (by hand) = x3 (by hand) = Now write a script to evaluate and print the results, by Assigning a value to a variable. Use the names above x1 = 7 + 3 * 6 / 2 - 1 x2 = ... x3 = ... Then print the type and contents of each variable. print(type(x1),x1) print(type(x2),x2) ... Question4: String Element Manipulation Define the string given below in quotes to a meaningful variable name. some_string ='Computational Thinking' Then Index and print all the elements from index positions 2 to 10. begin = ??? end = ??? print(some_string[begin:end]) Index and print the string 'Think'.","title":"<font color=darkblue>Week 1 Assignment </font>"},{"location":"2-Homework/ES1/es1-deploy/#week-1-assignment","text":"","title":"Week 1 Assignment "},{"location":"2-Homework/ES1/es1-deploy/#full-name","text":"","title":"Full name:"},{"location":"2-Homework/ES1/es1-deploy/#r","text":"","title":"R#:"},{"location":"2-Homework/ES1/es1-deploy/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"2-Homework/ES1/es1-deploy/#date","text":"","title":"Date:"},{"location":"2-Homework/ES1/es1-deploy/#question1-playing-with-a-markdown-cell","text":"","title":"Question1: Playing with a Markdown cell "},{"location":"2-Homework/ES1/es1-deploy/#use-the-empty-markdown-cell-below-set-it-up-so-that-it-meets-all-the-mentioned-requirements","text":"\"O Captain! my Captain! our fearful trip is done, The ship has weather\u2019d every rack, the prize we sought is won, The port is near, the bells I hear, the people all exulting, While follow eyes the steady keel, the vessel grim and daring;\" [Have this verse in blue and italic] But O heart! heart! heart! O the bleeding drops of red, Where on the deck my Captain lies, Fallen cold and dead. [Have this verse as indented quoting] O Captain! my Captain! rise up and hear the bells; Rise up\u2014for you the flag is flung\u2014for you the bugle trills, For you bouquets and ribbon\u2019d wreaths\u2014for you the shores a-crowding, For you they call, the swaying mass, their eager faces turning; [Have this verse in pink and bold] Here Captain! dear father! This arm beneath your head! It is some dream that on the deck, You\u2019ve fallen cold and dead. [Have this verse as indented quoting] My Captain does not answer, his lips are pale and still, My father does not feel my arm, he has no pulse nor will, The ship is anchor\u2019d safe and sound, its voyage closed and done, From fearful trip the victor ship comes in with object won; [Have this verse in orange, italic and bold] Exult O shores, and ring O bells! But I with mournful tread, Walk the deck my Captain lies, Fallen cold and dead.\" [Have this verse as indented quoting] [Attach an image of WALT WHITMAN from an online resource here.] EDIT THIS MARKDOWN CELL","title":"Use the empty Markdown cell below, set it up so that it meets all the mentioned requirements:"},{"location":"2-Homework/ES1/es1-deploy/#question2-to-quote-or-not-to-quote","text":"","title":"Question2: To quote or not to quote! "},{"location":"2-Homework/ES1/es1-deploy/#change-the-cell-below-to-a-code-cell-and-run-the-script","text":"print(3 + 7 ) print('3 + 7') MyNumber = 3+7 MyName = 'Dusty' print(MyName, MyNumber) print('MyName', 'MyNumber')","title":"Change the cell below to a code cell and run the script:"},{"location":"2-Homework/ES1/es1-deploy/#answer-the-following-questions-based-on-the-output","text":"What is the difference between print( 3 + 7 ) and print( '3 + 7') ? What is the difference between print( MyName, MyNumber) and print('MyName', 'MyNumber') Change MyNumber = 3+2 to MyNumber = 3+2.0 , and re-run the script, what happens? Why? Write your answers below: Question 1 Answer Question 2 Answer Question 3 Answer","title":"Answer the following questions based on the output"},{"location":"2-Homework/ES1/es1-deploy/#question3-arithmetic-and-expressions","text":"","title":"Question3: Arithmetic and Expressions "},{"location":"2-Homework/ES1/es1-deploy/#calculate-the-expressions-below-by-hand-taking-care-to-keep-track-of-result-type-integer-or-float","text":"x1 = 7 + 3 * 6 / 2 - 1 x2 = 2 % 2 + 2 * 2 - 2 / 2 x3 = ( 3 * 9 * ( 3 + ( 9 * 3 / ( 3 ) ) ) )","title":"Calculate the expressions below by hand taking care to keep track of result type (integer or float):"},{"location":"2-Homework/ES1/es1-deploy/#write-your-results-below","text":"x1 (by hand) = x2 (by hand) = x3 (by hand) =","title":"Write your results below"},{"location":"2-Homework/ES1/es1-deploy/#now-write-a-script-to-evaluate-and-print-the-results-by","text":"Assigning a value to a variable. Use the names above x1 = 7 + 3 * 6 / 2 - 1 x2 = ... x3 = ... Then print the type and contents of each variable. print(type(x1),x1) print(type(x2),x2) ...","title":"Now write a script to evaluate and print the results, by"},{"location":"2-Homework/ES1/es1-deploy/#question4-string-element-manipulation","text":"","title":"Question4: String Element Manipulation  "},{"location":"2-Homework/ES1/es1-deploy/#define-the-string-given-below-in-quotes-to-a-meaningful-variable-name","text":"some_string ='Computational Thinking'","title":"Define the string given below in quotes to a meaningful variable name."},{"location":"2-Homework/ES1/es1-deploy/#then","text":"Index and print all the elements from index positions 2 to 10. begin = ??? end = ??? print(some_string[begin:end]) Index and print the string 'Think'.","title":"Then"},{"location":"2-Homework/ES1/es1-solution/","text":"Week 1 Assignment Full name: R#: Title of the notebook: Date: Question1: Playing with a Markdown cell Use the empty Markdown cell below, set it up so that it meets all the mentioned requirements: \"O Captain! my Captain! our fearful trip is done, The ship has weather\u2019d every rack, the prize we sought is won, The port is near, the bells I hear, the people all exulting, While follow eyes the steady keel, the vessel grim and daring;\" [Have this verse in blue and italic] But O heart! heart! heart! O the bleeding drops of red, Where on the deck my Captain lies, Fallen cold and dead. [Have this verse as indented quoting] O Captain! my Captain! rise up and hear the bells; Rise up\u2014for you the flag is flung\u2014for you the bugle trills, For you bouquets and ribbon\u2019d wreaths\u2014for you the shores a-crowding, For you they call, the swaying mass, their eager faces turning; [Have this verse in pink and bold] Here Captain! dear father! This arm beneath your head! It is some dream that on the deck, You\u2019ve fallen cold and dead. [Have this verse as indented quoting] My Captain does not answer, his lips are pale and still, My father does not feel my arm, he has no pulse nor will, The ship is anchor\u2019d safe and sound, its voyage closed and done, From fearful trip the victor ship comes in with object won; [Have this verse in orange, italic and bold] Exult O shores, and ring O bells! But I with mournful tread, Walk the deck my Captain lies, Fallen cold and dead.\" [Have this verse as indented quoting] [Attach an image of WALT WHITMAN from an online resource here.] \"O Captain! my Captain! our fearful trip is done, The ship has weather\u2019d every rack, the prize we sought is won, The port is near, the bells I hear, the people all exulting, While follow eyes the steady keel, the vessel grim and daring;\" But O heart! heart! heart! O the bleeding drops of red, Where on the deck my Captain lies, Fallen cold and dead. O Captain! my Captain! rise up and hear the bells; Rise up\u2014for you the flag is flung\u2014for you the bugle trills, For you bouquets and ribbon\u2019d wreaths\u2014for you the shores a-crowding, For you they call, the swaying mass, their eager faces turning; Here Captain! dear father! This arm beneath your head! It is some dream that on the deck, You\u2019ve fallen cold and dead. My Captain does not answer, his lips are pale and still, My father does not feel my arm, he has no pulse nor will, The ship is anchor\u2019d safe and sound, its voyage closed and done, From fearful trip the victor ship comes in with object won; Exult O shores, and ring O bells! But I with mournful tread, Walk the deck my Captain lies, Fallen cold and dead.\" Question2: To quote or not to quote! Change the cell below to a code cell and run the script: print(3 + 7 ) print('3 + 7') MyNumber = 3+7 MyName = 'Dusty' print(MyName, MyNumber) print('MyName', 'MyNumber') MyNumber = 3+2.0 print(MyNumber) 10 3 + 7 Dusty 10 MyName MyNumber 5.0 Answer the following questions based on the output What is the difference between print( 3 + 7 ) and print( '3 + 7') ? What is the difference between print( MyName, MyNumber) and print('MyName', 'MyNumber') Change MyNumber = 3+2 to MyNumber = 3+2.0 , and re-run the script, what happens? Why? Write your answers below: Answer1: print( 3 + 7 ) returns the output of the summation of 3 and 7 which is equal to 10. print( '3 + 7' ) prints the string between quotation marks as is. Answer2: print( MyName, MyNumber) returns the content of \"MyName\" and \"MyNumber\" variables. But print( 'MyName', 'MyNumber') prints merely those strings. Answer3: After modifying the script, \"MyNumber\" is now updated to 5.0 which is a float. This is because we summed up an integer (3) and a float (2.0) which will always result in a float (5.0) Question3: Arithmetic and Expressions Calculate the expressions below by hand taking care to keep track of result type (integer or float): x1 = 7 + 3 * 6 / 2 - 1 x2 = 2 % 2 + 2 * 2 - 2 / 2 x3 = ( 3 * 9 * ( 3 + ( 9 * 3 / ( 3 ) ) ) ) Write your results below x1 (by hand) = 15 x2 (by hand) = 3 x3 (by hand) = 324 Now write a script to evaluate and print the results, by Assigning a value to a variable. Use the names above x1 = 7 + 3 * 6 / 2 - 1 x2 = ... x3 = ... Then print the type and contents of each variable. print(type(x1),x1) print(type(x2),x2) ... x1 = 7 + 3 * 6 / 2 - 1 x2 = 2 % 2 + 2 * 2 - 2 / 2 x3 = ( 3 * 9 * ( 3 + ( 9 * 3 / ( 3 ) ) ) ) print(type(x1),x1) print(type(x2),x2) print(type(x3),x3) <class 'float'> 15.0 <class 'float'> 3.0 <class 'float'> 324.0 Question4: String Element Manipulation Define the string given below in quotes to a meaningful variable name. some_string ='Computational Thinking' Then Index and print all the elements from index positions 2 to 10. begin = ??? end = ??? print(some_string[begin:end]) Index and print the string 'Think'. Q4string ='Computational Thinking' #Defining the given string print(Q4string[2:10]) #Indexing with [#first index:#last index] syntax and printing mputatio print(Q4string[14:19]) #Indexing so that we can get \"Think\" Think","title":"<font color=darkblue>Week 1 Assignment </font>"},{"location":"2-Homework/ES1/es1-solution/#week-1-assignment","text":"","title":"Week 1 Assignment "},{"location":"2-Homework/ES1/es1-solution/#full-name","text":"","title":"Full name:"},{"location":"2-Homework/ES1/es1-solution/#r","text":"","title":"R#:"},{"location":"2-Homework/ES1/es1-solution/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"2-Homework/ES1/es1-solution/#date","text":"","title":"Date:"},{"location":"2-Homework/ES1/es1-solution/#question1-playing-with-a-markdown-cell","text":"","title":"Question1: Playing with a Markdown cell "},{"location":"2-Homework/ES1/es1-solution/#use-the-empty-markdown-cell-below-set-it-up-so-that-it-meets-all-the-mentioned-requirements","text":"\"O Captain! my Captain! our fearful trip is done, The ship has weather\u2019d every rack, the prize we sought is won, The port is near, the bells I hear, the people all exulting, While follow eyes the steady keel, the vessel grim and daring;\" [Have this verse in blue and italic] But O heart! heart! heart! O the bleeding drops of red, Where on the deck my Captain lies, Fallen cold and dead. [Have this verse as indented quoting] O Captain! my Captain! rise up and hear the bells; Rise up\u2014for you the flag is flung\u2014for you the bugle trills, For you bouquets and ribbon\u2019d wreaths\u2014for you the shores a-crowding, For you they call, the swaying mass, their eager faces turning; [Have this verse in pink and bold] Here Captain! dear father! This arm beneath your head! It is some dream that on the deck, You\u2019ve fallen cold and dead. [Have this verse as indented quoting] My Captain does not answer, his lips are pale and still, My father does not feel my arm, he has no pulse nor will, The ship is anchor\u2019d safe and sound, its voyage closed and done, From fearful trip the victor ship comes in with object won; [Have this verse in orange, italic and bold] Exult O shores, and ring O bells! But I with mournful tread, Walk the deck my Captain lies, Fallen cold and dead.\" [Have this verse as indented quoting] [Attach an image of WALT WHITMAN from an online resource here.] \"O Captain! my Captain! our fearful trip is done, The ship has weather\u2019d every rack, the prize we sought is won, The port is near, the bells I hear, the people all exulting, While follow eyes the steady keel, the vessel grim and daring;\" But O heart! heart! heart! O the bleeding drops of red, Where on the deck my Captain lies, Fallen cold and dead. O Captain! my Captain! rise up and hear the bells; Rise up\u2014for you the flag is flung\u2014for you the bugle trills, For you bouquets and ribbon\u2019d wreaths\u2014for you the shores a-crowding, For you they call, the swaying mass, their eager faces turning; Here Captain! dear father! This arm beneath your head! It is some dream that on the deck, You\u2019ve fallen cold and dead. My Captain does not answer, his lips are pale and still, My father does not feel my arm, he has no pulse nor will, The ship is anchor\u2019d safe and sound, its voyage closed and done, From fearful trip the victor ship comes in with object won; Exult O shores, and ring O bells! But I with mournful tread, Walk the deck my Captain lies, Fallen cold and dead.\"","title":"Use the empty Markdown cell below, set it up so that it meets all the mentioned requirements:"},{"location":"2-Homework/ES1/es1-solution/#question2-to-quote-or-not-to-quote","text":"","title":"Question2: To quote or not to quote! "},{"location":"2-Homework/ES1/es1-solution/#change-the-cell-below-to-a-code-cell-and-run-the-script","text":"print(3 + 7 ) print('3 + 7') MyNumber = 3+7 MyName = 'Dusty' print(MyName, MyNumber) print('MyName', 'MyNumber') MyNumber = 3+2.0 print(MyNumber) 10 3 + 7 Dusty 10 MyName MyNumber 5.0","title":"Change the cell below to a code cell and run the script:"},{"location":"2-Homework/ES1/es1-solution/#answer-the-following-questions-based-on-the-output","text":"What is the difference between print( 3 + 7 ) and print( '3 + 7') ? What is the difference between print( MyName, MyNumber) and print('MyName', 'MyNumber') Change MyNumber = 3+2 to MyNumber = 3+2.0 , and re-run the script, what happens? Why? Write your answers below: Answer1: print( 3 + 7 ) returns the output of the summation of 3 and 7 which is equal to 10. print( '3 + 7' ) prints the string between quotation marks as is. Answer2: print( MyName, MyNumber) returns the content of \"MyName\" and \"MyNumber\" variables. But print( 'MyName', 'MyNumber') prints merely those strings. Answer3: After modifying the script, \"MyNumber\" is now updated to 5.0 which is a float. This is because we summed up an integer (3) and a float (2.0) which will always result in a float (5.0)","title":"Answer the following questions based on the output"},{"location":"2-Homework/ES1/es1-solution/#question3-arithmetic-and-expressions","text":"","title":"Question3: Arithmetic and Expressions "},{"location":"2-Homework/ES1/es1-solution/#calculate-the-expressions-below-by-hand-taking-care-to-keep-track-of-result-type-integer-or-float","text":"x1 = 7 + 3 * 6 / 2 - 1 x2 = 2 % 2 + 2 * 2 - 2 / 2 x3 = ( 3 * 9 * ( 3 + ( 9 * 3 / ( 3 ) ) ) )","title":"Calculate the expressions below by hand taking care to keep track of result type (integer or float):"},{"location":"2-Homework/ES1/es1-solution/#write-your-results-below","text":"x1 (by hand) = 15 x2 (by hand) = 3 x3 (by hand) = 324","title":"Write your results below"},{"location":"2-Homework/ES1/es1-solution/#now-write-a-script-to-evaluate-and-print-the-results-by","text":"Assigning a value to a variable. Use the names above x1 = 7 + 3 * 6 / 2 - 1 x2 = ... x3 = ... Then print the type and contents of each variable. print(type(x1),x1) print(type(x2),x2) ... x1 = 7 + 3 * 6 / 2 - 1 x2 = 2 % 2 + 2 * 2 - 2 / 2 x3 = ( 3 * 9 * ( 3 + ( 9 * 3 / ( 3 ) ) ) ) print(type(x1),x1) print(type(x2),x2) print(type(x3),x3) <class 'float'> 15.0 <class 'float'> 3.0 <class 'float'> 324.0","title":"Now write a script to evaluate and print the results, by"},{"location":"2-Homework/ES1/es1-solution/#question4-string-element-manipulation","text":"","title":"Question4: String Element Manipulation  "},{"location":"2-Homework/ES1/es1-solution/#define-the-string-given-below-in-quotes-to-a-meaningful-variable-name","text":"some_string ='Computational Thinking'","title":"Define the string given below in quotes to a meaningful variable name."},{"location":"2-Homework/ES1/es1-solution/#then","text":"Index and print all the elements from index positions 2 to 10. begin = ??? end = ??? print(some_string[begin:end]) Index and print the string 'Think'. Q4string ='Computational Thinking' #Defining the given string print(Q4string[2:10]) #Indexing with [#first index:#last index] syntax and printing mputatio print(Q4string[14:19]) #Indexing so that we can get \"Think\" Think","title":"Then"},{"location":"2-Homework/ES2/es2-deploy/","text":"Week 2 Assignment Full name: R#: Title of the notebook: Date: Question1: List Manipulation For the list given below, index and pick all the elements from index positions 3 to 10. Then, calculate the sum and the length of the elements from index positions 3 to 7. Print the sliced list and the values of the sum and the sliced list. [22, 45, 54, 87, 10, 97, 88, 75, 99, 11, 19, 39, 47, 81, 84] Question2: Dictionary Manipulation From the nested dictionary given below, index and pick the string 'hello'. {'k1':[1,2,3,{'tricky':['oh','man','inception',{'target':[1,2,3,'hello']}]}]} Question3: Use of Conditional Execution A student will not be allowed to sit in exam if his/her attendence is less than 75%. Take the following inputs from the user: 1. Number of classes held. 2. Number of classes attended. Compute the percentage of classes attended \\%_{attended} = \\frac{Classes_{attended}}{Classes_{total}}*100 Use the result to decide whether the student will be allowed to sit in the exam or not. Question4: RoboCop Modification You are driving too fast, and a robotic police officer stops you. The robot is programmed with conditional statements to return one of 3 possible results: \"No ticket\",\"One hundred dollar fine\", or \"Five hundred dollar fine\". according to the following rules If your speed is 60 or less, the result is \"No Ticket\". If speed is between 61 and 80 inclusive, the result is a fine of $100. If speed is 81 or more, the result is $500. If it is your birthday, your speed can be higher by a value of 5 in all cases. You discover you are able to hack into the robot and can modify the fine script. Modify it so that: If speed is between 75 and 85 inclusive, the result is a fine of $100. If speed is 86 or more, the result is $500. Leave the rest unchanged. # Original Script # Input Speed speed = int(input('How Fast ? (numeric)')) # Input Birthday yes = 0 # while loop while yes == 0: userInput = input('Is it your birthday? (Yes or No)') try: if userInput == 'Yes': is_birthday = True elif userInput == 'No': is_birthday = False yes = 1 except: print (\"You did not enter Yes or No, try again \\n\") # Exit the while loop when finally have a valid answer if is_birthday: alterspeed = speed-5 else: alterspeed = speed if alterspeed > 80: print('Fine = $500') elif alterspeed > 60: print('Fine = $100') else: print('No Ticket') Question5: A Loop for Leaps! 1904 was a leap year. Write a for loop that prints out all the leap years from in the 20th century (1904-1999). Question6: Trapped! Write a script that: - asks user for 3 inputs - a starting value for x - an ending value for x - a stepsize - creates and prints a sequence based on user's 3 numbers - calculates the value of y for each number in the defined sequence based on the rules below \\begin{gather} y = x~for~0 <= x < 1 \\\\ y = x^3~for~1 <= x < 2 \\\\ y = x + 2~for~2 <= x \\\\ \\end{gather} produces a table like this: x y(x) 0.0 1.0 2.0 3.0 4.0 5.0 *the increment can be different from 1.0 (unlike above). **Include error trapping that: Takes any numeric input for starting or ending x , and forces into a float. Takes any numeric input for stepsize. and forces into an integer. Takes any non-numeric input, issues a message that the input needs to be numeric, and makes the user try again. Test your script with the following inputs for x, x_increment, num_steps Case 1) fred , 0.5, 7 Case 2) 0.0, 0.5, 7 Case 3) -3.0, 0.5, 14","title":"<font color=darkblue>Week 2 Assignment </font>"},{"location":"2-Homework/ES2/es2-deploy/#week-2-assignment","text":"","title":"Week 2 Assignment "},{"location":"2-Homework/ES2/es2-deploy/#full-name","text":"","title":"Full name:"},{"location":"2-Homework/ES2/es2-deploy/#r","text":"","title":"R#:"},{"location":"2-Homework/ES2/es2-deploy/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"2-Homework/ES2/es2-deploy/#date","text":"","title":"Date:"},{"location":"2-Homework/ES2/es2-deploy/#question1-list-manipulation","text":"For the list given below, index and pick all the elements from index positions 3 to 10. Then, calculate the sum and the length of the elements from index positions 3 to 7. Print the sliced list and the values of the sum and the sliced list. [22, 45, 54, 87, 10, 97, 88, 75, 99, 11, 19, 39, 47, 81, 84]","title":"Question1: List Manipulation"},{"location":"2-Homework/ES2/es2-deploy/#question2-dictionary-manipulation","text":"From the nested dictionary given below, index and pick the string 'hello'. {'k1':[1,2,3,{'tricky':['oh','man','inception',{'target':[1,2,3,'hello']}]}]}","title":"Question2: Dictionary Manipulation "},{"location":"2-Homework/ES2/es2-deploy/#question3-use-of-conditional-execution","text":"A student will not be allowed to sit in exam if his/her attendence is less than 75%. Take the following inputs from the user: 1. Number of classes held. 2. Number of classes attended. Compute the percentage of classes attended \\%_{attended} = \\frac{Classes_{attended}}{Classes_{total}}*100 Use the result to decide whether the student will be allowed to sit in the exam or not.","title":"Question3: Use of Conditional Execution "},{"location":"2-Homework/ES2/es2-deploy/#question4-robocop-modification","text":"You are driving too fast, and a robotic police officer stops you. The robot is programmed with conditional statements to return one of 3 possible results: \"No ticket\",\"One hundred dollar fine\", or \"Five hundred dollar fine\". according to the following rules If your speed is 60 or less, the result is \"No Ticket\". If speed is between 61 and 80 inclusive, the result is a fine of $100. If speed is 81 or more, the result is $500. If it is your birthday, your speed can be higher by a value of 5 in all cases. You discover you are able to hack into the robot and can modify the fine script. Modify it so that: If speed is between 75 and 85 inclusive, the result is a fine of $100. If speed is 86 or more, the result is $500. Leave the rest unchanged. # Original Script # Input Speed speed = int(input('How Fast ? (numeric)')) # Input Birthday yes = 0 # while loop while yes == 0: userInput = input('Is it your birthday? (Yes or No)') try: if userInput == 'Yes': is_birthday = True elif userInput == 'No': is_birthday = False yes = 1 except: print (\"You did not enter Yes or No, try again \\n\") # Exit the while loop when finally have a valid answer if is_birthday: alterspeed = speed-5 else: alterspeed = speed if alterspeed > 80: print('Fine = $500') elif alterspeed > 60: print('Fine = $100') else: print('No Ticket')","title":"Question4: RoboCop Modification  "},{"location":"2-Homework/ES2/es2-deploy/#question5-a-loop-for-leaps","text":"1904 was a leap year. Write a for loop that prints out all the leap years from in the 20th century (1904-1999).","title":"Question5: A Loop for Leaps!  "},{"location":"2-Homework/ES2/es2-deploy/#question6-trapped","text":"Write a script that: - asks user for 3 inputs - a starting value for x - an ending value for x - a stepsize - creates and prints a sequence based on user's 3 numbers - calculates the value of y for each number in the defined sequence based on the rules below \\begin{gather} y = x~for~0 <= x < 1 \\\\ y = x^3~for~1 <= x < 2 \\\\ y = x + 2~for~2 <= x \\\\ \\end{gather} produces a table like this: x y(x) 0.0 1.0 2.0 3.0 4.0 5.0 *the increment can be different from 1.0 (unlike above). **Include error trapping that: Takes any numeric input for starting or ending x , and forces into a float. Takes any numeric input for stepsize. and forces into an integer. Takes any non-numeric input, issues a message that the input needs to be numeric, and makes the user try again. Test your script with the following inputs for x, x_increment, num_steps Case 1) fred , 0.5, 7 Case 2) 0.0, 0.5, 7 Case 3) -3.0, 0.5, 14","title":"Question6: Trapped!  "},{"location":"2-Homework/ES2/es2-solution/","text":"Week 2 Assignment Full name: R#: Title of the notebook: Date: Question1: List Manipulation For the list given below, index and pick all the elements from index positions 3 to 10. Then, calculate the sum and the length of the elements from index positions 3 to 7. Print the sliced list and the values of the sum and the sliced list. [22, 45, 54, 87, 10, 97, 88, 75, 99, 11, 19, 39, 47, 81, 84] mylist2 = [22, 45, 54, 87, 10, 97, 88, 75, 99, 11, 19, 39, 47, 81, 84] #Make the given list print(mylist2) #print it myslicelist2 = mylist2[3:11] #index and pick all elements from 3 to 10 | including 10 print(\"Sliced_list: \", myslicelist2) #print it mysubslice2 = myslicelist2[3:8] #slice the elements 3 to 7 | including 7 print(mysubslice2) #print it mysum = sum(mysubslice2) #calculate the sum print(\"Sum: \", mysum) #print it mylength = len(mysubslice2) #calculate the length print(\"Length: \", mylength) #print it [22, 45, 54, 87, 10, 97, 88, 75, 99, 11, 19, 39, 47, 81, 84] Sliced_list: [87, 10, 97, 88, 75, 99, 11, 19] [88, 75, 99, 11, 19] Sum: 292 Length: 5 Question2: Dictionary Manipulation From the nested dictionary given below, index and pick the string 'hello'. {'k1':[1,2,3,{'tricky':['oh','man','inception',{'target':[1,2,3,'hello']}]}]} Q2dict = {'k1':[1,2,3,{'tricky':['oh','man','inception',{'target':[1,2,3,'hello']}]}]} Q2dict['k1'][3]['tricky'][3]['target'][3] 'hello' Question3: Use of Conditional Execution A student will not be allowed to sit in exam if his/her attendence is less than 75%. Take the following inputs from the user: 1. Number of classes held. 2. Number of classes attended. Compute the percentage of classes attended \\%_{attended} = \\frac{Classes_{attended}}{Classes_{total}}*100 Use the result to decide whether the student will be allowed to sit in the exam or not. nc_held = input('Enter total classes held') #Ask user for Number of classes held nc_attended = input('Enter classes attended') # Ask user for Number of classes attended attendance = int(nc_attended)/int(nc_held)*100 #Calculate the %attended based on the given formula print('Attendance:', attendance,' percent') #print the calculated value if attendance >= 75: #Print the decision on whether the student will be allowed to sit in the exam or not print(\"Allowed to sit in exam\") else: print(\"Not allowed to sit in exam\") Enter total classes held 100 Enter classes attended 64 Attendance: 64.0 percent Not allowed to sit in exam Question4: RoboCop Modification You are driving too fast, and a robotic police officer stops you. The robot is programmed with conditional statements to return one of 3 possible results: \"No ticket\",\"One hundred dollar fine\", or \"Five hundred dollar fine\". according to the following rules If your speed is 60 or less, the result is \"No Ticket\". If speed is between 61 and 80 inclusive, the result is a fine of $100. If speed is 81 or more, the result is $500. If it is your birthday, your speed can be higher by a value of 5 in all cases. You discover you are able to hack into the robot and can modify the fine script. Modify it so that: If speed is between 75 and 85 inclusive, the result is a fine of $100. If speed is 86 or more, the result is $500. Leave the rest unchanged. # Original Script # Input Speed speed = int(input('How Fast ? (numeric)')) # Input Birthday yes = 0 # while loop while yes == 0: userInput = input('Is it your birthday? (Yes or No)') try: if userInput == 'Yes': is_birthday = True elif userInput == 'No': is_birthday = False yes = 1 except: print (\"You did not enter Yes or No, try again \\n\") # Exit the while loop when finally have a valid answer if is_birthday: alterspeed = speed-5 else: alterspeed = speed if alterspeed > 80: print('Fine = $100') elif alterspeed > 60: print('Fine = $100') else: print('No Ticket') How Fast ? (numeric) 76 Is it your birthday? (Yes or No) No Fine = $100 # Input Speed speed = int(input('How Fast ? (numeric)')) # Input Birthday yes = 0 # while loop while yes == 0: userInput = input('Is it your birthday? (Yes or No)') try: if userInput == 'Yes': is_birthday = True elif userInput == 'No': is_birthday = False yes = 1 except: print (\"You did not enter Yes or No, try again \\n\") # Exit the while loop when finally have a valid answer if is_birthday: alterspeed = speed-5 else: alterspeed = speed if alterspeed > 85: #Change this | the upper bound print('Fine = $500') elif alterspeed > 74: #Change this | the lower bound print('Fine = $100') else: print('No Ticket') How Fast ? (numeric) 86 Is it your birthday? (Yes or No) No Fine = $500 Question5: A Loop for Leaps! 1904 was a leap year. Write a for loop that prints out all the leap years from in the 20th century (1904-1999). for years in range(1904,2000,4): # a sequence from 1904 to 1999 with steps of 4 print(years) 1904 1908 1912 1916 1920 1924 1928 1932 1936 1940 1944 1948 1952 1956 1960 1964 1968 1972 1976 1980 1984 1988 1992 1996 Question6: Trapped! Write a script that: - asks user for 3 inputs - a starting value for x - an ending value for x - a stepsize - creates and prints a sequence based on user's 3 numbers - calculates the value of y for each number in the defined sequence based on the rules below \\begin{gather} y = x~for~0 <= x < 1 \\\\ y = x^3~for~1 <= x < 2 \\\\ y = x + 2~for~2 <= x \\\\ \\end{gather} produces a table like this: x y(x) 0.0 1.0 2.0 3.0 4.0 5.0 *the increment can be different from 1.0 (unlike above). **Include error trapping that: Takes any numeric input for starting or ending x , and forces into a float. Takes any numeric input for stepsize. and forces into an integer. Takes any non-numeric input, issues a message that the input needs to be numeric, and makes the user try again. Test your script with the following inputs for x, x_increment, num_steps Case 1) fred , 0.5, 7 Case 2) 0.0, 0.5, 7 Case 3) -3.0, 0.5, 14 from prettytable import PrettyTable try: userInput = input('Enter the starting value for x') #ask for user's input on the initial value start = float(userInput) userInput2 = input('Enter the ending value for x') #ask for user's input on the last value stop = float(userInput2) userInput3 = float(input('Enter the step size for x')) #ask for user's input on the step size step = int(userInput3) print(\"the range for x goes from\", start, \" to\", stop, \" by increments of\", step) t = PrettyTable(['x', 'y']) for x in range(int(start),int(stop),int(step)): if x >= 0 and x < 1: y = x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) elif x >= 1 and x < 2: y = x*x*x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) else: y = x+2 print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) print(t) except: print (\"the input needs to be numeric. Please try again!\") Enter the starting value for x 0 Enter the ending value for x 6 Enter the step size for x 1 the range for x goes from 0.0 to 6.0 by increments of 1 for x equal to 0 , y is equal to 0 for x equal to 1 , y is equal to 1 for x equal to 2 , y is equal to 4 for x equal to 3 , y is equal to 5 for x equal to 4 , y is equal to 6 for x equal to 5 , y is equal to 7 +---+---+ | x | y | +---+---+ | 0 | 0 | | 1 | 1 | | 2 | 4 | | 3 | 5 | | 4 | 6 | | 5 | 7 | +---+---+","title":"<font color=darkblue>Week 2 Assignment </font>"},{"location":"2-Homework/ES2/es2-solution/#week-2-assignment","text":"","title":"Week 2 Assignment "},{"location":"2-Homework/ES2/es2-solution/#full-name","text":"","title":"Full name:"},{"location":"2-Homework/ES2/es2-solution/#r","text":"","title":"R#:"},{"location":"2-Homework/ES2/es2-solution/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"2-Homework/ES2/es2-solution/#date","text":"","title":"Date:"},{"location":"2-Homework/ES2/es2-solution/#question1-list-manipulation","text":"For the list given below, index and pick all the elements from index positions 3 to 10. Then, calculate the sum and the length of the elements from index positions 3 to 7. Print the sliced list and the values of the sum and the sliced list. [22, 45, 54, 87, 10, 97, 88, 75, 99, 11, 19, 39, 47, 81, 84] mylist2 = [22, 45, 54, 87, 10, 97, 88, 75, 99, 11, 19, 39, 47, 81, 84] #Make the given list print(mylist2) #print it myslicelist2 = mylist2[3:11] #index and pick all elements from 3 to 10 | including 10 print(\"Sliced_list: \", myslicelist2) #print it mysubslice2 = myslicelist2[3:8] #slice the elements 3 to 7 | including 7 print(mysubslice2) #print it mysum = sum(mysubslice2) #calculate the sum print(\"Sum: \", mysum) #print it mylength = len(mysubslice2) #calculate the length print(\"Length: \", mylength) #print it [22, 45, 54, 87, 10, 97, 88, 75, 99, 11, 19, 39, 47, 81, 84] Sliced_list: [87, 10, 97, 88, 75, 99, 11, 19] [88, 75, 99, 11, 19] Sum: 292 Length: 5","title":"Question1: List Manipulation"},{"location":"2-Homework/ES2/es2-solution/#question2-dictionary-manipulation","text":"From the nested dictionary given below, index and pick the string 'hello'. {'k1':[1,2,3,{'tricky':['oh','man','inception',{'target':[1,2,3,'hello']}]}]} Q2dict = {'k1':[1,2,3,{'tricky':['oh','man','inception',{'target':[1,2,3,'hello']}]}]} Q2dict['k1'][3]['tricky'][3]['target'][3] 'hello'","title":"Question2: Dictionary Manipulation "},{"location":"2-Homework/ES2/es2-solution/#question3-use-of-conditional-execution","text":"A student will not be allowed to sit in exam if his/her attendence is less than 75%. Take the following inputs from the user: 1. Number of classes held. 2. Number of classes attended. Compute the percentage of classes attended \\%_{attended} = \\frac{Classes_{attended}}{Classes_{total}}*100 Use the result to decide whether the student will be allowed to sit in the exam or not. nc_held = input('Enter total classes held') #Ask user for Number of classes held nc_attended = input('Enter classes attended') # Ask user for Number of classes attended attendance = int(nc_attended)/int(nc_held)*100 #Calculate the %attended based on the given formula print('Attendance:', attendance,' percent') #print the calculated value if attendance >= 75: #Print the decision on whether the student will be allowed to sit in the exam or not print(\"Allowed to sit in exam\") else: print(\"Not allowed to sit in exam\") Enter total classes held 100 Enter classes attended 64 Attendance: 64.0 percent Not allowed to sit in exam","title":"Question3: Use of Conditional Execution "},{"location":"2-Homework/ES2/es2-solution/#question4-robocop-modification","text":"You are driving too fast, and a robotic police officer stops you. The robot is programmed with conditional statements to return one of 3 possible results: \"No ticket\",\"One hundred dollar fine\", or \"Five hundred dollar fine\". according to the following rules If your speed is 60 or less, the result is \"No Ticket\". If speed is between 61 and 80 inclusive, the result is a fine of $100. If speed is 81 or more, the result is $500. If it is your birthday, your speed can be higher by a value of 5 in all cases. You discover you are able to hack into the robot and can modify the fine script. Modify it so that: If speed is between 75 and 85 inclusive, the result is a fine of $100. If speed is 86 or more, the result is $500. Leave the rest unchanged. # Original Script # Input Speed speed = int(input('How Fast ? (numeric)')) # Input Birthday yes = 0 # while loop while yes == 0: userInput = input('Is it your birthday? (Yes or No)') try: if userInput == 'Yes': is_birthday = True elif userInput == 'No': is_birthday = False yes = 1 except: print (\"You did not enter Yes or No, try again \\n\") # Exit the while loop when finally have a valid answer if is_birthday: alterspeed = speed-5 else: alterspeed = speed if alterspeed > 80: print('Fine = $100') elif alterspeed > 60: print('Fine = $100') else: print('No Ticket') How Fast ? (numeric) 76 Is it your birthday? (Yes or No) No Fine = $100 # Input Speed speed = int(input('How Fast ? (numeric)')) # Input Birthday yes = 0 # while loop while yes == 0: userInput = input('Is it your birthday? (Yes or No)') try: if userInput == 'Yes': is_birthday = True elif userInput == 'No': is_birthday = False yes = 1 except: print (\"You did not enter Yes or No, try again \\n\") # Exit the while loop when finally have a valid answer if is_birthday: alterspeed = speed-5 else: alterspeed = speed if alterspeed > 85: #Change this | the upper bound print('Fine = $500') elif alterspeed > 74: #Change this | the lower bound print('Fine = $100') else: print('No Ticket') How Fast ? (numeric) 86 Is it your birthday? (Yes or No) No Fine = $500","title":"Question4: RoboCop Modification  "},{"location":"2-Homework/ES2/es2-solution/#question5-a-loop-for-leaps","text":"1904 was a leap year. Write a for loop that prints out all the leap years from in the 20th century (1904-1999). for years in range(1904,2000,4): # a sequence from 1904 to 1999 with steps of 4 print(years) 1904 1908 1912 1916 1920 1924 1928 1932 1936 1940 1944 1948 1952 1956 1960 1964 1968 1972 1976 1980 1984 1988 1992 1996","title":"Question5: A Loop for Leaps!  "},{"location":"2-Homework/ES2/es2-solution/#question6-trapped","text":"Write a script that: - asks user for 3 inputs - a starting value for x - an ending value for x - a stepsize - creates and prints a sequence based on user's 3 numbers - calculates the value of y for each number in the defined sequence based on the rules below \\begin{gather} y = x~for~0 <= x < 1 \\\\ y = x^3~for~1 <= x < 2 \\\\ y = x + 2~for~2 <= x \\\\ \\end{gather} produces a table like this: x y(x) 0.0 1.0 2.0 3.0 4.0 5.0 *the increment can be different from 1.0 (unlike above). **Include error trapping that: Takes any numeric input for starting or ending x , and forces into a float. Takes any numeric input for stepsize. and forces into an integer. Takes any non-numeric input, issues a message that the input needs to be numeric, and makes the user try again. Test your script with the following inputs for x, x_increment, num_steps Case 1) fred , 0.5, 7 Case 2) 0.0, 0.5, 7 Case 3) -3.0, 0.5, 14 from prettytable import PrettyTable try: userInput = input('Enter the starting value for x') #ask for user's input on the initial value start = float(userInput) userInput2 = input('Enter the ending value for x') #ask for user's input on the last value stop = float(userInput2) userInput3 = float(input('Enter the step size for x')) #ask for user's input on the step size step = int(userInput3) print(\"the range for x goes from\", start, \" to\", stop, \" by increments of\", step) t = PrettyTable(['x', 'y']) for x in range(int(start),int(stop),int(step)): if x >= 0 and x < 1: y = x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) elif x >= 1 and x < 2: y = x*x*x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) else: y = x+2 print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) print(t) except: print (\"the input needs to be numeric. Please try again!\") Enter the starting value for x 0 Enter the ending value for x 6 Enter the step size for x 1 the range for x goes from 0.0 to 6.0 by increments of 1 for x equal to 0 , y is equal to 0 for x equal to 1 , y is equal to 1 for x equal to 2 , y is equal to 4 for x equal to 3 , y is equal to 5 for x equal to 4 , y is equal to 6 for x equal to 5 , y is equal to 7 +---+---+ | x | y | +---+---+ | 0 | 0 | | 1 | 1 | | 2 | 4 | | 3 | 5 | | 4 | 6 | | 5 | 7 | +---+---+","title":"Question6: Trapped!  "},{"location":"2-Homework/ES3/es3-deploy/","text":"Week 3 Assignment Full name: R#: Title of the notebook: Date: Question1: User-Defined Function Create the function f(x) = e^x - 10 cos(x) - 100 as a function (i.e. use the def keyword) def name(parameters) : operations on parameters ... ... return (value, or null) Then apply your function to the value. Use your function to complete the table below: x f(x) 0.0 1.50 2.00 2.25 3.0 4.25 Question2: Power Plot Copy the wrapper script for the plotAline() function (from the Lab4_dev notebook), and modify the copy to create a plot of y = x^2 for x raging from 0 to 9 (inclusive) in steps of 1. Label the plot and the plot axes. Question3: Cars! Write a class named ' CarSpeed ' to calculate the speed (in miles/hour) of different brands of cars based on the information given in the table below. The formula to calculate speed is also given below. Based on your output, which brand of car has the highest speed? Speed = \\frac{Distance}{Time} Car brand Distance (miles) Time (hours) Ford 120 1.75 Ferrari 100 1.20 BMW 205 2.35 Porsche 155 1.85 Audi 190 2.10 Jaguar 255 2.45 Notes: Use docstrings to describe the purpose of the class. Create an object for each car brand and display the output as shown below. Ford_Speed (miles/hour): SPEED Ferrari_Speed (miles/hour): SPEED BMW_Speed (miles/hour): SPEED Porsche_Speed (miles/hour): SPEED Audi_Speed (miles/hour): SPEED Jaguar_Speed (miles/hour): SPEED The car with the highest speed is: CAR BRAND Question4: Tax Class Write a class named ' Tax ' to calculate the state tax (in dollars) of Employees at Texas Tech University based on their annual salary. The state tax is 16% if the annual salary is below 80,000 dollars and 22% if the salary is more than 80,000 dollars. Tax = Annual salary \\times State\\, tax\\, \\% Employee Annual salary (dollars) Bob 1,50,000 Mary 78,000 John 55,000 Danny 1,75,000 Notes: Use docstrings to describe the purpose of the class. Use if....else conditional statements within the method of the class to choose the relevant tax % based on the annual salary. Create an object for employee and display the output as shown below. Bob's tax amount (in dollars): AMOUNT Mary's tax amount (in dollars): AMOUNT John's tax amount (in dollars): AMOUNT Danny's tax amount (in dollars): AMOUNT","title":"<font color=darkblue>Week 3 Assignment </font>"},{"location":"2-Homework/ES3/es3-deploy/#week-3-assignment","text":"","title":"Week 3 Assignment "},{"location":"2-Homework/ES3/es3-deploy/#full-name","text":"","title":"Full name:"},{"location":"2-Homework/ES3/es3-deploy/#r","text":"","title":"R#:"},{"location":"2-Homework/ES3/es3-deploy/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"2-Homework/ES3/es3-deploy/#date","text":"","title":"Date:"},{"location":"2-Homework/ES3/es3-deploy/#question1-user-defined-function","text":"Create the function f(x) = e^x - 10 cos(x) - 100 as a function (i.e. use the def keyword) def name(parameters) : operations on parameters ... ... return (value, or null) Then apply your function to the value. Use your function to complete the table below: x f(x) 0.0 1.50 2.00 2.25 3.0 4.25","title":"Question1: User-Defined Function"},{"location":"2-Homework/ES3/es3-deploy/#question2-power-plot","text":"Copy the wrapper script for the plotAline() function (from the Lab4_dev notebook), and modify the copy to create a plot of y = x^2 for x raging from 0 to 9 (inclusive) in steps of 1. Label the plot and the plot axes.","title":"Question2: Power Plot "},{"location":"2-Homework/ES3/es3-deploy/#question3-cars","text":"Write a class named ' CarSpeed ' to calculate the speed (in miles/hour) of different brands of cars based on the information given in the table below. The formula to calculate speed is also given below. Based on your output, which brand of car has the highest speed? Speed = \\frac{Distance}{Time} Car brand Distance (miles) Time (hours) Ford 120 1.75 Ferrari 100 1.20 BMW 205 2.35 Porsche 155 1.85 Audi 190 2.10 Jaguar 255 2.45","title":"Question3: Cars! "},{"location":"2-Homework/ES3/es3-deploy/#notes","text":"Use docstrings to describe the purpose of the class. Create an object for each car brand and display the output as shown below. Ford_Speed (miles/hour): SPEED Ferrari_Speed (miles/hour): SPEED BMW_Speed (miles/hour): SPEED Porsche_Speed (miles/hour): SPEED Audi_Speed (miles/hour): SPEED Jaguar_Speed (miles/hour): SPEED The car with the highest speed is: CAR BRAND","title":"Notes:"},{"location":"2-Homework/ES3/es3-deploy/#question4-tax-class","text":"Write a class named ' Tax ' to calculate the state tax (in dollars) of Employees at Texas Tech University based on their annual salary. The state tax is 16% if the annual salary is below 80,000 dollars and 22% if the salary is more than 80,000 dollars. Tax = Annual salary \\times State\\, tax\\, \\% Employee Annual salary (dollars) Bob 1,50,000 Mary 78,000 John 55,000 Danny 1,75,000","title":"Question4: Tax Class  "},{"location":"2-Homework/ES3/es3-deploy/#notes_1","text":"Use docstrings to describe the purpose of the class. Use if....else conditional statements within the method of the class to choose the relevant tax % based on the annual salary. Create an object for employee and display the output as shown below. Bob's tax amount (in dollars): AMOUNT Mary's tax amount (in dollars): AMOUNT John's tax amount (in dollars): AMOUNT Danny's tax amount (in dollars): AMOUNT","title":"Notes:"},{"location":"2-Homework/ES3/es3-solution/","text":"Week 3 Assignment Full name: R#: Title of the notebook: Date: Question1: User-Defined Function Create the function f(x) = e^x - 10 cos(x) - 100 as a function (i.e. use the def keyword) def name(parameters) : operations on parameters ... ... return (value, or null) Then apply your function to the value. Use your function to complete the table below: x f(x) 0.0 1.50 2.00 2.25 3.0 4.25 from math import exp # package that contains exp function from math import cos # package that contains cosine function def gollum(x) : #define the function \"gollum\" precious = exp(x) - 10*cos(x) -100 #as stated by the question return precious from prettytable import PrettyTable #package that contains the PrettyTable function t = PrettyTable(['x', 'y']) #Define an empty table x = [0.0,1.5,2.0,2.25,3.0,4.25] #the list of x values according to the table for i in range(0,6,1): #a counter to go through the list x t.add_row([x[i], gollum(x[i])]) #for each x value, fill one row in table t with the value of x and the value of gollum function of that x print(t) Question2: Power Plot Copy the wrapper script for the plotAline() function (from the Lab4_dev notebook), and modify the copy to create a plot of y = x^2 for x raging from 0 to 9 (inclusive) in steps of 1. Label the plot and the plot axes. def plotAline(list1,list2,strx,stry,strtitle): # plot list1 on x, list2 on y, xlabel, ylabel, title from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() plt.plot( list1, list2, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis plt.title(strtitle)# add a title plt.ylabel(stry)# add a label to the x and y-axes plt.xlabel(strx) plt.show() # display the plot return #null return xxx = [] # null list yyy = [] # null list for i in range(0,10): xxx.append(1.0*i) #float i and append to the list for i in range(0,10): yyy.append(xxx[i]**2) plotAline(xxx,yyy,\"X\",\"Y\",\"Plot of Y = X^2 \") Question3: Cars! Write a class named ' CarSpeed ' to calculate the speed (in miles/hour) of different brands of cars based on the information given in the table below. The formula to calculate speed is also given below. Based on your output, which brand of car has the highest speed? Speed = \\frac{Distance}{Time} Car brand Distance (miles) Time (hours) Ford 120 1.75 Ferrari 100 1.20 BMW 205 2.35 Porsche 155 1.85 Audi 190 2.10 Jaguar 255 2.45 Notes: Use docstrings to describe the purpose of the class. Create an object for each car brand and display the output as shown below. Ford_Speed (miles/hour): SPEED Ferrari_Speed (miles/hour): SPEED BMW_Speed (miles/hour): SPEED Porsche_Speed (miles/hour): SPEED Audi_Speed (miles/hour): SPEED Jaguar_Speed (miles/hour): SPEED The car with the highest speed is: CAR BRAND class CarSpeed: \"\"\"This class calculates the speed of the car based on the given distance and time\"\"\" def __init__(self, distance, time): self.distance = distance self.time= time def speed(self): return self.distance/self.time ford = CarSpeed(120, 1.75) ferrari = CarSpeed(100, 1.20) bmw = CarSpeed(205, 2.35) porsche = CarSpeed(155, 1.85) audi = CarSpeed(190, 2.10) jaguar = CarSpeed(255, 2.45) print(\"Ford_Speed (miles/hour):\", ford.speed()) print(\"Ferrari_Speed (miles/hour):\", ferrari.speed()) print(\"BMW_Speed (miles/hour):\", bmw.speed()) print(\"Porsche_Speed (miles/hour):\", porsche.speed()) print(\"Audi_Speed (miles/hour):\", audi.speed()) print(\"Jaguar_Speed (miles/hour):\", jaguar.speed()) Ford_Speed (miles/hour): 68.57142857142857 Ferrari_Speed (miles/hour): 83.33333333333334 BMW_Speed (miles/hour): 87.23404255319149 Porsche_Speed (miles/hour): 83.78378378378378 Audi_Speed (miles/hour): 90.47619047619047 Jaguar_Speed (miles/hour): 104.08163265306122 # which brand of car has the highest speed? BrSp = {\"Ford\":ford.speed(),\"Ferrari\":ferrari.speed(),\"BMW\":bmw.speed(),\"Porsche\":porsche.speed(),\"Audi\":audi.speed(),\"Jaguar\":jaguar.speed()} #Create a dictionary with brands and speeds print(BrSp) max_key = max(BrSp, key=BrSp.get) #Find the key(brand) associated with maximum speed print(\"The brand of car with the highest speed is\",max_key) {'Ford': 68.57142857142857, 'Ferrari': 83.33333333333334, 'BMW': 87.23404255319149, 'Porsche': 83.78378378378378, 'Audi': 90.47619047619047, 'Jaguar': 104.08163265306122} The brand of car with the highest speed is Jaguar Question4: RoboCop Modification Write a class named ' Tax ' to calculate the state tax (in dollars) of Employees at Texas Tech University based on their annual salary. The state tax is 16% if the annual salary is below 80,000 dollars and 22% if the salary is more than 80,000 dollars. Tax = Annual salary \\times State\\, tax\\, \\% Employee Annual salary (dollars) Bob 1,50,000 Mary 78,000 John 55,000 Danny 1,75,000 Notes: Use docstrings to describe the purpose of the class. Use if....else conditional statements within the method of the class to choose the relevant tax % based on the annual salary. Create an object for employee and display the output as shown below. Bob's tax amount (in dollars): AMOUNT Mary's tax amount (in dollars): AMOUNT John's tax amount (in dollars): AMOUNT Danny's tax amount (in dollars): AMOUNT class Tax: \"\"\"This class calculates the tax amount based on the annual salary and the state tax %\"\"\" def __init__(self, salary): self.salary = salary def taxamount(self): if self.salary < 80000: return self.salary*(16/100) else: return self.salary*(22/100) bob = Tax(150000) mary = Tax(78000) john = Tax(55000) danny = Tax(175000) print(\"Bob's tax amount (in dollars):\", bob.taxamount()) print(\"Mary's tax amount (in dollars):\", mary.taxamount()) print(\"John's tax amount (in dollars):\", john.taxamount()) print(\"Danny's tax amount (in dollars):\", danny.taxamount()) Bob's tax amount (in dollars): 33000.0 Mary's tax amount (in dollars): 12480.0 John's tax amount (in dollars): 8800.0 Danny's tax amount (in dollars): 38500.0","title":"<font color=darkblue>Week 3 Assignment </font>"},{"location":"2-Homework/ES3/es3-solution/#week-3-assignment","text":"","title":"Week 3 Assignment "},{"location":"2-Homework/ES3/es3-solution/#full-name","text":"","title":"Full name:"},{"location":"2-Homework/ES3/es3-solution/#r","text":"","title":"R#:"},{"location":"2-Homework/ES3/es3-solution/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"2-Homework/ES3/es3-solution/#date","text":"","title":"Date:"},{"location":"2-Homework/ES3/es3-solution/#question1-user-defined-function","text":"Create the function f(x) = e^x - 10 cos(x) - 100 as a function (i.e. use the def keyword) def name(parameters) : operations on parameters ... ... return (value, or null) Then apply your function to the value. Use your function to complete the table below: x f(x) 0.0 1.50 2.00 2.25 3.0 4.25 from math import exp # package that contains exp function from math import cos # package that contains cosine function def gollum(x) : #define the function \"gollum\" precious = exp(x) - 10*cos(x) -100 #as stated by the question return precious from prettytable import PrettyTable #package that contains the PrettyTable function t = PrettyTable(['x', 'y']) #Define an empty table x = [0.0,1.5,2.0,2.25,3.0,4.25] #the list of x values according to the table for i in range(0,6,1): #a counter to go through the list x t.add_row([x[i], gollum(x[i])]) #for each x value, fill one row in table t with the value of x and the value of gollum function of that x print(t)","title":"Question1: User-Defined Function"},{"location":"2-Homework/ES3/es3-solution/#question2-power-plot","text":"Copy the wrapper script for the plotAline() function (from the Lab4_dev notebook), and modify the copy to create a plot of y = x^2 for x raging from 0 to 9 (inclusive) in steps of 1. Label the plot and the plot axes. def plotAline(list1,list2,strx,stry,strtitle): # plot list1 on x, list2 on y, xlabel, ylabel, title from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() plt.plot( list1, list2, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis plt.title(strtitle)# add a title plt.ylabel(stry)# add a label to the x and y-axes plt.xlabel(strx) plt.show() # display the plot return #null return xxx = [] # null list yyy = [] # null list for i in range(0,10): xxx.append(1.0*i) #float i and append to the list for i in range(0,10): yyy.append(xxx[i]**2) plotAline(xxx,yyy,\"X\",\"Y\",\"Plot of Y = X^2 \")","title":"Question2: Power Plot "},{"location":"2-Homework/ES3/es3-solution/#question3-cars","text":"Write a class named ' CarSpeed ' to calculate the speed (in miles/hour) of different brands of cars based on the information given in the table below. The formula to calculate speed is also given below. Based on your output, which brand of car has the highest speed? Speed = \\frac{Distance}{Time} Car brand Distance (miles) Time (hours) Ford 120 1.75 Ferrari 100 1.20 BMW 205 2.35 Porsche 155 1.85 Audi 190 2.10 Jaguar 255 2.45","title":"Question3: Cars! "},{"location":"2-Homework/ES3/es3-solution/#notes","text":"Use docstrings to describe the purpose of the class. Create an object for each car brand and display the output as shown below. Ford_Speed (miles/hour): SPEED Ferrari_Speed (miles/hour): SPEED BMW_Speed (miles/hour): SPEED Porsche_Speed (miles/hour): SPEED Audi_Speed (miles/hour): SPEED Jaguar_Speed (miles/hour): SPEED The car with the highest speed is: CAR BRAND class CarSpeed: \"\"\"This class calculates the speed of the car based on the given distance and time\"\"\" def __init__(self, distance, time): self.distance = distance self.time= time def speed(self): return self.distance/self.time ford = CarSpeed(120, 1.75) ferrari = CarSpeed(100, 1.20) bmw = CarSpeed(205, 2.35) porsche = CarSpeed(155, 1.85) audi = CarSpeed(190, 2.10) jaguar = CarSpeed(255, 2.45) print(\"Ford_Speed (miles/hour):\", ford.speed()) print(\"Ferrari_Speed (miles/hour):\", ferrari.speed()) print(\"BMW_Speed (miles/hour):\", bmw.speed()) print(\"Porsche_Speed (miles/hour):\", porsche.speed()) print(\"Audi_Speed (miles/hour):\", audi.speed()) print(\"Jaguar_Speed (miles/hour):\", jaguar.speed()) Ford_Speed (miles/hour): 68.57142857142857 Ferrari_Speed (miles/hour): 83.33333333333334 BMW_Speed (miles/hour): 87.23404255319149 Porsche_Speed (miles/hour): 83.78378378378378 Audi_Speed (miles/hour): 90.47619047619047 Jaguar_Speed (miles/hour): 104.08163265306122 # which brand of car has the highest speed? BrSp = {\"Ford\":ford.speed(),\"Ferrari\":ferrari.speed(),\"BMW\":bmw.speed(),\"Porsche\":porsche.speed(),\"Audi\":audi.speed(),\"Jaguar\":jaguar.speed()} #Create a dictionary with brands and speeds print(BrSp) max_key = max(BrSp, key=BrSp.get) #Find the key(brand) associated with maximum speed print(\"The brand of car with the highest speed is\",max_key) {'Ford': 68.57142857142857, 'Ferrari': 83.33333333333334, 'BMW': 87.23404255319149, 'Porsche': 83.78378378378378, 'Audi': 90.47619047619047, 'Jaguar': 104.08163265306122} The brand of car with the highest speed is Jaguar","title":"Notes:"},{"location":"2-Homework/ES3/es3-solution/#question4-robocop-modification","text":"Write a class named ' Tax ' to calculate the state tax (in dollars) of Employees at Texas Tech University based on their annual salary. The state tax is 16% if the annual salary is below 80,000 dollars and 22% if the salary is more than 80,000 dollars. Tax = Annual salary \\times State\\, tax\\, \\% Employee Annual salary (dollars) Bob 1,50,000 Mary 78,000 John 55,000 Danny 1,75,000","title":"Question4: RoboCop Modification  "},{"location":"2-Homework/ES3/es3-solution/#notes_1","text":"Use docstrings to describe the purpose of the class. Use if....else conditional statements within the method of the class to choose the relevant tax % based on the annual salary. Create an object for employee and display the output as shown below. Bob's tax amount (in dollars): AMOUNT Mary's tax amount (in dollars): AMOUNT John's tax amount (in dollars): AMOUNT Danny's tax amount (in dollars): AMOUNT class Tax: \"\"\"This class calculates the tax amount based on the annual salary and the state tax %\"\"\" def __init__(self, salary): self.salary = salary def taxamount(self): if self.salary < 80000: return self.salary*(16/100) else: return self.salary*(22/100) bob = Tax(150000) mary = Tax(78000) john = Tax(55000) danny = Tax(175000) print(\"Bob's tax amount (in dollars):\", bob.taxamount()) print(\"Mary's tax amount (in dollars):\", mary.taxamount()) print(\"John's tax amount (in dollars):\", john.taxamount()) print(\"Danny's tax amount (in dollars):\", danny.taxamount()) Bob's tax amount (in dollars): 33000.0 Mary's tax amount (in dollars): 12480.0 John's tax amount (in dollars): 8800.0 Danny's tax amount (in dollars): 38500.0","title":"Notes:"},{"location":"8-Labs/Lab0/Lab0_Dev/","text":"Laboratory 0: Yes, That's how we count in python! Link to Content Server Welcome to your first Jupyter Notebook . This is a medium that we will be using throughout the semester. Why is this called a notebook? Because you can write stuff in it! Is that it? Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!). How do we get this? There are online services that allow you create, modify, and export Jupyter notebooks. However, to have this on your local machines (computers), you can install Anaconda . Anaconda is a package of different software suits including \"Jupyter Notebook\". You can find videos on how to install Anaconda on your devices on BlackBoard: Go to Anaconda.com Scroll down to the bottom of the page or click on products > individual edition Download the right version for your system: Windows, MacOS, and Linux- This may take a while depending on your connection speed Once the installer file is downloaded, run it and install Anaconda on your machine. Anaconda requires almost 3 GB of free space Install it in a separate folder- Preferably on a drive with lots of free memory! BE PATIENT!- It will take a while. The Environment - Let's have a look around this window! The tabs File Edit View Insert Cell Kernel The Icons Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list) The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type. There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially). Code Cells: A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code. When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability. Markdown Cells: You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied: If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks: # title ## major headings ### subheadings #### 4th level subheadings ##### 5th level subheadings These codes are also quite useful: Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format: ![](put the image address here.image format) Raw Cells: Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook. Let's meet world's most popular python! What is python? \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language) How to have access to it? There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers: a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3 We can do the exact same thing in this notebook. But we need a CODE cell. print(\"Hello World\") Hello World This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens: print(\"This is my first notebook!\") This is my first notebook! How to save a notebook? As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE Exercise: Let's see who you are! Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!","title":"<font color=darkred>Laboratory 0: Yes, That's how we count in python!</font>"},{"location":"8-Labs/Lab0/Lab0_Dev/#laboratory-0-yes-thats-how-we-count-in-python","text":"Link to Content Server","title":"Laboratory 0: Yes, That's how we count in python!"},{"location":"8-Labs/Lab0/Lab0_Dev/#welcome-to-your-first-jupyter-notebook-this-is-a-medium-that-we-will-be-using-throughout-the-semester","text":"","title":"Welcome to your first Jupyter Notebook. This is a medium that we will be using throughout the semester."},{"location":"8-Labs/Lab0/Lab0_Dev/#why-is-this-called-a-notebook","text":"","title":"Why is this called a notebook?"},{"location":"8-Labs/Lab0/Lab0_Dev/#because-you-can-write-stuff-in-it","text":"","title":"Because you can write stuff in it!"},{"location":"8-Labs/Lab0/Lab0_Dev/#is-that-it","text":"","title":"Is that it?"},{"location":"8-Labs/Lab0/Lab0_Dev/#nope-you-can-write-and-run-code-in-this-notebook-plus-a-bunch-of-other-cool-stuff-such-as-making-graphs-running-tests-and-simulations-adding-images-and-prepare-documents-such-as-this-one","text":"","title":"Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!)."},{"location":"8-Labs/Lab0/Lab0_Dev/#how-do-we-get-this","text":"","title":"How do we get this?"},{"location":"8-Labs/Lab0/Lab0_Dev/#there-are-online-services-that-allow-you-create-modify-and-export-jupyter-notebooks-however-to-have-this-on-your-local-machines-computers-you-can-install-anaconda-anaconda-is-a-package-of-different-software-suits-including-jupyter-notebook-you-can-find-videos-on-how-to-install-anaconda-on-your-devices-on-blackboard","text":"Go to Anaconda.com Scroll down to the bottom of the page or click on products > individual edition Download the right version for your system: Windows, MacOS, and Linux- This may take a while depending on your connection speed Once the installer file is downloaded, run it and install Anaconda on your machine. Anaconda requires almost 3 GB of free space Install it in a separate folder- Preferably on a drive with lots of free memory! BE PATIENT!- It will take a while.","title":"There are online services that allow you create, modify, and export Jupyter notebooks. However, to have this on your local machines (computers), you can install Anaconda. Anaconda is a package of different software suits including \"Jupyter Notebook\". You can find videos on how to install Anaconda on your devices on BlackBoard:"},{"location":"8-Labs/Lab0/Lab0_Dev/#the-environment-lets-have-a-look-around-this-window","text":"The tabs File Edit View Insert Cell Kernel The Icons Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list)","title":"The Environment - Let's have a look around this window!"},{"location":"8-Labs/Lab0/Lab0_Dev/#the-notebook-consists-of-a-sequence-of-cells-a-cell-is-a-multiline-text-input-field-and-its-contents-can-be-executed-by-using-shift-enter-or-by-clicking-run-in-the-menu-bar-the-execution-behavior-of-a-cell-is-determined-by-the-cells-type","text":"","title":"The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type."},{"location":"8-Labs/Lab0/Lab0_Dev/#there-are-three-types-of-cells-code-cells-markdown-cells-and-raw-cells-every-cell-starts-off-being-a-code-cell-but-its-type-can-be-changed-by-using-a-drop-down-on-the-toolbar-which-will-be-code-initially","text":"","title":"There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially)."},{"location":"8-Labs/Lab0/Lab0_Dev/#code-cells","text":"","title":"Code Cells:"},{"location":"8-Labs/Lab0/Lab0_Dev/#a-code-cell-allows-you-to-edit-and-write-new-code-with-full-syntax-highlighting-and-tab-completion-the-programming-language-you-use-depends-on-the-kernel-what-we-will-use-for-this-course-and-the-default-kernel-ipython-runs-is-python-code","text":"","title":"A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code."},{"location":"8-Labs/Lab0/Lab0_Dev/#when-a-code-cell-is-executed-code-that-it-contains-is-sent-to-the-kernel-associated-with-the-notebook-the-results-that-are-returned-from-this-computation-are-then-displayed-in-the-notebook-as-the-cells-output-the-output-is-not-limited-to-text-with-many-other-possible-forms-of-output-are-also-possible-including-matplotlib-figures-and-html-tables-this-is-known-as-ipythons-rich-display-capability","text":"","title":"When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability."},{"location":"8-Labs/Lab0/Lab0_Dev/#markdown-cells","text":"","title":"Markdown Cells:"},{"location":"8-Labs/Lab0/Lab0_Dev/#you-can-document-the-computational-process-in-a-literate-way-alternating-descriptive-text-with-code-using-rich-text-in-ipython-this-is-accomplished-by-marking-up-text-with-the-markdown-language-the-corresponding-cells-are-called-markdown-cells-the-markdown-language-provides-a-simple-way-to-perform-this-text-markup-that-is-to-specify-which-parts-of-the-text-should-be-emphasized-italics-bold-form-lists-etc-in-fact-markdown-cells-allow-a-variety-of-cool-modifications-to-be-applied","text":"","title":"You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied:"},{"location":"8-Labs/Lab0/Lab0_Dev/#if-you-want-to-provide-structure-for-your-document-you-can-use-markdown-headings-markdown-headings-consist-of-1-to-5-hash-signs-followed-by-a-space-and-the-title-of-your-section-the-markdown-heading-will-be-converted-to-a-clickable-link-for-a-section-of-the-notebook-it-is-also-used-as-a-hint-when-exporting-to-other-document-formats-like-pdf-here-is-how-it-looks","text":"","title":"If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks:"},{"location":"8-Labs/Lab0/Lab0_Dev/#title","text":"","title":"# title"},{"location":"8-Labs/Lab0/Lab0_Dev/#major-headings","text":"","title":"## major headings"},{"location":"8-Labs/Lab0/Lab0_Dev/#subheadings","text":"","title":"### subheadings"},{"location":"8-Labs/Lab0/Lab0_Dev/#4th-level-subheadings","text":"","title":"#### 4th level subheadings"},{"location":"8-Labs/Lab0/Lab0_Dev/#5th-level-subheadings","text":"","title":"##### 5th level subheadings"},{"location":"8-Labs/Lab0/Lab0_Dev/#these-codes-are-also-quite-useful","text":"Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format: ![](put the image address here.image format)","title":"These codes are also quite useful:"},{"location":"8-Labs/Lab0/Lab0_Dev/#raw-cells","text":"","title":"Raw Cells:"},{"location":"8-Labs/Lab0/Lab0_Dev/#raw-cells-provide-a-place-in-which-you-can-write-output-directly-raw-cells-are-not-evaluated-by-the-notebook","text":"","title":"Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook."},{"location":"8-Labs/Lab0/Lab0_Dev/#lets-meet-worlds-most-popular-python","text":"","title":"Let's meet world's most popular python!"},{"location":"8-Labs/Lab0/Lab0_Dev/#what-is-python","text":"\"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language)","title":"What is python?"},{"location":"8-Labs/Lab0/Lab0_Dev/#how-to-have-access-to-it","text":"","title":"How to have access to it?"},{"location":"8-Labs/Lab0/Lab0_Dev/#there-are-plenty-of-ways-from-online-compilers-to-our-beloved-jupyter-notebook-on-your-local-machines-here-are-a-few-examples-of-online-compilers","text":"a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3","title":"There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers:"},{"location":"8-Labs/Lab0/Lab0_Dev/#we-can-do-the-exact-same-thing-in-this-notebook-but-we-need-a-code-cell","text":"print(\"Hello World\") Hello World","title":"We can do the exact same thing in this notebook. But we need a CODE cell."},{"location":"8-Labs/Lab0/Lab0_Dev/#this-is-the-classic-first-program-of-many-languages-the-script-input-is-quite-simple-we-instruct-the-computer-to-print-the-literal-string-hello-world-to-standard-inputoutput-device-which-is-the-console-lets-change-it-and-see-what-happens","text":"print(\"This is my first notebook!\") This is my first notebook!","title":"This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens:"},{"location":"8-Labs/Lab0/Lab0_Dev/#how-to-save-a-notebook","text":"As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE","title":"How to save a notebook?"},{"location":"8-Labs/Lab0/Lab0_Dev/#exercise-lets-see-who-you-are","text":"","title":"Exercise: Let's see who you are! "},{"location":"8-Labs/Lab0/Lab0_Dev/#similar-to-the-example-use-a-code-cell-and-print-a-paragraph-about-you-you-can-introduce-yourselves-and-write-about-interesting-things-to-and-about-you","text":"","title":"Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!"},{"location":"8-Labs/Lab1/Lab1_Dev/","text":"Laboratory 1: First Steps... Notice the code cell below! From this notebook forward please include and run the script in the cell, it will help in debugging a notebook. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Also, from now on, please make sure that you have the following markdown cell, filled with your own information, on top of your notebooks: Full name: R#: Title of the notebook: Date: Now, let's get to work! Variables Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float). TimeOfConcentration + 5 5.0 Naming Rules Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print, input, if, while, and for. There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables. Operators The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below: # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10. What's it with # ? Comments are added by writing a hashtag symbol (#) followed by any text of your choice. Any text that follows the hashtag symbol on the same line is ignored by the Python interpreter. Arithmetic Operators In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x ** y Raises value in x by value in y. ( e.g. xy) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0 Data Type In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary Integer Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309 Real (Float) A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427 String(Alphanumeric) A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) print(MyName[0:4]) # Notice how the string is sliced- This is Python: ALWAYS start counting from zero! All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Theo Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting. Changing Types A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens! Expressions Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15 Example: Simple Input/Output Get two floating point numbers via the input() function and store them under the variable names float1 and float2 . Then, compare them, and try a few operations on them! float1 = input(\"Please enter float1: \") float1 = float(float1) ... Print float1 and float2 to the output screen. print(\"float1:\", float1) ... Then check whether float1 is greater than or equal to float2 . float1 = input(\"Please enter float1: \") float2 = input(\"Please enter float2: \") Please enter float1: 2.5 Please enter float2: 5 print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5 float1 = float(float1) float2 = float(float2) print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5.0 float1>float2 False float1+float2 7.5 float1/float2 0.5 Here are some great reads on this topic: - \"Variables in Python\" by John Sturtz available at https://realpython.com/python-variables/ - \"A Beginner\u2019s Guide To Python Variables\" by Avijeet Biswal available at https://www.simplilearn.com/tutorials/python-tutorial/python-variables - \"A Very Basic Introduction to Variables in Python\" by Dr. Python available at *https://medium.com/@doctorsmonsters/a-very-basic-introduction-to-variables-in-python-4231e36dac52 Here are some great videos on these topics: - \"Python Tutorial for Absolute Beginners #1 - What Are Variables?\" by CS Dojo available at https://www.youtube.com/watch?v=Z1Yd7upQsXY - \"#4 Python Tutorial for Beginners | Variables in Python\" by Telusko available at https://www.youtube.com/watch?v=TqPzwenhMj0 - \"Variables and Types in Python\" by DataCamp available at *https://www.youtube.com/watch?v=OH86oLzVzzw Exercise: Integer or Float? Think of a few cases where one might need to convert a float into an integer. * Make sure to cite any resources that you may use.","title":"<font color=darkred>Laboratory 1: First Steps... </font>"},{"location":"8-Labs/Lab1/Lab1_Dev/#laboratory-1-first-steps","text":"","title":"Laboratory 1: First Steps... "},{"location":"8-Labs/Lab1/Lab1_Dev/#notice-the-code-cell-below","text":"","title":"Notice the code cell below!"},{"location":"8-Labs/Lab1/Lab1_Dev/#from-this-notebook-forward-please-include-and-run-the-script-in-the-cell-it-will-help-in-debugging-a-notebook","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"From this notebook forward please include and run the script in the cell, it will help in debugging a notebook."},{"location":"8-Labs/Lab1/Lab1_Dev/#also-from-now-on-please-make-sure-that-you-have-the-following-markdown-cell-filled-with-your-own-information-on-top-of-your-notebooks","text":"","title":"Also, from now on, please make sure that you have the following markdown cell, filled with your own information, on top of your notebooks:"},{"location":"8-Labs/Lab1/Lab1_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab1/Lab1_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab1/Lab1_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab1/Lab1_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab1/Lab1_Dev/#now-lets-get-to-work","text":"","title":"Now, let's get to work!"},{"location":"8-Labs/Lab1/Lab1_Dev/#variables","text":"Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float). TimeOfConcentration + 5 5.0","title":"Variables"},{"location":"8-Labs/Lab1/Lab1_Dev/#naming-rules","text":"Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print, input, if, while, and for. There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables.","title":"Naming Rules"},{"location":"8-Labs/Lab1/Lab1_Dev/#operators","text":"The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below: # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10.","title":"Operators"},{"location":"8-Labs/Lab1/Lab1_Dev/#whats-it-with","text":"Comments are added by writing a hashtag symbol (#) followed by any text of your choice. Any text that follows the hashtag symbol on the same line is ignored by the Python interpreter.","title":"What's it with # ?"},{"location":"8-Labs/Lab1/Lab1_Dev/#arithmetic-operators","text":"In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x ** y Raises value in x by value in y. ( e.g. xy) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0","title":"Arithmetic Operators"},{"location":"8-Labs/Lab1/Lab1_Dev/#data-type","text":"In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary","title":"Data Type"},{"location":"8-Labs/Lab1/Lab1_Dev/#integer","text":"Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309","title":"Integer"},{"location":"8-Labs/Lab1/Lab1_Dev/#real-float","text":"A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427","title":"Real (Float)"},{"location":"8-Labs/Lab1/Lab1_Dev/#stringalphanumeric","text":"A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) print(MyName[0:4]) # Notice how the string is sliced- This is Python: ALWAYS start counting from zero! All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Theo Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting.","title":"String(Alphanumeric)"},{"location":"8-Labs/Lab1/Lab1_Dev/#changing-types","text":"A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens!","title":"Changing Types"},{"location":"8-Labs/Lab1/Lab1_Dev/#expressions","text":"Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15","title":"Expressions"},{"location":"8-Labs/Lab1/Lab1_Dev/#example-simple-inputoutput","text":"Get two floating point numbers via the input() function and store them under the variable names float1 and float2 . Then, compare them, and try a few operations on them! float1 = input(\"Please enter float1: \") float1 = float(float1) ... Print float1 and float2 to the output screen. print(\"float1:\", float1) ... Then check whether float1 is greater than or equal to float2 . float1 = input(\"Please enter float1: \") float2 = input(\"Please enter float2: \") Please enter float1: 2.5 Please enter float2: 5 print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5 float1 = float(float1) float2 = float(float2) print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5.0 float1>float2 False float1+float2 7.5 float1/float2 0.5 Here are some great reads on this topic: - \"Variables in Python\" by John Sturtz available at https://realpython.com/python-variables/ - \"A Beginner\u2019s Guide To Python Variables\" by Avijeet Biswal available at https://www.simplilearn.com/tutorials/python-tutorial/python-variables - \"A Very Basic Introduction to Variables in Python\" by Dr. Python available at *https://medium.com/@doctorsmonsters/a-very-basic-introduction-to-variables-in-python-4231e36dac52 Here are some great videos on these topics: - \"Python Tutorial for Absolute Beginners #1 - What Are Variables?\" by CS Dojo available at https://www.youtube.com/watch?v=Z1Yd7upQsXY - \"#4 Python Tutorial for Beginners | Variables in Python\" by Telusko available at https://www.youtube.com/watch?v=TqPzwenhMj0 - \"Variables and Types in Python\" by DataCamp available at *https://www.youtube.com/watch?v=OH86oLzVzzw","title":"Example: Simple Input/Output"},{"location":"8-Labs/Lab1/Lab1_Dev/#exercise-integer-or-float","text":"","title":"Exercise: Integer or Float? "},{"location":"8-Labs/Lab1/Lab1_Dev/#think-of-a-few-cases-where-one-might-need-to-convert-a-float-into-an-integer","text":"","title":"Think of a few cases where one might need to convert a float into an integer."},{"location":"8-Labs/Lab1/Lab1_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab2/Lab2_Dev/","text":"Laboratory 2: Structures and Conditions. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Data Structures: List (Array) A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\ x_1= 11 \\ x_2= 5 \\ x_3= 9 \\ x_4= 13 \\ ... \\ x_N= 223 \\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO. Alot of other lnguages start at ONE. Its just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9. MyOtherList = [] #Create an empty list MyOtherList.append(765) #Add one item to the list print(MyOtherList) MyList = [7,11,5,9,13,66,99,223] #Define a list print(MyList) sublist = MyList[3:6] #slice a sublist print(\"sublist is: \", sublist) mysum = sum(sublist) #sum the numbers in the sublist print(\"Sum: \", mysum) mylength = len(sublist) #get the length of the sublist print(\"Length: \", mylength) [765] [7, 11, 5, 9, 13, 66, 99, 223] sublist is: [9, 13, 66] Sum: 88 Length: 3 Data Structures: Special List | Tuple A tuple is a special kind of list where the values cannot be changed after the list is created. It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") Data Structures: Special List | Dictionary A dictionary is a special kind of list where the items are related data PAIRS. It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Some examples follow: MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") MyTupleName ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec') MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} print(MyPetsNamesAndMass) MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print(MyPetsNamesAndMassToo) {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} # Tuples MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") # Access a Tuple print (\"5th element of the tuple:\", MyTupleName[4]) # Dictionary MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} # Access the Dictionary print (\"Aspen's mass = \", MyPetsNamesAndMass[\"Aspen\"]) # Change a value in a dictionary print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"]) MyPetsNamesAndMass[\"Merrimee\"] = 0.01 print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"], \"She lost weight !\") # Alternate dictionary MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print (\"Merrimee's mass\" , MyPetsNamesAndMassToo[\"Merrimee\"]) # Attempt to change a Tuple #MyTupleName[3]=(\"Fred\") # Activate this line and see what happens! 5th element of the tuple: May Aspen's mass = 6.3 Merrimee's mass 0.03 Merrimee's mass 0.01 She lost weight ! Merrimee's mass 0.03 Example: Nested Dictionary From the dictionary below, print \"Pandemic\" and \"Tokyo\": FD = {\"Quentin\":\"Tarantino\",\"2020\":[2020,\"COVID\",19,\"Pandemic\"],\"Bond\":[\"James\",\"Gun\",(\"Paris\",\"Tokyo\",\"London\")]} #A nested dictionary print(FD) {'Quentin': 'Tarantino', '2020': [2020, 'COVID', 19, 'Pandemic'], 'Bond': ['James', 'Gun', ('Paris', 'Tokyo', 'London')]} FD['2020'][3] 'Pandemic' FD['Bond'][2][1] 'Tokyo' Conditional Execution Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, butmore recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs. Conditional Execution: Comparison The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal. # Compare x = 7 y = 10 print(\"x =: \",x,\"y =: \",y) print(\"x is equal to y : \",x==y) print(\"x is not equal to y : \",x!=y) print(\"x is greater than y : \",x>y) print(\"x is less than y : \",x<y) x =: 7 y =: 10 x is equal to y : False x is not equal to y : True x is greater than y : False x is less than y : True # Logical operators print(\"5 == 5 and 5 < 6 ? \",5 == 5 and 5 < 6) print(\"4 > 3 or 17 > 20 \",4 > 3 or 17 > 20) print(\"not 5 == 5\",not 5 == 5) 5 == 5 and 5 < 6 ? True 4 > 3 or 17 > 20 True not 5 == 5 False Conditional Execution: Block if statement The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter. The next code fragment illustrates illustrates how the if statements work. The program asks the user for input. The use of raw_input() will let the program read any input as a string so non-numeric results will not throw an error. The input is stored in the variable named userInput . Next the statement if userInput == \"1\": compares the value of userInput with the string \"1\" . If the value in the variable is indeed \\1\", then the program will execute the block of code in the indentation after the colon. In this case it will execute print \"Hello World\" print \"How do you do? \" Alternatively, if the value of userInput is the string '2' , then the program will execute print \"Snakes on a plane \" For all other values the program will execute print \"You did not enter a valid number\" # Block if example userInput = input('Enter the number 1 or 2') # Use block if structure if userInput == '1': print(\"Hello World\") print(\"How do you do? \") elif userInput == '2': print(\"Snakes on a plane \") else: print(\"You did not enter a valid number\") Enter the number 1 or 21 Hello World How do you do? Conditional Execution: Inline if statement An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops. myInt = 0 num1 = 12 if (myInt == 0) else 13 num1 12 Example: Pass or Fail? Take the following inputs from the user: 1. Grade for Lesson 1 (from 0 to 5) 2. Grade for Lesson 2 (from 0 to 5) 3. Grade for Lesson 3 (from 0 to 5) Compute the average of the three grades. Use the result to decide whether the student will pass or fail. Lesson1 = int(input('Enter the grade for Lesson 1')) Lesson2 = int(input('Enter the grade for Lesson 2')) Lesson3 = int(input('Enter the grade for Lesson 3')) Average = int(Lesson1+Lesson2+Lesson3)/3 print('Average Course Grade:',Average) if Average >= 5: print(\"Passed\") else: print(\"Failed\") Enter the grade for Lesson 12 Enter the grade for Lesson 25 Enter the grade for Lesson 31 Average Course Grade: 2.6666666666666665 Failed Here are some great reads on this topic: - \"Common Python Data Structures (Guide)\" by Dan Bader available at https://realpython.com/python-data-structures/ - \"Data Structures You Need To Learn In Python\" by Akash available at https://www.edureka.co/blog/data-structures-in-python/ - \"Data Structures in Python\u2014 A Brief Introduction\" by Sowmya Krishnan available at https://towardsdatascience.com/data-structures-in-python-a-brief-introduction-b4135d7a9b7d - \"Everything you Should Know About Data Structures in Python\" by ANIRUDDHA BHANDARI available at https://www.analyticsvidhya.com/blog/2020/06/data-structures-python/ - \"Conditional Statements in Python\" by John Sturtz available at https://realpython.com/python-conditional-statements/ - \"Python If Statement explained with examples\" by CHAITANYA SINGH available at https://beginnersbook.com/2018/01/python-if-statement-example/ Here are some great videos on these topics: - \"Python: Data Structures - Lists, Tuples, Sets & Dictionaries tutorial\" by Joe James available at https://www.youtube.com/watch?v=R-HLU9Fl5ug&t=92s - \"Python Tutorial for Beginners 5: Dictionaries - Working with Key-Value Pairs\" by Corey Schafer available at https://www.youtube.com/watch?v=daefaLgNkw0 - \"How to Use If Else Statements in Python (Python Tutorial #2)\" by CS Dojo available at https://www.youtube.com/watch?v=AWek49wXGzI - \"Python If Statements | Python Tutorial #10\" by Amigoscode available at https://www.youtube.com/watch?v=wKQRmXR3jhc Exercise: Why dictionaries? Why do we need to use dictionaries in python? * Make sure to cite any resources that you may use.","title":"<font color=darkred>Laboratory 2: Structures and Conditions. </font>"},{"location":"8-Labs/Lab2/Lab2_Dev/#laboratory-2-structures-and-conditions","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 2: Structures and Conditions. "},{"location":"8-Labs/Lab2/Lab2_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab2/Lab2_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab2/Lab2_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab2/Lab2_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab2/Lab2_Dev/#data-structures-list-array","text":"A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\ x_1= 11 \\ x_2= 5 \\ x_3= 9 \\ x_4= 13 \\ ... \\ x_N= 223 \\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO. Alot of other lnguages start at ONE. Its just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9. MyOtherList = [] #Create an empty list MyOtherList.append(765) #Add one item to the list print(MyOtherList) MyList = [7,11,5,9,13,66,99,223] #Define a list print(MyList) sublist = MyList[3:6] #slice a sublist print(\"sublist is: \", sublist) mysum = sum(sublist) #sum the numbers in the sublist print(\"Sum: \", mysum) mylength = len(sublist) #get the length of the sublist print(\"Length: \", mylength) [765] [7, 11, 5, 9, 13, 66, 99, 223] sublist is: [9, 13, 66] Sum: 88 Length: 3","title":"Data Structures: List (Array)"},{"location":"8-Labs/Lab2/Lab2_Dev/#data-structures-special-list-tuple","text":"A tuple is a special kind of list where the values cannot be changed after the list is created. It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")","title":"Data Structures: Special List | Tuple"},{"location":"8-Labs/Lab2/Lab2_Dev/#data-structures-special-list-dictionary","text":"A dictionary is a special kind of list where the items are related data PAIRS. It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Some examples follow: MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") MyTupleName ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec') MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} print(MyPetsNamesAndMass) MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print(MyPetsNamesAndMassToo) {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} # Tuples MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") # Access a Tuple print (\"5th element of the tuple:\", MyTupleName[4]) # Dictionary MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} # Access the Dictionary print (\"Aspen's mass = \", MyPetsNamesAndMass[\"Aspen\"]) # Change a value in a dictionary print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"]) MyPetsNamesAndMass[\"Merrimee\"] = 0.01 print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"], \"She lost weight !\") # Alternate dictionary MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print (\"Merrimee's mass\" , MyPetsNamesAndMassToo[\"Merrimee\"]) # Attempt to change a Tuple #MyTupleName[3]=(\"Fred\") # Activate this line and see what happens! 5th element of the tuple: May Aspen's mass = 6.3 Merrimee's mass 0.03 Merrimee's mass 0.01 She lost weight ! Merrimee's mass 0.03","title":"Data Structures: Special List | Dictionary"},{"location":"8-Labs/Lab2/Lab2_Dev/#example-nested-dictionary","text":"From the dictionary below, print \"Pandemic\" and \"Tokyo\": FD = {\"Quentin\":\"Tarantino\",\"2020\":[2020,\"COVID\",19,\"Pandemic\"],\"Bond\":[\"James\",\"Gun\",(\"Paris\",\"Tokyo\",\"London\")]} #A nested dictionary print(FD) {'Quentin': 'Tarantino', '2020': [2020, 'COVID', 19, 'Pandemic'], 'Bond': ['James', 'Gun', ('Paris', 'Tokyo', 'London')]} FD['2020'][3] 'Pandemic' FD['Bond'][2][1] 'Tokyo'","title":"Example: Nested Dictionary"},{"location":"8-Labs/Lab2/Lab2_Dev/#conditional-execution","text":"Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, butmore recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs.","title":"Conditional Execution"},{"location":"8-Labs/Lab2/Lab2_Dev/#conditional-execution-comparison","text":"The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal. # Compare x = 7 y = 10 print(\"x =: \",x,\"y =: \",y) print(\"x is equal to y : \",x==y) print(\"x is not equal to y : \",x!=y) print(\"x is greater than y : \",x>y) print(\"x is less than y : \",x<y) x =: 7 y =: 10 x is equal to y : False x is not equal to y : True x is greater than y : False x is less than y : True # Logical operators print(\"5 == 5 and 5 < 6 ? \",5 == 5 and 5 < 6) print(\"4 > 3 or 17 > 20 \",4 > 3 or 17 > 20) print(\"not 5 == 5\",not 5 == 5) 5 == 5 and 5 < 6 ? True 4 > 3 or 17 > 20 True not 5 == 5 False","title":"Conditional Execution: Comparison"},{"location":"8-Labs/Lab2/Lab2_Dev/#conditional-execution-block-if-statement","text":"The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter. The next code fragment illustrates illustrates how the if statements work. The program asks the user for input. The use of raw_input() will let the program read any input as a string so non-numeric results will not throw an error. The input is stored in the variable named userInput . Next the statement if userInput == \"1\": compares the value of userInput with the string \"1\" . If the value in the variable is indeed \\1\", then the program will execute the block of code in the indentation after the colon. In this case it will execute print \"Hello World\" print \"How do you do? \" Alternatively, if the value of userInput is the string '2' , then the program will execute print \"Snakes on a plane \" For all other values the program will execute print \"You did not enter a valid number\" # Block if example userInput = input('Enter the number 1 or 2') # Use block if structure if userInput == '1': print(\"Hello World\") print(\"How do you do? \") elif userInput == '2': print(\"Snakes on a plane \") else: print(\"You did not enter a valid number\") Enter the number 1 or 21 Hello World How do you do?","title":"Conditional Execution:  Block if statement"},{"location":"8-Labs/Lab2/Lab2_Dev/#conditional-execution-inline-if-statement","text":"An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops. myInt = 0 num1 = 12 if (myInt == 0) else 13 num1 12","title":"Conditional Execution:  Inline if statement"},{"location":"8-Labs/Lab2/Lab2_Dev/#example-pass-or-fail","text":"Take the following inputs from the user: 1. Grade for Lesson 1 (from 0 to 5) 2. Grade for Lesson 2 (from 0 to 5) 3. Grade for Lesson 3 (from 0 to 5) Compute the average of the three grades. Use the result to decide whether the student will pass or fail. Lesson1 = int(input('Enter the grade for Lesson 1')) Lesson2 = int(input('Enter the grade for Lesson 2')) Lesson3 = int(input('Enter the grade for Lesson 3')) Average = int(Lesson1+Lesson2+Lesson3)/3 print('Average Course Grade:',Average) if Average >= 5: print(\"Passed\") else: print(\"Failed\") Enter the grade for Lesson 12 Enter the grade for Lesson 25 Enter the grade for Lesson 31 Average Course Grade: 2.6666666666666665 Failed Here are some great reads on this topic: - \"Common Python Data Structures (Guide)\" by Dan Bader available at https://realpython.com/python-data-structures/ - \"Data Structures You Need To Learn In Python\" by Akash available at https://www.edureka.co/blog/data-structures-in-python/ - \"Data Structures in Python\u2014 A Brief Introduction\" by Sowmya Krishnan available at https://towardsdatascience.com/data-structures-in-python-a-brief-introduction-b4135d7a9b7d - \"Everything you Should Know About Data Structures in Python\" by ANIRUDDHA BHANDARI available at https://www.analyticsvidhya.com/blog/2020/06/data-structures-python/ - \"Conditional Statements in Python\" by John Sturtz available at https://realpython.com/python-conditional-statements/ - \"Python If Statement explained with examples\" by CHAITANYA SINGH available at https://beginnersbook.com/2018/01/python-if-statement-example/ Here are some great videos on these topics: - \"Python: Data Structures - Lists, Tuples, Sets & Dictionaries tutorial\" by Joe James available at https://www.youtube.com/watch?v=R-HLU9Fl5ug&t=92s - \"Python Tutorial for Beginners 5: Dictionaries - Working with Key-Value Pairs\" by Corey Schafer available at https://www.youtube.com/watch?v=daefaLgNkw0 - \"How to Use If Else Statements in Python (Python Tutorial #2)\" by CS Dojo available at https://www.youtube.com/watch?v=AWek49wXGzI - \"Python If Statements | Python Tutorial #10\" by Amigoscode available at https://www.youtube.com/watch?v=wKQRmXR3jhc","title":"Example: Pass or Fail?"},{"location":"8-Labs/Lab2/Lab2_Dev/#exercise-why-dictionaries","text":"","title":"Exercise: Why dictionaries? "},{"location":"8-Labs/Lab2/Lab2_Dev/#why-do-we-need-to-use-dictionaries-in-python","text":"","title":"Why do we need to use dictionaries in python?"},{"location":"8-Labs/Lab2/Lab2_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab6/Lab6_Dev/","text":"Laboratory 6: Numpy for Bread! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Numpy Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is short for \u201cNumeric Python\u201d or \u201cNumerical Python\u201d. If you are curious about NumPy, this cheat sheet is recommended: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf Arrays A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension. In other words, an array contains information about the raw data, how to locate an element and how to interpret an element.To make a numpy array, you can just use the np.array() function. All you need to do is pass a list to it. Don\u2019t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. If you want to read more about the differences between a Python list and NumPy array, this link is recommended: https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference Example- 1D Arrays Let's create a 1D array from the 2000s (2000-2009): import numpy as np #First, we need to impoty \"numpy\" mylist = [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009] #Create a list of the years print(mylist) #Check how it looks np.array(mylist) #Define it as a numpy array [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009] array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]) Example- n-Dimensional Arrays Let's create a 5x2 array from the 2000s (2000-2009): myotherlist = [[2000,2001],[2002,2003],[2004,2005],[2006,2007],[2008,2009]] #Since I want a 5x2 array, I should group the years two by two print(myotherlist) #See how it looks as a list np.array(myotherlist) #See how it looks as a numpy array [[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]] array([[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]]) Arrays Arithmetic Once you have created the arrays, you can do basic Numpy operations. Numpy offers a variety of operations applicable on arrays. From basic operations such as summation, subtraction, multiplication and division to more advanced and essential operations such as matrix multiplication and other elementwise operations. In the examples below, we will go over some of these: Example- 1D Array Arithmetic Define a 1D array with [0,12,24,36,48,60,72,84,96] Multiple all elements by 2 Take all elements to the power of 2 Find the maximum value of the array and its position Find the minimum value of the array and its position Define another 1D array with [-12,0,12,24,36,48,60,72,84] Find the summation and subtraction of these two arrays Find the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([0,12,24,36,48,60,72,84,96]) #Step1: Define Array1 print(Array1) print(Array1*2) #Step2: Multiple all elements by 2 print(Array1**2) #Step3: Take all elements to the power of 2 print(np.power(Array1,2)) #Another way to do the same thing, by using a function in numpy print(np.max(Array1)) #Step4: Find the maximum value of the array print(np.argmax(Array1)) ##Step4: Find the postition of the maximum value print(np.min(Array1)) #Step5: Find the minimum value of the array print(np.argmin(Array1)) ##Step5: Find the postition of the minimum value Array2 = np.array([-12,0,12,24,36,48,60,72,84]) #Step6: Define Array2 print(Array2) print(Array1+Array2) #Step7: Find the summation of these two arrays print(Array1-Array2) #Step7: Find the subtraction of these two arrays print(Array1*Array2) #Step8: Find the multiplication of these two arrays [ 0 12 24 36 48 60 72 84 96] [ 0 24 48 72 96 120 144 168 192] [ 0 144 576 1296 2304 3600 5184 7056 9216] [ 0 144 576 1296 2304 3600 5184 7056 9216] 96 8 0 0 [-12 0 12 24 36 48 60 72 84] [-12 12 36 60 84 108 132 156 180] [12 12 12 12 12 12 12 12 12] [ 0 0 288 864 1728 2880 4320 6048 8064] Example- n-Dimensional Array Arithmetic Define a 2x2 array with [5,10,15,20] Define another 2x2 array with [3,6,9,12] Find the summation and subtraction of these two arrays Find the minimum number in the multiplication of these two arrays Find the position of the maximum in the multiplication of these two arrays Find the mean of the multiplication of these two arrays Find the mean of the first row of the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([[5,10],[15,20]]) #Step1: Define Array1 print(Array1) Array2 = np.array([[3,6],[9,12]]) #Step2: Define Array2 print(Array2) print(Array1+Array2) #Step3: Find the summation print(Array1-Array2) #Step3: Find the subtraction MultArray = Array1@Array2 #Step4: To perform a typical matrix multiplication (or matrix product) MultArray1 = Array1.dot(Array2) #Step4: Another way To perform a matrix multiplication print(MultArray) print(MultArray1) print(np.min(MultArray)) #Step4: Find the minimum value of the multiplication print(np.argmax(MultArray)) ##Step5: Find the postition of the maximum value print(np.mean(MultArray)) ##Step6: Find the mean of the multiplication of these two arrays print(np.mean(MultArray[0,:])) ##Step7: Find the mean of the first row of the multiplication of these two arrays [[ 5 10] [15 20]] [[ 3 6] [ 9 12]] [[ 8 16] [24 32]] [[2 4] [6 8]] [[105 150] [225 330]] [[105 150] [225 330]] 105 3 202.5 127.5 Arrays Comparison Comparing two NumPy arrays determines whether they are equivalent by checking if every element at each corresponding index are the same. Example- 1D Array Comparison Define a 1D array with [1.0,2.5,3.4,7,7] Define another 1D array with [5.0/5.0,5.0/2,6.8/2,21/3,14/2] Compare and see if the two arrays are equal Define another 1D array with [6,1.4,2.2,7.5,7] Compare and see if the first array is greater than or equal to the third array import numpy as np #import numpy Array1 = np.array([1.0,2.5,3.4,7,7]) #Step1: Define Array1 print(Array1) Array2 = np.array([5.0/5.0,5.0/2,6.8/2,21/3,14/2]) #Step2: Define Array1 print(Array2) print(np.equal(Array1, Array2)) #Step3: Compare and see if the two arrays are equal Array3 = np.array([6,1.4,2.2,7.5,7]) #Step4: Define Array3 print(Array3) print(np.greater_equal(Array1, Array3)) #Step3: Compare and see if the two arrays are equal [1. 2.5 3.4 7. 7. ] [1. 2.5 3.4 7. 7. ] [ True True True True True] [6. 1.4 2.2 7.5 7. ] [False True True False True] Arrays Manipulation numpy.copy() allows us to create a copy of an array. This is particularly useful when we need to manipulate an array while keeping an original copy in memory. The numpy.delete() function returns a new array with sub-arrays along an axis deleted. Let's have a look at the examples. Example- Copying and Deleting Arrays and Elements Define a 1D array, named \"x\" with [1,2,3] Define \"y\" so that \"y=x\" Define \"z\" as a copy of \"x\" Discuss the difference between y and z Delete the second element of x import numpy as np #import numpy x = np.array([1,2,3]) #Step1: Define x print(x) y = x #Step2: Define y as y=x print(y) z = np.copy(x) #Step3: Define z as a copy of x print(z) # For Step4: They look similar but check this out: x[1] = 8 # If we change x ... print(x) print(y) print(z) # By modifying x, y changes but z remains as a copy of the initial version of x. x = np.delete(x, 1) #Step5: Delete the second element of x print(x) [1 2 3] [1 2 3] [1 2 3] [1 8 3] [1 8 3] [1 2 3] [1 3] Sorting Arrays Sorting means putting elements in an ordered sequence. Ordered sequence is any sequence that has an order corresponding to elements, like numeric or alphabetical, ascending or descending. If you use the sort() method on a 2-D array, both arrays will be sorted. Example- Sorting 1D Arrays Define a 1D array as ['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed'] and print it out. Then, sort the array alphabetically. import numpy as np #import numpy games = np.array(['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed']) print(games) print(np.sort(games)) ['FIFA 2020' 'Red Dead Redemption' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed'] ['FIFA 2020' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed' 'Red Dead Redemption'] Example- Sorting n-Dimensional Arrays Define a 3x3 array with 17,-6,2,86,-12,0,0,23,12 and print it out. Then, sort the array. import numpy as np #import numpy a = np.array([[17,-6,2],[86,-12,0],[0,23,12]]) print(a) print (\"Along columns : \\n\", np.sort(a,axis = 0) ) #This will be sorting in each column print (\"Along rows : \\n\", np.sort(a,axis = 1) ) #This will be sorting in each row print (\"Sorting by default : \\n\", np.sort(a) ) #Same as above print (\"Along None Axis : \\n\", np.sort(a,axis = None) ) #This will be sorted like a 1D array [[ 17 -6 2] [ 86 -12 0] [ 0 23 12]] Along columns : [[ 0 -12 0] [ 17 -6 2] [ 86 23 12]] Along rows : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Sorting by default : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Along None Axis : [-12 -6 0 0 2 12 17 23 86] Partitioning (Slice) Arrays Slicing in python means taking elements from one given index to another given index. We can do slicing like this: [start:end]. We can also define the step, like this: [start:end:step]. If we don't pass start its considered 0 If we don't pass end its considered length of array in that dimension If we don't pass step its considered 1 Example- Slicing 1D Arrays Define a 1D array as [1,3,5,7,9], slice out the [3,5,7] and print it out. import numpy as np #import numpy a = np.array([1,3,5,7,9]) #Define the array print(a) aslice = a[1:4] #slice the [3,5,7] print(aslice) #print it out [1 3 5 7 9] [3 5 7] Example- Slicing n-Dimensional Arrays Define a 5x5 array with \"Superman, Batman, Jim Hammond, Captain America, Green Arrow, Aquaman, Wonder Woman, Martian Manhunter, Barry Allen, Hal Jordan, Hawkman, Ray Palmer, Spider Man, Thor, Hank Pym, Solar, Iron Man, Dr. Strange, Daredevil, Ted Kord, Captian Marvel, Black Panther, Wolverine, Booster Gold, Spawn \" and print it out. Then: - Slice the first column and print it out - Slice the third row and print it out - Slice 'Wolverine' and print it out - Slice a 3x3 array with 'Wonder Woman, Ray Palmer, Iron Man, Martian Manhunter, Spider Man, Dr. Strange, Barry Allen, Thor, Daredevil' import numpy as np #import numpy Superheroes = np.array([['Superman', 'Batman', 'Jim Hammond', 'Captain America', 'Green Arrow'], ['Aquaman', 'Wonder Woman', 'Martian Manhunter', 'Barry Allen', 'Hal Jordan'], ['Hawkman', 'Ray Palmer', 'Spider Man', 'Thor', 'Hank Pym'], ['Solar', 'Iron Man', 'Dr. Strange', 'Daredevil', 'Ted Kord'], ['Captian Marvel', 'Black Panther', 'Wolverine', 'Booster Gold', 'Spawn']]) print(Superheroes) #Step1 print(Superheroes[:,0]) print(Superheroes[2,:]) print(Superheroes[4,2]) print(Superheroes[1:4,1:4]) [['Superman' 'Batman' 'Jim Hammond' 'Captain America' 'Green Arrow'] ['Aquaman' 'Wonder Woman' 'Martian Manhunter' 'Barry Allen' 'Hal Jordan'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] ['Solar' 'Iron Man' 'Dr. Strange' 'Daredevil' 'Ted Kord'] ['Captian Marvel' 'Black Panther' 'Wolverine' 'Booster Gold' 'Spawn']] ['Superman' 'Aquaman' 'Hawkman' 'Solar' 'Captian Marvel'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] Wolverine [['Wonder Woman' 'Martian Manhunter' 'Barry Allen'] ['Ray Palmer' 'Spider Man' 'Thor'] ['Iron Man' 'Dr. Strange' 'Daredevil']] This is a Numpy Cheat Sheet- similar to the one you had on top of this notebook! Check out this link for more: https://blog.finxter.com/collection-10-best-numpy-cheat-sheets-every-python-coder-must-own/ Here are some of the resources used for creating this notebook: - Johnson, J. (2020). Python Numpy Tutorial (with Jupyter and Colab). Retrieved September 15, 2020, from https://cs231n.github.io/python-numpy-tutorial/ - Willems, K. (2019). (Tutorial) Python NUMPY Array TUTORIAL. Retrieved September 15, 2020, from https://www.datacamp.com/community/tutorials/python-numpy-tutorial?utm_source=adwords_ppc - Willems, K. (2017). NumPy Cheat Sheet: Data Analysis in Python. Retrieved September 15, 2020, from https://www.datacamp.com/community/blog/python-numpy-cheat-sheet - W3resource. (2020). NumPy: Compare two given arrays. Retrieved September 15, 2020, from https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-28.php Here are some great reads on this topic: - \"Python NumPy Tutorial\" available at https://www.geeksforgeeks.org/python-numpy-tutorial/ - \"What Is NumPy?\" a collection of blogs, available at https://realpython.com/tutorials/numpy/ - \"Look Ma, No For-Loops: Array Programming With NumPy\" by Brad Solomon available at https://realpython.com/numpy-array-programming/ - \"The Ultimate Beginner\u2019s Guide to NumPy\" by Anne Bonner available at https://towardsdatascience.com/the-ultimate-beginners-guide-to-numpy-f5a2f99aef54 Here are some great videos on these topics: - \"Learn NUMPY in 5 minutes - BEST Python Library!\" by Python Programmer available at https://www.youtube.com/watch?v=xECXZ3tyONo - \"Python NumPy Tutorial for Beginners\" by freeCodeCamp.org available at https://www.youtube.com/watch?v=QUT1VHiLmmI - \"Complete Python NumPy Tutorial (Creating Arrays, Indexing, Math, Statistics, Reshaping)\" by Keith Galli available at https://www.youtube.com/watch?v=GB9ByFAIAH4 - \"Python NumPy Tutorial | NumPy Array | Python Tutorial For Beginners | Python Training | Edureka\" by edureka! available at https://www.youtube.com/watch?v=8JfDAm9y_7s Exercise: Python List vs. Numpy Arrays? What are some differences between Python lists and Numpy arrays? * Make sure to cite any resources that you may use.","title":"<font color=darkred>Laboratory 6: Numpy for Bread! </font>"},{"location":"8-Labs/Lab6/Lab6_Dev/#laboratory-6-numpy-for-bread","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 6: Numpy for Bread! "},{"location":"8-Labs/Lab6/Lab6_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab6/Lab6_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab6/Lab6_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab6/Lab6_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab6/Lab6_Dev/#numpy","text":"Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is short for \u201cNumeric Python\u201d or \u201cNumerical Python\u201d. If you are curious about NumPy, this cheat sheet is recommended: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf","title":"Numpy"},{"location":"8-Labs/Lab6/Lab6_Dev/#arrays","text":"A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension. In other words, an array contains information about the raw data, how to locate an element and how to interpret an element.To make a numpy array, you can just use the np.array() function. All you need to do is pass a list to it. Don\u2019t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. If you want to read more about the differences between a Python list and NumPy array, this link is recommended: https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference","title":"Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-1d-arrays","text":"Let's create a 1D array from the 2000s (2000-2009): import numpy as np #First, we need to impoty \"numpy\" mylist = [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009] #Create a list of the years print(mylist) #Check how it looks np.array(mylist) #Define it as a numpy array [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009] array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009])","title":"Example- 1D Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-n-dimensional-arrays","text":"Let's create a 5x2 array from the 2000s (2000-2009): myotherlist = [[2000,2001],[2002,2003],[2004,2005],[2006,2007],[2008,2009]] #Since I want a 5x2 array, I should group the years two by two print(myotherlist) #See how it looks as a list np.array(myotherlist) #See how it looks as a numpy array [[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]] array([[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]])","title":"Example- n-Dimensional Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#arrays-arithmetic","text":"Once you have created the arrays, you can do basic Numpy operations. Numpy offers a variety of operations applicable on arrays. From basic operations such as summation, subtraction, multiplication and division to more advanced and essential operations such as matrix multiplication and other elementwise operations. In the examples below, we will go over some of these:","title":"Arrays Arithmetic"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-1d-array-arithmetic","text":"Define a 1D array with [0,12,24,36,48,60,72,84,96] Multiple all elements by 2 Take all elements to the power of 2 Find the maximum value of the array and its position Find the minimum value of the array and its position Define another 1D array with [-12,0,12,24,36,48,60,72,84] Find the summation and subtraction of these two arrays Find the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([0,12,24,36,48,60,72,84,96]) #Step1: Define Array1 print(Array1) print(Array1*2) #Step2: Multiple all elements by 2 print(Array1**2) #Step3: Take all elements to the power of 2 print(np.power(Array1,2)) #Another way to do the same thing, by using a function in numpy print(np.max(Array1)) #Step4: Find the maximum value of the array print(np.argmax(Array1)) ##Step4: Find the postition of the maximum value print(np.min(Array1)) #Step5: Find the minimum value of the array print(np.argmin(Array1)) ##Step5: Find the postition of the minimum value Array2 = np.array([-12,0,12,24,36,48,60,72,84]) #Step6: Define Array2 print(Array2) print(Array1+Array2) #Step7: Find the summation of these two arrays print(Array1-Array2) #Step7: Find the subtraction of these two arrays print(Array1*Array2) #Step8: Find the multiplication of these two arrays [ 0 12 24 36 48 60 72 84 96] [ 0 24 48 72 96 120 144 168 192] [ 0 144 576 1296 2304 3600 5184 7056 9216] [ 0 144 576 1296 2304 3600 5184 7056 9216] 96 8 0 0 [-12 0 12 24 36 48 60 72 84] [-12 12 36 60 84 108 132 156 180] [12 12 12 12 12 12 12 12 12] [ 0 0 288 864 1728 2880 4320 6048 8064]","title":"Example- 1D Array Arithmetic"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-n-dimensional-array-arithmetic","text":"Define a 2x2 array with [5,10,15,20] Define another 2x2 array with [3,6,9,12] Find the summation and subtraction of these two arrays Find the minimum number in the multiplication of these two arrays Find the position of the maximum in the multiplication of these two arrays Find the mean of the multiplication of these two arrays Find the mean of the first row of the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([[5,10],[15,20]]) #Step1: Define Array1 print(Array1) Array2 = np.array([[3,6],[9,12]]) #Step2: Define Array2 print(Array2) print(Array1+Array2) #Step3: Find the summation print(Array1-Array2) #Step3: Find the subtraction MultArray = Array1@Array2 #Step4: To perform a typical matrix multiplication (or matrix product) MultArray1 = Array1.dot(Array2) #Step4: Another way To perform a matrix multiplication print(MultArray) print(MultArray1) print(np.min(MultArray)) #Step4: Find the minimum value of the multiplication print(np.argmax(MultArray)) ##Step5: Find the postition of the maximum value print(np.mean(MultArray)) ##Step6: Find the mean of the multiplication of these two arrays print(np.mean(MultArray[0,:])) ##Step7: Find the mean of the first row of the multiplication of these two arrays [[ 5 10] [15 20]] [[ 3 6] [ 9 12]] [[ 8 16] [24 32]] [[2 4] [6 8]] [[105 150] [225 330]] [[105 150] [225 330]] 105 3 202.5 127.5","title":"Example- n-Dimensional Array Arithmetic"},{"location":"8-Labs/Lab6/Lab6_Dev/#arrays-comparison","text":"Comparing two NumPy arrays determines whether they are equivalent by checking if every element at each corresponding index are the same.","title":"Arrays Comparison"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-1d-array-comparison","text":"Define a 1D array with [1.0,2.5,3.4,7,7] Define another 1D array with [5.0/5.0,5.0/2,6.8/2,21/3,14/2] Compare and see if the two arrays are equal Define another 1D array with [6,1.4,2.2,7.5,7] Compare and see if the first array is greater than or equal to the third array import numpy as np #import numpy Array1 = np.array([1.0,2.5,3.4,7,7]) #Step1: Define Array1 print(Array1) Array2 = np.array([5.0/5.0,5.0/2,6.8/2,21/3,14/2]) #Step2: Define Array1 print(Array2) print(np.equal(Array1, Array2)) #Step3: Compare and see if the two arrays are equal Array3 = np.array([6,1.4,2.2,7.5,7]) #Step4: Define Array3 print(Array3) print(np.greater_equal(Array1, Array3)) #Step3: Compare and see if the two arrays are equal [1. 2.5 3.4 7. 7. ] [1. 2.5 3.4 7. 7. ] [ True True True True True] [6. 1.4 2.2 7.5 7. ] [False True True False True]","title":"Example- 1D Array Comparison"},{"location":"8-Labs/Lab6/Lab6_Dev/#arrays-manipulation","text":"numpy.copy() allows us to create a copy of an array. This is particularly useful when we need to manipulate an array while keeping an original copy in memory. The numpy.delete() function returns a new array with sub-arrays along an axis deleted. Let's have a look at the examples.","title":"Arrays Manipulation"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-copying-and-deleting-arrays-and-elements","text":"Define a 1D array, named \"x\" with [1,2,3] Define \"y\" so that \"y=x\" Define \"z\" as a copy of \"x\" Discuss the difference between y and z Delete the second element of x import numpy as np #import numpy x = np.array([1,2,3]) #Step1: Define x print(x) y = x #Step2: Define y as y=x print(y) z = np.copy(x) #Step3: Define z as a copy of x print(z) # For Step4: They look similar but check this out: x[1] = 8 # If we change x ... print(x) print(y) print(z) # By modifying x, y changes but z remains as a copy of the initial version of x. x = np.delete(x, 1) #Step5: Delete the second element of x print(x) [1 2 3] [1 2 3] [1 2 3] [1 8 3] [1 8 3] [1 2 3] [1 3]","title":"Example- Copying and Deleting Arrays and Elements"},{"location":"8-Labs/Lab6/Lab6_Dev/#sorting-arrays","text":"Sorting means putting elements in an ordered sequence. Ordered sequence is any sequence that has an order corresponding to elements, like numeric or alphabetical, ascending or descending. If you use the sort() method on a 2-D array, both arrays will be sorted.","title":"Sorting Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-sorting-1d-arrays","text":"Define a 1D array as ['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed'] and print it out. Then, sort the array alphabetically. import numpy as np #import numpy games = np.array(['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed']) print(games) print(np.sort(games)) ['FIFA 2020' 'Red Dead Redemption' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed'] ['FIFA 2020' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed' 'Red Dead Redemption']","title":"Example- Sorting 1D Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-sorting-n-dimensional-arrays","text":"Define a 3x3 array with 17,-6,2,86,-12,0,0,23,12 and print it out. Then, sort the array. import numpy as np #import numpy a = np.array([[17,-6,2],[86,-12,0],[0,23,12]]) print(a) print (\"Along columns : \\n\", np.sort(a,axis = 0) ) #This will be sorting in each column print (\"Along rows : \\n\", np.sort(a,axis = 1) ) #This will be sorting in each row print (\"Sorting by default : \\n\", np.sort(a) ) #Same as above print (\"Along None Axis : \\n\", np.sort(a,axis = None) ) #This will be sorted like a 1D array [[ 17 -6 2] [ 86 -12 0] [ 0 23 12]] Along columns : [[ 0 -12 0] [ 17 -6 2] [ 86 23 12]] Along rows : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Sorting by default : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Along None Axis : [-12 -6 0 0 2 12 17 23 86]","title":"Example- Sorting n-Dimensional Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#partitioning-slice-arrays","text":"Slicing in python means taking elements from one given index to another given index. We can do slicing like this: [start:end]. We can also define the step, like this: [start:end:step]. If we don't pass start its considered 0 If we don't pass end its considered length of array in that dimension If we don't pass step its considered 1","title":"Partitioning (Slice) Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-slicing-1d-arrays","text":"Define a 1D array as [1,3,5,7,9], slice out the [3,5,7] and print it out. import numpy as np #import numpy a = np.array([1,3,5,7,9]) #Define the array print(a) aslice = a[1:4] #slice the [3,5,7] print(aslice) #print it out [1 3 5 7 9] [3 5 7]","title":"Example- Slicing 1D Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-slicing-n-dimensional-arrays","text":"Define a 5x5 array with \"Superman, Batman, Jim Hammond, Captain America, Green Arrow, Aquaman, Wonder Woman, Martian Manhunter, Barry Allen, Hal Jordan, Hawkman, Ray Palmer, Spider Man, Thor, Hank Pym, Solar, Iron Man, Dr. Strange, Daredevil, Ted Kord, Captian Marvel, Black Panther, Wolverine, Booster Gold, Spawn \" and print it out. Then: - Slice the first column and print it out - Slice the third row and print it out - Slice 'Wolverine' and print it out - Slice a 3x3 array with 'Wonder Woman, Ray Palmer, Iron Man, Martian Manhunter, Spider Man, Dr. Strange, Barry Allen, Thor, Daredevil' import numpy as np #import numpy Superheroes = np.array([['Superman', 'Batman', 'Jim Hammond', 'Captain America', 'Green Arrow'], ['Aquaman', 'Wonder Woman', 'Martian Manhunter', 'Barry Allen', 'Hal Jordan'], ['Hawkman', 'Ray Palmer', 'Spider Man', 'Thor', 'Hank Pym'], ['Solar', 'Iron Man', 'Dr. Strange', 'Daredevil', 'Ted Kord'], ['Captian Marvel', 'Black Panther', 'Wolverine', 'Booster Gold', 'Spawn']]) print(Superheroes) #Step1 print(Superheroes[:,0]) print(Superheroes[2,:]) print(Superheroes[4,2]) print(Superheroes[1:4,1:4]) [['Superman' 'Batman' 'Jim Hammond' 'Captain America' 'Green Arrow'] ['Aquaman' 'Wonder Woman' 'Martian Manhunter' 'Barry Allen' 'Hal Jordan'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] ['Solar' 'Iron Man' 'Dr. Strange' 'Daredevil' 'Ted Kord'] ['Captian Marvel' 'Black Panther' 'Wolverine' 'Booster Gold' 'Spawn']] ['Superman' 'Aquaman' 'Hawkman' 'Solar' 'Captian Marvel'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] Wolverine [['Wonder Woman' 'Martian Manhunter' 'Barry Allen'] ['Ray Palmer' 'Spider Man' 'Thor'] ['Iron Man' 'Dr. Strange' 'Daredevil']]","title":"Example- Slicing n-Dimensional Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#this-is-a-numpy-cheat-sheet-similar-to-the-one-you-had-on-top-of-this-notebook","text":"","title":"This is a Numpy Cheat Sheet- similar to the one you had on top of this notebook!"},{"location":"8-Labs/Lab6/Lab6_Dev/#check-out-this-link-for-more","text":"https://blog.finxter.com/collection-10-best-numpy-cheat-sheets-every-python-coder-must-own/ Here are some of the resources used for creating this notebook: - Johnson, J. (2020). Python Numpy Tutorial (with Jupyter and Colab). Retrieved September 15, 2020, from https://cs231n.github.io/python-numpy-tutorial/ - Willems, K. (2019). (Tutorial) Python NUMPY Array TUTORIAL. Retrieved September 15, 2020, from https://www.datacamp.com/community/tutorials/python-numpy-tutorial?utm_source=adwords_ppc - Willems, K. (2017). NumPy Cheat Sheet: Data Analysis in Python. Retrieved September 15, 2020, from https://www.datacamp.com/community/blog/python-numpy-cheat-sheet - W3resource. (2020). NumPy: Compare two given arrays. Retrieved September 15, 2020, from https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-28.php Here are some great reads on this topic: - \"Python NumPy Tutorial\" available at https://www.geeksforgeeks.org/python-numpy-tutorial/ - \"What Is NumPy?\" a collection of blogs, available at https://realpython.com/tutorials/numpy/ - \"Look Ma, No For-Loops: Array Programming With NumPy\" by Brad Solomon available at https://realpython.com/numpy-array-programming/ - \"The Ultimate Beginner\u2019s Guide to NumPy\" by Anne Bonner available at https://towardsdatascience.com/the-ultimate-beginners-guide-to-numpy-f5a2f99aef54 Here are some great videos on these topics: - \"Learn NUMPY in 5 minutes - BEST Python Library!\" by Python Programmer available at https://www.youtube.com/watch?v=xECXZ3tyONo - \"Python NumPy Tutorial for Beginners\" by freeCodeCamp.org available at https://www.youtube.com/watch?v=QUT1VHiLmmI - \"Complete Python NumPy Tutorial (Creating Arrays, Indexing, Math, Statistics, Reshaping)\" by Keith Galli available at https://www.youtube.com/watch?v=GB9ByFAIAH4 - \"Python NumPy Tutorial | NumPy Array | Python Tutorial For Beginners | Python Training | Edureka\" by edureka! available at https://www.youtube.com/watch?v=8JfDAm9y_7s","title":"Check out this link for more: "},{"location":"8-Labs/Lab6/Lab6_Dev/#exercise-python-list-vs-numpy-arrays","text":"","title":"Exercise: Python List vs. Numpy Arrays? "},{"location":"8-Labs/Lab6/Lab6_Dev/#what-are-some-differences-between-python-lists-and-numpy-arrays","text":"","title":"What are some differences between Python lists and Numpy arrays?"},{"location":"8-Labs/Lab6/Lab6_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab7/Lab7_Dev/","text":"Laboratory 7: Pandas for Butter! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Pandas A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. To use pandas, we need to import the module, generally pandas has numpy as a dependency so it also must be imported import numpy as np #Importing NumPy library as \"np\" import pandas as pd #Importing Pandas library as \"pd\" Dataframe-structure using primative python First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. mytabular = np.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction print(mytabular) [[61 82 48 85] [45 36 97 72] [91 3 22 35] [18 65 30 63] [79 71 8 45]] The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 61, 82, 48, 85] ['B', 45, 36, 97, 72] ['C', 91, 3, 22, 35] ['D', 18, 65, 30, 63] ['E', 79, 71, 8, 45] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 91, 3, 22, 35] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 82 36 3 65 71 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 22 Create a proper dataframe We will now do the same using pandas df = pd.DataFrame(np.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 We can also turn our table into a dataframe, notice how the constructor adds header row and index column df1 = pd.DataFrame(mytable) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 61 82 48 85 2 B 45 36 97 72 3 C 91 3 22 35 4 D 18 65 30 63 5 E 79 71 8 45 To get proper behavior, we can just reuse our original objects df2 = pd.DataFrame(mytabular,myrowname,mycolname) df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 61 82 48 85 B 45 36 97 72 C 91 3 22 35 D 18 65 30 63 E 79 71 8 45 Getting the shape of dataframes The shape method will return the row and column rank (count) of a dataframe. df.shape (5, 4) df1.shape (6, 5) df2.shape (5, 4) Appending new columns To append a column simply assign a value to a new column name to the dataframe df['new']= 'NA' df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA Appending new rows A bit trickier but we can create a copy of a row and concatenate it back into the dataframe. newrow = df.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pd.concat([df,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA X 9 46 81 84 NA Removing Rows and Columns To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 Indexing We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 34 B 40 D 51 E 46 X 46 Name: X, dtype: int64 newtable[['X','W']] #Selecing multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 34 52 B 40 11 D 51 96 E 46 9 X 46 9 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 9 X 46 Y 81 Z 84 Name: E, dtype: int64 newtable.loc[['E','X','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 9 46 81 84 X 9 46 81 84 B 11 40 9 69 newtable.loc[['B','E','D'],['X','Y']] #Selecting elemens via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 40 9 E 46 81 D 51 89 Conditional Selection df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? df[df['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? df[df['col2']==df['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object Descriptor Functions #Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach head method Returns the first few rows, useful to infer structure #Returns only the first five rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit info method Returns the data model (data column count, names, data types) #Info about the dataframe df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes describe method Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000 Counting and Sum methods There are also methods for counts and sums by specific columns df['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) df['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values df['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) df['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64 Using functions in dataframes - symbolic apply The power of pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. Its pretty complicated but quite handy, best shown by an example def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(df) print('Apply the times2 function to col2') df['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64 Sorts df.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit Aggregating (Grouping Values) dataframe contents #Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } df1 = pd.DataFrame(data) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' df1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' df1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27 Filtering out missing values #Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach Reading a File into a Dataframe Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. import pandas as pd readfilecsv = pd.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 Writing a dataframe to file #Creating and writing to a .csv file readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pd.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pd.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') readfileexcel = pd.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 Unnamed: 0.1 a b c d 0 0 0 0 1 2 3 1 1 1 4 5 6 7 2 2 2 8 9 10 11 3 3 3 12 13 14 15 This is a Pandas Cheat Sheet Here are some of the resources used for creating this notebook: Pandas foundations. Retrieved February 15, 2021, from https://www.datacamp.com/courses/pandas-foundations Pandas tutorial. Retrieved February 15, 2021, from https://www.w3schools.com/python/pandas/default.asp Pandas tutorial: Dataframes in Python. Retrieved February 15, 2021, from https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python Here are some great reads on this topic: - \"Introduction to Pandas in Python\" available at https://www.geeksforgeeks.org/introduction-to-pandas-in-python/ - \"Pandas Introduction & Tutorials for Beginners\" by Walker Rowe , available at https://www.bmc.com/blogs/pandas-basics/ - \"Using Pandas and Python to Explore Your Dataset\" by Reka Horvath available at https://realpython.com/pandas-python-explore-dataset/ - \"Python Pandas Tutorial: A Complete Introduction for Beginners\" by George McIntire, Lauren Washington, and Brendan Martin available at https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/ Here are some great videos on these topics: - \"Python: Pandas Tutorial | Intro to DataFrames\" by Joe James available at https://www.youtube.com/watch?v=e60ItwlZTKM - \"Complete Python Pandas Data Science Tutorial! (Reading CSV/Excel files, Sorting, Filtering, Groupby)\" by Keith Galli available at https://www.youtube.com/watch?v=vmEHCJofslg - \"What is Pandas? Why and How to Use Pandas in Python\" by Python Programmer available at *https://www.youtube.com/watch?v=dcqPhpY7tWk Exercise: Pandas of Data Pandas library supports three major types of data structures: Series, DataFrames, and Panels. What are some differences between the three structures? * Make sure to cite any resources that you may use.","title":"<font color=darkred>Laboratory 7: Pandas for Butter! </font>"},{"location":"8-Labs/Lab7/Lab7_Dev/#laboratory-7-pandas-for-butter","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Laboratory 7: Pandas for Butter! "},{"location":"8-Labs/Lab7/Lab7_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab7/Lab7_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab7/Lab7_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab7/Lab7_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab7/Lab7_Dev/#pandas","text":"A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. To use pandas, we need to import the module, generally pandas has numpy as a dependency so it also must be imported import numpy as np #Importing NumPy library as \"np\" import pandas as pd #Importing Pandas library as \"pd\"","title":"Pandas"},{"location":"8-Labs/Lab7/Lab7_Dev/#dataframe-structure-using-primative-python","text":"First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. mytabular = np.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction print(mytabular) [[61 82 48 85] [45 36 97 72] [91 3 22 35] [18 65 30 63] [79 71 8 45]] The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 61, 82, 48, 85] ['B', 45, 36, 97, 72] ['C', 91, 3, 22, 35] ['D', 18, 65, 30, 63] ['E', 79, 71, 8, 45] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 91, 3, 22, 35] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 82 36 3 65 71 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 22","title":"Dataframe-structure using primative python"},{"location":"8-Labs/Lab7/Lab7_Dev/#create-a-proper-dataframe","text":"We will now do the same using pandas df = pd.DataFrame(np.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 We can also turn our table into a dataframe, notice how the constructor adds header row and index column df1 = pd.DataFrame(mytable) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 61 82 48 85 2 B 45 36 97 72 3 C 91 3 22 35 4 D 18 65 30 63 5 E 79 71 8 45 To get proper behavior, we can just reuse our original objects df2 = pd.DataFrame(mytabular,myrowname,mycolname) df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 61 82 48 85 B 45 36 97 72 C 91 3 22 35 D 18 65 30 63 E 79 71 8 45","title":"Create a proper dataframe"},{"location":"8-Labs/Lab7/Lab7_Dev/#getting-the-shape-of-dataframes","text":"The shape method will return the row and column rank (count) of a dataframe. df.shape (5, 4) df1.shape (6, 5) df2.shape (5, 4)","title":"Getting the shape of dataframes"},{"location":"8-Labs/Lab7/Lab7_Dev/#appending-new-columns","text":"To append a column simply assign a value to a new column name to the dataframe df['new']= 'NA' df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA","title":"Appending new columns"},{"location":"8-Labs/Lab7/Lab7_Dev/#appending-new-rows","text":"A bit trickier but we can create a copy of a row and concatenate it back into the dataframe. newrow = df.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pd.concat([df,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA X 9 46 81 84 NA","title":"Appending new rows"},{"location":"8-Labs/Lab7/Lab7_Dev/#removing-rows-and-columns","text":"To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84","title":"Removing Rows and Columns"},{"location":"8-Labs/Lab7/Lab7_Dev/#indexing","text":"We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 34 B 40 D 51 E 46 X 46 Name: X, dtype: int64 newtable[['X','W']] #Selecing multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 34 52 B 40 11 D 51 96 E 46 9 X 46 9 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 9 X 46 Y 81 Z 84 Name: E, dtype: int64 newtable.loc[['E','X','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 9 46 81 84 X 9 46 81 84 B 11 40 9 69 newtable.loc[['B','E','D'],['X','Y']] #Selecting elemens via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 40 9 E 46 81 D 51 89","title":"Indexing"},{"location":"8-Labs/Lab7/Lab7_Dev/#conditional-selection","text":"df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? df[df['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? df[df['col2']==df['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object","title":"Conditional Selection"},{"location":"8-Labs/Lab7/Lab7_Dev/#descriptor-functions","text":"#Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach","title":"Descriptor Functions"},{"location":"8-Labs/Lab7/Lab7_Dev/#head-method","text":"Returns the first few rows, useful to infer structure #Returns only the first five rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit","title":"head method"},{"location":"8-Labs/Lab7/Lab7_Dev/#info-method","text":"Returns the data model (data column count, names, data types) #Info about the dataframe df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes","title":"info method"},{"location":"8-Labs/Lab7/Lab7_Dev/#describe-method","text":"Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000","title":"describe method"},{"location":"8-Labs/Lab7/Lab7_Dev/#counting-and-sum-methods","text":"There are also methods for counts and sums by specific columns df['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) df['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values df['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) df['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64","title":"Counting and Sum methods"},{"location":"8-Labs/Lab7/Lab7_Dev/#using-functions-in-dataframes-symbolic-apply","text":"The power of pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. Its pretty complicated but quite handy, best shown by an example def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(df) print('Apply the times2 function to col2') df['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64","title":"Using functions in dataframes - symbolic apply"},{"location":"8-Labs/Lab7/Lab7_Dev/#sorts","text":"df.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit","title":"Sorts"},{"location":"8-Labs/Lab7/Lab7_Dev/#aggregating-grouping-values-dataframe-contents","text":"#Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } df1 = pd.DataFrame(data) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' df1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' df1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27","title":"Aggregating (Grouping Values) dataframe contents"},{"location":"8-Labs/Lab7/Lab7_Dev/#filtering-out-missing-values","text":"#Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach","title":"Filtering out missing values"},{"location":"8-Labs/Lab7/Lab7_Dev/#reading-a-file-into-a-dataframe","text":"Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. import pandas as pd readfilecsv = pd.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15","title":"Reading a File into a Dataframe"},{"location":"8-Labs/Lab7/Lab7_Dev/#writing-a-dataframe-to-file","text":"#Creating and writing to a .csv file readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pd.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pd.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') readfileexcel = pd.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 Unnamed: 0.1 a b c d 0 0 0 0 1 2 3 1 1 1 4 5 6 7 2 2 2 8 9 10 11 3 3 3 12 13 14 15","title":"Writing a dataframe to file"},{"location":"8-Labs/Lab7/Lab7_Dev/#this-is-a-pandas-cheat-sheet","text":"Here are some of the resources used for creating this notebook: Pandas foundations. Retrieved February 15, 2021, from https://www.datacamp.com/courses/pandas-foundations Pandas tutorial. Retrieved February 15, 2021, from https://www.w3schools.com/python/pandas/default.asp Pandas tutorial: Dataframes in Python. Retrieved February 15, 2021, from https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python Here are some great reads on this topic: - \"Introduction to Pandas in Python\" available at https://www.geeksforgeeks.org/introduction-to-pandas-in-python/ - \"Pandas Introduction & Tutorials for Beginners\" by Walker Rowe , available at https://www.bmc.com/blogs/pandas-basics/ - \"Using Pandas and Python to Explore Your Dataset\" by Reka Horvath available at https://realpython.com/pandas-python-explore-dataset/ - \"Python Pandas Tutorial: A Complete Introduction for Beginners\" by George McIntire, Lauren Washington, and Brendan Martin available at https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/ Here are some great videos on these topics: - \"Python: Pandas Tutorial | Intro to DataFrames\" by Joe James available at https://www.youtube.com/watch?v=e60ItwlZTKM - \"Complete Python Pandas Data Science Tutorial! (Reading CSV/Excel files, Sorting, Filtering, Groupby)\" by Keith Galli available at https://www.youtube.com/watch?v=vmEHCJofslg - \"What is Pandas? Why and How to Use Pandas in Python\" by Python Programmer available at *https://www.youtube.com/watch?v=dcqPhpY7tWk","title":"This is a Pandas Cheat Sheet"},{"location":"8-Labs/Lab7/Lab7_Dev/#exercise-pandas-of-data","text":"","title":"Exercise: Pandas of Data  "},{"location":"8-Labs/Lab7/Lab7_Dev/#pandas-library-supports-three-major-types-of-data-structures-series-dataframes-and-panels-what-are-some-differences-between-the-three-structures","text":"","title":"Pandas library supports three major types of data structures: Series, DataFrames, and Panels. What are some differences between the three structures?"},{"location":"8-Labs/Lab7/Lab7_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab8/Lab8_Dev/","text":"Laboratory 8: Matplotlib for Jam! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Matplotlip and Visual Display of Data This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib About matplotlib Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis). Background Data are not always numerical. Data can music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) They can also be categorical into which you can place individuals: - The individuals are cartons of ice-cream, and the category is the flavor in the carton - The individuals are professional basketball players, and the category is the player's team. Bar Graphs Bar charts (graphs) are good display tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='maroon', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='orange', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.bar(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:xlabel='Flavor'> Example- Language Bars! Consider the data set \"data\" defined as data = {'C':20, 'C++':15, 'Java':30, 'Python':35} which lists student count by programming language in some school. Produce a bar chart of number of students in each language, where language is the classification, and student count is the variable. # Code and run your solution here import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show() Plot it as a horizontal bar chart: # Code and run your solution here # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.barh(courses, values, color ='maroon', height = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show() Line Charts A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application. Example- Speed vs Time Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() From examination of the plot, estimate the speed at time t = 5.0 (eyeball estimate) Example- Add a linear fit Using the same series from Exercise 1, Plot the speed vs time (speed on y-axis, time on x-axis) using a line plot. Plot a second line based on the linear model y = mx + b , where b=0~\\text{and}~m=7.6 . # Code and run your solution here: def ymodel(xmodel,slope,intercept): ymodel = slope*xmodel+intercept return(ymodel) yseries = [] slope = 7.6 intercept = 0.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0.5) # basic line plot plt.plot(time, yseries, c='blue') plt.show() Example- Find a better fit Using trial and error try to improve the 'fit' of the model, by adjusting values of m~\\text{and}~b . # Code and run your solution here: yseries = [] slope = 7.6 intercept = -8.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0) # basic scatter plot plt.plot(time, yseries, c='blue') plt.show() Scatter Plots A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot Example- Examine the dataset with heights of fathers, mothers and sons df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists dad = df['father'] ; mom = df['mother'] ; son = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red' , label='Father') # one plot series plt.scatter(son, mom, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\") Histograms Quoting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson.[1] To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\" Example- Explore the \"top_movies\" dataset and draw histograms for Gross and Year. import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Year\"]].hist() array([[<AxesSubplot:title={'center':'Year'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) This is a Matplotlib Cheat Sheet Here are some of the resources used for creating this notebook: \"Discrete distribution as horizontal bar chart\" available at *https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html \"Bar Plot in Matplotlib\" available at *https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ Here are some great reads on this topic: - \"Python | Introduction to Matplotlib\" available at https://www.geeksforgeeks.org/python-introduction-matplotlib/ - \"Visualization with Matplotlib\" available at https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html - \"Introduction to Matplotlib \u2014 Data Visualization in Python\" by Ehi Aigiomawu available at https://heartbeat.fritz.ai/introduction-to-matplotlib-data-visualization-in-python-d9143287ae39 - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"Matplotlib Tutorial (Part 1): Creating and Customizing Our First Plots\" by Corey Schafer available at https://www.youtube.com/watch?v=UO98lJQ3QGI - \"Intro to Data Analysis / Visualization with Python, Matplotlib and Pandas | Matplotlib Tutorial\" by CS Dojo available at https://www.youtube.com/watch?v=a9UrKTVEeZA - \"Intro to Data Visualization in Python with Matplotlib! (line graph, bar chart, title, labels, size)\" by Keith Galli available at *https://www.youtube.com/watch?v=DAQNHzOcO5A Exercise: Bins, Bins, Bins! Selecting the number of bins is an important decision when working with histograms. Are there any rules or recommendations for choosing the number or width of bins? What happens if we use too many or too few bins? * Make sure to cite any resources that you may use.","title":"<font color=darkred>Laboratory 8: Matplotlib for Jam! </font>"},{"location":"8-Labs/Lab8/Lab8_Dev/#laboratory-8-matplotlib-for-jam","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Laboratory 8: Matplotlib for Jam! "},{"location":"8-Labs/Lab8/Lab8_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab8/Lab8_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab8/Lab8_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab8/Lab8_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab8/Lab8_Dev/#matplotlip-and-visual-display-of-data","text":"This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib","title":"Matplotlip and Visual Display of Data"},{"location":"8-Labs/Lab8/Lab8_Dev/#about-matplotlib","text":"Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis).","title":"About matplotlib"},{"location":"8-Labs/Lab8/Lab8_Dev/#background","text":"Data are not always numerical. Data can music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) They can also be categorical into which you can place individuals: - The individuals are cartons of ice-cream, and the category is the flavor in the carton - The individuals are professional basketball players, and the category is the player's team.","title":"Background"},{"location":"8-Labs/Lab8/Lab8_Dev/#bar-graphs","text":"Bar charts (graphs) are good display tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='maroon', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='orange', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.bar(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:xlabel='Flavor'>","title":"Bar Graphs"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-language-bars","text":"Consider the data set \"data\" defined as data = {'C':20, 'C++':15, 'Java':30, 'Python':35} which lists student count by programming language in some school. Produce a bar chart of number of students in each language, where language is the classification, and student count is the variable. # Code and run your solution here import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show()","title":"Example- Language Bars!"},{"location":"8-Labs/Lab8/Lab8_Dev/#plot-it-as-a-horizontal-bar-chart","text":"# Code and run your solution here # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.barh(courses, values, color ='maroon', height = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show()","title":"Plot it as a horizontal bar chart:"},{"location":"8-Labs/Lab8/Lab8_Dev/#line-charts","text":"A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application.","title":"Line Charts"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-speed-vs-time","text":"Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() From examination of the plot, estimate the speed at time t = 5.0 (eyeball estimate)","title":"Example- Speed vs Time"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-add-a-linear-fit","text":"Using the same series from Exercise 1, Plot the speed vs time (speed on y-axis, time on x-axis) using a line plot. Plot a second line based on the linear model y = mx + b , where b=0~\\text{and}~m=7.6 . # Code and run your solution here: def ymodel(xmodel,slope,intercept): ymodel = slope*xmodel+intercept return(ymodel) yseries = [] slope = 7.6 intercept = 0.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0.5) # basic line plot plt.plot(time, yseries, c='blue') plt.show()","title":"Example- Add a linear fit"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-find-a-better-fit","text":"Using trial and error try to improve the 'fit' of the model, by adjusting values of m~\\text{and}~b . # Code and run your solution here: yseries = [] slope = 7.6 intercept = -8.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0) # basic scatter plot plt.plot(time, yseries, c='blue') plt.show()","title":"Example- Find a better fit"},{"location":"8-Labs/Lab8/Lab8_Dev/#scatter-plots","text":"A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot","title":"Scatter Plots"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-examine-the-dataset-with-heights-of-fathers-mothers-and-sons","text":"df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists dad = df['father'] ; mom = df['mother'] ; son = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red' , label='Father') # one plot series plt.scatter(son, mom, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\")","title":"Example- Examine the dataset with heights of fathers, mothers and sons"},{"location":"8-Labs/Lab8/Lab8_Dev/#histograms","text":"Quoting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson.[1] To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\"","title":"Histograms"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-explore-the-top_movies-dataset-and-draw-histograms-for-gross-and-year","text":"import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Year\"]].hist() array([[<AxesSubplot:title={'center':'Year'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100)","title":"Example- Explore the \"top_movies\" dataset and draw histograms for Gross and Year."},{"location":"8-Labs/Lab8/Lab8_Dev/#this-is-a-matplotlib-cheat-sheet","text":"Here are some of the resources used for creating this notebook: \"Discrete distribution as horizontal bar chart\" available at *https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html \"Bar Plot in Matplotlib\" available at *https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ Here are some great reads on this topic: - \"Python | Introduction to Matplotlib\" available at https://www.geeksforgeeks.org/python-introduction-matplotlib/ - \"Visualization with Matplotlib\" available at https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html - \"Introduction to Matplotlib \u2014 Data Visualization in Python\" by Ehi Aigiomawu available at https://heartbeat.fritz.ai/introduction-to-matplotlib-data-visualization-in-python-d9143287ae39 - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"Matplotlib Tutorial (Part 1): Creating and Customizing Our First Plots\" by Corey Schafer available at https://www.youtube.com/watch?v=UO98lJQ3QGI - \"Intro to Data Analysis / Visualization with Python, Matplotlib and Pandas | Matplotlib Tutorial\" by CS Dojo available at https://www.youtube.com/watch?v=a9UrKTVEeZA - \"Intro to Data Visualization in Python with Matplotlib! (line graph, bar chart, title, labels, size)\" by Keith Galli available at *https://www.youtube.com/watch?v=DAQNHzOcO5A","title":"This is a Matplotlib Cheat Sheet"},{"location":"8-Labs/Lab8/Lab8_Dev/#exercise-bins-bins-bins","text":"","title":"Exercise: Bins, Bins, Bins!  "},{"location":"8-Labs/Lab8/Lab8_Dev/#selecting-the-number-of-bins-is-an-important-decision-when-working-with-histograms-are-there-any-rules-or-recommendations-for-choosing-the-number-or-width-of-bins-what-happens-if-we-use-too-many-or-too-few-bins","text":"","title":"Selecting the number of bins is an important decision when working with histograms. Are there any rules or recommendations for choosing the number or width of bins? What happens if we use too many or too few bins?"},{"location":"8-Labs/Lab8/Lab8_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab9/Lab9_Dev/","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: HEX: Title of the notebook Date: Lab9: Simulation Example1: Simulate a game of Russian Roulette: For 2 rounds For 5 rounds For 10 rounds import numpy as np #import numpy revolver = np.array([1,0,0,0,0,0]) #create a numpy array with 1 bullet and 5 empty chambers print(np.random.choice(revolver,2)) #randomly select a value from revolver - simulation [0 0] print(np.random.choice(revolver,5)) [0 0 0 1 1] print(np.random.choice(revolver,10)) [0 0 0 0 0 0 0 0 0 0] Exercise 1: Simulate the results of throwing a D6 (regular dice) for 10 times. Example2: Assume the following rules: If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. Define a function to simulate a game with the above rules, assuming a D6, and compute the net gain of the player over any given number of rolls. Compute the net gain for 5, 50, and 500 rolls def D6game(nrolls): import numpy as np #import numpy dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 rolls = np.random.choice(dice,nrolls) #randomly selecting a value from dice for nrolls times- simulation gainlist =[] #create an empty list for gains|losses for i in np.arange(len(rolls)): #Apply the rules if rolls[i]<=2: gainlist.append(-1) elif rolls[i]<=4: gainlist.append(0) elif rolls[i]<=6: gainlist.append(+1) return (np.sum(gainlist)) #sum up all gains|losses # return (gainlist,\"The net gain is equal to:\",np.sum(gainlist)) D6game(5) -2 D6game(50) -4 D6game(500) -16 Exercise2: Assume the following rules: If the dice shows 1 or 2 spots, my net gain is (-2*value of dice) dollars. If the dice shows 3 or 4 spots, my net gain is 1 dollars. If the dice shows 5 spots, my net gain is (2*value of dice) dollars. If the dice shows 6 spots, my net gain is -5 dollars. Define a function to simulate a game with the above rules, assuming a D6, and compute the net gain of the player over any given number of rolls. Compute the net gain for 5, 50, and 500 rolls # Define the function # Run for 5 rounds # Run for 50 rounds # Run for 500 rounds Example3: Simulate Monty Hall Game for 1000 times. Use a barplot and discuss whether players are better off sticking to their initial choice, or switching doors? def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 1', 'Goat 2', 'Car'] Goat 1 Goat 2 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining for i in np.arange(1000): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list import pandas as pd #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Car Goat 1 Goat 2 1 Car Goat 2 Goat 1 2 Car Goat 2 Goat 1 3 Goat 2 Goat 1 Car 4 Goat 2 Goat 1 Car ... ... ... ... 995 Car Goat 2 Goat 1 996 Car Goat 2 Goat 1 997 Goat 2 Goat 1 Car 998 Goat 2 Goat 1 Car 999 Goat 2 Goat 1 Car 1000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot import matplotlib.pyplot as plt # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show() According to the plot, it is statitically beneficial for the players to switch doors because the initial chance for being correct is only 1/3 Example4: What if there were 4 doors and 3 goats? import numpy as np import pandas as pd import matplotlib.pyplot as plt Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\",\"Goat 3\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\",\"Goat 3\"]) #Define a list for goats! def othergoat12(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" def othergoat23(x): #Define a function to return \"the other goat\"! if x == \"Goat 2\": return \"Goat 3\" elif x == \"Goat 3\": return \"Goat 2\" def othergoat13(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 3\" elif x == \"Goat 3\": return \"Goat 1\" ##################################### def othergoat123(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return np.random.choice([\"Goat 2\",\"Goat 3\"]) elif x == \"Goat 2\": return np.random.choice([\"Goat 1\",\"Goat 3\"]) elif x == \"Goat 3\": return np.random.choice([\"Goat 1\",\"Goat 2\"]) def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"unrevealed1\", \"unrevealed2\"] goats = np.array([\"Goat 1\" , \"Goat 2\",\"Goat 3\"]) userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": #If the user chooses Goat 1 revealed = np.random.choice(goats[np.arange(len(goats))!=0]) unrevealed1 = othergoat23(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Goat 2\": #If the user chooses Goat 2 revealed = np.random.choice(goats[np.arange(len(goats))!=1]) unrevealed1 = othergoat13(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Goat 3\": #If the user chooses Goat 3 revealed = np.random.choice(goats[np.arange(len(goats))!=2]) unrevealed1 = othergoat12(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Car\": #If the user chooses Car revealed = np.random.choice(goats) newgoat = goats[goats != revealed] unrevealed1 = newgoat[0] unrevealed2 = newgoat[1] return [userguess, revealed,unrevealed1,unrevealed2] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) print(a[3]) ['Car', 'Goat 1', 'Goat 2', 'Goat 3'] Car Goat 1 Goat 2 Goat 3 c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining1 c4 = [] #Create an empty list for the remaining2 for i in np.arange(1000): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list1 c4.append(game[3]) #In each round, add the fourth element to the remaining list2 import pandas as pd #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining1':c3, 'Remaining2':c4}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining1 Remaining2 0 Goat 3 Goat 2 Goat 1 Car 1 Goat 3 Goat 2 Goat 1 Car 2 Goat 1 Goat 3 Goat 2 Car 3 Goat 1 Goat 2 Goat 3 Car 4 Goat 2 Goat 1 Goat 3 Car ... ... ... ... ... 995 Goat 1 Goat 2 Goat 3 Car 996 Car Goat 1 Goat 2 Goat 3 997 Goat 2 Goat 1 Goat 3 Car 998 Goat 3 Goat 1 Goat 2 Car 999 Goat 2 Goat 1 Goat 3 Car 1000 rows \u00d7 4 columns # Get the count of each item in the first and (3rd+4th) column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining1 == 'Car'].shape[0] + gamedf[gamedf.Remaining2 == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining1 == 'Goat 1'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining1 == 'Goat 2'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 2'].shape[0] original_g3 =gamedf[gamedf.Guess == 'Goat 3'].shape[0] remaining_g3 =gamedf[gamedf.Remaining1 == 'Goat 3'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 3'].shape[0] # Let's plot a grouped barplot import matplotlib.pyplot as plt # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2,original_g3] bars2 = [remaining_car,remaining_g1,remaining_g2,remaining_g3] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2','Goat 3']) # Create legend & Show graphic plt.legend() plt.show() Comparison of the plots show that as the number of doors (and goats) increases, it makes even more sense to switch! Exercise3: Run the modified Monty Hall game for 10,100, and 1000 rounds. Show the bar plots for each series and explain the difference. #Define necessary functions #Run and plot for 10 rounds #Run and plot for 100 rounds #Run and plot for 1000 rounds","title":"Lab9 Dev"},{"location":"8-Labs/Lab9/Lab9_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab9/Lab9_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab9/Lab9_Dev/#hex","text":"","title":"HEX:"},{"location":"8-Labs/Lab9/Lab9_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook"},{"location":"8-Labs/Lab9/Lab9_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab9/Lab9_Dev/#lab9-simulation","text":"","title":"Lab9: Simulation"},{"location":"8-Labs/Lab9/Lab9_Dev/#example1-simulate-a-game-of-russian-roulette","text":"For 2 rounds For 5 rounds For 10 rounds import numpy as np #import numpy revolver = np.array([1,0,0,0,0,0]) #create a numpy array with 1 bullet and 5 empty chambers print(np.random.choice(revolver,2)) #randomly select a value from revolver - simulation [0 0] print(np.random.choice(revolver,5)) [0 0 0 1 1] print(np.random.choice(revolver,10)) [0 0 0 0 0 0 0 0 0 0]","title":"Example1: Simulate a game of Russian Roulette:"},{"location":"8-Labs/Lab9/Lab9_Dev/#exercise-1-simulate-the-results-of-throwing-a-d6-regular-dice-for-10-times","text":"","title":"Exercise 1: Simulate the results of throwing a D6 (regular dice) for 10 times."},{"location":"8-Labs/Lab9/Lab9_Dev/#example2-assume-the-following-rules","text":"If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. Define a function to simulate a game with the above rules, assuming a D6, and compute the net gain of the player over any given number of rolls. Compute the net gain for 5, 50, and 500 rolls def D6game(nrolls): import numpy as np #import numpy dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 rolls = np.random.choice(dice,nrolls) #randomly selecting a value from dice for nrolls times- simulation gainlist =[] #create an empty list for gains|losses for i in np.arange(len(rolls)): #Apply the rules if rolls[i]<=2: gainlist.append(-1) elif rolls[i]<=4: gainlist.append(0) elif rolls[i]<=6: gainlist.append(+1) return (np.sum(gainlist)) #sum up all gains|losses # return (gainlist,\"The net gain is equal to:\",np.sum(gainlist)) D6game(5) -2 D6game(50) -4 D6game(500) -16","title":"Example2: Assume the following rules:"},{"location":"8-Labs/Lab9/Lab9_Dev/#exercise2-assume-the-following-rules","text":"If the dice shows 1 or 2 spots, my net gain is (-2*value of dice) dollars. If the dice shows 3 or 4 spots, my net gain is 1 dollars. If the dice shows 5 spots, my net gain is (2*value of dice) dollars. If the dice shows 6 spots, my net gain is -5 dollars. Define a function to simulate a game with the above rules, assuming a D6, and compute the net gain of the player over any given number of rolls. Compute the net gain for 5, 50, and 500 rolls # Define the function # Run for 5 rounds # Run for 50 rounds # Run for 500 rounds","title":"Exercise2: Assume the following rules:"},{"location":"8-Labs/Lab9/Lab9_Dev/#example3-simulate-monty-hall-game-for-1000-times-use-a-barplot-and-discuss-whether-players-are-better-off-sticking-to-their-initial-choice-or-switching-doors","text":"def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 1', 'Goat 2', 'Car'] Goat 1 Goat 2 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining for i in np.arange(1000): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list import pandas as pd #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Car Goat 1 Goat 2 1 Car Goat 2 Goat 1 2 Car Goat 2 Goat 1 3 Goat 2 Goat 1 Car 4 Goat 2 Goat 1 Car ... ... ... ... 995 Car Goat 2 Goat 1 996 Car Goat 2 Goat 1 997 Goat 2 Goat 1 Car 998 Goat 2 Goat 1 Car 999 Goat 2 Goat 1 Car 1000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot import matplotlib.pyplot as plt # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show() According to the plot, it is statitically beneficial for the players to switch doors because the initial chance for being correct is only 1/3","title":"Example3: Simulate Monty Hall Game for 1000 times. Use a barplot and discuss whether players are better off sticking to their initial choice, or switching doors?"},{"location":"8-Labs/Lab9/Lab9_Dev/#example4-what-if-there-were-4-doors-and-3-goats","text":"import numpy as np import pandas as pd import matplotlib.pyplot as plt Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\",\"Goat 3\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\",\"Goat 3\"]) #Define a list for goats! def othergoat12(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" def othergoat23(x): #Define a function to return \"the other goat\"! if x == \"Goat 2\": return \"Goat 3\" elif x == \"Goat 3\": return \"Goat 2\" def othergoat13(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 3\" elif x == \"Goat 3\": return \"Goat 1\" ##################################### def othergoat123(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return np.random.choice([\"Goat 2\",\"Goat 3\"]) elif x == \"Goat 2\": return np.random.choice([\"Goat 1\",\"Goat 3\"]) elif x == \"Goat 3\": return np.random.choice([\"Goat 1\",\"Goat 2\"]) def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"unrevealed1\", \"unrevealed2\"] goats = np.array([\"Goat 1\" , \"Goat 2\",\"Goat 3\"]) userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": #If the user chooses Goat 1 revealed = np.random.choice(goats[np.arange(len(goats))!=0]) unrevealed1 = othergoat23(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Goat 2\": #If the user chooses Goat 2 revealed = np.random.choice(goats[np.arange(len(goats))!=1]) unrevealed1 = othergoat13(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Goat 3\": #If the user chooses Goat 3 revealed = np.random.choice(goats[np.arange(len(goats))!=2]) unrevealed1 = othergoat12(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Car\": #If the user chooses Car revealed = np.random.choice(goats) newgoat = goats[goats != revealed] unrevealed1 = newgoat[0] unrevealed2 = newgoat[1] return [userguess, revealed,unrevealed1,unrevealed2] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) print(a[3]) ['Car', 'Goat 1', 'Goat 2', 'Goat 3'] Car Goat 1 Goat 2 Goat 3 c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining1 c4 = [] #Create an empty list for the remaining2 for i in np.arange(1000): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list1 c4.append(game[3]) #In each round, add the fourth element to the remaining list2 import pandas as pd #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining1':c3, 'Remaining2':c4}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining1 Remaining2 0 Goat 3 Goat 2 Goat 1 Car 1 Goat 3 Goat 2 Goat 1 Car 2 Goat 1 Goat 3 Goat 2 Car 3 Goat 1 Goat 2 Goat 3 Car 4 Goat 2 Goat 1 Goat 3 Car ... ... ... ... ... 995 Goat 1 Goat 2 Goat 3 Car 996 Car Goat 1 Goat 2 Goat 3 997 Goat 2 Goat 1 Goat 3 Car 998 Goat 3 Goat 1 Goat 2 Car 999 Goat 2 Goat 1 Goat 3 Car 1000 rows \u00d7 4 columns # Get the count of each item in the first and (3rd+4th) column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining1 == 'Car'].shape[0] + gamedf[gamedf.Remaining2 == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining1 == 'Goat 1'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining1 == 'Goat 2'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 2'].shape[0] original_g3 =gamedf[gamedf.Guess == 'Goat 3'].shape[0] remaining_g3 =gamedf[gamedf.Remaining1 == 'Goat 3'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 3'].shape[0] # Let's plot a grouped barplot import matplotlib.pyplot as plt # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2,original_g3] bars2 = [remaining_car,remaining_g1,remaining_g2,remaining_g3] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2','Goat 3']) # Create legend & Show graphic plt.legend() plt.show() Comparison of the plots show that as the number of doors (and goats) increases, it makes even more sense to switch!","title":"Example4: What if there were 4 doors and 3 goats?"},{"location":"8-Labs/Lab9/Lab9_Dev/#exercise3-run-the-modified-monty-hall-game-for-10100-and-1000-rounds-show-the-bar-plots-for-each-series-and-explain-the-difference","text":"#Define necessary functions #Run and plot for 10 rounds #Run and plot for 100 rounds #Run and plot for 1000 rounds","title":"Exercise3: Run the modified Monty Hall game for 10,100, and 1000 rounds. Show the bar plots for each series and explain the difference."},{"location":"docs.pc/","text":"Computational Thinking and Data Science A WebBook to Accompany ENGR 1330 at TTU by Theodore G. Cleveland and Farhang Forghanparast with contributions from : Dinesh Sundaravadivelu Devarajan, Turgut Batuhan Baturalp (Batu), Tanja Karp, Long Nguyen, and Mona Rizvi Introduction This on-line workbook is a collection of lessons and workshop contents for ENGR-1330 sections taught bt the first two authors; students in other sections are welcome to use this as a resource with proper attribution (check with your instructor regarding what they will consider acceptable) suggested citation goes here The webbook hyperlinks to two additional local (to the host server) resources for a total of three broad categories WebBook WebCourse WebData Topical Chapters - subtoic - subtoic A semester course framework - lessons - workshops(labs) - exercises - exams Topical The entire course content is served here and can be accessed from blackboard.ttu.edu or directly using the public URL. Homeworks and exams must be uploaded to blackboard.ttu.edu to be graded. Solutions are posted after due dates have passed, these links are updated weekly-ish. Document History This document is a living document and is updated frequently, Python is an ever evolving tool and stuff that works today will be constructively broken by the development team (python.org) in their quest for continuous improvement. Generally these changes occur in the packages (libraries, external modules) and primative python is quite stable. Administrator Notes The lead author built this webbook on a Raspberry Pi 4B (4GB) running Ubuntu 20.XX, an Apache Web Server, a JupyterHub (fully encrypted) with iPython extensions, R core, Latex, and MkDocs with extensions. The deployment hardware is an Amazon Web Services Virtual Private Server (Lightsail Instance) in the West Virginia Server Farm (typically the container is run on x86-64 Xeon hardware) Direct access to the notebook directories is on the to-do-list. A backup is maintained at https://github.com/dustykat/1330-textbook-webroot .","title":"<p style=\"text-align:center\"> Computational Thinking and Data Science </p>"},{"location":"docs.pc/#computational-thinking-and-data-science","text":"","title":" Computational Thinking and Data Science "},{"location":"docs.pc/#a-webbook-to-accompany-engr-1330-at-ttu","text":"by Theodore G. Cleveland and Farhang Forghanparast with contributions from : Dinesh Sundaravadivelu Devarajan, Turgut Batuhan Baturalp (Batu), Tanja Karp, Long Nguyen, and Mona Rizvi","title":"A WebBook to Accompany ENGR 1330 at TTU "},{"location":"docs.pc/#introduction","text":"This on-line workbook is a collection of lessons and workshop contents for ENGR-1330 sections taught bt the first two authors; students in other sections are welcome to use this as a resource with proper attribution (check with your instructor regarding what they will consider acceptable) suggested citation goes here The webbook hyperlinks to two additional local (to the host server) resources for a total of three broad categories WebBook WebCourse WebData Topical Chapters - subtoic - subtoic A semester course framework - lessons - workshops(labs) - exercises - exams Topical The entire course content is served here and can be accessed from blackboard.ttu.edu or directly using the public URL. Homeworks and exams must be uploaded to blackboard.ttu.edu to be graded. Solutions are posted after due dates have passed, these links are updated weekly-ish.","title":"Introduction"},{"location":"docs.pc/#document-history","text":"This document is a living document and is updated frequently, Python is an ever evolving tool and stuff that works today will be constructively broken by the development team (python.org) in their quest for continuous improvement. Generally these changes occur in the packages (libraries, external modules) and primative python is quite stable.","title":"Document History"},{"location":"docs.pc/#administrator-notes","text":"The lead author built this webbook on a Raspberry Pi 4B (4GB) running Ubuntu 20.XX, an Apache Web Server, a JupyterHub (fully encrypted) with iPython extensions, R core, Latex, and MkDocs with extensions. The deployment hardware is an Amazon Web Services Virtual Private Server (Lightsail Instance) in the West Virginia Server Farm (typically the container is run on x86-64 Xeon hardware) Direct access to the notebook directories is on the to-do-list. A backup is maintained at https://github.com/dustykat/1330-textbook-webroot .","title":"Administrator Notes"},{"location":"docs.pc/untitled/","text":"About this document Put something here about the document, authors, copyright (GPL or MIT Open License) On-Line Book Author's Notes Inserting Code Fragments To insert a code fragment such as print('Hello World') simply indent in the source file used to generate the document print('hello world') These fragments can be cut-and-paste into a JupyterLab notebook. Inserting Images If the image is taken from a URL, use the following: ![image-name (a local tag)](url_to_image_source) Such as: ![image-name](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqn40YbupkMAzY63jYtA6auEmjRfCOvCd0FA&usqp=CAU) Which will render a black swan: If the image is local to the host replace the url with the path to the image. Inserting URL links This is a variation of images, but without the ! , such as [link-name-that-will-display](url_to_link_destimation) For example the code below will link to the black swan search results: [link-to-images-of-black-swans](https://www.google.com/search?q=images+of+black+swan&client=safari&rls=en&sxsrf=ALeKk03oIoQ387TWjJoKzX-D_b7o1to43Q:1613002985584&tbm=isch&source=iu&ictx=1&fir=L2P5MiS1ICLTxM%252CC6BDdJoXT9KcEM%252C_&vet=1&usg=AI4_-kTXrBMpj__xL5IkGCshrXTp04fX3w&sa=X&ved=2ahUKEwiCneivyODuAhVJBs0KHY88CaAQ9QF6BAgUEAE&biw=1447&bih=975#imgrc=i_lxoojURNE3XM) link-to-images-of-black-swans","title":"About this document"},{"location":"docs.pc/untitled/#about-this-document","text":"Put something here about the document, authors, copyright (GPL or MIT Open License)","title":"About this document"},{"location":"docs.pc/untitled/#on-line-book-authors-notes","text":"Inserting Code Fragments To insert a code fragment such as print('Hello World') simply indent in the source file used to generate the document print('hello world') These fragments can be cut-and-paste into a JupyterLab notebook. Inserting Images If the image is taken from a URL, use the following: ![image-name (a local tag)](url_to_image_source) Such as: ![image-name](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqn40YbupkMAzY63jYtA6auEmjRfCOvCd0FA&usqp=CAU) Which will render a black swan: If the image is local to the host replace the url with the path to the image. Inserting URL links This is a variation of images, but without the ! , such as [link-name-that-will-display](url_to_link_destimation) For example the code below will link to the black swan search results: [link-to-images-of-black-swans](https://www.google.com/search?q=images+of+black+swan&client=safari&rls=en&sxsrf=ALeKk03oIoQ387TWjJoKzX-D_b7o1to43Q:1613002985584&tbm=isch&source=iu&ictx=1&fir=L2P5MiS1ICLTxM%252CC6BDdJoXT9KcEM%252C_&vet=1&usg=AI4_-kTXrBMpj__xL5IkGCshrXTp04fX3w&sa=X&ved=2ahUKEwiCneivyODuAhVJBs0KHY88CaAQ9QF6BAgUEAE&biw=1447&bih=975#imgrc=i_lxoojURNE3XM) link-to-images-of-black-swans","title":"On-Line Book Author's Notes"},{"location":"engr-1330-webroot/","text":"ENGR-1330 Computational Thinking and Data Science This is a repository for a course at Texas Tech University, specificially the sections taught by Dr. Theodore G. Cleveland. Purpose The purpose of the repository is to maintain a convienent back-up of course content for rapid migration across servers. Special Notes The structure is written to work on a web host, with hostname == atomickitty.ddns.net , if you clone to another server you will have the lovely task of changing the links. The string editor sed will become your friend! Materials herein come from many sources, in particular the Data8 repository from UC Berkeley. Sources in notebooks are at least cited by a URL. As the content is matured, proper citations are to be inserted. The 3-Readings directory contains copyrighted materials and should be exposed with care on a web server; generally no-one reads anymore, so its probably safe enought to protect using .htaccess simple uid:pwd approach. I use the materials during lectures to point out where I obtain various computational ideas. How to Use Clone the entire repository to /var/www/html/engr-1330-webroot. Have your main index point to this directory i.e. http://your-fqdn-server.org/engr-1330-webroot/ You can see working example at https://3.137.111.182/engr-1330-webroot/ (You will have to set a browser exception to accept the self-signed certificate) Syncronization Notes: Sync with 3.137.111.182/engr-1330-webroot/ (AWS server -- primary and live website copy) Sync with 75.3.84.227:192.168.1.75/ (Raspberry Pi -- developer and backup website copy) Sync with 75.3.84.227:192.168.1.79/ (Macintosh -- developer copy)","title":"ENGR-1330 Computational Thinking and Data Science"},{"location":"engr-1330-webroot/#engr-1330-computational-thinking-and-data-science","text":"This is a repository for a course at Texas Tech University, specificially the sections taught by Dr. Theodore G. Cleveland.","title":"ENGR-1330 Computational Thinking and Data Science"},{"location":"engr-1330-webroot/#purpose","text":"The purpose of the repository is to maintain a convienent back-up of course content for rapid migration across servers.","title":"Purpose"},{"location":"engr-1330-webroot/#special-notes","text":"The structure is written to work on a web host, with hostname == atomickitty.ddns.net , if you clone to another server you will have the lovely task of changing the links. The string editor sed will become your friend! Materials herein come from many sources, in particular the Data8 repository from UC Berkeley. Sources in notebooks are at least cited by a URL. As the content is matured, proper citations are to be inserted. The 3-Readings directory contains copyrighted materials and should be exposed with care on a web server; generally no-one reads anymore, so its probably safe enought to protect using .htaccess simple uid:pwd approach. I use the materials during lectures to point out where I obtain various computational ideas.","title":"Special Notes"},{"location":"engr-1330-webroot/#how-to-use","text":"Clone the entire repository to /var/www/html/engr-1330-webroot. Have your main index point to this directory i.e. http://your-fqdn-server.org/engr-1330-webroot/ You can see working example at https://3.137.111.182/engr-1330-webroot/ (You will have to set a browser exception to accept the self-signed certificate)","title":"How to Use"},{"location":"engr-1330-webroot/#syncronization-notes","text":"Sync with 3.137.111.182/engr-1330-webroot/ (AWS server -- primary and live website copy) Sync with 75.3.84.227:192.168.1.75/ (Raspberry Pi -- developer and backup website copy) Sync with 75.3.84.227:192.168.1.79/ (Macintosh -- developer copy)","title":"Syncronization Notes:"},{"location":"implicit_equations/NewtonsMethod/","text":"# Environment Check -- Deactivate on a working host import sys print(sys.executable) print(sys.version) print(sys.version_info) /opt/jupyterhub/bin/python3 3.8.5 (default, Jan 27 2021, 15:41:15) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Copyright \u00a9 DATE Author, all rights reserved Transporting Wind Turbine Blades Consider the transport of Turbine Blades, they are rather long, and often the transport compaines have to navigate difficult turns as in Figure 1. Consider route planning using a simple case: Two intersecting mountain road cuts (think of vertical walls) meet at an angle of 123 ^o , as shown in Figure 2. The East-West road is 7 feet wide, while the Southwest-Northeast road is 9 feet wide. What is the longest blade that can negotiate the turn? You can neglect the blade thickness (think of it as a line segment) and cannot tip it to make it through the corner. Build a tool (solution script) that can allow for general use where the angle A is variable as are the road widths. Analysis Visualize the turbine blade in sucessive positions as we transport it around the corner; there will be some critical position where each end touch the road cut walls while a point on the blade touches the corner of the intersection. If we analyze the various triangles formed by the turbine blade we can express the lengths in terms of the widths and angles. For the part of the blade on the East-West portion we obtain: l_1 = \\frac{w_2}{sin(B)} For the part of the blade on the Southeast-Northwest portion we obtain: l_2 = \\frac{w_1}{sin(C)} The angles are related as: B = \\pi - A - C And the turbine total length is: l = l_1 + l_2 = \\frac{w_2}{sin(B)} + \\frac{w_1}{sin(C)} Substitute our expression for B , and we have everything in terms on road widths, and intersection angle: l = \\frac{w_2}{sin(\\pi - A - C)} + \\frac{w_1}{sin(C)} Now we want to find the smallest l as a function of C , the necessary condition for such a minimum is \\frac{dl}{dC}=0 which by application of Calculus produces: \\frac{dl}{dC}=\\frac{w_2 cos(\\pi - A - C)}{sin^2(\\pi - A - C)} - \\frac{w_1 cos(C)}{sin^2(C)}=0 where A , w_1 , and w_2 are known, now we have to find a solution to this equation. and once we have found the value of C that satisfies \\frac{dl}{dC}=0 we can recover the length from l = \\frac{w_2}{sin(\\pi - A - C)} + \\frac{w_1}{sin(C)} Lets consider some methods to find the length: Plot the function, find the value from the plot Apply a search method (grid searching) Newton's method (with finite difference approximations to derivative) # Plot the function, find the value from the plot ## forward define prototype functions # Our plotting function import matplotlib.pyplot as plt def makeAplot(listx1,listy1,strlablx,strlably,strtitle): mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(listx1,listy1, c='red', marker='o',linewidth=1) # basic data plot plt.xlabel(strlablx) plt.ylabel(strlably) plt.title(strtitle) plt.show() # prototype dl/dC function import math def func(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 =w2*math.cos(math.pi - A - x) denom1 = math.sin(math.pi - A - x)**2 numer2 = w1*math.cos(x) denom2 = math.sin(x)**2 func = numer1/denom1 - numer2/denom2 return(func) # prototype length function def blade_length(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 = w2 denom1 = math.sin(math.pi - A - x) numer2 = w1 denom2 = math.sin(x) blade_length = numer1/denom1 + numer2/denom2 return(blade_length) # Plot the function c_angle = [] # empty list to populate f_value = [] # empty list to populate # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet stepsize = 0.01 # increments of angle (degrees) total_steps = int(Cm/stepsize) tolerance = 5 # Only save a result close to the zero for i in range(1,total_steps): test_angle = float(i)*math.pi/total_steps test_value = func(test_angle) if abs(test_value) <= tolerance: c_angle.append((test_angle*180)/math.pi) # angle in degrees f_value.append(test_value) # value of dl/dC function # print((test_angle*180)/math.pi,test_value) # activate to examine values makeAplot(c_angle,f_value,'strlablx','strlably','strtitle') # Now get the length from our best guess from plot; trial and error to refine guess visual_angle = 43.888 # based on eyeball fit! print('Maximum blade length for C angle of ',round(visual_angle,1),' degrees is ',round(blade_length(visual_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(visual_angle*math.pi/180.),3)) Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.002 Grid Search Describe method then show example # Apply a search method # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet tolerance = 0.01 # define close enough to zero stepsize = 0.001 # increments of angle (degrees) total_steps = int(Cm/stepsize) best_size = 1e9 for i in range(1,total_steps): test_angle = float(i)*math.pi/total_steps test_value = func(test_angle) if abs(test_value) <= tolerance: # are we close to zero? if abs(test_value) < best_size: # are better than last close to zero? best_size = abs(test_value) best_angle = test_angle # print(float(i),test_value,best_size) else: pass #print(((best_angle*180)/math.pi),best_size) # Now report values print('Searched ',total_steps,' interference angles in steps of ',stepsize,' degrees of arc') print(' Lower C angle ',round(float(1)*math.pi/total_steps,3),'degrees of arc') #print(' Blade Length to Fit Intersection is ',round(blade_length(float(1)*math.pi/total_steps),3),' feet') print(' Upper C angle ',round(Cm,3), 'degrees of arc') print('Best C angle ',round(((best_angle*180)/math.pi),3), 'degrees of arc') print('Longest Blade Length to Fit Intersection is ',round(blade_length(best_angle),3),' feet') Searched 89900 interference angles in steps of 0.001 degrees of arc Lower C angle 0.0 degrees of arc Upper C angle 89.9 degrees of arc Best C angle 43.889 degrees of arc Longest Blade Length to Fit Intersection is 50.93 feet Single Variable Newtons Method Newton's method is an iterative technique that can produce good estimates of solutions to implicit equations. The method is employed by rewriting the equation in the form f(x) = 0 , then successively manipulating guesses for x until the function evaluates to a value close enough to zero for the modeler to accept.In the turbine blade case the \\frac{dl}{dC}=0 function is already in the correct form. Background The figure above is a graph of some function whose intercept with the x-axis is unknown. The goal of Newton's method is to find this intersection (root) from a realistic first guess. Suppose the first guess is x1 , shown on the figure as the right-most specific value of x . The value of the function at this location is f(x1) . Because x1 is supposed to be a root the difference from the value zero represents an error in the estimate. Newton's method simply provides a recipe for corrections to this error. Provided x1 is not near a minimum or maximum (slope of the function is not zero) then a better estimate of the root can be obtained by extending a tangent line from x1, f(x1) to the x-axis . The intersection of this line with the axis represents a better estimate of the root. This new estimate is x2 . A formula for x2 can be derived from the geometry of the triangle x2,f(x1),x1 . Recall from calculus that the tangent to a function at a particular point is the first derivative of the function. Therefore, from the geometry of the triangle and the definition of tangent we can write, \\begin{equation} tan(\\theta)=\\frac{df}{dx}\\Biggr\\vert_{x_1} = \\frac{f(x_1)}{x_1 - x_2} \\end{equation} Solving the equation for x 2 results in a formula that expresses x2 in terms of the first guess plus a correction term. \\begin{equation} x_2=x_1 - \\frac{f(x_1)}{\\frac{df}{dx}\\vert_{x_1}} \\end{equation} The second term on the right hand side is the correction term to the estimate on the right hand side. Once x2 is calculated we can repeat the formula substituting x2 for x1 and x3 for x2 in the formula. Repeated application usually leads to one of three outcomes: 1. a root; 2. divergence to +/- \\inf ; or 3. cycling. These three outcomes are discussed below in various subsections along with some remedies. The generalized formula is \\begin{equation} x_{k+1}=x_{k} - \\frac{ f(x_{k}) }{ \\frac{df}{dx}\\rvert_{x_k} } \\label{eqn:NewtonFormula} \\end{equation} If the derivative is evaluated using analytical derivatives the method is called Newton's method, if approximations to the derivative are used, it is called a quasi-Newton method. Newton's Method --- Using analytical derivatives This subsection is an example in Python of implementing Newton's method with analytical derivatives. The recipe itself is: Write the function in proper form, and code it into a computer. Write the derivative in proper form and code it into a computer. Make an initial guess of the solution (0 and 1 are always convenient guesses). Evaluate the function, evaluate the derivative, calculate their ratio. Subtract the ratio from the current guess and save the result as the update. Test for stopping: Did the update stay the same value? Yes, then stop, probably have a solution. Is the function nearly zero? Yes, then stop we probably have a solution. Have we tried too many updates? Yes, then stop the process is probably cycling, stop. If stopping is indicated proceed to next step, otherwise proceed back to step 4. Stopping indicated, report last update as the result (or report failure to find solution), and related information about the status of the numerical method. The following example illustrates these step as well as an ipython implementation of Newton's method. Suppose we wish to find a root (value of x ) that satisfies: \\begin{equation} f(x) = e^x - 10 cos(x) -100 \\end{equation} Then we will need to code it into a script. Here is a code fragment that will generate the prototype function # import built in function for e^x, cosine from math import exp, cos, sin # Define the function def func(x): func = exp(x) - 10*cos(x) - 100 #using the name as the temp var return func Notice in the code fragment we import three built-in functions from the Python math package, specifically \\exp() , \\sin() , and \\cos () . The next step is to code the derivative. In this case the derivative is \\begin{equation} \\frac{df}{dx}\\vert{(x)} = e^x + 10 \\sin(x) \\end{equation} and the prototype function is coded as def dfdx(x): dfdx = exp(x) + 10*sin(x) return dfdx Next we will need script to read in an initial guess, and ask us how many trials we will use to try to find a solution, as well as how close to zero we should be before we declare victory. # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 2 Enter iteration maximum 9 Enter a solution tolerance (e.g. 1e-06) 1e-6 The use of HowSmall is called a zero tolerance. We will use the same numerical value for two tolerance tests. Also notice how we are using error traps to force numeric input. Probably overkill for this example, but because we already wrote the try-except code in an earlier lesson, might as well reuse the code. Professional codes do a lot of error checking before launching into the actual processing - especially if the processing part is time consuming, its worth the time to check for obvious errors before running for a few hours then at some point failing because of an input value error that was predictable. Now back to the tolerance tests. The first test is to determine if the update has changed or not. If it has not, we may not have a correct answer, but there is no point continuing because the update is unlikely to move further. The test is something like \\begin{equation} \\text{IF}~\\lvert x_{k+1} - x_{k} \\rvert < \\text{Tol.~ THEN Exit and Report Results} \\end{equation} The second test is if the function value is close to zero. The structure of the test is similar, just an different argument. The second test is something like \\begin{equation} \\text{IF}~\\lvert f(x_{k+1}) \\rvert < \\text{Tol.~ THEN Exit and Report Results} \\end{equation} One can see from the nature of the two tests that a programmer might want to make the tolerance values different. This modification is left as a reader exercise. Checking for maximum iterations is relatively easy, we just include code that checks for normal exit the loop. Here is code fragment that implements the method, makes the various tests, and reports results. # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Update not changing \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") Update not changing Function value = 1.4210854715202004e-14 Root value = 4.593209147284144 End of NewtonMethod.py Now we simply connect the three fragments, and we would have a working Python script that implements Newton's method for the example equation. The example is specific to the particular function provided, but the programmer could move the two functions func and dfdx into a user specified module, and then load that module in the program to make it even more generic. The next section will use such an approach to illustrate the ability to build a generalized Newton method and only have to program the function itself Newton's Method --- Using finite-differences to estimate the derivative A practical difficulty in using Newton's method is determining the value of the derivative in cases where differentiation is difficult. In these cases we can replace the derivative by a finite difference equation and then proceed as in Newton's method. Recall from calculus that the derivative was defined as the limit of the difference quotient: \\begin{equation} \\frac{df}{dx}\\vert_{x} = \\lim_{\\Delta x \\rightarrow 0}\\frac{f(x + \\Delta x) - f(x) }{\\Delta x} \\end{equation} A good approximation to the derivative should be possible by using this formula with a small, but non-zero value for \\Delta x . \\begin{equation} \\frac{df}{dx}\\vert_{x} \\approx \\frac{f(x + \\Delta x) - f(x) }{\\Delta x} \\end{equation} When one replaces the derivative with the difference formula the root finding method the resulting update formula is \\begin{equation} x_{k+1}=x_k - \\frac{f(x_k) \\Delta x}{f(x_k + \\Delta x)-f(x_k)} \\end{equation} This root-finding method is called a quasi-Newton method. Here is the code fragment that we change by commenting out the analytical derivative and replacing it with a first-order finite difference approximation of the derivative. The numerical value 1e-06 is called the step size ( \\Delta x ) and should be an input value (rather than built-in to the code as shown here) like the tolerance test values, and be passed to the function as another argument. # reset the notebook %reset -f # import built in function for e^x, cosine from math import exp, cos, sin # Define the function def func(x): func = exp(x) - 10*cos(x) - 100 #using the name as the temp var return func def dfdx(x): # dfdx = exp(x) + 10*sin(x) dfdx = (func(x + 1e-06) - func(x) )/ (1e-06) return (dfdx) # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 1 Enter iteration maximum 10 Enter a solution tolerance (e.g. 1e-06) 1e-6 # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Update not changing \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") Iteration Limit Reached Function value = 0.00017750521082859905 Root value = 4.593211144371335 End of NewtonMethod.py Pretty much the same result, but now we dont have to determine the analytical derivative. Turbine Example using Newton's Method All we have to do is redefine the various functions, copy from above (because we reset the notebook twice, we lost these objects) # reset the notebook %reset -f # prototype dl/dC function import math def func(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 =w2*math.cos(math.pi - A - x) denom1 = math.sin(math.pi - A - x)**2 numer2 = w1*math.cos(x) denom2 = math.sin(x)**2 func = numer1/denom1 - numer2/denom2 return(func) # prototype length function def blade_length(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 = w2 denom1 = math.sin(math.pi - A - x) numer2 = w1 denom2 = math.sin(x) blade_length = numer1/denom1 + numer2/denom2 return(blade_length) # prototype finite difference approximation to derivative def dfdx(x): dfdx = (func(x + 1e-09) - func(x) )/ (1e-09) return (dfdx) # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 30 Enter iteration maximum 12 Enter a solution tolerance (e.g. 1e-06) 1e-6 # need to convert to radians xnow = xnow*math.pi/180. # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) print(xnow,func(xnow),xnew,func(xnew)) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Iteration\",i,\" Update not changing \\n\",) print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") 0.5235987755982988 -46.15910654595476 0.6916206270990959 -11.659686294276273 0.6916206270990959 -11.659686294276273 0.7630538719365358 -0.4517499985484186 0.7630538719365358 -0.4517499985484186 0.7660021307306737 -0.00012395061385106487 0.7660021307306737 -0.00012395061385106487 0.7660029400921291 -2.327027459614328e-12 Iteration 3 Update not changing Function value = -2.327027459614328e-12 Root value = 0.7660029400921291 End of NewtonMethod.py # need to convert back to degrees newton_angle = xnew*180/math.pi print(\"C angle \",newton_angle) C angle 43.88873556189143 # Now get the length from our best guess from plot #newton_angle = 43.8887 # based on newton method fit! print('Maximum blade length for C angle of ',round(newton_angle,1),' degrees is ',round(blade_length(newton_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(newton_angle*math.pi/180.),3)) Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.0 # Newton's method using scipy import scipy.optimize myguess = 30 myguess = myguess*math.pi/180. newton_angle = scipy.optimize.newton(func, myguess)*180/math.pi print(\"Using scipy the C angle is \",newton_angle) print('Maximum blade length for C angle of ',round(newton_angle,1),' degrees is ',round(blade_length(newton_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(newton_angle*math.pi/180.),3)) Using scipy the C angle is 43.888735561892304 Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.0","title":"Implicit Equations"},{"location":"implicit_equations/NewtonsMethod/#transporting-wind-turbine-blades","text":"Consider the transport of Turbine Blades, they are rather long, and often the transport compaines have to navigate difficult turns as in Figure 1. Consider route planning using a simple case: Two intersecting mountain road cuts (think of vertical walls) meet at an angle of 123 ^o , as shown in Figure 2. The East-West road is 7 feet wide, while the Southwest-Northeast road is 9 feet wide. What is the longest blade that can negotiate the turn? You can neglect the blade thickness (think of it as a line segment) and cannot tip it to make it through the corner. Build a tool (solution script) that can allow for general use where the angle A is variable as are the road widths.","title":"Transporting Wind Turbine Blades"},{"location":"implicit_equations/NewtonsMethod/#analysis","text":"Visualize the turbine blade in sucessive positions as we transport it around the corner; there will be some critical position where each end touch the road cut walls while a point on the blade touches the corner of the intersection. If we analyze the various triangles formed by the turbine blade we can express the lengths in terms of the widths and angles. For the part of the blade on the East-West portion we obtain: l_1 = \\frac{w_2}{sin(B)} For the part of the blade on the Southeast-Northwest portion we obtain: l_2 = \\frac{w_1}{sin(C)} The angles are related as: B = \\pi - A - C And the turbine total length is: l = l_1 + l_2 = \\frac{w_2}{sin(B)} + \\frac{w_1}{sin(C)} Substitute our expression for B , and we have everything in terms on road widths, and intersection angle: l = \\frac{w_2}{sin(\\pi - A - C)} + \\frac{w_1}{sin(C)} Now we want to find the smallest l as a function of C , the necessary condition for such a minimum is \\frac{dl}{dC}=0 which by application of Calculus produces: \\frac{dl}{dC}=\\frac{w_2 cos(\\pi - A - C)}{sin^2(\\pi - A - C)} - \\frac{w_1 cos(C)}{sin^2(C)}=0 where A , w_1 , and w_2 are known, now we have to find a solution to this equation. and once we have found the value of C that satisfies \\frac{dl}{dC}=0 we can recover the length from l = \\frac{w_2}{sin(\\pi - A - C)} + \\frac{w_1}{sin(C)} Lets consider some methods to find the length: Plot the function, find the value from the plot Apply a search method (grid searching) Newton's method (with finite difference approximations to derivative) # Plot the function, find the value from the plot ## forward define prototype functions # Our plotting function import matplotlib.pyplot as plt def makeAplot(listx1,listy1,strlablx,strlably,strtitle): mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(listx1,listy1, c='red', marker='o',linewidth=1) # basic data plot plt.xlabel(strlablx) plt.ylabel(strlably) plt.title(strtitle) plt.show() # prototype dl/dC function import math def func(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 =w2*math.cos(math.pi - A - x) denom1 = math.sin(math.pi - A - x)**2 numer2 = w1*math.cos(x) denom2 = math.sin(x)**2 func = numer1/denom1 - numer2/denom2 return(func) # prototype length function def blade_length(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 = w2 denom1 = math.sin(math.pi - A - x) numer2 = w1 denom2 = math.sin(x) blade_length = numer1/denom1 + numer2/denom2 return(blade_length) # Plot the function c_angle = [] # empty list to populate f_value = [] # empty list to populate # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet stepsize = 0.01 # increments of angle (degrees) total_steps = int(Cm/stepsize) tolerance = 5 # Only save a result close to the zero for i in range(1,total_steps): test_angle = float(i)*math.pi/total_steps test_value = func(test_angle) if abs(test_value) <= tolerance: c_angle.append((test_angle*180)/math.pi) # angle in degrees f_value.append(test_value) # value of dl/dC function # print((test_angle*180)/math.pi,test_value) # activate to examine values makeAplot(c_angle,f_value,'strlablx','strlably','strtitle') # Now get the length from our best guess from plot; trial and error to refine guess visual_angle = 43.888 # based on eyeball fit! print('Maximum blade length for C angle of ',round(visual_angle,1),' degrees is ',round(blade_length(visual_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(visual_angle*math.pi/180.),3)) Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.002","title":"Analysis"},{"location":"implicit_equations/NewtonsMethod/#grid-search","text":"Describe method then show example # Apply a search method # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet tolerance = 0.01 # define close enough to zero stepsize = 0.001 # increments of angle (degrees) total_steps = int(Cm/stepsize) best_size = 1e9 for i in range(1,total_steps): test_angle = float(i)*math.pi/total_steps test_value = func(test_angle) if abs(test_value) <= tolerance: # are we close to zero? if abs(test_value) < best_size: # are better than last close to zero? best_size = abs(test_value) best_angle = test_angle # print(float(i),test_value,best_size) else: pass #print(((best_angle*180)/math.pi),best_size) # Now report values print('Searched ',total_steps,' interference angles in steps of ',stepsize,' degrees of arc') print(' Lower C angle ',round(float(1)*math.pi/total_steps,3),'degrees of arc') #print(' Blade Length to Fit Intersection is ',round(blade_length(float(1)*math.pi/total_steps),3),' feet') print(' Upper C angle ',round(Cm,3), 'degrees of arc') print('Best C angle ',round(((best_angle*180)/math.pi),3), 'degrees of arc') print('Longest Blade Length to Fit Intersection is ',round(blade_length(best_angle),3),' feet') Searched 89900 interference angles in steps of 0.001 degrees of arc Lower C angle 0.0 degrees of arc Upper C angle 89.9 degrees of arc Best C angle 43.889 degrees of arc Longest Blade Length to Fit Intersection is 50.93 feet","title":"Grid Search"},{"location":"implicit_equations/NewtonsMethod/#single-variable-newtons-method","text":"Newton's method is an iterative technique that can produce good estimates of solutions to implicit equations. The method is employed by rewriting the equation in the form f(x) = 0 , then successively manipulating guesses for x until the function evaluates to a value close enough to zero for the modeler to accept.In the turbine blade case the \\frac{dl}{dC}=0 function is already in the correct form.","title":"Single Variable Newtons Method"},{"location":"implicit_equations/NewtonsMethod/#background","text":"The figure above is a graph of some function whose intercept with the x-axis is unknown. The goal of Newton's method is to find this intersection (root) from a realistic first guess. Suppose the first guess is x1 , shown on the figure as the right-most specific value of x . The value of the function at this location is f(x1) . Because x1 is supposed to be a root the difference from the value zero represents an error in the estimate. Newton's method simply provides a recipe for corrections to this error. Provided x1 is not near a minimum or maximum (slope of the function is not zero) then a better estimate of the root can be obtained by extending a tangent line from x1, f(x1) to the x-axis . The intersection of this line with the axis represents a better estimate of the root. This new estimate is x2 . A formula for x2 can be derived from the geometry of the triangle x2,f(x1),x1 . Recall from calculus that the tangent to a function at a particular point is the first derivative of the function. Therefore, from the geometry of the triangle and the definition of tangent we can write, \\begin{equation} tan(\\theta)=\\frac{df}{dx}\\Biggr\\vert_{x_1} = \\frac{f(x_1)}{x_1 - x_2} \\end{equation} Solving the equation for x 2 results in a formula that expresses x2 in terms of the first guess plus a correction term. \\begin{equation} x_2=x_1 - \\frac{f(x_1)}{\\frac{df}{dx}\\vert_{x_1}} \\end{equation} The second term on the right hand side is the correction term to the estimate on the right hand side. Once x2 is calculated we can repeat the formula substituting x2 for x1 and x3 for x2 in the formula. Repeated application usually leads to one of three outcomes: 1. a root; 2. divergence to +/- \\inf ; or 3. cycling. These three outcomes are discussed below in various subsections along with some remedies. The generalized formula is \\begin{equation} x_{k+1}=x_{k} - \\frac{ f(x_{k}) }{ \\frac{df}{dx}\\rvert_{x_k} } \\label{eqn:NewtonFormula} \\end{equation} If the derivative is evaluated using analytical derivatives the method is called Newton's method, if approximations to the derivative are used, it is called a quasi-Newton method.","title":"Background"},{"location":"implicit_equations/NewtonsMethod/#newtons-method-using-analytical-derivatives","text":"This subsection is an example in Python of implementing Newton's method with analytical derivatives. The recipe itself is: Write the function in proper form, and code it into a computer. Write the derivative in proper form and code it into a computer. Make an initial guess of the solution (0 and 1 are always convenient guesses). Evaluate the function, evaluate the derivative, calculate their ratio. Subtract the ratio from the current guess and save the result as the update. Test for stopping: Did the update stay the same value? Yes, then stop, probably have a solution. Is the function nearly zero? Yes, then stop we probably have a solution. Have we tried too many updates? Yes, then stop the process is probably cycling, stop. If stopping is indicated proceed to next step, otherwise proceed back to step 4. Stopping indicated, report last update as the result (or report failure to find solution), and related information about the status of the numerical method. The following example illustrates these step as well as an ipython implementation of Newton's method. Suppose we wish to find a root (value of x ) that satisfies: \\begin{equation} f(x) = e^x - 10 cos(x) -100 \\end{equation} Then we will need to code it into a script. Here is a code fragment that will generate the prototype function # import built in function for e^x, cosine from math import exp, cos, sin # Define the function def func(x): func = exp(x) - 10*cos(x) - 100 #using the name as the temp var return func Notice in the code fragment we import three built-in functions from the Python math package, specifically \\exp() , \\sin() , and \\cos () . The next step is to code the derivative. In this case the derivative is \\begin{equation} \\frac{df}{dx}\\vert{(x)} = e^x + 10 \\sin(x) \\end{equation} and the prototype function is coded as def dfdx(x): dfdx = exp(x) + 10*sin(x) return dfdx Next we will need script to read in an initial guess, and ask us how many trials we will use to try to find a solution, as well as how close to zero we should be before we declare victory. # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 2 Enter iteration maximum 9 Enter a solution tolerance (e.g. 1e-06) 1e-6 The use of HowSmall is called a zero tolerance. We will use the same numerical value for two tolerance tests. Also notice how we are using error traps to force numeric input. Probably overkill for this example, but because we already wrote the try-except code in an earlier lesson, might as well reuse the code. Professional codes do a lot of error checking before launching into the actual processing - especially if the processing part is time consuming, its worth the time to check for obvious errors before running for a few hours then at some point failing because of an input value error that was predictable. Now back to the tolerance tests. The first test is to determine if the update has changed or not. If it has not, we may not have a correct answer, but there is no point continuing because the update is unlikely to move further. The test is something like \\begin{equation} \\text{IF}~\\lvert x_{k+1} - x_{k} \\rvert < \\text{Tol.~ THEN Exit and Report Results} \\end{equation} The second test is if the function value is close to zero. The structure of the test is similar, just an different argument. The second test is something like \\begin{equation} \\text{IF}~\\lvert f(x_{k+1}) \\rvert < \\text{Tol.~ THEN Exit and Report Results} \\end{equation} One can see from the nature of the two tests that a programmer might want to make the tolerance values different. This modification is left as a reader exercise. Checking for maximum iterations is relatively easy, we just include code that checks for normal exit the loop. Here is code fragment that implements the method, makes the various tests, and reports results. # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Update not changing \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") Update not changing Function value = 1.4210854715202004e-14 Root value = 4.593209147284144 End of NewtonMethod.py Now we simply connect the three fragments, and we would have a working Python script that implements Newton's method for the example equation. The example is specific to the particular function provided, but the programmer could move the two functions func and dfdx into a user specified module, and then load that module in the program to make it even more generic. The next section will use such an approach to illustrate the ability to build a generalized Newton method and only have to program the function itself","title":"Newton's Method --- Using analytical derivatives"},{"location":"implicit_equations/NewtonsMethod/#newtons-method-using-finite-differences-to-estimate-the-derivative","text":"A practical difficulty in using Newton's method is determining the value of the derivative in cases where differentiation is difficult. In these cases we can replace the derivative by a finite difference equation and then proceed as in Newton's method. Recall from calculus that the derivative was defined as the limit of the difference quotient: \\begin{equation} \\frac{df}{dx}\\vert_{x} = \\lim_{\\Delta x \\rightarrow 0}\\frac{f(x + \\Delta x) - f(x) }{\\Delta x} \\end{equation} A good approximation to the derivative should be possible by using this formula with a small, but non-zero value for \\Delta x . \\begin{equation} \\frac{df}{dx}\\vert_{x} \\approx \\frac{f(x + \\Delta x) - f(x) }{\\Delta x} \\end{equation} When one replaces the derivative with the difference formula the root finding method the resulting update formula is \\begin{equation} x_{k+1}=x_k - \\frac{f(x_k) \\Delta x}{f(x_k + \\Delta x)-f(x_k)} \\end{equation} This root-finding method is called a quasi-Newton method. Here is the code fragment that we change by commenting out the analytical derivative and replacing it with a first-order finite difference approximation of the derivative. The numerical value 1e-06 is called the step size ( \\Delta x ) and should be an input value (rather than built-in to the code as shown here) like the tolerance test values, and be passed to the function as another argument. # reset the notebook %reset -f # import built in function for e^x, cosine from math import exp, cos, sin # Define the function def func(x): func = exp(x) - 10*cos(x) - 100 #using the name as the temp var return func def dfdx(x): # dfdx = exp(x) + 10*sin(x) dfdx = (func(x + 1e-06) - func(x) )/ (1e-06) return (dfdx) # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 1 Enter iteration maximum 10 Enter a solution tolerance (e.g. 1e-06) 1e-6 # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Update not changing \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") Iteration Limit Reached Function value = 0.00017750521082859905 Root value = 4.593211144371335 End of NewtonMethod.py Pretty much the same result, but now we dont have to determine the analytical derivative.","title":"Newton's Method --- Using finite-differences to estimate the derivative"},{"location":"implicit_equations/NewtonsMethod/#turbine-example-using-newtons-method","text":"All we have to do is redefine the various functions, copy from above (because we reset the notebook twice, we lost these objects) # reset the notebook %reset -f # prototype dl/dC function import math def func(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 =w2*math.cos(math.pi - A - x) denom1 = math.sin(math.pi - A - x)**2 numer2 = w1*math.cos(x) denom2 = math.sin(x)**2 func = numer1/denom1 - numer2/denom2 return(func) # prototype length function def blade_length(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 = w2 denom1 = math.sin(math.pi - A - x) numer2 = w1 denom2 = math.sin(x) blade_length = numer1/denom1 + numer2/denom2 return(blade_length) # prototype finite difference approximation to derivative def dfdx(x): dfdx = (func(x + 1e-09) - func(x) )/ (1e-09) return (dfdx) # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 30 Enter iteration maximum 12 Enter a solution tolerance (e.g. 1e-06) 1e-6 # need to convert to radians xnow = xnow*math.pi/180. # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) print(xnow,func(xnow),xnew,func(xnew)) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Iteration\",i,\" Update not changing \\n\",) print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") 0.5235987755982988 -46.15910654595476 0.6916206270990959 -11.659686294276273 0.6916206270990959 -11.659686294276273 0.7630538719365358 -0.4517499985484186 0.7630538719365358 -0.4517499985484186 0.7660021307306737 -0.00012395061385106487 0.7660021307306737 -0.00012395061385106487 0.7660029400921291 -2.327027459614328e-12 Iteration 3 Update not changing Function value = -2.327027459614328e-12 Root value = 0.7660029400921291 End of NewtonMethod.py # need to convert back to degrees newton_angle = xnew*180/math.pi print(\"C angle \",newton_angle) C angle 43.88873556189143 # Now get the length from our best guess from plot #newton_angle = 43.8887 # based on newton method fit! print('Maximum blade length for C angle of ',round(newton_angle,1),' degrees is ',round(blade_length(newton_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(newton_angle*math.pi/180.),3)) Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.0 # Newton's method using scipy import scipy.optimize myguess = 30 myguess = myguess*math.pi/180. newton_angle = scipy.optimize.newton(func, myguess)*180/math.pi print(\"Using scipy the C angle is \",newton_angle) print('Maximum blade length for C angle of ',round(newton_angle,1),' degrees is ',round(blade_length(newton_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(newton_angle*math.pi/180.),3)) Using scipy the C angle is 43.888735561892304 Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.0","title":"Turbine Example using Newton's Method"},{"location":"integration_differentiation/numerical_integration/","text":"Integration of Functions At this point we have enough Python to consider doing some useful computations. We will start with numerical integration because it is useful and only requires count-controlled repetition and single subscript lists. Background Numerical integration is the numerical approximation of \\begin{equation} I = \\int_a^b f(x)dx \\end{equation} Consider the problem of determining the shaded area under the curve y = f(x) from x = a to x = b , as depicted in the figure below, and suppose that analytical integration is not feasible. The function may be known in tabular form from experimental measurements or it may be known in an analytical form. The function is taken to be continuous within the interval a < x < b . We may divide the area into n vertical panels, each of width \\Delta x = (b - a)/n , and then add the areas of all strips to obtain A~\\approx \\int ydx . A representative panel of area A_i is shown with darker shading in the figure. Three useful numerical approximations are listed in the following sections. The approximations differ in how the function is represented by the panels --- in all cases the function is approximated by known polynomial models between the panel end points. In each case the greater the number of strips, and correspondingly smaller value of \\Delta x , the more accurate the approximation. Typically, one can begin with a relatively small number of panels and increase the number until the resulting area approximation stops changing. Rectangular Panels The figure below is a schematic of a rectangular panels. The figure is assuming the function structure is known and can be evaluated at an arbitrary location in the \\Delta x dimension. Each panel is treated as a rectangle, as shown by the representative panel whose height y_m is chosen visually so that the small cross-hatched areas are as nearly equal as possible. Thus, we form the sum \\sum y_m of the effective heights and multiply by \\Delta x . For a function known in analytical form, a value for y_m equal to that of the function at the midpoint x_i + \\Delta x /2 may be calculated and used in the summation. For tabulated functions, we have to choose to either take y_m as the value at the left endpoint or right endpoint. This limitation is often quite handy when we are trying to integrate a function that is integrable, but undefined on one endpoint. Lets try some examples in Python. Find the area under the curve y= x\\sqrt{1+x^2} from x = 0 to x = 2 . First lets read in the value for the lowerlimit, we will do some limited error checks to be sure user enters a number, but won't check that the number is non-negative. # RectangularPanels.py # Numerical Integration print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Program finds area under curve y = x * sqrt(1+x) Verify that value is indeed what we entered print(x_low) Now do the same for the upper limit, notice how we are using the yes variable. We set a \"fail\" value, and demand input until we get \"success\". The structure used here is called a try -- exception structure and is very common in programming. Error checking is really important so that garbled input does not hang things up. yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Again verify! print(x_high) Now use the try - exception structure to input how many panels we wish to use. Notice you can enter a negative value which will ultimately break things. Also observe this value is an integer. yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Again verify! print(how_many) Now we can actually perform the integration by evaluating the function at the panel half-widths. In this example we are using primitive arithmetic, so the \\sqrt{} is accomplished by exponentation, the syntax is c = a ** b is the operation c = a^b . The integration uses an accumulator, which is a memory location where subsquent results are added (accumulated) back into the accumulator. This structure is so common that there are alternate, compact syntax to perform this task, here it is all out in the open. The counting loop where we evaluate the function at different x values, starts at 1 and ends at how_many+1 because python for loops use an increment skip if equal structure. When the value in range equals how_many the for loop exits ( break is implied.) A loop control structure starting from 0 is shown in the code as a comment line. Simply uncomment this line, and comment the line just below to have the structure typical in python scripts. In the start from 1 case, we want to evaluate at the last value of how_many . # OK we should have the three things we need for evaluating the integral delta_x = (x_high - x_low)/float(how_many) # compute panel width xx = x_low + delta_x/2 # initial value for x ### OK THIS IS THE ACTUAL INTEGRATOR PART ### accumulated_area = 0.0 # initial value in an accumulator #for i in range(0,how_many,1): #note we are counting from 0 for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * ( (1+xx**2)**(0.5) ) ) * delta_x xx = xx + delta_x ### AND WE ARE DONE INTEGRATING ############# Finally, we want to report our result print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) # the backslash \\ # \" to x = ..... lets us use multiple lines # the \\n is a \"newline\" character The code implements rudimentary error checking -- it forces us to enter numeric values for the lower and upper values of x as well as the number of panels to use. It does not check for undefined ranges and such, but you should get the idea -- notice that a large fraction of the entire program is error trapping; this devotion to error trapping is typical for professional programs where you are going to distribute executable modules and not expect the end user to be a programmer. Using the math package The actual computations are done rather crudely -- there is a math package that would give us the ability to compute the square root as a function call rather than exponentiation to a real values exponent. That is illustrated below # RectangularPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator xx = x_low + delta_x/2 # initial value for x for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * sqrt(1+xx**2) ) * delta_x xx = xx + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) Trapezoidal Panels The trapezoidal panels are approximated as shown in the figure below. The area A_i is the average height (y_i + y_{i+1} )/2 times \\Delta x . Adding the areas gives the area approximation as tabulated. For the example with the curvature shown, the approximation will be on the low side. For the reverse curvature, the approximation will be on the high side. The trapezoidal approximation is commonly used with tabulated values. The script below illustrates the trapezoidal method for approximating an integral. In the example, the left and right panel endpoints in x are set as separate variables x_{left} and x_{right} and incremented by \\Delta x as we step through the count-controlled repetition to accumulate the area. The corresponding y values are computed within the loop and averaged, then multiplied by \\Delta x and added to the accumulator. Finally the x values are incremented --- for grins, we used the += operator on the accumulator # TrapezoidalPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_right = x_left + delta_x # initial value for x_right edge panel for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left* sqrt(1+x_left**2) ) y_right = ( x_right* sqrt(1+x_right**2) ) accumulated_area += + (1./2.) * ( y_left + y_right ) * delta_x x_left += delta_x x_right += delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) Parabolic Panels Parabolic panels approximate the shape of the panel with a parabola. The area between the chord and the curve (neglected in the trapezoidal solution) may be accounted for by approximating the function with a parabola passing through the points defined by three successive values of y . This area may be calculated from the geometry of the parabola and added to the trapezoidal area of the pair of strips to give the area \\Delta A of the pair as illustrated. Adding all of the \\Delta A s produces the tabulation shown, which is known as Simpson's rule. To use Simpson's rule, the number n of strips must be even. The same example as presented for rectangular panels is repeated, except using parabolic panels. The code is changed yet again because we will evaluate at each end of the panel as well as at an intermediate value. # ParabolicPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_middle = x_left + delta_x # initial value for x_middle edge panel x_right = x_middle + delta_x # initial value for x_right edge panel how_many = int(how_many/2) # using 2 panels every step, so 1/2 many steps -- force integer result for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left * sqrt(1+ x_left**2) ) y_middle = ( x_middle * sqrt(1+ x_middle**2) ) y_right = ( x_right * sqrt(1+ x_right**2) ) accumulated_area = accumulated_area + \\ (1./3.) * ( y_left + 4.* y_middle + y_right ) * delta_x x_left = x_left + 2*delta_x x_middle = x_left + delta_x x_right = x_middle + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) If we study all the forms of the numerical method we observe that the numerical integration method is really the sum of function values at specific locations in the interval of interest, with each value multiplied by a specific weight. In this development the weights were based on polynomials, but other method use different weighting functions. An extremely important method is called gaussian quadrature. This method is valuable because one can approximate convolution integrals quite effectively using quadrature routines, while the number of function evaluations for a polynomial based approximation could be hopeless. When the function values are tabular, we are going to have to accept the rectangular (with adaptations) and trapezoidal as our best tools to approximate an integral because we don't have any really effective way to evaluate the function between the tabulated values. Integration of Tabular Data This section is going to work with tabular data -- different from function evaluation, but similar. To be really useful, we need to learn how to read data from a file; manually entering tabular data is really time consuming, error prone, and just plain idiotic. So in this chapter we will learn how to read data from a file into a list, then we can process the list as if it were a function and integrate its contents. Reading from a file --- open, read, close files First, lets consider a file named MyFile.txt . The extension is important so that the Shell does not think it is a Python script. The contents of MyFile.txt are: 1 1 2 4 3 9 4 16 5 25 The code fragment below, will let us look at the file (already existing in our local directory) import subprocess # lets us run \"shell\" commands and recover stdio stream usefull_cat_call = subprocess.run([\"cat\",\"MyFile.txt\"], stdout=subprocess.PIPE, text=True) # this is the call to run the bash command \"cat MyFile.txt\" which will display the contents of the file if it exists. print(usefull_cat_call.stdout) 1 1 2 4 3 9 4 16 5 25 Now that we know that the file exists,to read the contents into a Python script we have to do the following: Open a connection to the file --- this is a concept common to all languages, it might be called something different, but the program needs to somehow know the location and name of the file. Read the contents into an object --- we have a lot of control on how this gets done, for the time being we won't exercise much control yet. When you do substantial programs, you will depend on the control of the reads (and writes). Disconnect the file --- this too is common to all languages. Its a really easy step to forget. Not a big deal if the program ends as planned but terrible if there is a error in the program and the connection is still open. Usually nothing bad happens, but with an open connection it is possible for the file to get damaged. If that file represents millions of customers credit card numbers, that's kind of a problem, and time to go work on your resume, or get your passport collection out and choose a country without extradition. The code fragment below performs these three tasks and prints the things we read Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read the five lines line1 = Afile.readline() line2 = Afile.readline() line3 = Afile.readline() line4 = Afile.readline() line5 = Afile.readline() Afile.close() # disconnect from the file # echo the input print(line1,end=\"\") print(line2,end=\"\") print(line3,end=\"\") print(line4,end=\"\") print(line5,end=\"\") 1 1 2 4 3 9 4 16 5 25 Read into a list A far more useful and elegant way to read from a file is to use a for loop. The attribute line within a file is an iterable, hence construction the loop is pretty straightforward. A script fragment below does the same thing as the example above, but uses a for loop to accomplish stepping through the file. Additionally, I have added a counter to keep track of how many lines were read --- in a lot of engineering programs, the number of things read becomes important later in a program, hence it is usually a good idea to capture the count when the data are first read. First lets work out if we can automatically detect the end of the file. So this script just reads and prints the attribute line from object Afile . Notice how the print statement is changed, to suppress the extra line feed. Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) Now we will add a list to receive the input, here it reads the file above as a string into a list xy , then splits that list and places the contents into two other lists, x and y . The script has several parts to discuss. First, the destination variables (lists) must be created -- I used the null list concept here because I don't know how big the list is until I read the list. Next I used the .append() method which operates on the xy list. The arguments of the method [str(n) for n in line.strip().split()] tells the program that the elements are to be interpreted as a string, and to split (split) the line into sub-strings based on a null delimiter (whitespace), and to remove all the whitespace (strip) characters. Once the line is split, the strings are appended into the xy list. The xy list is printed to show that it is a list of 5 things, each thing being a string comprised of two sets of characters separated by a comma. xy is a list of strings. The next section of the code then uses the pair function within another .append() method to break the character sets in each element of xy into two parts x and y . Lastly during the pair operation, the code also converts the data into real values (float) and then prints the data in two columns. This seems like a lot of work, but we could easily get this code to be super reliable, then save it as a function and never have to write it again. That too comes later -- suffice to say for now we can read a file, parse its contents into two lists x and y . Thus we are now able to integrate tabular data. xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) The list is: [['1', '1'], ['2', '4'], ['3', '9'], ['4', '16'], ['5', '25']] x = 1.0 y = 1.0 x = 2.0 y = 4.0 x = 3.0 y = 9.0 x = 4.0 y = 16.0 x = 5.0 y = 25.0 Integrating the Tabular Data Suppose instead of a function we only have tabulations and wist to estimate the area under the curve represented by the tabular values. Then our integration rules from the prior chapter still work more or less, except the rectangular panels will have to be shifted to either the left edge or right edge of a panel (where the tabulation exists). Lets just examine an example. Suppose some measurement technology produced a table of related values. The excitation variable is x and f(x) is the response. x f(x) 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 To integrate this table using the trapezoidal method is straightforward. We will modify our earlier code to read the table (which we put into a file), and compute the integral. # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 File has 9 records (lines) The list is: [['1.0', '1.543'], ['1.1', '1.668'], ['1.2', '1.811'], ['1.3', '1.971'], ['1.4', '2.151'], ['1.5', '2.352'], ['1.6', '2.577'], ['1.7', '2.828'], ['1.8', '3.107']] x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Cool, it seems to work -- now tidy the code a bit by suppressing extra outputs # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: ##print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nRecords read =: \",how_many_lines) ##print(\"The list is: \",end=\"\") ##print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result Records read =: 9 x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Realistically the only other simple integration method for tabular data is the rectangular rule, either using the left edge of a panel or the right edge of a panel (and I suppose you could do both and average the result which would be the trapezoidal method). Exercises 1) Approximate \\int_0^2 f(x) dx from the tabulation in the Table below: x f(x) 0.00 1.0000 0.12 0.8869 0.53 0.5886 0.87 0.4190 1.08 0.3396 1.43 0.2393 2.00 0.1353 # 2) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{9.0} cosh(x) dx from the tabulation above. 3) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{4.2} cosh(x) dx from the tabulation above. Briefly explain how you handle starting and stopping the integration from values that are intermediate and are tabulated. # 4) (Advanced) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{4.0} cosh(x) dx from the tabulation above. Explain how handled working with values that fall between tabulated values.","title":"Interpolation, Integration, and Differentiation"},{"location":"integration_differentiation/numerical_integration/#integration-of-functions","text":"At this point we have enough Python to consider doing some useful computations. We will start with numerical integration because it is useful and only requires count-controlled repetition and single subscript lists.","title":"Integration of Functions"},{"location":"integration_differentiation/numerical_integration/#background","text":"Numerical integration is the numerical approximation of \\begin{equation} I = \\int_a^b f(x)dx \\end{equation} Consider the problem of determining the shaded area under the curve y = f(x) from x = a to x = b , as depicted in the figure below, and suppose that analytical integration is not feasible. The function may be known in tabular form from experimental measurements or it may be known in an analytical form. The function is taken to be continuous within the interval a < x < b . We may divide the area into n vertical panels, each of width \\Delta x = (b - a)/n , and then add the areas of all strips to obtain A~\\approx \\int ydx . A representative panel of area A_i is shown with darker shading in the figure. Three useful numerical approximations are listed in the following sections. The approximations differ in how the function is represented by the panels --- in all cases the function is approximated by known polynomial models between the panel end points. In each case the greater the number of strips, and correspondingly smaller value of \\Delta x , the more accurate the approximation. Typically, one can begin with a relatively small number of panels and increase the number until the resulting area approximation stops changing.","title":"Background"},{"location":"integration_differentiation/numerical_integration/#rectangular-panels","text":"The figure below is a schematic of a rectangular panels. The figure is assuming the function structure is known and can be evaluated at an arbitrary location in the \\Delta x dimension. Each panel is treated as a rectangle, as shown by the representative panel whose height y_m is chosen visually so that the small cross-hatched areas are as nearly equal as possible. Thus, we form the sum \\sum y_m of the effective heights and multiply by \\Delta x . For a function known in analytical form, a value for y_m equal to that of the function at the midpoint x_i + \\Delta x /2 may be calculated and used in the summation. For tabulated functions, we have to choose to either take y_m as the value at the left endpoint or right endpoint. This limitation is often quite handy when we are trying to integrate a function that is integrable, but undefined on one endpoint. Lets try some examples in Python. Find the area under the curve y= x\\sqrt{1+x^2} from x = 0 to x = 2 . First lets read in the value for the lowerlimit, we will do some limited error checks to be sure user enters a number, but won't check that the number is non-negative. # RectangularPanels.py # Numerical Integration print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Program finds area under curve y = x * sqrt(1+x) Verify that value is indeed what we entered print(x_low) Now do the same for the upper limit, notice how we are using the yes variable. We set a \"fail\" value, and demand input until we get \"success\". The structure used here is called a try -- exception structure and is very common in programming. Error checking is really important so that garbled input does not hang things up. yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Again verify! print(x_high) Now use the try - exception structure to input how many panels we wish to use. Notice you can enter a negative value which will ultimately break things. Also observe this value is an integer. yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Again verify! print(how_many) Now we can actually perform the integration by evaluating the function at the panel half-widths. In this example we are using primitive arithmetic, so the \\sqrt{} is accomplished by exponentation, the syntax is c = a ** b is the operation c = a^b . The integration uses an accumulator, which is a memory location where subsquent results are added (accumulated) back into the accumulator. This structure is so common that there are alternate, compact syntax to perform this task, here it is all out in the open. The counting loop where we evaluate the function at different x values, starts at 1 and ends at how_many+1 because python for loops use an increment skip if equal structure. When the value in range equals how_many the for loop exits ( break is implied.) A loop control structure starting from 0 is shown in the code as a comment line. Simply uncomment this line, and comment the line just below to have the structure typical in python scripts. In the start from 1 case, we want to evaluate at the last value of how_many . # OK we should have the three things we need for evaluating the integral delta_x = (x_high - x_low)/float(how_many) # compute panel width xx = x_low + delta_x/2 # initial value for x ### OK THIS IS THE ACTUAL INTEGRATOR PART ### accumulated_area = 0.0 # initial value in an accumulator #for i in range(0,how_many,1): #note we are counting from 0 for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * ( (1+xx**2)**(0.5) ) ) * delta_x xx = xx + delta_x ### AND WE ARE DONE INTEGRATING ############# Finally, we want to report our result print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) # the backslash \\ # \" to x = ..... lets us use multiple lines # the \\n is a \"newline\" character The code implements rudimentary error checking -- it forces us to enter numeric values for the lower and upper values of x as well as the number of panels to use. It does not check for undefined ranges and such, but you should get the idea -- notice that a large fraction of the entire program is error trapping; this devotion to error trapping is typical for professional programs where you are going to distribute executable modules and not expect the end user to be a programmer.","title":"Rectangular Panels"},{"location":"integration_differentiation/numerical_integration/#using-the-math-package","text":"The actual computations are done rather crudely -- there is a math package that would give us the ability to compute the square root as a function call rather than exponentiation to a real values exponent. That is illustrated below # RectangularPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator xx = x_low + delta_x/2 # initial value for x for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * sqrt(1+xx**2) ) * delta_x xx = xx + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area)","title":"Using the math package"},{"location":"integration_differentiation/numerical_integration/#trapezoidal-panels","text":"The trapezoidal panels are approximated as shown in the figure below. The area A_i is the average height (y_i + y_{i+1} )/2 times \\Delta x . Adding the areas gives the area approximation as tabulated. For the example with the curvature shown, the approximation will be on the low side. For the reverse curvature, the approximation will be on the high side. The trapezoidal approximation is commonly used with tabulated values. The script below illustrates the trapezoidal method for approximating an integral. In the example, the left and right panel endpoints in x are set as separate variables x_{left} and x_{right} and incremented by \\Delta x as we step through the count-controlled repetition to accumulate the area. The corresponding y values are computed within the loop and averaged, then multiplied by \\Delta x and added to the accumulator. Finally the x values are incremented --- for grins, we used the += operator on the accumulator # TrapezoidalPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_right = x_left + delta_x # initial value for x_right edge panel for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left* sqrt(1+x_left**2) ) y_right = ( x_right* sqrt(1+x_right**2) ) accumulated_area += + (1./2.) * ( y_left + y_right ) * delta_x x_left += delta_x x_right += delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area)","title":"Trapezoidal Panels"},{"location":"integration_differentiation/numerical_integration/#parabolic-panels","text":"Parabolic panels approximate the shape of the panel with a parabola. The area between the chord and the curve (neglected in the trapezoidal solution) may be accounted for by approximating the function with a parabola passing through the points defined by three successive values of y . This area may be calculated from the geometry of the parabola and added to the trapezoidal area of the pair of strips to give the area \\Delta A of the pair as illustrated. Adding all of the \\Delta A s produces the tabulation shown, which is known as Simpson's rule. To use Simpson's rule, the number n of strips must be even. The same example as presented for rectangular panels is repeated, except using parabolic panels. The code is changed yet again because we will evaluate at each end of the panel as well as at an intermediate value. # ParabolicPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_middle = x_left + delta_x # initial value for x_middle edge panel x_right = x_middle + delta_x # initial value for x_right edge panel how_many = int(how_many/2) # using 2 panels every step, so 1/2 many steps -- force integer result for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left * sqrt(1+ x_left**2) ) y_middle = ( x_middle * sqrt(1+ x_middle**2) ) y_right = ( x_right * sqrt(1+ x_right**2) ) accumulated_area = accumulated_area + \\ (1./3.) * ( y_left + 4.* y_middle + y_right ) * delta_x x_left = x_left + 2*delta_x x_middle = x_left + delta_x x_right = x_middle + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) If we study all the forms of the numerical method we observe that the numerical integration method is really the sum of function values at specific locations in the interval of interest, with each value multiplied by a specific weight. In this development the weights were based on polynomials, but other method use different weighting functions. An extremely important method is called gaussian quadrature. This method is valuable because one can approximate convolution integrals quite effectively using quadrature routines, while the number of function evaluations for a polynomial based approximation could be hopeless. When the function values are tabular, we are going to have to accept the rectangular (with adaptations) and trapezoidal as our best tools to approximate an integral because we don't have any really effective way to evaluate the function between the tabulated values.","title":"Parabolic Panels"},{"location":"integration_differentiation/numerical_integration/#integration-of-tabular-data","text":"This section is going to work with tabular data -- different from function evaluation, but similar. To be really useful, we need to learn how to read data from a file; manually entering tabular data is really time consuming, error prone, and just plain idiotic. So in this chapter we will learn how to read data from a file into a list, then we can process the list as if it were a function and integrate its contents.","title":"Integration of Tabular Data"},{"location":"integration_differentiation/numerical_integration/#reading-from-a-file-open-read-close-files","text":"First, lets consider a file named MyFile.txt . The extension is important so that the Shell does not think it is a Python script. The contents of MyFile.txt are: 1 1 2 4 3 9 4 16 5 25 The code fragment below, will let us look at the file (already existing in our local directory) import subprocess # lets us run \"shell\" commands and recover stdio stream usefull_cat_call = subprocess.run([\"cat\",\"MyFile.txt\"], stdout=subprocess.PIPE, text=True) # this is the call to run the bash command \"cat MyFile.txt\" which will display the contents of the file if it exists. print(usefull_cat_call.stdout) 1 1 2 4 3 9 4 16 5 25 Now that we know that the file exists,to read the contents into a Python script we have to do the following: Open a connection to the file --- this is a concept common to all languages, it might be called something different, but the program needs to somehow know the location and name of the file. Read the contents into an object --- we have a lot of control on how this gets done, for the time being we won't exercise much control yet. When you do substantial programs, you will depend on the control of the reads (and writes). Disconnect the file --- this too is common to all languages. Its a really easy step to forget. Not a big deal if the program ends as planned but terrible if there is a error in the program and the connection is still open. Usually nothing bad happens, but with an open connection it is possible for the file to get damaged. If that file represents millions of customers credit card numbers, that's kind of a problem, and time to go work on your resume, or get your passport collection out and choose a country without extradition. The code fragment below performs these three tasks and prints the things we read Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read the five lines line1 = Afile.readline() line2 = Afile.readline() line3 = Afile.readline() line4 = Afile.readline() line5 = Afile.readline() Afile.close() # disconnect from the file # echo the input print(line1,end=\"\") print(line2,end=\"\") print(line3,end=\"\") print(line4,end=\"\") print(line5,end=\"\") 1 1 2 4 3 9 4 16 5 25","title":"Reading from a file --- open, read, close files"},{"location":"integration_differentiation/numerical_integration/#read-into-a-list","text":"A far more useful and elegant way to read from a file is to use a for loop. The attribute line within a file is an iterable, hence construction the loop is pretty straightforward. A script fragment below does the same thing as the example above, but uses a for loop to accomplish stepping through the file. Additionally, I have added a counter to keep track of how many lines were read --- in a lot of engineering programs, the number of things read becomes important later in a program, hence it is usually a good idea to capture the count when the data are first read. First lets work out if we can automatically detect the end of the file. So this script just reads and prints the attribute line from object Afile . Notice how the print statement is changed, to suppress the extra line feed. Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) Now we will add a list to receive the input, here it reads the file above as a string into a list xy , then splits that list and places the contents into two other lists, x and y . The script has several parts to discuss. First, the destination variables (lists) must be created -- I used the null list concept here because I don't know how big the list is until I read the list. Next I used the .append() method which operates on the xy list. The arguments of the method [str(n) for n in line.strip().split()] tells the program that the elements are to be interpreted as a string, and to split (split) the line into sub-strings based on a null delimiter (whitespace), and to remove all the whitespace (strip) characters. Once the line is split, the strings are appended into the xy list. The xy list is printed to show that it is a list of 5 things, each thing being a string comprised of two sets of characters separated by a comma. xy is a list of strings. The next section of the code then uses the pair function within another .append() method to break the character sets in each element of xy into two parts x and y . Lastly during the pair operation, the code also converts the data into real values (float) and then prints the data in two columns. This seems like a lot of work, but we could easily get this code to be super reliable, then save it as a function and never have to write it again. That too comes later -- suffice to say for now we can read a file, parse its contents into two lists x and y . Thus we are now able to integrate tabular data. xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) The list is: [['1', '1'], ['2', '4'], ['3', '9'], ['4', '16'], ['5', '25']] x = 1.0 y = 1.0 x = 2.0 y = 4.0 x = 3.0 y = 9.0 x = 4.0 y = 16.0 x = 5.0 y = 25.0","title":"Read into a list"},{"location":"integration_differentiation/numerical_integration/#integrating-the-tabular-data","text":"Suppose instead of a function we only have tabulations and wist to estimate the area under the curve represented by the tabular values. Then our integration rules from the prior chapter still work more or less, except the rectangular panels will have to be shifted to either the left edge or right edge of a panel (where the tabulation exists). Lets just examine an example. Suppose some measurement technology produced a table of related values. The excitation variable is x and f(x) is the response. x f(x) 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 To integrate this table using the trapezoidal method is straightforward. We will modify our earlier code to read the table (which we put into a file), and compute the integral. # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 File has 9 records (lines) The list is: [['1.0', '1.543'], ['1.1', '1.668'], ['1.2', '1.811'], ['1.3', '1.971'], ['1.4', '2.151'], ['1.5', '2.352'], ['1.6', '2.577'], ['1.7', '2.828'], ['1.8', '3.107']] x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Cool, it seems to work -- now tidy the code a bit by suppressing extra outputs # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: ##print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nRecords read =: \",how_many_lines) ##print(\"The list is: \",end=\"\") ##print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result Records read =: 9 x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Realistically the only other simple integration method for tabular data is the rectangular rule, either using the left edge of a panel or the right edge of a panel (and I suppose you could do both and average the result which would be the trapezoidal method).","title":"Integrating the Tabular Data"},{"location":"integration_differentiation/numerical_integration/#exercises","text":"1) Approximate \\int_0^2 f(x) dx from the tabulation in the Table below: x f(x) 0.00 1.0000 0.12 0.8869 0.53 0.5886 0.87 0.4190 1.08 0.3396 1.43 0.2393 2.00 0.1353 # 2) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{9.0} cosh(x) dx from the tabulation above.","title":"Exercises"},{"location":"lesson0/lesson0/","text":"table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 13 January 2021 Lesson 0 Introduction to Computational Thinking with Data Science: Computational thinking concepts Data science and practices JupyterLab (iPython) as a programming environment Programming as a problem solving process CCMR Approach Special Script Blocks In the lesson notebooks there will usually be two script blocks, identical to the ones below. The first block identifies the particular computer, the user, and the python kernel in use. The second block sets markdown tables to left edge when rendering. I usually put both blocks at the top of the notebook, just after some kind of title block, as done here. Computational Thinking Concepts Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. Much of what follows is borrowed from (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2696102/). Computational thinking is taking an approach to solving problems, designing systems and understanding human behaviour that draws on concepts fundamental to computing (http://www.cs.cmu.edu/~15110-s13/Wing06-ct.pdf). Computational thinking is a kind of analytical thinking: It shares with mathematical thinking in the general ways in which we might approach solving a problem. It shares with engineering thinking in the general ways in which we might approach designing and evaluating a large, complex system that operates within the constraints of the real world. - It shares with scientific thinking in the general ways in which we might approach understanding computability, intelligence, the mind and human behaviour. The essence of computational thinking is abstraction and automation . In computing, we abstract notions beyond the physical dimensions of time and space. Our abstractions are extremely general because they are symbolic, where numeric abstractions are just a special case. CT Foundations CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation) Decomposition Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Examples include: - Writing a paper: - Introduction - Body - Conclusion Wide-viewed (Panorama) image: Taking multiple overlapped photos Stitch them Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it also will require assembly (system integration) to produce a desired solution. Abstraction Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve. Books in an online bookstore Important NOT important title Cover color ISBN Author\u2019s hometown Authors ... ... ... Algorithms Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. Image from https://www.newyorker.com/magazine/2021/01/18/whats-wrong-with-the-way-we-work?utm_source=pocket-newtab An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation. Algorithms are unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, can incorporate random input. System Integration (implementation) System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Data Science and Practice Data science is leveraging existing data sources, to create new ones as needed in order to extract meaningful information and actionable insights through business domain expertise, effective communication and results interpretation. Data science uses relevant statistical techniques, programming languages, software packages and libraries, and data infrastructure; The insights are used to drive business decisions and take actions intended to achieve business goals. Why is this important for engineers? Because engineering is a business! A list of typical skills (https://elitedatascience.com/data-science-resources): Foundational Skills Programming and Data Manipulation Statistics and Probability Technical Skills Data Collection SQL Data Visualization Applied Machine Learning Business Skills Communication Creativity and Innovation Operations and Strategy Business Analytics Supplementary Skills Natural Language Processing Recommendation Systems Time Series Analysis Practice Projects Competitions Problem Solving Challenges JupyterLab (iPython) Environment The tools: JupyterLab (https://jupyter.org/) is a web-based interactive development environment for Jupyter notebooks, code, and data. Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data organizing and transformation, numerical simulation, statistical modeling, visualization, machine learning, and other similar types of uses. JupyterHub (https://github.com/jupyterhub/jupyterhub) is a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server. All these tools allow use of various coding languages; Python is the choice for ENGR 1330. Installing JupyterLab on your own computer is relatively straightforward if it is an Intel-based Linux, Macintosh, or Windows machine - simply use Anaconda (https://www.anaconda.com/) as the installer. Installing onto an ARM-based machine is more difficult, but possible (this notebook was created on a Raspberry Pi). With both Apple and Microsoft abandoning Intel you can expect Anaconda builds for aarch64 (ARM) in the future. This course: You will create and use Jupyter Notebooks that use the ipython kernel, the notebook files will look like filename.ipynb ; these are ASCII files that the JupyterLab interprets and runs. Python The programming language we will use is Python (actually iPython). Python is an example of a high-level language; other high-level languages include C, C++, PHP, FORTRAN, ADA, Pascal, Go, Java, etc (there are a lot). As you might infer from the name high-level language, there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous. First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with few or no modifications. Low-level programs can run on only one kind of computer and have to be rewritten to run on another. Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications, and for device drivers. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. Interpreted Program. Image from (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) A compiler reads the program and translates it completely before the program starts running. In this case, the high-level program is called the source code, and the translated program is called the object code or the executable. Once a program is compiled, you can execute it repeatedly without further translation. Compiled Prorgam. Image from: (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) Many modern languages use both processes. They are first compiled into a lower level language, called byte code, and then interpreted by a program called a virtual machine. Python uses both processes, but because of the way programmers interact with it, it is usually considered an interpreted language. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Formal languages are languages that are designed by people for specific applications. For example, the notation that mathematicians use is a formal language that is particularly good at denoting relationships among numbers and symbols. Chemists use a formal language to represent the chemical structure of molecules. Programming languages are formal languages that have been designed to express computations. Formal languages have strict rules about syntax. For example, 3+3=6 is a syntactically correct mathematical statement, but 3=+6& is not. Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. One of the problems with 3=+6& is that & is not a legal token in mathematics (at least as far as we know). The second type of syntax rule pertains to the structure of a statement\u2014 that is, the way the tokens are arranged. The statement 3=+6& is structurally illegal (in mathematics) because you don\u2019t place a plus sign immediately after an equal sign (of course we will in python!). When you read a sentence in English or a statement in a formal language, you have to figure out what the structure of the sentence is; This process is called parsing . For example, when you hear the sentence, \u201cThe other shoe fell\u201d, you understand that the other shoe is the subject and fell is the verb. Once you have parsed a sentence, you can figure out what it means, or the semantics of the sentence. Assuming that you know what a shoe is and what it means to fall, you will understand the general implication of this sentence. Good Resources: Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science) Programming as a problem solving process The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method (https://en.wikipedia.org/wiki/Scientific_method) is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Generate and evaluate potential solutions Refine and implement a solution Verify and test the solution. For actual computational methods the protocol becomes: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4). Example 1 Problem Solving Process Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background. CCMR Approach A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /home/sensei/1330-textbook-webroot/docs/lesson0 /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Artifical Neural Networks"},{"location":"lesson0/lesson0/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 13 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"lesson0/lesson0/#lesson-0-introduction-to-computational-thinking-with-data-science","text":"Computational thinking concepts Data science and practices JupyterLab (iPython) as a programming environment Programming as a problem solving process CCMR Approach","title":"Lesson 0 Introduction to Computational Thinking with Data Science:"},{"location":"lesson0/lesson0/#special-script-blocks","text":"In the lesson notebooks there will usually be two script blocks, identical to the ones below. The first block identifies the particular computer, the user, and the python kernel in use. The second block sets markdown tables to left edge when rendering. I usually put both blocks at the top of the notebook, just after some kind of title block, as done here.","title":"Special Script Blocks"},{"location":"lesson0/lesson0/#computational-thinking-concepts","text":"Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. Much of what follows is borrowed from (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2696102/). Computational thinking is taking an approach to solving problems, designing systems and understanding human behaviour that draws on concepts fundamental to computing (http://www.cs.cmu.edu/~15110-s13/Wing06-ct.pdf). Computational thinking is a kind of analytical thinking: It shares with mathematical thinking in the general ways in which we might approach solving a problem. It shares with engineering thinking in the general ways in which we might approach designing and evaluating a large, complex system that operates within the constraints of the real world. - It shares with scientific thinking in the general ways in which we might approach understanding computability, intelligence, the mind and human behaviour. The essence of computational thinking is abstraction and automation . In computing, we abstract notions beyond the physical dimensions of time and space. Our abstractions are extremely general because they are symbolic, where numeric abstractions are just a special case.","title":"Computational Thinking Concepts"},{"location":"lesson0/lesson0/#ct-foundations","text":"CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation)","title":"CT Foundations"},{"location":"lesson0/lesson0/#decomposition","text":"Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Examples include: - Writing a paper: - Introduction - Body - Conclusion Wide-viewed (Panorama) image: Taking multiple overlapped photos Stitch them Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution.","title":"Decomposition"},{"location":"lesson0/lesson0/#pattern-recognition","text":"Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it also will require assembly (system integration) to produce a desired solution.","title":"Pattern Recognition"},{"location":"lesson0/lesson0/#abstraction","text":"Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve. Books in an online bookstore Important NOT important title Cover color ISBN Author\u2019s hometown Authors ... ... ...","title":"Abstraction"},{"location":"lesson0/lesson0/#algorithms","text":"Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. Image from https://www.newyorker.com/magazine/2021/01/18/whats-wrong-with-the-way-we-work?utm_source=pocket-newtab An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation. Algorithms are unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, can incorporate random input.","title":"Algorithms"},{"location":"lesson0/lesson0/#system-integration-implementation","text":"System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand.","title":"System Integration (implementation)"},{"location":"lesson0/lesson0/#data-science-and-practice","text":"Data science is leveraging existing data sources, to create new ones as needed in order to extract meaningful information and actionable insights through business domain expertise, effective communication and results interpretation. Data science uses relevant statistical techniques, programming languages, software packages and libraries, and data infrastructure; The insights are used to drive business decisions and take actions intended to achieve business goals. Why is this important for engineers? Because engineering is a business! A list of typical skills (https://elitedatascience.com/data-science-resources): Foundational Skills Programming and Data Manipulation Statistics and Probability Technical Skills Data Collection SQL Data Visualization Applied Machine Learning Business Skills Communication Creativity and Innovation Operations and Strategy Business Analytics Supplementary Skills Natural Language Processing Recommendation Systems Time Series Analysis Practice Projects Competitions Problem Solving Challenges","title":"Data Science and Practice"},{"location":"lesson0/lesson0/#jupyterlab-ipython-environment","text":"","title":"JupyterLab (iPython) Environment"},{"location":"lesson0/lesson0/#the-tools","text":"JupyterLab (https://jupyter.org/) is a web-based interactive development environment for Jupyter notebooks, code, and data. Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data organizing and transformation, numerical simulation, statistical modeling, visualization, machine learning, and other similar types of uses. JupyterHub (https://github.com/jupyterhub/jupyterhub) is a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server. All these tools allow use of various coding languages; Python is the choice for ENGR 1330. Installing JupyterLab on your own computer is relatively straightforward if it is an Intel-based Linux, Macintosh, or Windows machine - simply use Anaconda (https://www.anaconda.com/) as the installer. Installing onto an ARM-based machine is more difficult, but possible (this notebook was created on a Raspberry Pi). With both Apple and Microsoft abandoning Intel you can expect Anaconda builds for aarch64 (ARM) in the future.","title":"The tools:"},{"location":"lesson0/lesson0/#this-course","text":"You will create and use Jupyter Notebooks that use the ipython kernel, the notebook files will look like filename.ipynb ; these are ASCII files that the JupyterLab interprets and runs.","title":"This course:"},{"location":"lesson0/lesson0/#python","text":"The programming language we will use is Python (actually iPython). Python is an example of a high-level language; other high-level languages include C, C++, PHP, FORTRAN, ADA, Pascal, Go, Java, etc (there are a lot). As you might infer from the name high-level language, there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous. First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with few or no modifications. Low-level programs can run on only one kind of computer and have to be rewritten to run on another. Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications, and for device drivers. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. Interpreted Program. Image from (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) A compiler reads the program and translates it completely before the program starts running. In this case, the high-level program is called the source code, and the translated program is called the object code or the executable. Once a program is compiled, you can execute it repeatedly without further translation. Compiled Prorgam. Image from: (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) Many modern languages use both processes. They are first compiled into a lower level language, called byte code, and then interpreted by a program called a virtual machine. Python uses both processes, but because of the way programmers interact with it, it is usually considered an interpreted language. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Formal languages are languages that are designed by people for specific applications. For example, the notation that mathematicians use is a formal language that is particularly good at denoting relationships among numbers and symbols. Chemists use a formal language to represent the chemical structure of molecules. Programming languages are formal languages that have been designed to express computations. Formal languages have strict rules about syntax. For example, 3+3=6 is a syntactically correct mathematical statement, but 3=+6& is not. Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. One of the problems with 3=+6& is that & is not a legal token in mathematics (at least as far as we know). The second type of syntax rule pertains to the structure of a statement\u2014 that is, the way the tokens are arranged. The statement 3=+6& is structurally illegal (in mathematics) because you don\u2019t place a plus sign immediately after an equal sign (of course we will in python!). When you read a sentence in English or a statement in a formal language, you have to figure out what the structure of the sentence is; This process is called parsing . For example, when you hear the sentence, \u201cThe other shoe fell\u201d, you understand that the other shoe is the subject and fell is the verb. Once you have parsed a sentence, you can figure out what it means, or the semantics of the sentence. Assuming that you know what a shoe is and what it means to fall, you will understand the general implication of this sentence.","title":"Python"},{"location":"lesson0/lesson0/#good-resources","text":"Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science)","title":"Good Resources:"},{"location":"lesson0/lesson0/#programming-as-a-problem-solving-process","text":"The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method (https://en.wikipedia.org/wiki/Scientific_method) is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Generate and evaluate potential solutions Refine and implement a solution Verify and test the solution. For actual computational methods the protocol becomes: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4).","title":"Programming as a problem solving process"},{"location":"lesson0/lesson0/#example-1-problem-solving-process","text":"Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background.","title":"Example 1 Problem Solving Process"},{"location":"lesson0/lesson0/#ccmr-approach","text":"A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity.","title":"CCMR Approach"},{"location":"lesson0/lesson0/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /home/sensei/1330-textbook-webroot/docs/lesson0 /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Readings"},{"location":"lesson1/lesson1/","text":"table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 19 January 2021 Lesson 1 Programming Fundamentals: iPython, tokens, and structure Data types (int, float, string, bool) Variables, operators, expressions, basic I/O String functions and operations Programming Fundamentals Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. CT is a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Recall the 5 fundamental CT concepts are: Decomposition: the process of taking a complex problem and breaking it into more manageable sub-problems. Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition: finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method ( automation ) for each occurrence of the pattern. Abstraction : Determine important characteristics of the problem and use these characteristics to create a representation of the problem. Algorithms : Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. System Integration: the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Programming is (generally) writing code in a specific programming language to address a certain problem. In the above list it is largely contained within the algorithms concept. iPython The programming language we will use is Python (actually iPython). Python is an example of a high-level language; there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous: First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with just a few modifications. Low-level programs can run on only one kind of computer (chipset-specific for sure, in some cases hardware specific) and have to be rewritten to run on other processors. (e.g. x86-64 vs. arm7 vs. aarch64 vs. PowerPC ...) Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. The second type of syntax rule pertains to the structure of a statement specifically in the way the tokens are arranged. Tokens and Structure Consider the relativistic equation relating energy, mass, and the speed of light e = m \\cdot c^2 In this equation the tokens are e , m , c , = , \\cdot , and the structure is parsed from left to right as into the token named e place the result of the product of the contents of the tokens m and c^2 . Given that the speed of light is some universal constant, the only things that can change are the contents of m and the resulting change in e . In the above discourse, the tokens e , m , c are names for things that can have values -- we will call these variables (or constants as appropriate). The tokens = , \\cdot , and ~^2 are symbols for various arithmetic operations -- we will call these operators. The structure of the equation is specific -- we will call it a statement. When we attempt to write and execute python scripts - we will make various mistakes; these will generate warnings and errors, which we will repair to make a working program. Consider our equation: #clear all variables# Example Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-4-1c1f1fa5363a> in <module> 1 #clear all variables# Example ----> 2 Energy = Mass * SpeedOfLight**2 NameError: name 'Mass' is not defined Notice how the interpreter tells us that Mass is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-5-a4a52966e6df> in <module> 1 # Example 2 Mass = 1000000 ----> 3 Energy = Mass * SpeedOfLight**2 NameError: name 'SpeedOfLight' is not defined Notice how the interpreter now tells us that SpeedOfLight is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 Now the script ran without any reported errors, but we have not instructed the program on how to produce output. To keep the example simple we will just add a generic print statement. # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 print(\"Energy is:\", Energy, \"Newton meters\") Energy is: 89875517873681764000000 Newton meters Now lets examine our program. Identify the tokens that have values, Identify the tokens that are symbols of operations, identify the structure. Variables Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float). Naming Rules Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print , input , if , while , and for . There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables. Operators The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10. Arithmetic Operators In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x y Raises value in x by value in y. ( e.g. x y) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0 Data Type In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary Integer Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309 Real (Float) A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427 String(Alphanumeric) A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting. Changing Types A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens! 234 876.543 What is your name? Integer as float 234.0 Float as integer 876 Integer as string 234 Integer as hexadecimal 0xea Integer Type <class 'int'> Expressions Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15 Summary So far consider our story - a tool to help with problem solving is CT leading to an algorithm. The tool to implement the algorithm is the program and in our case JupyterLab running iPython interpreter for us. As a formal language we introduced: - tokens - structure From these two constructs we further introduced variables (a kind of token), data types (an abstraction, and arguably a decomposition), and expressions (a structure). We created simple scripts (with errors), examined the errors, corrected our script, and eventually got an answer. So we are well on our way in CT as it applies in Engineering. Programming as a problem solving process Recall the entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). Recall our suggested problem solving protocol: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Refine general solution for deployment (frequent use) Another protocol with the same goal is at https://3.137.111.182/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint/HowToBuildAProgram.html Notice the similarity! Example 2 Problem Solving Process Consider an engineering material problem where we wish to classify whether a material in loaded in the elastic or inelastic region as determined the stress (solid pressure) in a rod for some applied load. The yield stress is the classifier, and once the material yields (begins to fail) it will not carry any additional load (until ultimate failure, when it carries no load). Step 1. Compute the material stress under an applied load; determine if value exceedes yield stress, and report the loading condition Step 2. - Inputs: applied load, cross sectional area, yield stress - Governing equation: \\sigma = \\frac{P}{A} when \\frac{P}{A} is less than the yield stress, and is equal to the yield stress otherwise. - Outputs: The material stress \\sigma , and the classification elastic or inelastic. Step 3. Work a sample problem by-hand for testing the general solution. Assuming the yield stress is 1 million psi (units matter in an actual problem - kind of glossed over here) Applied Load (lbf) Cross Section Area (sq.in.) Stress (psi) Classification 10,000 1.0 10,000 Elastic 10,000 0.1 100,000 Elastic 100,000 0.1 1,000,000 Inelastic The stress requires us to read in the load value, read in the cross sectional area, divide the load by the area, and compare the result to the yield stress. If it exceeds the yield stress, then the actual stress is the yield stress, and the loading is inelastic, otherwise elastic \\sigma = \\frac{P}{A} If \\sigma >= \\text{Yield Stress Report Inelastic} Step 4. Develop a general solution (code) In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. We have not yet learned prompts to get input we simply direct assign values as below (and the conditional execution is the subject of a later lesson) In a simple JupyterLab script # Example 2 Problem Solving Process yield_stress = 1e6 applied_load = 1e5 cross_section = 0.1 computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",computed_stress) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",yield_stress) Inelastic Region: Stress = 1000000.0 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the inputs by user entry,and tidy the output by rounding to only two decimal places. A little CCMR from https://www.geeksforgeeks.org/taking-input-in-python/ gives us a way to deal with the inputs and typecasting. Some more CCMR from https://www.programiz.com/python-programming/methods/built-in/round gets us rounded out! # Example 2 Problem Solving Process yield_stress = float(input('Yield Stress (psi)')) applied_load = float(input('Applied Load (lbf)')) cross_section = float(input('Cross Section Area (sq.in.)')) computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",round(computed_stress,2)) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",round(yield_stress,2)) Yield Stress (psi) 1000000 Applied Load (lbf) 100000 Cross Section Area (sq.in.) 1 Elastic Region: Stress = 100000.0 So the simple task of computing the stress, is a bit more complex when decomposed, that it first appears, but illustrates a five step process (with a refinement step). CCMR Approach A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 3 https://www.inferentialthinking.com/chapters/03/programming-in-python.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python in One Day and Learn It Well. Python for Beginners with Hands-on Project. (Learn Coding Fast with Hands-On Project Book -- Kindle Edition by LCF Publishing (Author), Jamie Chan https://www.amazon.com/Python-2nd-Beginners-Hands-Project-ebook/dp/B071Z2Q6TQ/ref=sr_1_3?dchild=1&keywords=learn+python+in+a+day&qid=1611108340&sr=8-3 Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science) # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Lesson1"},{"location":"lesson1/lesson1/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 19 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"lesson1/lesson1/#lesson-1-programming-fundamentals","text":"iPython, tokens, and structure Data types (int, float, string, bool) Variables, operators, expressions, basic I/O String functions and operations","title":"Lesson 1 Programming Fundamentals:"},{"location":"lesson1/lesson1/#programming-fundamentals","text":"Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. CT is a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Recall the 5 fundamental CT concepts are: Decomposition: the process of taking a complex problem and breaking it into more manageable sub-problems. Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition: finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method ( automation ) for each occurrence of the pattern. Abstraction : Determine important characteristics of the problem and use these characteristics to create a representation of the problem. Algorithms : Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. System Integration: the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Programming is (generally) writing code in a specific programming language to address a certain problem. In the above list it is largely contained within the algorithms concept.","title":"Programming Fundamentals"},{"location":"lesson1/lesson1/#ipython","text":"The programming language we will use is Python (actually iPython). Python is an example of a high-level language; there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous: First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with just a few modifications. Low-level programs can run on only one kind of computer (chipset-specific for sure, in some cases hardware specific) and have to be rewritten to run on other processors. (e.g. x86-64 vs. arm7 vs. aarch64 vs. PowerPC ...) Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. The second type of syntax rule pertains to the structure of a statement specifically in the way the tokens are arranged.","title":"iPython"},{"location":"lesson1/lesson1/#tokens-and-structure","text":"Consider the relativistic equation relating energy, mass, and the speed of light e = m \\cdot c^2 In this equation the tokens are e , m , c , = , \\cdot , and the structure is parsed from left to right as into the token named e place the result of the product of the contents of the tokens m and c^2 . Given that the speed of light is some universal constant, the only things that can change are the contents of m and the resulting change in e . In the above discourse, the tokens e , m , c are names for things that can have values -- we will call these variables (or constants as appropriate). The tokens = , \\cdot , and ~^2 are symbols for various arithmetic operations -- we will call these operators. The structure of the equation is specific -- we will call it a statement. When we attempt to write and execute python scripts - we will make various mistakes; these will generate warnings and errors, which we will repair to make a working program. Consider our equation: #clear all variables# Example Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-4-1c1f1fa5363a> in <module> 1 #clear all variables# Example ----> 2 Energy = Mass * SpeedOfLight**2 NameError: name 'Mass' is not defined Notice how the interpreter tells us that Mass is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-5-a4a52966e6df> in <module> 1 # Example 2 Mass = 1000000 ----> 3 Energy = Mass * SpeedOfLight**2 NameError: name 'SpeedOfLight' is not defined Notice how the interpreter now tells us that SpeedOfLight is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 Now the script ran without any reported errors, but we have not instructed the program on how to produce output. To keep the example simple we will just add a generic print statement. # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 print(\"Energy is:\", Energy, \"Newton meters\") Energy is: 89875517873681764000000 Newton meters Now lets examine our program. Identify the tokens that have values, Identify the tokens that are symbols of operations, identify the structure.","title":"Tokens and Structure"},{"location":"lesson1/lesson1/#variables","text":"Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float).","title":"Variables"},{"location":"lesson1/lesson1/#naming-rules","text":"Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print , input , if , while , and for . There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables.","title":"Naming Rules"},{"location":"lesson1/lesson1/#operators","text":"The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10.","title":"Operators"},{"location":"lesson1/lesson1/#arithmetic-operators","text":"In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x y Raises value in x by value in y. ( e.g. x y) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0","title":"Arithmetic Operators"},{"location":"lesson1/lesson1/#data-type","text":"In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary","title":"Data Type"},{"location":"lesson1/lesson1/#integer","text":"Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309","title":"Integer"},{"location":"lesson1/lesson1/#real-float","text":"A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427","title":"Real (Float)"},{"location":"lesson1/lesson1/#stringalphanumeric","text":"A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting.","title":"String(Alphanumeric)"},{"location":"lesson1/lesson1/#changing-types","text":"A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens! 234 876.543 What is your name? Integer as float 234.0 Float as integer 876 Integer as string 234 Integer as hexadecimal 0xea Integer Type <class 'int'>","title":"Changing Types"},{"location":"lesson1/lesson1/#expressions","text":"Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15","title":"Expressions"},{"location":"lesson1/lesson1/#summary","text":"So far consider our story - a tool to help with problem solving is CT leading to an algorithm. The tool to implement the algorithm is the program and in our case JupyterLab running iPython interpreter for us. As a formal language we introduced: - tokens - structure From these two constructs we further introduced variables (a kind of token), data types (an abstraction, and arguably a decomposition), and expressions (a structure). We created simple scripts (with errors), examined the errors, corrected our script, and eventually got an answer. So we are well on our way in CT as it applies in Engineering.","title":"Summary"},{"location":"lesson1/lesson1/#programming-as-a-problem-solving-process","text":"Recall the entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). Recall our suggested problem solving protocol: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Refine general solution for deployment (frequent use) Another protocol with the same goal is at https://3.137.111.182/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint/HowToBuildAProgram.html Notice the similarity!","title":"Programming as a problem solving process"},{"location":"lesson1/lesson1/#example-2-problem-solving-process","text":"Consider an engineering material problem where we wish to classify whether a material in loaded in the elastic or inelastic region as determined the stress (solid pressure) in a rod for some applied load. The yield stress is the classifier, and once the material yields (begins to fail) it will not carry any additional load (until ultimate failure, when it carries no load). Step 1. Compute the material stress under an applied load; determine if value exceedes yield stress, and report the loading condition Step 2. - Inputs: applied load, cross sectional area, yield stress - Governing equation: \\sigma = \\frac{P}{A} when \\frac{P}{A} is less than the yield stress, and is equal to the yield stress otherwise. - Outputs: The material stress \\sigma , and the classification elastic or inelastic. Step 3. Work a sample problem by-hand for testing the general solution. Assuming the yield stress is 1 million psi (units matter in an actual problem - kind of glossed over here) Applied Load (lbf) Cross Section Area (sq.in.) Stress (psi) Classification 10,000 1.0 10,000 Elastic 10,000 0.1 100,000 Elastic 100,000 0.1 1,000,000 Inelastic The stress requires us to read in the load value, read in the cross sectional area, divide the load by the area, and compare the result to the yield stress. If it exceeds the yield stress, then the actual stress is the yield stress, and the loading is inelastic, otherwise elastic \\sigma = \\frac{P}{A} If \\sigma >= \\text{Yield Stress Report Inelastic} Step 4. Develop a general solution (code) In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. We have not yet learned prompts to get input we simply direct assign values as below (and the conditional execution is the subject of a later lesson) In a simple JupyterLab script # Example 2 Problem Solving Process yield_stress = 1e6 applied_load = 1e5 cross_section = 0.1 computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",computed_stress) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",yield_stress) Inelastic Region: Stress = 1000000.0 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the inputs by user entry,and tidy the output by rounding to only two decimal places. A little CCMR from https://www.geeksforgeeks.org/taking-input-in-python/ gives us a way to deal with the inputs and typecasting. Some more CCMR from https://www.programiz.com/python-programming/methods/built-in/round gets us rounded out! # Example 2 Problem Solving Process yield_stress = float(input('Yield Stress (psi)')) applied_load = float(input('Applied Load (lbf)')) cross_section = float(input('Cross Section Area (sq.in.)')) computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",round(computed_stress,2)) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",round(yield_stress,2)) Yield Stress (psi) 1000000 Applied Load (lbf) 100000 Cross Section Area (sq.in.) 1 Elastic Region: Stress = 100000.0 So the simple task of computing the stress, is a bit more complex when decomposed, that it first appears, but illustrates a five step process (with a refinement step).","title":"Example 2 Problem Solving Process"},{"location":"lesson1/lesson1/#ccmr-approach","text":"A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity.","title":"CCMR Approach"},{"location":"lesson1/lesson1/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 3 https://www.inferentialthinking.com/chapters/03/programming-in-python.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python in One Day and Learn It Well. Python for Beginners with Hands-on Project. (Learn Coding Fast with Hands-On Project Book -- Kindle Edition by LCF Publishing (Author), Jamie Chan https://www.amazon.com/Python-2nd-Beginners-Hands-Project-ebook/dp/B071Z2Q6TQ/ref=sr_1_3?dchild=1&keywords=learn+python+in+a+day&qid=1611108340&sr=8-3 Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science) # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Readings"},{"location":"lesson2/lesson2/","text":"%%html <!-- Script Block to set tables to left alignment --> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 25 January 2021 Lesson 2 Data Structures and Conditional Statements: Data structures; lists, arrays, tuples, sets, dictionaries Name, index, contents; keys Conditional structures; logical compares, block and in-line if Objectives 1) Develop awareness of data structures available in Python to store and manipulate data - Implement arrays (lists), dictionaries, and tuples - Address contents of lists , dictionaries, and tuples 2) Develop awareness of decision making in Python - Implement decision making in Python using using if-then ... conditional statements Data Structures and Conditional Statements Computational thinking (CT) concepts involved are: Decomposition : Data interpretation, manipulation, and analysis of NumPy arrays Abstraction : Data structures; Arrays, lists, tuples, sets, and dictionaries Algorithms : Conditional statements What is a data structure? Data Structures are a specialized means of organizing and storing data in computers in such a way that we can perform operations on the stored data more efficiently. In our iPython world the structures are illustrated in the figure below Lists A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\\\ x_1= 11 \\\\ x_2= 5 \\\\ x_3= 9 \\\\ x_4= 13 \\\\ \\dots \\\\ x_N= 223 \\\\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO . A lot of other languages start at ONE. It's just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9. Arrays Arrays are lists that are used to store only elements of a specific data type - Ordered: Elements in an array can be indexed - Mutable: Elements in an array can be altered Data type that an array must hold is specified using the type code when it is created - \u2018f\u2019 for float - \u2018d\u2019 for double - \u2018i\u2019 for signed int - \u2018I\u2019 for unsigned int More types are listed below Type Code C Data Type Python Data Type Minimum Size in Bytes 'b' signed char int 1 'B' unsigned char int 1 'h' signed short int 2 'H' unsigned short int 2 'i' signed int int 2 'I' unsigned int int 2 'l' signed long int 4 'L' unsigned long int 4 'q' signed long long int 8 'Q' unsigned long long int 8 'f' float float 4 'd' double float 8 To use arrays, a library named \u2018array\u2019 must be imported import array Creating an array that contains signed integer numbers myarray = array.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 import array as arr #import using an alias so the calls dont look so funny myarray = arr.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 Lists: Can store elements of different data types; like arrays they are (arrays are lists, but lists are not quite arrays!) - Ordered: Elements in a list can be indexed - Mutable: Elements in a list can be altered - Mathematical operations must be applied to each element of the list Tuple - A special list A tuple is a special kind of list where the values cannot be changed after the list is created. Such a property is called immutable It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") Tuples are often created as output from packages and functions. Dictionary - A special list A dictionary is a special kind of list where the items are related data PAIRS . It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Dictionary properties - Unordered: Elements in a dictionary cannot be - Mutable elements: Elements in a dictionary can be altered - Immutable keys: Keys in a dictionary cannot be altered Sets - A special list Sets: Are used to store elements of different data types - Unordered: Elements in a set cannot be indexed - Mutable: Elements in a set can be altered - Non-repetition: Elements in a set are unique Elements of a set are enclosed in curly brackets { } - Creating sets that contains different data types - Sets cannot be nested What's the difference between a set and dictionary? From https://stackoverflow.com/questions/34370599/difference-between-dict-and-set-python \"Well, a set is like a dict with keys but no values, and they're both implemented using a hash table. But yes, it's a little annoying that the {} notation denotes an empty dict rather than an empty set , but that's a historical artifact.\" Conditional Statements Decision making via conditional statements is an important step in algorithm design; they control the flow of execution of a program. Conditional statements in Python include: if statement if....else statements if....elif....else statements Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Expressed in a flowchart a block if statement looks like: As psuedo code: if(condition is true): do stuff Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, but more recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs. Comparison The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal. Block if statement The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter. Inline if statement An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 Subpart 3 https://www.inferentialthinking.com/chapters/04/3/Comparison.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson03/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Lesson2"},{"location":"lesson2/lesson2/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 25 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"lesson2/lesson2/#lesson-2-data-structures-and-conditional-statements","text":"Data structures; lists, arrays, tuples, sets, dictionaries Name, index, contents; keys Conditional structures; logical compares, block and in-line if","title":"Lesson 2 Data Structures and Conditional Statements:"},{"location":"lesson2/lesson2/#objectives","text":"1) Develop awareness of data structures available in Python to store and manipulate data - Implement arrays (lists), dictionaries, and tuples - Address contents of lists , dictionaries, and tuples 2) Develop awareness of decision making in Python - Implement decision making in Python using using if-then ... conditional statements","title":"Objectives"},{"location":"lesson2/lesson2/#data-structures-and-conditional-statements","text":"Computational thinking (CT) concepts involved are: Decomposition : Data interpretation, manipulation, and analysis of NumPy arrays Abstraction : Data structures; Arrays, lists, tuples, sets, and dictionaries Algorithms : Conditional statements","title":"Data Structures and Conditional Statements"},{"location":"lesson2/lesson2/#what-is-a-data-structure","text":"Data Structures are a specialized means of organizing and storing data in computers in such a way that we can perform operations on the stored data more efficiently. In our iPython world the structures are illustrated in the figure below","title":"What is a data structure?"},{"location":"lesson2/lesson2/#lists","text":"A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\\\ x_1= 11 \\\\ x_2= 5 \\\\ x_3= 9 \\\\ x_4= 13 \\\\ \\dots \\\\ x_N= 223 \\\\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO . A lot of other languages start at ONE. It's just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9.","title":"Lists"},{"location":"lesson2/lesson2/#arrays","text":"Arrays are lists that are used to store only elements of a specific data type - Ordered: Elements in an array can be indexed - Mutable: Elements in an array can be altered Data type that an array must hold is specified using the type code when it is created - \u2018f\u2019 for float - \u2018d\u2019 for double - \u2018i\u2019 for signed int - \u2018I\u2019 for unsigned int More types are listed below Type Code C Data Type Python Data Type Minimum Size in Bytes 'b' signed char int 1 'B' unsigned char int 1 'h' signed short int 2 'H' unsigned short int 2 'i' signed int int 2 'I' unsigned int int 2 'l' signed long int 4 'L' unsigned long int 4 'q' signed long long int 8 'Q' unsigned long long int 8 'f' float float 4 'd' double float 8 To use arrays, a library named \u2018array\u2019 must be imported import array Creating an array that contains signed integer numbers myarray = array.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 import array as arr #import using an alias so the calls dont look so funny myarray = arr.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 Lists: Can store elements of different data types; like arrays they are (arrays are lists, but lists are not quite arrays!) - Ordered: Elements in a list can be indexed - Mutable: Elements in a list can be altered - Mathematical operations must be applied to each element of the list","title":"Arrays"},{"location":"lesson2/lesson2/#tuple-a-special-list","text":"A tuple is a special kind of list where the values cannot be changed after the list is created. Such a property is called immutable It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") Tuples are often created as output from packages and functions.","title":"Tuple - A special list"},{"location":"lesson2/lesson2/#dictionary-a-special-list","text":"A dictionary is a special kind of list where the items are related data PAIRS . It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Dictionary properties - Unordered: Elements in a dictionary cannot be - Mutable elements: Elements in a dictionary can be altered - Immutable keys: Keys in a dictionary cannot be altered","title":"Dictionary - A special list"},{"location":"lesson2/lesson2/#sets-a-special-list","text":"Sets: Are used to store elements of different data types - Unordered: Elements in a set cannot be indexed - Mutable: Elements in a set can be altered - Non-repetition: Elements in a set are unique Elements of a set are enclosed in curly brackets { } - Creating sets that contains different data types - Sets cannot be nested","title":"Sets - A special list"},{"location":"lesson2/lesson2/#whats-the-difference-between-a-set-and-dictionary","text":"From https://stackoverflow.com/questions/34370599/difference-between-dict-and-set-python \"Well, a set is like a dict with keys but no values, and they're both implemented using a hash table. But yes, it's a little annoying that the {} notation denotes an empty dict rather than an empty set , but that's a historical artifact.\"","title":"What's the difference between a set and dictionary?"},{"location":"lesson2/lesson2/#conditional-statements","text":"Decision making via conditional statements is an important step in algorithm design; they control the flow of execution of a program. Conditional statements in Python include: if statement if....else statements if....elif....else statements Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Expressed in a flowchart a block if statement looks like: As psuedo code: if(condition is true): do stuff Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, but more recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs.","title":"Conditional Statements"},{"location":"lesson2/lesson2/#comparison","text":"The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal.","title":"Comparison"},{"location":"lesson2/lesson2/#block-if-statement","text":"The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter.","title":"Block if statement"},{"location":"lesson2/lesson2/#inline-if-statement","text":"An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops.","title":"Inline if statement"},{"location":"lesson2/lesson2/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 Subpart 3 https://www.inferentialthinking.com/chapters/04/3/Comparison.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson03/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Readings"},{"location":"lesson6/lesson6/","text":"ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 31 January 2021 Lesson 6 Classes, Objects, and File Handling: Classes and Objects Files Create (new), Open (existing) Read from .... Write to ... Close (save) Delete Special Script Blocks %%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} Objectives To understand the use of classes and objects to do effective coding in Python To understand the basic idea of how to manipulate the data in a file using file handling options in Python Classes and Objects In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated. When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class. Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state. Class definitions, like function definitions (def statements) must be executed before they have any effect. (You could conceivably place a class definition in a branch of an if statement, or inside a function.) In practice, the statements inside a class definition will usually be function definitions, but other statements are allowed, and sometimes useful \u2014 we\u2019ll come back to this later. The function definitions inside a class normally have a peculiar form of argument list, dictated by the calling conventions for methods \u2014 again, this is explained later. When a class definition is entered, a new namespace is created, and used as the local scope \u2014 thus, all assignments to local variables go into this new namespace. In particular, function definitions bind the name of the new function here. When a class definition is left normally (via the end), a class object is created. This is basically a wrapper around the contents of the namespace created by the class definition; we\u2019ll learn more about class objects in the next section. The original local scope (the one in effect just before the class definition was entered) is reinstated, and the class object is bound here to the class name given in the class definition header (ClassName in the example). What is an object? An object is simply a collection of data (variables) and methods (functions) that act on those data. Similarly, a class is a blueprint for that object. We can think of class as a sketch (prototype) of a house. It contains all the details about the floors, doors, windows etc. Based on these descriptions we build the house. House is the object. As many houses can be made from a house's blueprint, we can create many objects from a class. An object is also called an instance of a class and the process of creating this object is called instantiation Learn more at 1. https://docs.python.org/3/tutorial/classes.html 2. https://en.wikipedia.org/wiki/Class_(computer_programming) An Example: Write a class named ' Tax ' to calculate the state tax (in dollars) of Employees at Texas Tech University based on their annual salary. The state tax is 16% if the annual salary is below 80,000 dollars and 22% if the salary is more than 80,000 dollars. Employee Annual salary (dollars) Bob 150,000 Mary 78,000 John 55,000 Danny 175,000 Notes: Use docstrings to describe the purpose of the class. Use if....else conditional statements within the method of the class to choose the relevant tax % based on the annual salary. Create an object for employee and display the output as shown below. Bob's tax amount (in dollars): AMOUNT Mary's tax amount (in dollars): AMOUNT John's tax amount (in dollars): AMOUNT Danny's tax amount (in dollars): AMOUNT class Tax: \"\"\"This class calculates the tax amount based on the annual salary and the state tax %\"\"\" def __init__(self, salary): # here is the instantiation constructor self.salary = salary def taxamount(self): # here is a method (function) that can operate on the class once created if self.salary < 80000: return self.salary*(16/100) else: return self.salary*(22/100) bob = Tax(150000) dir(bob) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] bob = Tax(150000) # objects constructed using Tax class mary = Tax(78000) john = Tax(55000) danny = Tax(175000) dir(Tax) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'taxamount'] dir(mary) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] print(\"bobz salary \", bob.salary ) print(\"Bob's tax amount (in dollars):\", bob.taxamount() ) print(\"Mary's tax amount (in dollars):\", mary.taxamount()) print(\"John's tax amount (in dollars):\", john.taxamount()) print(\"Danny's tax amount (in dollars):\", danny.taxamount()) bobz salary 150000 Bob's tax amount (in dollars): 33000.0 Mary's tax amount (in dollars): 12480.0 John's tax amount (in dollars): 8800.0 Danny's tax amount (in dollars): 38500.0 Numbers, strings, lists, and dictionaries are all objects that are instances of a parent class print(type(0)) <class 'int'> print(type(\"\")) <class 'str'> print(type([1, 2, 3, 4])) <class 'list'> To get more information about the built-in classes and objects, use dir( ) and help( ) functions print(dir(int)) print(help(\"\")) User-defined classes: Defining docstrings class Dog: \"\"\"This class enables the dog to say its name and age in dog years\"\"\" def __init__(self, name, years): \"\"\"This function contains all the necessary attributes\"\"\" self.name = name self.years = years self.dog_age = years*9 def sound(self): \"\"\"This function enables the dog to speak\"\"\" print(\"woof! I am {} and I am {} dog years old! woof!\".format(self.name, self.dog_age)) fudge = Dog(\"Fudge\", 2) maple = Dog(\"Maple\", 1.5) fudge.sound() maple.sound() woof! I am Fudge and I am 18 dog years old! woof! woof! I am Maple and I am 13.5 dog years old! woof! help(Dog) Help on class Dog in module __main__: class Dog(builtins.object) | Dog(name, years) | | This class enables the dog to say its name and age in dog years | | Methods defined here: | | __init__(self, name, years) | This function contains all the necessary attributes | | sound(self) | This function enables the dog to speak | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) Files and Filesystems Background A computer file is a computer resource for recording data discretely (not in the secretive context, but specifically somewhere on a piece of hardware) in a computer storage device. Just as words can be written to paper, so can information be written to a computer file. Files can be edited and transferred through the internet on that particular computer system. There are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once. By using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times. Typically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access. File system In computing, a file system or filesystem, controls how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stops and the next begins. By separating the data into pieces and giving each piece a name, the data is isolated and identified. Taking its name from the way paper-based data management system is named, each group of data is called a \u201cfile\u201d. The structure and logic rules used to manage the groups of data and their names is called a \u201cfile system\u201d. Path A path, the general form of the name of a file or directory, specifies a unique location in a file system. A path points to a file system location by following the directory tree hierarchy expressed in a string of characters in which path components, separated by a delimiting character, represent each directory. The delimiting character is most commonly the slash (\u201d/\u201d), the backslash character (\u201d\\\u201d), or colon (\u201d:\u201d), though some operating systems may use a different delimiter. Paths are used extensively in computer science to represent the directory/file relationships common in modern operating systems, and are essential in the construction of Uniform Resource Locators (URLs). Resources can be represented by either absolute or relative paths. As an example consider the following two files: /Users/theodore/MyGit/@atomickitty/hurri-sensors/.git/Guest.conf /etc/apache2/users/Guest.conf They both have the same file name, but are located on different paths. Failure to provide the path when addressing the file can be a problem. Another way to interpret is that the two unique files actually have different names, and only part of those names is common (Guest.conf) The two names above (including the path) are called fully qualified filenames (or absolute names), a relative path (usually relative to the file or program of interest depends on where in the directory structure the file lives. If we are currently in the .git directory (the first file) the path to the file is just the filename. We have experienced path issues with dependencies on .png files - in general your JupyterLab notebooks on CoCalc can only look at the local directory which is why we have to copy files into the directory for things to work. File Types Text Files. Text files are regular files that contain information readable by the user. This information is stored in ASCII. You can display and print these files. The lines of a text file must not contain NULL characters, and none can exceed a prescribed (by architecture) length, including the new-line character. The term text file does not prevent the inclusion of control or other nonprintable characters (other than NUL). Therefore, standard utilities that list text files as inputs or outputs are either able to process the special characters gracefully or they explicitly describe their limitations within their individual sections. Binary Files. Binary files are regular files that contain information readable by the computer. Binary files may be executable files that instruct the system to accomplish a job. Commands and programs are stored in executable, binary files. Special compiling programs translate ASCII text into binary code. The only difference between text and binary files is that text files have lines of less than some length, with no NULL characters, each terminated by a new-line character. Directory Files. Directory files contain information the system needs to access all types of files, but they do not contain the actual file data. As a result, directories occupy less space than a regular file and give the file system structure flexibility and depth. Each directory entry represents either a file or a subdirectory. Each entry contains the name of the file and the file's index node reference number (i-node). The i-node points to the unique index node assigned to the file. The i-node describes the location of the data associated with the file. Directories are created and controlled by a separate set of commands. File Manipulation For this lesson we examine just a handfull of file manipulations which are quite useful. Files can be \"created\",\"read\",\"updated\", or \"deleted\" (CRUD). Example: Create a file, write to it. Below is an example of creating a file that does not yet exist. The script is a bit pendandic on purpose. First will use some system commands to view the contents of the local directory import sys # on a Mac/Linux ! rm -rf myfirstfile.txt # delete file if it exists ! pwd # list name of working directory, note it includes path, so it is an absolute path # on Winderz #! del myfirstfile.txt # delete file if it exists #! %pwd # list name of working directory, note it includes path, so it is an absolute path /home/sensei/1330-textbook-webroot/docs/lesson6 # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 752 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt # create file example externalfile = open(\"myfirstfile.txt\",'w') # create connection to file, set to write (w), file does not need to exist mymessage = 'message in a bottle' #some object to write, in this case a string externalfile.write(mymessage)# write the contents of mymessage to the file externalfile.close() # close the file connection At this point our new file should exist, lets list the directory and see if that is so # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 756 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 19 Feb 17 17:35 myfirstfile.txt -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt Sure enough, its there, we will use a bash command cat to look at the contents of the file. ! cat myfirstfile.txt # Mac/Linux # ! type myfirstfile.txt # Winderz message in a bottle Example: Read from an existing file. We will continue using the file we just made, and read from it the example is below # read file example externalfile = open(\"myfirstfile.txt\",'r') # create connection to file, set to read (r), file must exist silly_string = externalfile.read() # read the contents externalfile.close() # close the file connection print(silly_string) message in a bottle Example: Update a file. This example continues with our same file, but we will now add contents without destroying existing contents. The keyword is append externalfile = open(\"myfirstfile.txt\",'a') # create connection to file, set to append (a), file does not need to exist externalfile.write('\\n') # adds a newline character what_to_add = 'I love rock-and-roll, put another dime in the jukebox baby ... \\n' externalfile.write(what_to_add) # add a string including the linefeed what_to_add = '... the waiting is the hardest part \\n' externalfile.write(what_to_add) # add a string including the linefeed mylist = [1,2,3,4,5] # a list of numbers what_to_add = ','.join(map(repr, mylist)) + \"\\n\" # one way to write the list externalfile.write(what_to_add) what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" # another way to write the list externalfile.write(what_to_add) externalfile.close() As before we can examine the contents using a shell command sent from the notebook. ! cat myfirstfile.txt # ! type myfirstfile.txt # Winderz message in a bottle I love rock-and-roll, put another dime in the jukebox baby ... ... the waiting is the hardest part 1,2,3,4,5 1,2,3,4,5 Example: Delete a file Delete can be done by a system call as we did above to clear the local directory In a JupyterLab notebook, we can either use import sys ! rm -rf myfirstfile.txt # delete file if it exists or import os os.remove(\"myfirstfile.txt\") they both have same effect, both equally dangerous to your filesystem. Learn more about CRUD with text files at https://www.guru99.com/reading-and-writing-files-in-python.html Learn more about file delete at https://www.dummies.com/programming/python/how-to-delete-a-file-in-python/ import os file2kill = \"myfirstfile.txt\" try: os.remove(file2kill) # file must exist or will generate an exception except: pass # example of using pass to improve readability print(file2kill, \" missing or deleted !\") myfirstfile.txt missing or deleted ! A little discussion on the part where we wrote numbers what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" Here are descriptions of the two functions map and repr map(function, iterable, ...) Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list. repr(object) Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse quotes). It is sometimes useful to be able to access this operation as an ordinary function. For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() , otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. A class can control what this function returns for its instances by defining a repr() method. What they do in this script is important. The statement: what_to_add = \u2019,\u2019.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" is building a string that will be comprised of elements of mylist[0:len(mylist)]. The repr() function gets these elements as they are represented in the computer, the delimiter a comma is added using the join method in Python, and because everything is now a string the ... + \"\\n\" puts a linefeed character at the end of the string so the output will start a new line the next time something is written. Example create a text file, name it \"MyFavoriteQuotation\" . Write your favorite quotation in the file. Read the file. Add this string to it in a new line : \"And that's something I wish I had said...\" Show the final outcome. # create the \"My Favorite Quotation\" file: externalfile = open(\"MyFavoriteQuotation.txt\",'w') # create connection to file, set to write (w) myquotation = 'The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.' #My choice: quotation from Pulp Fiction externalfile.write(myquotation)# write the contents of mymessage to the file externalfile.close() # close the file connection #Let's read the file ! cat MyFavoriteQuotation.txt # Let's add the string externalfile = open(\"MyFavoriteQuotation.txt\",'a') #create connection to file, set to append (a) externalfile.write('\\n') # adds a newline character what_to_add = \"And that's something I wish I had said ... \\n\" externalfile.write(what_to_add) externalfile.close() #Let's read the file one last time ! cat MyFavoriteQuotation.txt # ! type MyFavoriteQuotation # Winderz The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you. And that's something I wish I had said ... References Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty compthink /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"lesson6/lesson6/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 31 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"lesson6/lesson6/#lesson-6-classes-objects-and-file-handling","text":"Classes and Objects Files Create (new), Open (existing) Read from .... Write to ... Close (save) Delete","title":"Lesson 6 Classes, Objects, and File Handling:"},{"location":"lesson6/lesson6/#special-script-blocks","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;}","title":"Special Script Blocks"},{"location":"lesson6/lesson6/#objectives","text":"To understand the use of classes and objects to do effective coding in Python To understand the basic idea of how to manipulate the data in a file using file handling options in Python","title":"Objectives"},{"location":"lesson6/lesson6/#classes-and-objects","text":"In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated. When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class. Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state. Class definitions, like function definitions (def statements) must be executed before they have any effect. (You could conceivably place a class definition in a branch of an if statement, or inside a function.) In practice, the statements inside a class definition will usually be function definitions, but other statements are allowed, and sometimes useful \u2014 we\u2019ll come back to this later. The function definitions inside a class normally have a peculiar form of argument list, dictated by the calling conventions for methods \u2014 again, this is explained later. When a class definition is entered, a new namespace is created, and used as the local scope \u2014 thus, all assignments to local variables go into this new namespace. In particular, function definitions bind the name of the new function here. When a class definition is left normally (via the end), a class object is created. This is basically a wrapper around the contents of the namespace created by the class definition; we\u2019ll learn more about class objects in the next section. The original local scope (the one in effect just before the class definition was entered) is reinstated, and the class object is bound here to the class name given in the class definition header (ClassName in the example).","title":"Classes and Objects"},{"location":"lesson6/lesson6/#what-is-an-object","text":"An object is simply a collection of data (variables) and methods (functions) that act on those data. Similarly, a class is a blueprint for that object. We can think of class as a sketch (prototype) of a house. It contains all the details about the floors, doors, windows etc. Based on these descriptions we build the house. House is the object. As many houses can be made from a house's blueprint, we can create many objects from a class. An object is also called an instance of a class and the process of creating this object is called instantiation Learn more at 1. https://docs.python.org/3/tutorial/classes.html 2. https://en.wikipedia.org/wiki/Class_(computer_programming)","title":"What is an object?"},{"location":"lesson6/lesson6/#an-example","text":"Write a class named ' Tax ' to calculate the state tax (in dollars) of Employees at Texas Tech University based on their annual salary. The state tax is 16% if the annual salary is below 80,000 dollars and 22% if the salary is more than 80,000 dollars. Employee Annual salary (dollars) Bob 150,000 Mary 78,000 John 55,000 Danny 175,000","title":"An Example:"},{"location":"lesson6/lesson6/#notes","text":"Use docstrings to describe the purpose of the class. Use if....else conditional statements within the method of the class to choose the relevant tax % based on the annual salary. Create an object for employee and display the output as shown below. Bob's tax amount (in dollars): AMOUNT Mary's tax amount (in dollars): AMOUNT John's tax amount (in dollars): AMOUNT Danny's tax amount (in dollars): AMOUNT class Tax: \"\"\"This class calculates the tax amount based on the annual salary and the state tax %\"\"\" def __init__(self, salary): # here is the instantiation constructor self.salary = salary def taxamount(self): # here is a method (function) that can operate on the class once created if self.salary < 80000: return self.salary*(16/100) else: return self.salary*(22/100) bob = Tax(150000) dir(bob) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] bob = Tax(150000) # objects constructed using Tax class mary = Tax(78000) john = Tax(55000) danny = Tax(175000) dir(Tax) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'taxamount'] dir(mary) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] print(\"bobz salary \", bob.salary ) print(\"Bob's tax amount (in dollars):\", bob.taxamount() ) print(\"Mary's tax amount (in dollars):\", mary.taxamount()) print(\"John's tax amount (in dollars):\", john.taxamount()) print(\"Danny's tax amount (in dollars):\", danny.taxamount()) bobz salary 150000 Bob's tax amount (in dollars): 33000.0 Mary's tax amount (in dollars): 12480.0 John's tax amount (in dollars): 8800.0 Danny's tax amount (in dollars): 38500.0 Numbers, strings, lists, and dictionaries are all objects that are instances of a parent class print(type(0)) <class 'int'> print(type(\"\")) <class 'str'> print(type([1, 2, 3, 4])) <class 'list'> To get more information about the built-in classes and objects, use dir( ) and help( ) functions print(dir(int)) print(help(\"\")) User-defined classes: Defining docstrings class Dog: \"\"\"This class enables the dog to say its name and age in dog years\"\"\" def __init__(self, name, years): \"\"\"This function contains all the necessary attributes\"\"\" self.name = name self.years = years self.dog_age = years*9 def sound(self): \"\"\"This function enables the dog to speak\"\"\" print(\"woof! I am {} and I am {} dog years old! woof!\".format(self.name, self.dog_age)) fudge = Dog(\"Fudge\", 2) maple = Dog(\"Maple\", 1.5) fudge.sound() maple.sound() woof! I am Fudge and I am 18 dog years old! woof! woof! I am Maple and I am 13.5 dog years old! woof! help(Dog) Help on class Dog in module __main__: class Dog(builtins.object) | Dog(name, years) | | This class enables the dog to say its name and age in dog years | | Methods defined here: | | __init__(self, name, years) | This function contains all the necessary attributes | | sound(self) | This function enables the dog to speak | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined)","title":"Notes:"},{"location":"lesson6/lesson6/#files-and-filesystems","text":"","title":"Files and Filesystems"},{"location":"lesson6/lesson6/#background","text":"A computer file is a computer resource for recording data discretely (not in the secretive context, but specifically somewhere on a piece of hardware) in a computer storage device. Just as words can be written to paper, so can information be written to a computer file. Files can be edited and transferred through the internet on that particular computer system. There are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once. By using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times. Typically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access.","title":"Background"},{"location":"lesson6/lesson6/#file-system","text":"In computing, a file system or filesystem, controls how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stops and the next begins. By separating the data into pieces and giving each piece a name, the data is isolated and identified. Taking its name from the way paper-based data management system is named, each group of data is called a \u201cfile\u201d. The structure and logic rules used to manage the groups of data and their names is called a \u201cfile system\u201d.","title":"File system"},{"location":"lesson6/lesson6/#path","text":"A path, the general form of the name of a file or directory, specifies a unique location in a file system. A path points to a file system location by following the directory tree hierarchy expressed in a string of characters in which path components, separated by a delimiting character, represent each directory. The delimiting character is most commonly the slash (\u201d/\u201d), the backslash character (\u201d\\\u201d), or colon (\u201d:\u201d), though some operating systems may use a different delimiter. Paths are used extensively in computer science to represent the directory/file relationships common in modern operating systems, and are essential in the construction of Uniform Resource Locators (URLs). Resources can be represented by either absolute or relative paths. As an example consider the following two files: /Users/theodore/MyGit/@atomickitty/hurri-sensors/.git/Guest.conf /etc/apache2/users/Guest.conf They both have the same file name, but are located on different paths. Failure to provide the path when addressing the file can be a problem. Another way to interpret is that the two unique files actually have different names, and only part of those names is common (Guest.conf) The two names above (including the path) are called fully qualified filenames (or absolute names), a relative path (usually relative to the file or program of interest depends on where in the directory structure the file lives. If we are currently in the .git directory (the first file) the path to the file is just the filename. We have experienced path issues with dependencies on .png files - in general your JupyterLab notebooks on CoCalc can only look at the local directory which is why we have to copy files into the directory for things to work.","title":"Path"},{"location":"lesson6/lesson6/#file-types","text":"Text Files. Text files are regular files that contain information readable by the user. This information is stored in ASCII. You can display and print these files. The lines of a text file must not contain NULL characters, and none can exceed a prescribed (by architecture) length, including the new-line character. The term text file does not prevent the inclusion of control or other nonprintable characters (other than NUL). Therefore, standard utilities that list text files as inputs or outputs are either able to process the special characters gracefully or they explicitly describe their limitations within their individual sections. Binary Files. Binary files are regular files that contain information readable by the computer. Binary files may be executable files that instruct the system to accomplish a job. Commands and programs are stored in executable, binary files. Special compiling programs translate ASCII text into binary code. The only difference between text and binary files is that text files have lines of less than some length, with no NULL characters, each terminated by a new-line character. Directory Files. Directory files contain information the system needs to access all types of files, but they do not contain the actual file data. As a result, directories occupy less space than a regular file and give the file system structure flexibility and depth. Each directory entry represents either a file or a subdirectory. Each entry contains the name of the file and the file's index node reference number (i-node). The i-node points to the unique index node assigned to the file. The i-node describes the location of the data associated with the file. Directories are created and controlled by a separate set of commands.","title":"File Types"},{"location":"lesson6/lesson6/#file-manipulation","text":"For this lesson we examine just a handfull of file manipulations which are quite useful. Files can be \"created\",\"read\",\"updated\", or \"deleted\" (CRUD).","title":"File Manipulation"},{"location":"lesson6/lesson6/#example-create-a-file-write-to-it","text":"Below is an example of creating a file that does not yet exist. The script is a bit pendandic on purpose. First will use some system commands to view the contents of the local directory import sys # on a Mac/Linux ! rm -rf myfirstfile.txt # delete file if it exists ! pwd # list name of working directory, note it includes path, so it is an absolute path # on Winderz #! del myfirstfile.txt # delete file if it exists #! %pwd # list name of working directory, note it includes path, so it is an absolute path /home/sensei/1330-textbook-webroot/docs/lesson6 # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 752 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt # create file example externalfile = open(\"myfirstfile.txt\",'w') # create connection to file, set to write (w), file does not need to exist mymessage = 'message in a bottle' #some object to write, in this case a string externalfile.write(mymessage)# write the contents of mymessage to the file externalfile.close() # close the file connection At this point our new file should exist, lets list the directory and see if that is so # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 756 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 19 Feb 17 17:35 myfirstfile.txt -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt Sure enough, its there, we will use a bash command cat to look at the contents of the file. ! cat myfirstfile.txt # Mac/Linux # ! type myfirstfile.txt # Winderz message in a bottle","title":"Example: Create a file, write to it."},{"location":"lesson6/lesson6/#example-read-from-an-existing-file","text":"We will continue using the file we just made, and read from it the example is below # read file example externalfile = open(\"myfirstfile.txt\",'r') # create connection to file, set to read (r), file must exist silly_string = externalfile.read() # read the contents externalfile.close() # close the file connection print(silly_string) message in a bottle","title":"Example: Read from an existing file."},{"location":"lesson6/lesson6/#example-update-a-file","text":"This example continues with our same file, but we will now add contents without destroying existing contents. The keyword is append externalfile = open(\"myfirstfile.txt\",'a') # create connection to file, set to append (a), file does not need to exist externalfile.write('\\n') # adds a newline character what_to_add = 'I love rock-and-roll, put another dime in the jukebox baby ... \\n' externalfile.write(what_to_add) # add a string including the linefeed what_to_add = '... the waiting is the hardest part \\n' externalfile.write(what_to_add) # add a string including the linefeed mylist = [1,2,3,4,5] # a list of numbers what_to_add = ','.join(map(repr, mylist)) + \"\\n\" # one way to write the list externalfile.write(what_to_add) what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" # another way to write the list externalfile.write(what_to_add) externalfile.close() As before we can examine the contents using a shell command sent from the notebook. ! cat myfirstfile.txt # ! type myfirstfile.txt # Winderz message in a bottle I love rock-and-roll, put another dime in the jukebox baby ... ... the waiting is the hardest part 1,2,3,4,5 1,2,3,4,5","title":"Example: Update a file."},{"location":"lesson6/lesson6/#example-delete-a-file","text":"Delete can be done by a system call as we did above to clear the local directory In a JupyterLab notebook, we can either use import sys ! rm -rf myfirstfile.txt # delete file if it exists or import os os.remove(\"myfirstfile.txt\") they both have same effect, both equally dangerous to your filesystem. Learn more about CRUD with text files at https://www.guru99.com/reading-and-writing-files-in-python.html Learn more about file delete at https://www.dummies.com/programming/python/how-to-delete-a-file-in-python/ import os file2kill = \"myfirstfile.txt\" try: os.remove(file2kill) # file must exist or will generate an exception except: pass # example of using pass to improve readability print(file2kill, \" missing or deleted !\") myfirstfile.txt missing or deleted ! A little discussion on the part where we wrote numbers what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" Here are descriptions of the two functions map and repr map(function, iterable, ...) Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list. repr(object) Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse quotes). It is sometimes useful to be able to access this operation as an ordinary function. For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() , otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. A class can control what this function returns for its instances by defining a repr() method. What they do in this script is important. The statement: what_to_add = \u2019,\u2019.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" is building a string that will be comprised of elements of mylist[0:len(mylist)]. The repr() function gets these elements as they are represented in the computer, the delimiter a comma is added using the join method in Python, and because everything is now a string the ... + \"\\n\" puts a linefeed character at the end of the string so the output will start a new line the next time something is written.","title":"Example: Delete a file"},{"location":"lesson6/lesson6/#example","text":"create a text file, name it \"MyFavoriteQuotation\" . Write your favorite quotation in the file. Read the file. Add this string to it in a new line : \"And that's something I wish I had said...\" Show the final outcome. # create the \"My Favorite Quotation\" file: externalfile = open(\"MyFavoriteQuotation.txt\",'w') # create connection to file, set to write (w) myquotation = 'The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.' #My choice: quotation from Pulp Fiction externalfile.write(myquotation)# write the contents of mymessage to the file externalfile.close() # close the file connection #Let's read the file ! cat MyFavoriteQuotation.txt # Let's add the string externalfile = open(\"MyFavoriteQuotation.txt\",'a') #create connection to file, set to append (a) externalfile.write('\\n') # adds a newline character what_to_add = \"And that's something I wish I had said ... \\n\" externalfile.write(what_to_add) externalfile.close() #Let's read the file one last time ! cat MyFavoriteQuotation.txt # ! type MyFavoriteQuotation # Winderz The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you. And that's something I wish I had said ...","title":"Example"},{"location":"lesson6/lesson6/#references","text":"Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty compthink /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"References"},{"location":"lesson7/lesson7/","text":"ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 14 February 2021 Lesson 7 The Pandas module About Pandas How to install Anaconda JupyterHub/Lab (on Linux) JupyterHub/Lab (on MacOS) JupyterHub/Lab (on Windoze) The Dataframe Primatives Using Pandas Create, Modify, Delete datagrames Slice Dataframes Conditional Selection Synthetic Programming (Symbolic Function Application) Files Access Files from a remote Web Server Get file contents Get the actual file Adaptations for encrypted servers (future semester) Special Script Blocks %%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} Objectives To understand the dataframe abstraction as implemented in the Pandas library(module). To be able to access and manipulate data within a dataframe To be able to obtain basic statistical measures of data within a dataframe Read/Write from/to files MS Excel-type files (.xls,.xlsx,.csv) (LibreOffice files use the MS .xml standard) Ordinary ASCII (.txt) files Access files directly from a URL (advanced concept) Using a wget-type function Using a curl-type function Using API keys (future versions) Pandas: Pandas is the core library for dataframe manipulation in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is derived from the term \u2018Panel Data\u2019. If you are curious about Pandas, this cheat sheet is recommended: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf Data Structure The Primary data structure is called a dataframe. It is an abstraction where data are represented as a 2-dimensional mutable and heterogenous tabular data structure; much like a Worksheet in MS Excel. The structure itself is popular among statisticians and data scientists and business executives. According to the marketing department \"Pandas Provides rich data structures and functions designed to make working with data fast, easy, and expressive. It is useful in data manipulation, cleaning, and analysis; Pandas excels in performance and productivity \" The Dataframe A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. Like MS Excel we can query the dataframe to find the contents of a particular cell using its row name and column name , or operate on entire rows and columns To use pandas, we need to import the module. Computational Thinking Concepts The CT concepts expressed within Pandas include: Decomposition : Data interpretation, manipulation, and analysis of Pandas dataframes is an act of decomposition -- although the dataframes can be quite complex. Abstraction : The dataframe is a data representation abstraction that allows for placeholder operations, later substituted with specific contents for a problem; enhances reuse and readability. We leverage the principle of algebraic replacement using these abstractions. Algorithms : Data interpretation, manipulation, and analysis of dataframes are generally implemented as part of a supervisory algorithm. Module Set-Up In principle, Pandas should be available in a default Anaconda install - You should not have to do any extra installation steps to install the library in Python - You do have to import the library in your scripts How to check - Simply open a code cell and run import pandas if the notebook does not protest (i.e. pink block of error), the youis good to go. import pandas If you do get an error, that means that you will have to install using conda or pip ; you are on-your-own here! On the content server the process is: Open a new terminal from the launcher Change to root user su then enter the root password sudo -H /opt/jupyterhib/bin/python3 -m pip install pandas Wait until the install is complete; for security, user compthink is not in the sudo group Verify the install by trying to execute import pandas as above. The process above will be similar on a Macintosh, or Windows if you did not use an Anaconda distribution. Best is to have a sucessful anaconda install, or go to the GoodJobUntilMyOrgansGetHarvested . If you have to do this kind of install, you will have to do some reading, some references I find useful are: 1. https://jupyterlab.readthedocs.io/en/stable/user/extensions.html 2. https://www.pugetsystems.com/labs/hpc/Note-How-To-Install-JupyterHub-on-a-Local-Server-1673/#InstallJupyterHub 3. https://jupyterhub.readthedocs.io/en/stable/installation-guide-hard.html (This is the approach on the content server which has a functioning JupyterHub) Dataframe-type Structure using primative python First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. import numpy mytabular = numpy.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 19, 21, 22, 81] ['B', 94, 75, 66, 44] ['C', 70, 56, 47, 63] ['D', 56, 80, 39, 39] ['E', 66, 78, 39, 15] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 70, 56, 47, 63] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 21 75 56 80 78 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 47 Now we shall create a proper dataframe We will now do the same using pandas mydf = pandas.DataFrame(numpy.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 We can also turn our table into a dataframe, notice how the constructor adds header row and index column mydf1 = pandas.DataFrame(mytable) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 19 21 22 81 2 B 94 75 66 44 3 C 70 56 47 63 4 D 56 80 39 39 5 E 66 78 39 15 To get proper behavior, we can just reuse our original objects mydf2 = pandas.DataFrame(mytabular,myrowname,mycolname) mydf2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 19 21 22 81 B 94 75 66 44 C 70 56 47 63 D 56 80 39 39 E 66 78 39 15 Why are mydf and mydf2 different? Getting the shape of dataframes The shape method, which is available after the dataframe is constructed, will return the row and column rank (count) of a dataframe. mydf.shape (5, 4) mydf1.shape (6, 5) mydf2.shape (5, 4) Appending new columns To append a column simply assign a value to a new column name to the dataframe mydf['new']= 'NA' mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA Appending new rows This is sometimes a bit trickier but here is one way: - create a copy of a row, give it a new name. - concatenate it back into the dataframe. newrow = mydf.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pandas.concat([mydf,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA X 80 67 4 24 NA Removing Rows and Columns To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 # or just use drop with axis specify newtable.drop('X', axis=0, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 Indexing We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 20 B 8 D 83 E 67 Name: X, dtype: int64 newtable[['X','W']] #Selecing a multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 20 97 B 8 74 D 83 92 E 67 80 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 80 X 67 Y 4 Z 24 Name: E, dtype: int64 newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 newtable.loc[['E','D','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 80 67 4 24 D 92 83 90 28 B 74 8 7 99 newtable.loc[['B','E','D'],['X','Y']] #Selecting elements via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 8 7 E 67 4 D 83 90 Conditional Selection mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? mydf[mydf['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? mydf[mydf['col2']==mydf['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object Descriptor Functions #Creating a dataframe from a dictionary mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach head method Returns the first few rows, useful to infer structure #Returns only the first five rows mydf.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit info method Returns the data model (data column count, names, data types) #Info about the dataframe mydf.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes describe method Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe mydf.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000 Counting and Sum methods There are also methods for counts and sums by specific columns mydf['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) mydf['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values mydf['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) mydf['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64 Using functions in dataframes - symbolic apply The power of Pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. This employs principles of pattern matching , abstraction , and algorithm development ; a holy trinity of Computational Thinning. It's somewhat complicated but quite handy, best shown by an example: def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(mydf) print('Apply the times2 function to col2') mydf['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64 Sorts mydf.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit mydf.sort_values('col3', ascending = True) #Lexiographic sort .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 1 2 555 apple 6 7 222 banana 2 3 666 grape 4 5 666 jackfruit 3 4 444 mango 0 1 444 orange 7 8 222 peach 5 6 111 watermelon Aggregating (Grouping Values) dataframe contents #Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } mydf1 = pandas.DataFrame(data) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' mydf1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' mydf1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27 Filtering out missing values Filtering and cleaning are often used to describe the process where data that does not support a narrative is removed ;typically for maintenance of profit applications, if the data are actually missing that is common situation where cleaning is justified. #Creating a dataframe from a dictionary df = pandas.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach Reading a File into a Dataframe Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) # xlsx reads deprecated here is a hack using openpyxl readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 Writing a dataframe to file #Creating and writing to a .csv file readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pandas.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pandas.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', index = False, engine='openpyxl') readfileexcel = pandas.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 Downloading files from websites (optional) This section shows how to get files from a remote computer. There are several ways to get the files, most importantly you need the FQDN to the file. Method 1: Get data from a file on a remote server (unencrypted) This section shows how to obtain data files from public URLs. Prerequesites: You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://... Web Developer Notes If you want to distribute files (web developers) the files need to be in the server webroot, but can be deep into the heirarchial structure. Here we will do an example with a file that contains topographic data in XYZ format, without header information. The first few lines of the remote file look like: 74.90959724 93.21251922 0 75.17907367 64.40278759 0 94.9935575 93.07951286 0 95.26234119 64.60091165 0 54.04976655 64.21159095 0 54.52914363 35.06934342 0 75.44993558 34.93079513 0 75.09317373 5.462959114 0 74.87357468 10.43130083 0 74.86249082 15.72938748 0 And importantly it is tab delimited. The module to manipulate url in python is called urllib Google search to learn more, here we are using only a small component without exception trapping. #Step 1: import needed modules to interact with the internet from urllib.request import urlopen # import a method that will connect to a url and read file contents import pandas #import pandas This next code fragment sets a string called remote_url ; it is just a variable, name can be anything that honors python naming rules. Then the urllib function urlopen with read and decode methods is employed, the result is stored in an object named elevationXYZ #Step 2: make the connection to the remote file (actually its implementing \"bash curl -O http://fqdn/path ...\") remote_url = 'http://www.rtfmps.com/share_files/pip-corner-sumps.txt' # elevationXYZ = urlopen(remote_url).read().decode().split() # Gets the file contents as a single vector, comma delimited, file is not retained locally At this point the object exists as a single vector with hundreds of elements. We now need to structure the content. Here using python primatives, and knowing how the data are supposed to look, we prepare variables to recieve the structured results #Step 3 Python primatives to structure the data, or use fancy modules (probably easy in numpy) howmany = len(elevationXYZ) # how long is the vector? nrow = int(howmany/3) xyz = [[0 for j in range(3)] for j in range(nrow)] # null space to receive data define columnX Now that everything is ready, we can extract from the object the values we want into xyz #Step4 Now will build xyz as a matrix with 3 columns index = 0 for irow in range(0,nrow): xyz[irow][0]=float(elevationXYZ[index]) xyz[irow][1]=float(elevationXYZ[index+1]) xyz[irow][2]=float(elevationXYZ[index+2]) index += 3 #increment the index xyz is now a 3-column float array and can now probably be treated as a data frame. Here we use a pandas method to build the dataframe. df = pandas.DataFrame(xyz) Get some info, yep three columns (ordered triples to be precise!) df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 774 entries, 0 to 773 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 774 non-null float64 1 1 774 non-null float64 2 2 774 non-null float64 dtypes: float64(3) memory usage: 18.3 KB And some summary statistics (meaningless for these data), but now have taken data from the internet and prepared it for analysis. df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 count 774.000000 774.000000 774.000000 mean 52.064621 48.770060 2.364341 std 30.883400 32.886277 1.497413 min -2.113554 -11.360960 0.000000 25% 25.640786 21.809579 2.000000 50% 55.795821 49.059950 2.000000 75% 76.752290 75.015933 4.000000 max 111.726727 115.123931 4.000000 And lets look at the first few rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 74.909597 93.212519 0.0 1 75.179074 64.402788 0.0 2 94.993557 93.079513 0.0 3 95.262341 64.600912 0.0 4 54.049767 64.211591 0.0 Method 2: Get the actual file from a remote web server (unencrypted) You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://... We will need a module to interface with the remote server, in this example lets use something different than urllib . Here we will use requests , so first we load the module import requests # Module to process http/https requests Now we will generate a GET request to the remote http server. I chose to do so using a variable to store the remote URL so I can reuse code in future projects. The GET request (an http/https method) is generated with the requests method get and assigned to an object named rget -- the name is arbitrary. Next we extract the file from the rget object and write it to a local file with the name of the remote file - esentially automating the download process. Then we import the pandas module. remote_url=\"http://54.243.252.9/engr-1330-psuedo-course/MyJupyterNotebooks/42-DataScience-EvaporationAnalysis/all_quads_gross_evaporation.csv\" # set the url rget = requests.get(remote_url, allow_redirects=True) # get the remote resource, follow imbedded links open('all_quads_gross_evaporation.csv','wb').write(rget.content) # extract from the remote the contents, assign to a local file same name import pandas as pd # Module to process dataframes (not absolutely needed but somewhat easier than using primatives, and gives graphing tools) # verify file exists ! pwd ! ls -la /home/sensei/1330-textbook-webroot/docs/lesson7 total 1412 drwxrwxr-x 3 sensei sensei 4096 Feb 16 20:57 . drwxr-xr-x 10 sensei sensei 4096 Feb 16 20:30 .. drwxrwxr-x 2 sensei sensei 4096 Feb 16 20:53 .ipynb_checkpoints -rw-rw-r-- 1 sensei sensei 21150 Feb 15 15:58 01-table-dataframe.png -rw-rw-r-- 1 sensei sensei 51 Feb 15 15:58 CSV_ReadingFile.csv -rw-rw-r-- 1 sensei sensei 55 Feb 16 20:59 CSV_WritingFile1.csv -rw-rw-r-- 1 sensei sensei 46 Feb 16 20:59 CSV_WritingFile2.csv -rw-rw-r-- 1 sensei sensei 693687 Feb 15 15:58 ENGR-1330-Lesson8-Dev.html -rw-rw-r-- 1 sensei sensei 166938 Feb 15 15:58 ENGR-1330-Lesson8-Dev.ipynb -rw-rw-r-- 1 sensei sensei 5508 Feb 15 15:58 Excel_ReadingFile.xlsx -rw-rw-r-- 1 sensei sensei 5041 Feb 16 20:59 Excel_WritingFile.xlsx -rw-rw-r-- 1 sensei sensei 363498 Feb 16 20:59 all_quads_gross_evaporation.csv -rw-rw-r-- 1 sensei sensei 108222 Feb 16 20:57 lesson7.ipynb -rw-rw-r-- 1 sensei sensei 40566 Feb 15 15:58 output_126_1.png Now we can read the file contents and check its structure, before proceeding. evapdf = pd.read_csv(\"all_quads_gross_evaporation.csv\",parse_dates=[\"YYYY-MM\"]) # Read the file as a .CSV assign to a dataframe evapdf evapdf.head() # check structure .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } YYYY-MM 104 105 106 107 108 204 205 206 207 ... 911 912 1008 1009 1010 1011 1108 1109 1110 1210 0 1954-01-01 1.80 1.80 2.02 2.24 2.24 2.34 1.89 1.80 1.99 ... 1.42 1.30 2.50 2.42 1.94 1.29 2.59 2.49 2.22 2.27 1 1954-02-01 4.27 4.27 4.13 3.98 3.90 4.18 4.26 4.27 4.26 ... 2.59 2.51 4.71 4.30 3.84 2.50 5.07 4.62 4.05 4.18 2 1954-03-01 4.98 4.98 4.62 4.25 4.20 5.01 4.98 4.98 4.68 ... 3.21 3.21 6.21 6.06 5.02 3.21 6.32 6.20 5.68 5.70 3 1954-04-01 6.09 5.94 5.94 6.07 5.27 6.31 5.98 5.89 5.72 ... 3.83 3.54 6.45 6.25 4.92 3.54 6.59 6.44 5.88 5.95 4 1954-05-01 5.41 5.09 5.14 4.40 3.61 5.57 4.56 4.47 4.18 ... 3.48 3.97 7.92 8.13 6.31 3.99 7.75 7.98 7.40 7.40 5 rows \u00d7 93 columns Structure looks like a spreadsheet as expected; lets plot the time series for cell '911' evapdf.plot.line(x='YYYY-MM',y='911') # Plot quadrant 911 evaporation time series <AxesSubplot:xlabel='YYYY-MM'> Method 3: Get the actual file from an encrypted server This section is saved for future semesters References Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"lesson7/lesson7/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 14 February 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"lesson7/lesson7/#lesson-7-the-pandas-module","text":"About Pandas How to install Anaconda JupyterHub/Lab (on Linux) JupyterHub/Lab (on MacOS) JupyterHub/Lab (on Windoze) The Dataframe Primatives Using Pandas Create, Modify, Delete datagrames Slice Dataframes Conditional Selection Synthetic Programming (Symbolic Function Application) Files Access Files from a remote Web Server Get file contents Get the actual file Adaptations for encrypted servers (future semester)","title":"Lesson 7 The Pandas module"},{"location":"lesson7/lesson7/#special-script-blocks","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;}","title":"Special Script Blocks"},{"location":"lesson7/lesson7/#objectives","text":"To understand the dataframe abstraction as implemented in the Pandas library(module). To be able to access and manipulate data within a dataframe To be able to obtain basic statistical measures of data within a dataframe Read/Write from/to files MS Excel-type files (.xls,.xlsx,.csv) (LibreOffice files use the MS .xml standard) Ordinary ASCII (.txt) files Access files directly from a URL (advanced concept) Using a wget-type function Using a curl-type function Using API keys (future versions)","title":"Objectives"},{"location":"lesson7/lesson7/#pandas","text":"Pandas is the core library for dataframe manipulation in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is derived from the term \u2018Panel Data\u2019. If you are curious about Pandas, this cheat sheet is recommended: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf","title":"Pandas:"},{"location":"lesson7/lesson7/#data-structure","text":"The Primary data structure is called a dataframe. It is an abstraction where data are represented as a 2-dimensional mutable and heterogenous tabular data structure; much like a Worksheet in MS Excel. The structure itself is popular among statisticians and data scientists and business executives. According to the marketing department \"Pandas Provides rich data structures and functions designed to make working with data fast, easy, and expressive. It is useful in data manipulation, cleaning, and analysis; Pandas excels in performance and productivity \"","title":"Data Structure"},{"location":"lesson7/lesson7/#the-dataframe","text":"A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. Like MS Excel we can query the dataframe to find the contents of a particular cell using its row name and column name , or operate on entire rows and columns To use pandas, we need to import the module.","title":"The Dataframe"},{"location":"lesson7/lesson7/#computational-thinking-concepts","text":"The CT concepts expressed within Pandas include: Decomposition : Data interpretation, manipulation, and analysis of Pandas dataframes is an act of decomposition -- although the dataframes can be quite complex. Abstraction : The dataframe is a data representation abstraction that allows for placeholder operations, later substituted with specific contents for a problem; enhances reuse and readability. We leverage the principle of algebraic replacement using these abstractions. Algorithms : Data interpretation, manipulation, and analysis of dataframes are generally implemented as part of a supervisory algorithm.","title":"Computational Thinking Concepts"},{"location":"lesson7/lesson7/#module-set-up","text":"In principle, Pandas should be available in a default Anaconda install - You should not have to do any extra installation steps to install the library in Python - You do have to import the library in your scripts How to check - Simply open a code cell and run import pandas if the notebook does not protest (i.e. pink block of error), the youis good to go. import pandas If you do get an error, that means that you will have to install using conda or pip ; you are on-your-own here! On the content server the process is: Open a new terminal from the launcher Change to root user su then enter the root password sudo -H /opt/jupyterhib/bin/python3 -m pip install pandas Wait until the install is complete; for security, user compthink is not in the sudo group Verify the install by trying to execute import pandas as above. The process above will be similar on a Macintosh, or Windows if you did not use an Anaconda distribution. Best is to have a sucessful anaconda install, or go to the GoodJobUntilMyOrgansGetHarvested . If you have to do this kind of install, you will have to do some reading, some references I find useful are: 1. https://jupyterlab.readthedocs.io/en/stable/user/extensions.html 2. https://www.pugetsystems.com/labs/hpc/Note-How-To-Install-JupyterHub-on-a-Local-Server-1673/#InstallJupyterHub 3. https://jupyterhub.readthedocs.io/en/stable/installation-guide-hard.html (This is the approach on the content server which has a functioning JupyterHub)","title":"Module Set-Up"},{"location":"lesson7/lesson7/#dataframe-type-structure-using-primative-python","text":"First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. import numpy mytabular = numpy.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 19, 21, 22, 81] ['B', 94, 75, 66, 44] ['C', 70, 56, 47, 63] ['D', 56, 80, 39, 39] ['E', 66, 78, 39, 15] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 70, 56, 47, 63] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 21 75 56 80 78 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 47","title":"Dataframe-type Structure using primative python"},{"location":"lesson7/lesson7/#now-we-shall-create-a-proper-dataframe","text":"We will now do the same using pandas mydf = pandas.DataFrame(numpy.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 We can also turn our table into a dataframe, notice how the constructor adds header row and index column mydf1 = pandas.DataFrame(mytable) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 19 21 22 81 2 B 94 75 66 44 3 C 70 56 47 63 4 D 56 80 39 39 5 E 66 78 39 15 To get proper behavior, we can just reuse our original objects mydf2 = pandas.DataFrame(mytabular,myrowname,mycolname) mydf2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 19 21 22 81 B 94 75 66 44 C 70 56 47 63 D 56 80 39 39 E 66 78 39 15 Why are mydf and mydf2 different?","title":"Now we shall create a proper dataframe"},{"location":"lesson7/lesson7/#getting-the-shape-of-dataframes","text":"The shape method, which is available after the dataframe is constructed, will return the row and column rank (count) of a dataframe. mydf.shape (5, 4) mydf1.shape (6, 5) mydf2.shape (5, 4)","title":"Getting the shape of dataframes"},{"location":"lesson7/lesson7/#appending-new-columns","text":"To append a column simply assign a value to a new column name to the dataframe mydf['new']= 'NA' mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA","title":"Appending new columns"},{"location":"lesson7/lesson7/#appending-new-rows","text":"This is sometimes a bit trickier but here is one way: - create a copy of a row, give it a new name. - concatenate it back into the dataframe. newrow = mydf.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pandas.concat([mydf,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA X 80 67 4 24 NA","title":"Appending new rows"},{"location":"lesson7/lesson7/#removing-rows-and-columns","text":"To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 # or just use drop with axis specify newtable.drop('X', axis=0, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24","title":"Removing Rows and Columns"},{"location":"lesson7/lesson7/#indexing","text":"We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 20 B 8 D 83 E 67 Name: X, dtype: int64 newtable[['X','W']] #Selecing a multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 20 97 B 8 74 D 83 92 E 67 80 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 80 X 67 Y 4 Z 24 Name: E, dtype: int64 newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 newtable.loc[['E','D','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 80 67 4 24 D 92 83 90 28 B 74 8 7 99 newtable.loc[['B','E','D'],['X','Y']] #Selecting elements via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 8 7 E 67 4 D 83 90","title":"Indexing"},{"location":"lesson7/lesson7/#conditional-selection","text":"mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? mydf[mydf['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? mydf[mydf['col2']==mydf['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object","title":"Conditional Selection"},{"location":"lesson7/lesson7/#descriptor-functions","text":"#Creating a dataframe from a dictionary mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach","title":"Descriptor Functions"},{"location":"lesson7/lesson7/#head-method","text":"Returns the first few rows, useful to infer structure #Returns only the first five rows mydf.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit","title":"head method"},{"location":"lesson7/lesson7/#info-method","text":"Returns the data model (data column count, names, data types) #Info about the dataframe mydf.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes","title":"info method"},{"location":"lesson7/lesson7/#describe-method","text":"Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe mydf.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000","title":"describe method"},{"location":"lesson7/lesson7/#counting-and-sum-methods","text":"There are also methods for counts and sums by specific columns mydf['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) mydf['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values mydf['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) mydf['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64","title":"Counting and Sum methods"},{"location":"lesson7/lesson7/#using-functions-in-dataframes-symbolic-apply","text":"The power of Pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. This employs principles of pattern matching , abstraction , and algorithm development ; a holy trinity of Computational Thinning. It's somewhat complicated but quite handy, best shown by an example: def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(mydf) print('Apply the times2 function to col2') mydf['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64","title":"Using functions in dataframes - symbolic apply"},{"location":"lesson7/lesson7/#sorts","text":"mydf.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit mydf.sort_values('col3', ascending = True) #Lexiographic sort .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 1 2 555 apple 6 7 222 banana 2 3 666 grape 4 5 666 jackfruit 3 4 444 mango 0 1 444 orange 7 8 222 peach 5 6 111 watermelon","title":"Sorts"},{"location":"lesson7/lesson7/#aggregating-grouping-values-dataframe-contents","text":"#Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } mydf1 = pandas.DataFrame(data) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' mydf1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' mydf1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27","title":"Aggregating (Grouping Values) dataframe contents"},{"location":"lesson7/lesson7/#filtering-out-missing-values","text":"Filtering and cleaning are often used to describe the process where data that does not support a narrative is removed ;typically for maintenance of profit applications, if the data are actually missing that is common situation where cleaning is justified. #Creating a dataframe from a dictionary df = pandas.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach","title":"Filtering out missing values"},{"location":"lesson7/lesson7/#reading-a-file-into-a-dataframe","text":"Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) # xlsx reads deprecated here is a hack using openpyxl readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15","title":"Reading a File into a Dataframe"},{"location":"lesson7/lesson7/#writing-a-dataframe-to-file","text":"#Creating and writing to a .csv file readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pandas.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pandas.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', index = False, engine='openpyxl') readfileexcel = pandas.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15","title":"Writing a dataframe to file"},{"location":"lesson7/lesson7/#downloading-files-from-websites-optional","text":"This section shows how to get files from a remote computer. There are several ways to get the files, most importantly you need the FQDN to the file.","title":"Downloading files from websites (optional)"},{"location":"lesson7/lesson7/#method-1-get-data-from-a-file-on-a-remote-server-unencrypted","text":"This section shows how to obtain data files from public URLs. Prerequesites: You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://...","title":"Method 1: Get data from a file on a remote server (unencrypted)"},{"location":"lesson7/lesson7/#web-developer-notes","text":"If you want to distribute files (web developers) the files need to be in the server webroot, but can be deep into the heirarchial structure. Here we will do an example with a file that contains topographic data in XYZ format, without header information. The first few lines of the remote file look like: 74.90959724 93.21251922 0 75.17907367 64.40278759 0 94.9935575 93.07951286 0 95.26234119 64.60091165 0 54.04976655 64.21159095 0 54.52914363 35.06934342 0 75.44993558 34.93079513 0 75.09317373 5.462959114 0 74.87357468 10.43130083 0 74.86249082 15.72938748 0 And importantly it is tab delimited. The module to manipulate url in python is called urllib Google search to learn more, here we are using only a small component without exception trapping. #Step 1: import needed modules to interact with the internet from urllib.request import urlopen # import a method that will connect to a url and read file contents import pandas #import pandas This next code fragment sets a string called remote_url ; it is just a variable, name can be anything that honors python naming rules. Then the urllib function urlopen with read and decode methods is employed, the result is stored in an object named elevationXYZ #Step 2: make the connection to the remote file (actually its implementing \"bash curl -O http://fqdn/path ...\") remote_url = 'http://www.rtfmps.com/share_files/pip-corner-sumps.txt' # elevationXYZ = urlopen(remote_url).read().decode().split() # Gets the file contents as a single vector, comma delimited, file is not retained locally At this point the object exists as a single vector with hundreds of elements. We now need to structure the content. Here using python primatives, and knowing how the data are supposed to look, we prepare variables to recieve the structured results #Step 3 Python primatives to structure the data, or use fancy modules (probably easy in numpy) howmany = len(elevationXYZ) # how long is the vector? nrow = int(howmany/3) xyz = [[0 for j in range(3)] for j in range(nrow)] # null space to receive data define columnX Now that everything is ready, we can extract from the object the values we want into xyz #Step4 Now will build xyz as a matrix with 3 columns index = 0 for irow in range(0,nrow): xyz[irow][0]=float(elevationXYZ[index]) xyz[irow][1]=float(elevationXYZ[index+1]) xyz[irow][2]=float(elevationXYZ[index+2]) index += 3 #increment the index xyz is now a 3-column float array and can now probably be treated as a data frame. Here we use a pandas method to build the dataframe. df = pandas.DataFrame(xyz) Get some info, yep three columns (ordered triples to be precise!) df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 774 entries, 0 to 773 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 774 non-null float64 1 1 774 non-null float64 2 2 774 non-null float64 dtypes: float64(3) memory usage: 18.3 KB And some summary statistics (meaningless for these data), but now have taken data from the internet and prepared it for analysis. df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 count 774.000000 774.000000 774.000000 mean 52.064621 48.770060 2.364341 std 30.883400 32.886277 1.497413 min -2.113554 -11.360960 0.000000 25% 25.640786 21.809579 2.000000 50% 55.795821 49.059950 2.000000 75% 76.752290 75.015933 4.000000 max 111.726727 115.123931 4.000000 And lets look at the first few rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 74.909597 93.212519 0.0 1 75.179074 64.402788 0.0 2 94.993557 93.079513 0.0 3 95.262341 64.600912 0.0 4 54.049767 64.211591 0.0","title":"Web Developer Notes"},{"location":"lesson7/lesson7/#method-2-get-the-actual-file-from-a-remote-web-server-unencrypted","text":"You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://... We will need a module to interface with the remote server, in this example lets use something different than urllib . Here we will use requests , so first we load the module import requests # Module to process http/https requests Now we will generate a GET request to the remote http server. I chose to do so using a variable to store the remote URL so I can reuse code in future projects. The GET request (an http/https method) is generated with the requests method get and assigned to an object named rget -- the name is arbitrary. Next we extract the file from the rget object and write it to a local file with the name of the remote file - esentially automating the download process. Then we import the pandas module. remote_url=\"http://54.243.252.9/engr-1330-psuedo-course/MyJupyterNotebooks/42-DataScience-EvaporationAnalysis/all_quads_gross_evaporation.csv\" # set the url rget = requests.get(remote_url, allow_redirects=True) # get the remote resource, follow imbedded links open('all_quads_gross_evaporation.csv','wb').write(rget.content) # extract from the remote the contents, assign to a local file same name import pandas as pd # Module to process dataframes (not absolutely needed but somewhat easier than using primatives, and gives graphing tools) # verify file exists ! pwd ! ls -la /home/sensei/1330-textbook-webroot/docs/lesson7 total 1412 drwxrwxr-x 3 sensei sensei 4096 Feb 16 20:57 . drwxr-xr-x 10 sensei sensei 4096 Feb 16 20:30 .. drwxrwxr-x 2 sensei sensei 4096 Feb 16 20:53 .ipynb_checkpoints -rw-rw-r-- 1 sensei sensei 21150 Feb 15 15:58 01-table-dataframe.png -rw-rw-r-- 1 sensei sensei 51 Feb 15 15:58 CSV_ReadingFile.csv -rw-rw-r-- 1 sensei sensei 55 Feb 16 20:59 CSV_WritingFile1.csv -rw-rw-r-- 1 sensei sensei 46 Feb 16 20:59 CSV_WritingFile2.csv -rw-rw-r-- 1 sensei sensei 693687 Feb 15 15:58 ENGR-1330-Lesson8-Dev.html -rw-rw-r-- 1 sensei sensei 166938 Feb 15 15:58 ENGR-1330-Lesson8-Dev.ipynb -rw-rw-r-- 1 sensei sensei 5508 Feb 15 15:58 Excel_ReadingFile.xlsx -rw-rw-r-- 1 sensei sensei 5041 Feb 16 20:59 Excel_WritingFile.xlsx -rw-rw-r-- 1 sensei sensei 363498 Feb 16 20:59 all_quads_gross_evaporation.csv -rw-rw-r-- 1 sensei sensei 108222 Feb 16 20:57 lesson7.ipynb -rw-rw-r-- 1 sensei sensei 40566 Feb 15 15:58 output_126_1.png Now we can read the file contents and check its structure, before proceeding. evapdf = pd.read_csv(\"all_quads_gross_evaporation.csv\",parse_dates=[\"YYYY-MM\"]) # Read the file as a .CSV assign to a dataframe evapdf evapdf.head() # check structure .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } YYYY-MM 104 105 106 107 108 204 205 206 207 ... 911 912 1008 1009 1010 1011 1108 1109 1110 1210 0 1954-01-01 1.80 1.80 2.02 2.24 2.24 2.34 1.89 1.80 1.99 ... 1.42 1.30 2.50 2.42 1.94 1.29 2.59 2.49 2.22 2.27 1 1954-02-01 4.27 4.27 4.13 3.98 3.90 4.18 4.26 4.27 4.26 ... 2.59 2.51 4.71 4.30 3.84 2.50 5.07 4.62 4.05 4.18 2 1954-03-01 4.98 4.98 4.62 4.25 4.20 5.01 4.98 4.98 4.68 ... 3.21 3.21 6.21 6.06 5.02 3.21 6.32 6.20 5.68 5.70 3 1954-04-01 6.09 5.94 5.94 6.07 5.27 6.31 5.98 5.89 5.72 ... 3.83 3.54 6.45 6.25 4.92 3.54 6.59 6.44 5.88 5.95 4 1954-05-01 5.41 5.09 5.14 4.40 3.61 5.57 4.56 4.47 4.18 ... 3.48 3.97 7.92 8.13 6.31 3.99 7.75 7.98 7.40 7.40 5 rows \u00d7 93 columns Structure looks like a spreadsheet as expected; lets plot the time series for cell '911' evapdf.plot.line(x='YYYY-MM',y='911') # Plot quadrant 911 evaporation time series <AxesSubplot:xlabel='YYYY-MM'>","title":"Method 2: Get the actual file from a remote web server (unencrypted)"},{"location":"lesson7/lesson7/#method-3-get-the-actual-file-from-an-encrypted-server","text":"This section is saved for future semesters","title":"Method 3: Get the actual file from an encrypted server"},{"location":"lesson7/lesson7/#references","text":"Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"References"},{"location":"lesson8/lesson8/","text":"ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 18 February 2021 Lesson 8 Visual Display of Data This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, box plot, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib Graphic Standards for Plots Parts of a Plot Building Plots using matplotlib external package Objectives Define the ordinate, abscissa, independent and dependent variables Identify the parts of a proper plot Define how to plot experimental data and theoretical data About matplotlib Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis). Computational thinking (CT) concepts involved are: Decomposition : Break a problem down into smaller pieces; separating plotting from other parts of analysis simplifies maintenace of scripts Abstraction : Pulling out specific differences to make one solution work for multiple problems; wrappers around generic plot calls enhances reuse Algorithms : A list of steps that you can follow to finish a task; Often the last step and most important to make professional graphics to justify the expense (of paying you to do engineering) to the client. Graphics Conventions for Plots Terminology: Ordinate, Abscissa, Dependent and Independent Variables A few terms are used in describing plots: - Abscissa \u2013 the horizontal axis on a plot (the left-right axis) - Ordinate \u2013 the vertical axis on a plot (the up-down axis) A few terms in describing data models - Independent Variable (Explainatory, Predictor, Feature, ...) \u2013 a variable that can be controlled/manipulated in an experiment or theoretical analysis - Dependent Variable (Response, Prediction, ...) \u2013 the variable that measured/observed as a function of the independent variable Plotting convention in most cases assigns explainatory variables to the horizontal axis (e.g. Independent variable is plotted on the Abscissa) and the response variable(s) to the vertical axis (e.g. Dependent Variable is plotted on the Ordinate) Conventions for Proper Plots Include a title OR a caption with a brief description of the plot Label both axes clearly Include the variable name, the variable, and the unit in each label If possible, select increments for both the x and y axes that provide for easy interpolation Include gridlines Show experimental measurements as symbols Show model (theoretical) relationships as lines Use portrait orientation when making your plot Make the plot large enough to be easily read If more than one experimental dataset is plotted Use different shapes for each dataset Use different colors for each dataset Include a legend defining the datasets Background Data are not always numerical. Data can be music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) and other types. Categorical data is a type where you can place individual components into a category: For example visualize a freezer where a business stores ice cream, the product is categorized by flavor, each carton is a component. - The individual components are cartons of ice-cream, and the category is the flavor in the carton Bar Graphs Bar charts (graphs) are useful tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! type(flavors) list myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio <Figure size 720x360 with 0 Axes> # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='magenta', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='green', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now lets deconstruct the script a bit: ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! This part of the code creates a dictionary object, keys are the flavors, values are the carton counts (not the best way, but good for our learning needs). Next we import the python plotting library from matplotlib and name it plt to keep the script a bit easier to read. Next we use the list method to create two lists from the dictionary, flavors and cartons . Keep this in mind plotting is usually done on lists, so we need to prepare the structures properly. The next statement myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio Uses the figure class in pyplot from matplotlib to make a figure object named myfigure, the plot is built into this object. Every call to a method in plt adds content to myfigure until we send the instruction to render the plot ( plt.show() ) The next portion of the script builds the plot: plt.bar(flavors, cartons, color ='orange', width = 0.4) # Build a bar chart, plot series flavor on x-axis, plot series carton on y-axis. Make the bars orange, set bar width (units unspecified) plt.xlabel(\"Flavors\") # Label the x-axis as Flavors plt.ylabel(\"No. of Cartons in Stock\") # Label the x-axis as Flavors plt.title(\"Current Ice Cream in Storage\") # Title for the whole plot This last statement renders the plot to the graphics device (probably localhost in the web browser) plt.show() Now lets add another set of categories to the plot and see what happens ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beastcount = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='gray', width = 0.4) plt.bar(animals, beastcount, color ='brown', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now suppose we want horizontal bars we can search pyplot for such a thing. If one types horizontal bar chart into the pyplot search engine there is a link that leads to: Which has the right look! If we examine the script there is a method called barh so lets try that. ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beasts = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.barh(flavors, cartons, color ='orange') plt.barh(animals, beasts, color ='green') plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.barh(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:ylabel='Flavor'> import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show() Line Charts A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application. Example Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() time = [0,1.0,4.0,5.0,6.0,2.0,3.0] speed = [0,3,20,30,45.6,7,12] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='green', marker='o',linewidth=1) # basic line plot plt.show() # Estimate acceleration (naive) dvdt = (max(speed) - min(speed))/(max(time)-min(time)) plottitle = 'Average acceleration %.3f' % (dvdt) + ' m/sec/sec' seriesnames = ['Data','Model'] modely = [min(speed),max(speed)] modelx = [min(time),max(time)] mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=0) # basic line plot plt.plot(modelx, modely, c='blue',linewidth=1) # basic line plot plt.xlabel('Time (sec)') plt.ylabel('Speed (m/sec)') plt.legend(seriesnames) plt.title(plottitle) plt.show() Scatter Plots A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot # Example 1. A data file containing heights of fathers, mothers, and sons is to be examined df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists daddy = df['father'] ; mommy = df['mother'] ; baby = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red' , label='Father') # one plot series plt.scatter(baby, mommy, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\") df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> Histograms Quilting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson. To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\" import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Gross Gross (Adjusted) Year count 2.000000e+02 2.000000e+02 200.000000 mean 2.216196e+08 5.041983e+08 1986.620000 std 1.441574e+08 2.159814e+08 20.493548 min 9.183673e+06 3.222619e+08 1921.000000 25% 1.087824e+08 3.677804e+08 1973.000000 50% 2.001273e+08 4.388570e+08 1990.000000 75% 3.069535e+08 5.512131e+08 2003.250000 max 9.067234e+08 1.757788e+09 2015.000000 References Constructing Horizontal Bar Charts (matplotlib.org documentation) https://matplotlib.org/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html?highlight=horizontal%20bar%20chart How to make a bar chart https://www.geeksforgeeks.org/bar-plot-in-matplotlib/","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"lesson8/lesson8/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 18 February 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"lesson8/lesson8/#lesson-8-visual-display-of-data","text":"This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, box plot, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib Graphic Standards for Plots Parts of a Plot Building Plots using matplotlib external package","title":"Lesson 8 Visual Display of Data"},{"location":"lesson8/lesson8/#objectives","text":"Define the ordinate, abscissa, independent and dependent variables Identify the parts of a proper plot Define how to plot experimental data and theoretical data","title":"Objectives"},{"location":"lesson8/lesson8/#about-matplotlib","text":"Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis). Computational thinking (CT) concepts involved are: Decomposition : Break a problem down into smaller pieces; separating plotting from other parts of analysis simplifies maintenace of scripts Abstraction : Pulling out specific differences to make one solution work for multiple problems; wrappers around generic plot calls enhances reuse Algorithms : A list of steps that you can follow to finish a task; Often the last step and most important to make professional graphics to justify the expense (of paying you to do engineering) to the client.","title":"About matplotlib"},{"location":"lesson8/lesson8/#graphics-conventions-for-plots","text":"","title":"Graphics Conventions for Plots"},{"location":"lesson8/lesson8/#terminology-ordinate-abscissa-dependent-and-independent-variables","text":"A few terms are used in describing plots: - Abscissa \u2013 the horizontal axis on a plot (the left-right axis) - Ordinate \u2013 the vertical axis on a plot (the up-down axis) A few terms in describing data models - Independent Variable (Explainatory, Predictor, Feature, ...) \u2013 a variable that can be controlled/manipulated in an experiment or theoretical analysis - Dependent Variable (Response, Prediction, ...) \u2013 the variable that measured/observed as a function of the independent variable Plotting convention in most cases assigns explainatory variables to the horizontal axis (e.g. Independent variable is plotted on the Abscissa) and the response variable(s) to the vertical axis (e.g. Dependent Variable is plotted on the Ordinate)","title":"Terminology: Ordinate, Abscissa, Dependent and Independent Variables"},{"location":"lesson8/lesson8/#conventions-for-proper-plots","text":"Include a title OR a caption with a brief description of the plot Label both axes clearly Include the variable name, the variable, and the unit in each label If possible, select increments for both the x and y axes that provide for easy interpolation Include gridlines Show experimental measurements as symbols Show model (theoretical) relationships as lines Use portrait orientation when making your plot Make the plot large enough to be easily read If more than one experimental dataset is plotted Use different shapes for each dataset Use different colors for each dataset Include a legend defining the datasets","title":"Conventions for Proper Plots"},{"location":"lesson8/lesson8/#background","text":"Data are not always numerical. Data can be music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) and other types. Categorical data is a type where you can place individual components into a category: For example visualize a freezer where a business stores ice cream, the product is categorized by flavor, each carton is a component. - The individual components are cartons of ice-cream, and the category is the flavor in the carton","title":"Background"},{"location":"lesson8/lesson8/#bar-graphs","text":"Bar charts (graphs) are useful tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! type(flavors) list myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio <Figure size 720x360 with 0 Axes> # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='magenta', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='green', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now lets deconstruct the script a bit: ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! This part of the code creates a dictionary object, keys are the flavors, values are the carton counts (not the best way, but good for our learning needs). Next we import the python plotting library from matplotlib and name it plt to keep the script a bit easier to read. Next we use the list method to create two lists from the dictionary, flavors and cartons . Keep this in mind plotting is usually done on lists, so we need to prepare the structures properly. The next statement myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio Uses the figure class in pyplot from matplotlib to make a figure object named myfigure, the plot is built into this object. Every call to a method in plt adds content to myfigure until we send the instruction to render the plot ( plt.show() ) The next portion of the script builds the plot: plt.bar(flavors, cartons, color ='orange', width = 0.4) # Build a bar chart, plot series flavor on x-axis, plot series carton on y-axis. Make the bars orange, set bar width (units unspecified) plt.xlabel(\"Flavors\") # Label the x-axis as Flavors plt.ylabel(\"No. of Cartons in Stock\") # Label the x-axis as Flavors plt.title(\"Current Ice Cream in Storage\") # Title for the whole plot This last statement renders the plot to the graphics device (probably localhost in the web browser) plt.show() Now lets add another set of categories to the plot and see what happens ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beastcount = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='gray', width = 0.4) plt.bar(animals, beastcount, color ='brown', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now suppose we want horizontal bars we can search pyplot for such a thing. If one types horizontal bar chart into the pyplot search engine there is a link that leads to: Which has the right look! If we examine the script there is a method called barh so lets try that. ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beasts = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.barh(flavors, cartons, color ='orange') plt.barh(animals, beasts, color ='green') plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.barh(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:ylabel='Flavor'> import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show()","title":"Bar Graphs"},{"location":"lesson8/lesson8/#line-charts","text":"A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application. Example Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() time = [0,1.0,4.0,5.0,6.0,2.0,3.0] speed = [0,3,20,30,45.6,7,12] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='green', marker='o',linewidth=1) # basic line plot plt.show() # Estimate acceleration (naive) dvdt = (max(speed) - min(speed))/(max(time)-min(time)) plottitle = 'Average acceleration %.3f' % (dvdt) + ' m/sec/sec' seriesnames = ['Data','Model'] modely = [min(speed),max(speed)] modelx = [min(time),max(time)] mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=0) # basic line plot plt.plot(modelx, modely, c='blue',linewidth=1) # basic line plot plt.xlabel('Time (sec)') plt.ylabel('Speed (m/sec)') plt.legend(seriesnames) plt.title(plottitle) plt.show()","title":"Line Charts"},{"location":"lesson8/lesson8/#scatter-plots","text":"A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot # Example 1. A data file containing heights of fathers, mothers, and sons is to be examined df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists daddy = df['father'] ; mommy = df['mother'] ; baby = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red' , label='Father') # one plot series plt.scatter(baby, mommy, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\") df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'>","title":"Scatter Plots"},{"location":"lesson8/lesson8/#histograms","text":"Quilting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson. To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\" import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Gross Gross (Adjusted) Year count 2.000000e+02 2.000000e+02 200.000000 mean 2.216196e+08 5.041983e+08 1986.620000 std 1.441574e+08 2.159814e+08 20.493548 min 9.183673e+06 3.222619e+08 1921.000000 25% 1.087824e+08 3.677804e+08 1973.000000 50% 2.001273e+08 4.388570e+08 1990.000000 75% 3.069535e+08 5.512131e+08 2003.250000 max 9.067234e+08 1.757788e+09 2015.000000","title":"Histograms"},{"location":"lesson8/lesson8/#references","text":"Constructing Horizontal Bar Charts (matplotlib.org documentation) https://matplotlib.org/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html?highlight=horizontal%20bar%20chart How to make a bar chart https://www.geeksforgeeks.org/bar-plot-in-matplotlib/","title":"References"},{"location":"lesson9/lesson9/","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 18 February 2021 Lesson 9 Data Modeling: Statistical Approach This lesson covers concepts related to modeling data - it is the start of several lessons on the subject. The ultimate goal is to explain observed behavior with a model (like \\textbf{F} = m\\textbf{a} ) so that the model can be used to predict behavior. If we are predicting between existing observations, that's interpolation and is relatively straightforward. If we are predicting beyond existing observations, that's called extrapolation and is less straightforward. To get started we will examine the concepts of causality (cause => effect) and correlation, and the use of simulation to generate probability estimates. Objectives To understand the fundamental concepts involved in causality; and the difference between cause and correlation. To understand the fundamental concepts involved in iteration. To understand the fundamental concepts involved in simulation Computational Thinking Concepts The CT concepts include: Algorithm Design => Causality, Iteration, Simulation System Integration => Iteration, Simulation Correlation and Causality What is causality? (A long winded psuedo definition!) Causality is the relationship between causes and effects. The notion of causality does not have a uniform definition in the sciences, and is studied using philosophy and statistics. From the perspective of physics, it is generally believed that causality cannot occur between an effect and an event that is not in the back (past) light cone of said effect. Similarly, a cause could not have an effect outside its front (future) light cone. Here are some recent articles regarding Closed Time Loops, that explains causal consistency. The second paper is by an undergraduate student! https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.125.040605 https://iopscience.iop.org/article/10.1088/1361-6382/aba4bc Both to some extent theoretically support our popular notion of time travel (aka Dr. Who) without pesky paradoxes; someone with creative writing juices, could have a good science fiction career using these papers as a starting thesis! In classical physics, an effect cannot occur before its cause. In Einstein's theory of special relativity, causality means that an effect can not occur from a cause that is not in the back (past) light cone of that event. Similarly, a cause cannot have an effect outside its front (future) light cone. These restrictions are consistent with the assumption that causal influences cannot travel faster than the speed of light and/or backwards in time. In quantum field theory, observables of events with a spacelike relationship, \"elsewhere\", have to commute, so the order of observations or measurements of such observables do not impact each other. Causality in this context should not be confused with Newton's second law, which is related to the conservation of momentum, and is a consequence of the spatial homogeneity of physical laws. The word causality in this context means that all effects must have specific causes. Another requirement, at least valid at the level of human experience, is that cause and effect be mediated across space and time (requirement of contiguity). This requirement has been very influential in the past, in the first place as a result of direct observation of causal processes (like pushing a cart), in the second place as a problematic aspect of Newton's theory of gravitation (attraction of the earth by the sun by means of action at a distance) replacing mechanistic proposals like Descartes' vortex theory; in the third place as an incentive to develop dynamic field theories (e.g., Maxwell's electrodynamics and Einstein's general theory of relativity) restoring contiguity in the transmission of influences in a more successful way than in Descartes' theory. Yada yada bla bla bla ... Correlation (Causality's mimic!) The literary (as in writing!) formulation of causality is a \"why?, because ...\" structure (sort of like if=>then) The answer to a because question, should be the \"cause.\" Many authors use \"since\" to imply cause, but it is incorrect grammar - since answers the question of when? Think \"CAUSE\" => \"EFFECT\" Correlation doesn\u2019t mean cause (although it is a really good predictor of the crap we all buy - its why Amazon is sucessfull) Consider the chart below The correlation between money spent on pets and the number of lawyers is quite good (nearly perfect), so does having pets cause lawyers? Of course not, the general social economic conditions that improve general wealth, and create sufficient disposable income to have pets (here we mean companion animals, not food on the hoof) also creates conditions for laywers to proliferate, hence a good correlation. Nice video : Correlation and Causation https://www.youtube.com/watch?v=1Sa2v7kVEc0 Quoting from http://water.usgs.gov/pubs/twri/twri4a3/ Concentrations of atrazine and nitrate in shallow groundwaters are measured in wells over a several county area. For each sample, the concentration of one is plotted versus the concentration of the other. As atrazine concentrations increase, so do nitrate. How might the strength of this association be measured and summarized? Streams draining the Sierra Nevada mountains in California usually receive less precipitation in November than in other months. Has the amount of November precipitation significantly changed over the last 70 years, showing a gradual change in the climate of the area? How might this be tested? The above situations require a measure of the strength of association between two continuous variables, such as between two chemical concentrations, or between amount of precipitation and time. How do they co-vary? One class of measures are called correlation coefficients. Also important is how the significance of that association can be tested for, to determine whether the observed pattern differs from what is expected due entirely to chance. Whenever a correlation coefficient is calculated, the data should be plotted on a scatterplot. No single numerical measure can substitute for the visual insight gained from a plot. Many different patterns can produce the same correlation coefficient, and similar strengths of relationships can produce differing coefficients, depending on the curvature of the relationship. Implications Most research questions attempt to explain cause and effect. - In experimental research, the relationship is constructed and the experiment is somewhat of a failure if none of the presumed causal (causal == explainatory) variables influence the response (response == effect) - In a data science experimental context, causality may be impossible to establish, however correlations can be established and exploited. In data science, many studies involve observations on a group of individuals, a factor of interest called a treatment (explainatory variable, predictor variable, predictor feature ...), and an outcome (response, effect, state, predicted value ...) measured on each individual. The presumptive establishment of causality takes place in two stages. First, an association is observed. Any relation between the treatment and the outcome is called an association (we can measure the strength of the association using correlation coefficients!). Second, A more careful analysis is used to establish causality. a. One approach would be to control all variables other than the suspected (explainatory) variables, which for any meaningful process is essentially impossible. b. Another approach is to establish randomized control studies: 1. Start with a sample from a population (e.g. volunteers to test Covid 19 vaccines) 2. Randomly assign members to either a. Control group b. Treatment group 3. Expose the two groups identically, except the control group recieves a false (null) treatment 4. Compare the responses of the two groups, if they are same, there exists no evidence that the treatment variable CAUSES a response These concepts can be extended with some ingenuity to engineered systems and natural systems. Consider Data Science Questions: - Does going to school cause flu? - Does flu cause school attendance? - Does going to school contribute to the spread of flu? - Does the spread of flu contribute to the school attendance? - Are there other variables that affects both? a. These are called \u201cconfounding factors\u201d or \u201clurking variables\u201d. b. Cold weather?, more indoor time?, more interaction? Confounding Factors An underlying difference between the two groups (other than the treatment) is called a confounding factor, because it might confound you (that is, mess you up) when you try to reach a conclusion. For example, Cold weather in the previous example. Confounding also occurs when explainatory variables are correlated to another, for instance flood flows are well correlated to drainage area, main channel length, mean annual precipitation, main channel slope, and elevation. However main channel length is itself strongly correlated to drainage area, so much so as to be nearly useless as an explainatory variable when drainage area is retained in a data model. It would be a \"confounding variable\" in this context. Randomization To establish presumptive causality in our data science experiments, we need randomization tools. We can use Python to make psuedo-random choices. There are built-in functions in numpy library under random submodule. The choice function randomly picks one item from an array. The syntax is np.random.choice(array_name) , where array_name is the name of the array from which to make the choice.\u200b #Making Random Choice from an Array (or list) import numpy as np two_groups = np.array(['treatment', 'control']) np.random.choice(two_groups,1) # mylist = ['treatment', 'control'] # this works too # np.random.choice(mylist) array(['treatment'], dtype='<U9') The difference of this function from others that we learned so far, is that it doesn\u2019t give the same result every time. We can roll a dice using this function by randomly selecting from an array from 1 to 6. my_die = np.array(['one', 'two','three', 'four','five', 'six']) np.random.choice(my_die) 'six' # now a bunch of rolls print('roll #1 ',np.random.choice(my_die) ) print('roll #2 ',np.random.choice(my_die) ) print('roll #3 ',np.random.choice(my_die) ) print('roll #4 ',np.random.choice(my_die) ) print('roll #5 ',np.random.choice(my_die) ) print('roll #6 ',np.random.choice(my_die) ) roll #1 four roll #2 four roll #3 four roll #4 five roll #5 six roll #6 one # or multiple rolls, single call myDiceRolls = np.random.choice(my_die,6) print(myDiceRolls) ['six' 'two' 'two' 'six' 'one' 'five'] 'six' We might need to repeat a process multiple times to reach better results or cover more results. Let\u2019s create a game with following rules: If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. my_wallet = 1 # start with 1 dollars def place_a_bet(wallet): print(\"Place your bet!\") if wallet == 0: print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Single play print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) print(\"Amount in my account =:\",my_wallet) Amount in my account =: 1 Place your bet! Roll the die! You Lose, Bummer! Amount in my account =: 0 A more automated solution is to use a for statement to loop over the contents of a sequence. Each result is called iteration. Here we use a for statement in a more realistic way: we print the results of betting five times on the die as described earlier. This process is called simulating the results of five bets. We use the word simulating to remind ourselves that we are not physically rolling dice and exchanging money but using Python to mimic the process. # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' my_wallet = 10 how_many_throws = 1 for i in range(how_many_throws): print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) print(\"After \",i+1,\" plays\") print(CRED + \"Amount in my account =:\",my_wallet,CEND) print(\"_______________________\") Amount in my account =: 10 Place your bet! Roll the die! You win a dollar! After 1 plays \u001b[91mAmount in my account =: 11 \u001b[0m _______________________ Simulation of multiple gamblers/multiple visits to the Casino https://www.inferentialthinking.com/chapters/09/3/Simulation.html outcomes = np.array([]) #null array to store outcomes # redefine functions to suppress output def place_a_bet(wallet): # print(\"Place your bet!\") if wallet == 0: # print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" # print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: #print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: #print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: #print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' how_many_simulations = 100000 for j in range(how_many_simulations): my_wallet = 1 how_many_throws = 30 for i in range(how_many_throws): # print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) # print(\"After \",i+1,\" plays\") # print(CRED + \"Amount in my account =:\",my_wallet,CEND) # print(\"_______________________\") outcomes = np.append(outcomes,my_wallet) # build a histogram chart - outcomes is an array import matplotlib.pyplot as plt from scipy.stats import gamma #ax.hist(r, density=True, histtype='stepfilled', alpha=0.2) plt.hist(outcomes, density=True, bins = 20) plt.xlabel(\"Dollars in Gamer's Wallet\") plt.ylabel('Relative Frequency') #### just a data model, gamma distribution ############## # code below adapted from https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html a = 5 # bit of trial and error x = np.linspace(gamma.ppf(0.001, a),gamma.ppf(0.999, a), 1000) plt.plot(x, gamma.pdf(x, a, loc=-1.25, scale=1),'r-', lw=5, alpha=1.0, label='gamma pdf') ######################################################### # Render the plot plt.show() #print(\"Expected value of wallet (mean) =: \",outcomes.mean()) import pandas as pd df = pd.DataFrame(outcomes) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 count 100000.000000 mean 1.632990 std 1.651133 min 0.000000 25% 0.000000 50% 1.000000 75% 2.000000 max 14.000000 Simulation Simulation is the process of using a computer to mimic a real experiment or process. In this class, those experiments will almost invariably involve chance. To summarize from: https://www.inferentialthinking.com/chapters/09/3/Simulation.html Step 1: What to Simulate: Specify the quantity you want to simulate. For example, you might decide that you want to simulate the outcomes of tosses of a coin. Step 2: Simulating One Value: Figure out how to simulate one value of the quantity you specified in Step 1. (usually turn into a function for readability) Step 3: Number of Repetitions: Decide how many times you want to simulate the quantity. You will have to repeat Step 2 that many times. Step 4: Coding the Simulation: Put it all together in code. Step 5: Interpret the results (plots, Simulation Example Should I change my choice? Based on Monty Hall example from https://youtu.be/Xp6V_lO1ZKA But we already have a small car! (Also watch https://www.youtube.com/watch?v=6Ewq_ytHA7g to learn significance of the small car!) Consider The gist of the game is that a contestent chooses a door, the host reveals one of the unselected doors and offers the contestant a chance to change their choice. Should the contestant stick with her initial choice, or switch to the other door? That is the Monty Hall problem. Using classical probability theory it is straightforward to show that: The chance that the car is behind the originally chosen door is 1/3. After Monty opens the door with the goat, the chance distribution changes. If the contestant switches the decision, he/she doubles the chance. Suppose we have harder situations, can we use this simple problem to learn how to ask complex questions? import numpy as np import pandas as pd import matplotlib.pyplot as plt def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 1', 'Goat 2', 'Car'] Goat 1 Goat 2 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining how_many_games = 10000 for i in np.arange(how_many_games): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Goat 2 Goat 1 Car 1 Goat 1 Goat 2 Car 2 Goat 1 Goat 2 Car 3 Goat 2 Goat 1 Car 4 Goat 2 Goat 1 Car ... ... ... ... 9995 Car Goat 2 Goat 1 9996 Car Goat 1 Goat 2 9997 Car Goat 2 Goat 1 9998 Car Goat 2 Goat 1 9999 Goat 1 Goat 2 Car 10000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show() Interpret Results According to the plot, it is beneficial for the players to switch doors because the initial chance for being correct is only 1/3 Does changing doors have a CAUSAL effect on outcome? ## Various Examples Defect Chances A sample of four electronic components is taken from the output of a production line. The probabilities of the various outcomes are calculated to be: Pr [0 defectives] = 0.6561, Pr [1 defective] = 0.2916, Pr [2 defectives] = 0.0486, Pr [3 defectives] = 0.0036, Pr [4 defectives] = 0.0001. What is the probability of at least one defective? #Method-1 pr_atleast1 = 1-0.6561 print(pr_atleast1) 0.3439 #Method-2 pr_atleast1 = 0.2916+0.0483+0.0036+0.0001 print(pr_atleast1) 0.3436 Common is a Birthday? A class of engineering students consists of 45 people. What is the probability that no two students have birthdays on the same day, not considering the year of birth? To simplify the calculation, assume that there are 365 days in the year and that births are equally likely on all of them. Then what is the probability that some members of the class have birthdays on the same day? Also, vary the number of students in the class from 2 to 200 to see its effect on the probability values. #A student in the class states his birthday. So the probability that he/she has the birthday on that date is 1 pr_first = 1 print(pr_first) 1 #Probability that the second student has different birthday than the first student is 364/365 pr_second = 364/365 print(pr_second) 0.9972602739726028 #Probability that the third student has different birthday than the first and the second students is 363/365 pr_third = 363/365 print(pr_third) 0.9945205479452055 #Probability that the fourth student has different birthday than the first, the second, and the third students is 362/365 pr_fourth = 362/365 print(pr_fourth) 0.9917808219178083 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-1: Looping over a list student_ids = list(range(2,46,1)) pr_nosame = 1 for i in student_ids: pr_nosame = pr_nosame*((365-i+1)/365) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) 0.05902410053422507 0.940975899465775 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop student_ids = np.arange(2,46,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame = np.prod(pr_eachstudent) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-19-e397c0f6a5ec> in <module> 7 #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop 8 ----> 9 student_ids = np.arange(2,46,1) 10 11 pr_eachstudent = ((365-student_ids+1)/365) NameError: name 'np' is not defined #Simulation: Getting the probability for different numbers of total students in the class total_students = np.arange(2,201,1) pr_nosame = [] pr_same = [] for i in total_students: student_ids = np.arange(2,i,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame_total = np.prod(pr_eachstudent) pr_nosame.append(pr_nosame_total) pr_same.append(1 - pr_nosame_total) #Creating a dataframe with columns - number of students and probability import pandas as pd final_data = {'Number of students': total_students, 'Probability': pr_same} df = pd.DataFrame(final_data) print(df) #Creating a scatter plot between number of students and probability that at least a pair of students have the same birthday import matplotlib.pyplot as plt plt.scatter(total_students, pr_same, color = 'blue') plt.xlabel('No. of students in the class') plt.ylabel('P [same birthday]') plt.title('Effect of sample size on the chance of success') Making Hole (and money!) An oil company is bidding for the rights to drill a well in field A and a well in field B. The probability it will drill a well in field A is 40%. If it does, the probability the well will be successful is 45%. The probability it will drill a well in field B is 30%. If it does, the probability the well will be successful is 55%. Calculate each of the following probabilities: a) What is the probability of a successful well in field A? pr_successA = 0.40*0.45 pr_successA 0.18000000000000002 b) What is the probability of a successful well in field B? pr_successB = 0.30*0.55 pr_successB 0.165 c) What is the probability of both a successful well in field A and a successful well in field B? pr_successAB = pr_successA*pr_successB pr_successAB 0.029700000000000004 d) What is the probability of at least one successful well in the two fields together? pr_onesuccess = pr_successA + pr_successB - pr_successAB pr_onesuccess 0.3153 e) What is the probability of no successful well in field A? pr_nosuccessA = (1-0.4)+(0.4*0.55) pr_nosuccessA 0.8200000000000001 f) What is the probability of no successful well in field B? pr_nosuccessB = (1-0.3)+(0.3*0.45) pr_nosuccessB 0.835 g) What is the probability of no successful well in the two fields together? pr_nosuccessAB = 1 - pr_onesuccess pr_nosuccessAB 0.6847 h) What is the probability of exactly one successful well in the two fields together? pr_exactonesuccess = (0.18*0.835)+(0.165*0.82) pr_exactonesuccess 0.28559999999999997 References Ford, Martin. 2009 The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future (p. 107). Acculant Publishing. Kindle Edition. Computational and Inferential Thinking: The Foundations of Data Science. By Ani Adhikari and John DeNero, with Contributions by David Wagner and Henry Milner. Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0). https://www.inferentialthinking.com/chapters/09/Randomness.html # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) ! pwd atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) /home/sensei/1330-textbook-webroot/docs/lesson9","title":"Lesson9"},{"location":"lesson9/lesson9/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 18 February 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"lesson9/lesson9/#lesson-9-data-modeling-statistical-approach","text":"This lesson covers concepts related to modeling data - it is the start of several lessons on the subject. The ultimate goal is to explain observed behavior with a model (like \\textbf{F} = m\\textbf{a} ) so that the model can be used to predict behavior. If we are predicting between existing observations, that's interpolation and is relatively straightforward. If we are predicting beyond existing observations, that's called extrapolation and is less straightforward. To get started we will examine the concepts of causality (cause => effect) and correlation, and the use of simulation to generate probability estimates.","title":"Lesson 9 Data Modeling: Statistical Approach"},{"location":"lesson9/lesson9/#objectives","text":"To understand the fundamental concepts involved in causality; and the difference between cause and correlation. To understand the fundamental concepts involved in iteration. To understand the fundamental concepts involved in simulation","title":"Objectives"},{"location":"lesson9/lesson9/#computational-thinking-concepts","text":"The CT concepts include: Algorithm Design => Causality, Iteration, Simulation System Integration => Iteration, Simulation","title":"Computational Thinking Concepts"},{"location":"lesson9/lesson9/#correlation-and-causality","text":"","title":"Correlation and Causality"},{"location":"lesson9/lesson9/#what-is-causality-a-long-winded-psuedo-definition","text":"Causality is the relationship between causes and effects. The notion of causality does not have a uniform definition in the sciences, and is studied using philosophy and statistics. From the perspective of physics, it is generally believed that causality cannot occur between an effect and an event that is not in the back (past) light cone of said effect. Similarly, a cause could not have an effect outside its front (future) light cone. Here are some recent articles regarding Closed Time Loops, that explains causal consistency. The second paper is by an undergraduate student! https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.125.040605 https://iopscience.iop.org/article/10.1088/1361-6382/aba4bc Both to some extent theoretically support our popular notion of time travel (aka Dr. Who) without pesky paradoxes; someone with creative writing juices, could have a good science fiction career using these papers as a starting thesis! In classical physics, an effect cannot occur before its cause. In Einstein's theory of special relativity, causality means that an effect can not occur from a cause that is not in the back (past) light cone of that event. Similarly, a cause cannot have an effect outside its front (future) light cone. These restrictions are consistent with the assumption that causal influences cannot travel faster than the speed of light and/or backwards in time. In quantum field theory, observables of events with a spacelike relationship, \"elsewhere\", have to commute, so the order of observations or measurements of such observables do not impact each other. Causality in this context should not be confused with Newton's second law, which is related to the conservation of momentum, and is a consequence of the spatial homogeneity of physical laws. The word causality in this context means that all effects must have specific causes. Another requirement, at least valid at the level of human experience, is that cause and effect be mediated across space and time (requirement of contiguity). This requirement has been very influential in the past, in the first place as a result of direct observation of causal processes (like pushing a cart), in the second place as a problematic aspect of Newton's theory of gravitation (attraction of the earth by the sun by means of action at a distance) replacing mechanistic proposals like Descartes' vortex theory; in the third place as an incentive to develop dynamic field theories (e.g., Maxwell's electrodynamics and Einstein's general theory of relativity) restoring contiguity in the transmission of influences in a more successful way than in Descartes' theory. Yada yada bla bla bla ...","title":"What is causality? (A long winded psuedo definition!)"},{"location":"lesson9/lesson9/#correlation-causalitys-mimic","text":"The literary (as in writing!) formulation of causality is a \"why?, because ...\" structure (sort of like if=>then) The answer to a because question, should be the \"cause.\" Many authors use \"since\" to imply cause, but it is incorrect grammar - since answers the question of when? Think \"CAUSE\" => \"EFFECT\" Correlation doesn\u2019t mean cause (although it is a really good predictor of the crap we all buy - its why Amazon is sucessfull) Consider the chart below The correlation between money spent on pets and the number of lawyers is quite good (nearly perfect), so does having pets cause lawyers? Of course not, the general social economic conditions that improve general wealth, and create sufficient disposable income to have pets (here we mean companion animals, not food on the hoof) also creates conditions for laywers to proliferate, hence a good correlation. Nice video : Correlation and Causation https://www.youtube.com/watch?v=1Sa2v7kVEc0 Quoting from http://water.usgs.gov/pubs/twri/twri4a3/ Concentrations of atrazine and nitrate in shallow groundwaters are measured in wells over a several county area. For each sample, the concentration of one is plotted versus the concentration of the other. As atrazine concentrations increase, so do nitrate. How might the strength of this association be measured and summarized? Streams draining the Sierra Nevada mountains in California usually receive less precipitation in November than in other months. Has the amount of November precipitation significantly changed over the last 70 years, showing a gradual change in the climate of the area? How might this be tested? The above situations require a measure of the strength of association between two continuous variables, such as between two chemical concentrations, or between amount of precipitation and time. How do they co-vary? One class of measures are called correlation coefficients. Also important is how the significance of that association can be tested for, to determine whether the observed pattern differs from what is expected due entirely to chance. Whenever a correlation coefficient is calculated, the data should be plotted on a scatterplot. No single numerical measure can substitute for the visual insight gained from a plot. Many different patterns can produce the same correlation coefficient, and similar strengths of relationships can produce differing coefficients, depending on the curvature of the relationship.","title":"Correlation (Causality's mimic!)"},{"location":"lesson9/lesson9/#implications","text":"Most research questions attempt to explain cause and effect. - In experimental research, the relationship is constructed and the experiment is somewhat of a failure if none of the presumed causal (causal == explainatory) variables influence the response (response == effect) - In a data science experimental context, causality may be impossible to establish, however correlations can be established and exploited. In data science, many studies involve observations on a group of individuals, a factor of interest called a treatment (explainatory variable, predictor variable, predictor feature ...), and an outcome (response, effect, state, predicted value ...) measured on each individual. The presumptive establishment of causality takes place in two stages. First, an association is observed. Any relation between the treatment and the outcome is called an association (we can measure the strength of the association using correlation coefficients!). Second, A more careful analysis is used to establish causality. a. One approach would be to control all variables other than the suspected (explainatory) variables, which for any meaningful process is essentially impossible. b. Another approach is to establish randomized control studies: 1. Start with a sample from a population (e.g. volunteers to test Covid 19 vaccines) 2. Randomly assign members to either a. Control group b. Treatment group 3. Expose the two groups identically, except the control group recieves a false (null) treatment 4. Compare the responses of the two groups, if they are same, there exists no evidence that the treatment variable CAUSES a response These concepts can be extended with some ingenuity to engineered systems and natural systems. Consider Data Science Questions: - Does going to school cause flu? - Does flu cause school attendance? - Does going to school contribute to the spread of flu? - Does the spread of flu contribute to the school attendance? - Are there other variables that affects both? a. These are called \u201cconfounding factors\u201d or \u201clurking variables\u201d. b. Cold weather?, more indoor time?, more interaction?","title":"Implications"},{"location":"lesson9/lesson9/#confounding-factors","text":"An underlying difference between the two groups (other than the treatment) is called a confounding factor, because it might confound you (that is, mess you up) when you try to reach a conclusion. For example, Cold weather in the previous example. Confounding also occurs when explainatory variables are correlated to another, for instance flood flows are well correlated to drainage area, main channel length, mean annual precipitation, main channel slope, and elevation. However main channel length is itself strongly correlated to drainage area, so much so as to be nearly useless as an explainatory variable when drainage area is retained in a data model. It would be a \"confounding variable\" in this context.","title":"Confounding Factors"},{"location":"lesson9/lesson9/#randomization","text":"To establish presumptive causality in our data science experiments, we need randomization tools. We can use Python to make psuedo-random choices. There are built-in functions in numpy library under random submodule. The choice function randomly picks one item from an array. The syntax is np.random.choice(array_name) , where array_name is the name of the array from which to make the choice.\u200b #Making Random Choice from an Array (or list) import numpy as np two_groups = np.array(['treatment', 'control']) np.random.choice(two_groups,1) # mylist = ['treatment', 'control'] # this works too # np.random.choice(mylist) array(['treatment'], dtype='<U9') The difference of this function from others that we learned so far, is that it doesn\u2019t give the same result every time. We can roll a dice using this function by randomly selecting from an array from 1 to 6. my_die = np.array(['one', 'two','three', 'four','five', 'six']) np.random.choice(my_die) 'six' # now a bunch of rolls print('roll #1 ',np.random.choice(my_die) ) print('roll #2 ',np.random.choice(my_die) ) print('roll #3 ',np.random.choice(my_die) ) print('roll #4 ',np.random.choice(my_die) ) print('roll #5 ',np.random.choice(my_die) ) print('roll #6 ',np.random.choice(my_die) ) roll #1 four roll #2 four roll #3 four roll #4 five roll #5 six roll #6 one # or multiple rolls, single call myDiceRolls = np.random.choice(my_die,6) print(myDiceRolls) ['six' 'two' 'two' 'six' 'one' 'five'] 'six' We might need to repeat a process multiple times to reach better results or cover more results. Let\u2019s create a game with following rules: If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. my_wallet = 1 # start with 1 dollars def place_a_bet(wallet): print(\"Place your bet!\") if wallet == 0: print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Single play print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) print(\"Amount in my account =:\",my_wallet) Amount in my account =: 1 Place your bet! Roll the die! You Lose, Bummer! Amount in my account =: 0 A more automated solution is to use a for statement to loop over the contents of a sequence. Each result is called iteration. Here we use a for statement in a more realistic way: we print the results of betting five times on the die as described earlier. This process is called simulating the results of five bets. We use the word simulating to remind ourselves that we are not physically rolling dice and exchanging money but using Python to mimic the process. # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' my_wallet = 10 how_many_throws = 1 for i in range(how_many_throws): print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) print(\"After \",i+1,\" plays\") print(CRED + \"Amount in my account =:\",my_wallet,CEND) print(\"_______________________\") Amount in my account =: 10 Place your bet! Roll the die! You win a dollar! After 1 plays \u001b[91mAmount in my account =: 11 \u001b[0m _______________________","title":"Randomization"},{"location":"lesson9/lesson9/#simulation-of-multiple-gamblersmultiple-visits-to-the-casino","text":"https://www.inferentialthinking.com/chapters/09/3/Simulation.html outcomes = np.array([]) #null array to store outcomes # redefine functions to suppress output def place_a_bet(wallet): # print(\"Place your bet!\") if wallet == 0: # print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" # print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: #print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: #print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: #print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' how_many_simulations = 100000 for j in range(how_many_simulations): my_wallet = 1 how_many_throws = 30 for i in range(how_many_throws): # print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) # print(\"After \",i+1,\" plays\") # print(CRED + \"Amount in my account =:\",my_wallet,CEND) # print(\"_______________________\") outcomes = np.append(outcomes,my_wallet) # build a histogram chart - outcomes is an array import matplotlib.pyplot as plt from scipy.stats import gamma #ax.hist(r, density=True, histtype='stepfilled', alpha=0.2) plt.hist(outcomes, density=True, bins = 20) plt.xlabel(\"Dollars in Gamer's Wallet\") plt.ylabel('Relative Frequency') #### just a data model, gamma distribution ############## # code below adapted from https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html a = 5 # bit of trial and error x = np.linspace(gamma.ppf(0.001, a),gamma.ppf(0.999, a), 1000) plt.plot(x, gamma.pdf(x, a, loc=-1.25, scale=1),'r-', lw=5, alpha=1.0, label='gamma pdf') ######################################################### # Render the plot plt.show() #print(\"Expected value of wallet (mean) =: \",outcomes.mean()) import pandas as pd df = pd.DataFrame(outcomes) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 count 100000.000000 mean 1.632990 std 1.651133 min 0.000000 25% 0.000000 50% 1.000000 75% 2.000000 max 14.000000","title":"Simulation of multiple gamblers/multiple visits to the Casino"},{"location":"lesson9/lesson9/#simulation","text":"Simulation is the process of using a computer to mimic a real experiment or process. In this class, those experiments will almost invariably involve chance. To summarize from: https://www.inferentialthinking.com/chapters/09/3/Simulation.html Step 1: What to Simulate: Specify the quantity you want to simulate. For example, you might decide that you want to simulate the outcomes of tosses of a coin. Step 2: Simulating One Value: Figure out how to simulate one value of the quantity you specified in Step 1. (usually turn into a function for readability) Step 3: Number of Repetitions: Decide how many times you want to simulate the quantity. You will have to repeat Step 2 that many times. Step 4: Coding the Simulation: Put it all together in code. Step 5: Interpret the results (plots,","title":"Simulation"},{"location":"lesson9/lesson9/#simulation-example","text":"Should I change my choice? Based on Monty Hall example from https://youtu.be/Xp6V_lO1ZKA But we already have a small car! (Also watch https://www.youtube.com/watch?v=6Ewq_ytHA7g to learn significance of the small car!) Consider The gist of the game is that a contestent chooses a door, the host reveals one of the unselected doors and offers the contestant a chance to change their choice. Should the contestant stick with her initial choice, or switch to the other door? That is the Monty Hall problem. Using classical probability theory it is straightforward to show that: The chance that the car is behind the originally chosen door is 1/3. After Monty opens the door with the goat, the chance distribution changes. If the contestant switches the decision, he/she doubles the chance. Suppose we have harder situations, can we use this simple problem to learn how to ask complex questions? import numpy as np import pandas as pd import matplotlib.pyplot as plt def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 1', 'Goat 2', 'Car'] Goat 1 Goat 2 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining how_many_games = 10000 for i in np.arange(how_many_games): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Goat 2 Goat 1 Car 1 Goat 1 Goat 2 Car 2 Goat 1 Goat 2 Car 3 Goat 2 Goat 1 Car 4 Goat 2 Goat 1 Car ... ... ... ... 9995 Car Goat 2 Goat 1 9996 Car Goat 1 Goat 2 9997 Car Goat 2 Goat 1 9998 Car Goat 2 Goat 1 9999 Goat 1 Goat 2 Car 10000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show()","title":"Simulation Example"},{"location":"lesson9/lesson9/#interpret-results","text":"According to the plot, it is beneficial for the players to switch doors because the initial chance for being correct is only 1/3 Does changing doors have a CAUSAL effect on outcome? ## Various Examples","title":"Interpret Results"},{"location":"lesson9/lesson9/#defect-chances","text":"A sample of four electronic components is taken from the output of a production line. The probabilities of the various outcomes are calculated to be: Pr [0 defectives] = 0.6561, Pr [1 defective] = 0.2916, Pr [2 defectives] = 0.0486, Pr [3 defectives] = 0.0036, Pr [4 defectives] = 0.0001. What is the probability of at least one defective? #Method-1 pr_atleast1 = 1-0.6561 print(pr_atleast1) 0.3439 #Method-2 pr_atleast1 = 0.2916+0.0483+0.0036+0.0001 print(pr_atleast1) 0.3436","title":"Defect Chances"},{"location":"lesson9/lesson9/#common-is-a-birthday","text":"A class of engineering students consists of 45 people. What is the probability that no two students have birthdays on the same day, not considering the year of birth? To simplify the calculation, assume that there are 365 days in the year and that births are equally likely on all of them. Then what is the probability that some members of the class have birthdays on the same day? Also, vary the number of students in the class from 2 to 200 to see its effect on the probability values. #A student in the class states his birthday. So the probability that he/she has the birthday on that date is 1 pr_first = 1 print(pr_first) 1 #Probability that the second student has different birthday than the first student is 364/365 pr_second = 364/365 print(pr_second) 0.9972602739726028 #Probability that the third student has different birthday than the first and the second students is 363/365 pr_third = 363/365 print(pr_third) 0.9945205479452055 #Probability that the fourth student has different birthday than the first, the second, and the third students is 362/365 pr_fourth = 362/365 print(pr_fourth) 0.9917808219178083 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-1: Looping over a list student_ids = list(range(2,46,1)) pr_nosame = 1 for i in student_ids: pr_nosame = pr_nosame*((365-i+1)/365) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) 0.05902410053422507 0.940975899465775 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop student_ids = np.arange(2,46,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame = np.prod(pr_eachstudent) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-19-e397c0f6a5ec> in <module> 7 #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop 8 ----> 9 student_ids = np.arange(2,46,1) 10 11 pr_eachstudent = ((365-student_ids+1)/365) NameError: name 'np' is not defined #Simulation: Getting the probability for different numbers of total students in the class total_students = np.arange(2,201,1) pr_nosame = [] pr_same = [] for i in total_students: student_ids = np.arange(2,i,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame_total = np.prod(pr_eachstudent) pr_nosame.append(pr_nosame_total) pr_same.append(1 - pr_nosame_total) #Creating a dataframe with columns - number of students and probability import pandas as pd final_data = {'Number of students': total_students, 'Probability': pr_same} df = pd.DataFrame(final_data) print(df) #Creating a scatter plot between number of students and probability that at least a pair of students have the same birthday import matplotlib.pyplot as plt plt.scatter(total_students, pr_same, color = 'blue') plt.xlabel('No. of students in the class') plt.ylabel('P [same birthday]') plt.title('Effect of sample size on the chance of success')","title":"Common is a Birthday?"},{"location":"lesson9/lesson9/#making-hole-and-money","text":"An oil company is bidding for the rights to drill a well in field A and a well in field B. The probability it will drill a well in field A is 40%. If it does, the probability the well will be successful is 45%. The probability it will drill a well in field B is 30%. If it does, the probability the well will be successful is 55%. Calculate each of the following probabilities: a) What is the probability of a successful well in field A? pr_successA = 0.40*0.45 pr_successA 0.18000000000000002 b) What is the probability of a successful well in field B? pr_successB = 0.30*0.55 pr_successB 0.165 c) What is the probability of both a successful well in field A and a successful well in field B? pr_successAB = pr_successA*pr_successB pr_successAB 0.029700000000000004 d) What is the probability of at least one successful well in the two fields together? pr_onesuccess = pr_successA + pr_successB - pr_successAB pr_onesuccess 0.3153 e) What is the probability of no successful well in field A? pr_nosuccessA = (1-0.4)+(0.4*0.55) pr_nosuccessA 0.8200000000000001 f) What is the probability of no successful well in field B? pr_nosuccessB = (1-0.3)+(0.3*0.45) pr_nosuccessB 0.835 g) What is the probability of no successful well in the two fields together? pr_nosuccessAB = 1 - pr_onesuccess pr_nosuccessAB 0.6847 h) What is the probability of exactly one successful well in the two fields together? pr_exactonesuccess = (0.18*0.835)+(0.165*0.82) pr_exactonesuccess 0.28559999999999997","title":"Making Hole (and money!)"},{"location":"lesson9/lesson9/#references","text":"Ford, Martin. 2009 The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future (p. 107). Acculant Publishing. Kindle Edition. Computational and Inferential Thinking: The Foundations of Data Science. By Ani Adhikari and John DeNero, with Contributions by David Wagner and Henry Milner. Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0). https://www.inferentialthinking.com/chapters/09/Randomness.html # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) ! pwd atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) /home/sensei/1330-textbook-webroot/docs/lesson9","title":"References"},{"location":"scraps/about/","text":"About this document Put something here about the document, authors, copyright (GPL or MIT Open License) On-Line Book Author's Notes Inserting Code Fragments To insert a code fragment such as print('Hello World') simply indent in the source file used to generate the document print('hello world') These fragments can be cut-and-paste into a JupyterLab notebook. Inserting Images If the image is taken from a URL, use the following: ![image-name (a local tag)](url_to_image_source) Such as: ![image-name](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqn40YbupkMAzY63jYtA6auEmjRfCOvCd0FA&usqp=CAU) Which will render a black swan: If the image is local to the host replace the url with the path to the image. Inserting URL links This is a variation of images, but without the ! , such as [link-name-that-will-display](url_to_link_destimation) For example the code below will link to the black swan search results: [link-to-images-of-black-swans](https://www.google.com/search?q=images+of+black+swan&client=safari&rls=en&sxsrf=ALeKk03oIoQ387TWjJoKzX-D_b7o1to43Q:1613002985584&tbm=isch&source=iu&ictx=1&fir=L2P5MiS1ICLTxM%252CC6BDdJoXT9KcEM%252C_&vet=1&usg=AI4_-kTXrBMpj__xL5IkGCshrXTp04fX3w&sa=X&ved=2ahUKEwiCneivyODuAhVJBs0KHY88CaAQ9QF6BAgUEAE&biw=1447&bih=975#imgrc=i_lxoojURNE3XM) link-to-images-of-black-swans","title":"About this document"},{"location":"scraps/about/#about-this-document","text":"Put something here about the document, authors, copyright (GPL or MIT Open License)","title":"About this document"},{"location":"scraps/about/#on-line-book-authors-notes","text":"Inserting Code Fragments To insert a code fragment such as print('Hello World') simply indent in the source file used to generate the document print('hello world') These fragments can be cut-and-paste into a JupyterLab notebook. Inserting Images If the image is taken from a URL, use the following: ![image-name (a local tag)](url_to_image_source) Such as: ![image-name](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqn40YbupkMAzY63jYtA6auEmjRfCOvCd0FA&usqp=CAU) Which will render a black swan: If the image is local to the host replace the url with the path to the image. Inserting URL links This is a variation of images, but without the ! , such as [link-name-that-will-display](url_to_link_destimation) For example the code below will link to the black swan search results: [link-to-images-of-black-swans](https://www.google.com/search?q=images+of+black+swan&client=safari&rls=en&sxsrf=ALeKk03oIoQ387TWjJoKzX-D_b7o1to43Q:1613002985584&tbm=isch&source=iu&ictx=1&fir=L2P5MiS1ICLTxM%252CC6BDdJoXT9KcEM%252C_&vet=1&usg=AI4_-kTXrBMpj__xL5IkGCshrXTp04fX3w&sa=X&ved=2ahUKEwiCneivyODuAhVJBs0KHY88CaAQ9QF6BAgUEAE&biw=1447&bih=975#imgrc=i_lxoojURNE3XM) link-to-images-of-black-swans","title":"On-Line Book Author's Notes"},{"location":"syllabus/syllabus/","text":"%%html <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Course Description: Introducion to Python programming, its relevant modules and libraries, and computational thinking for solving problems in Data Science. Data science approaches for importing, manipulating, and analyzing data. Modeling and visualizing real-world data sets in various science and engineering disciplines. 3 credit hours comprising of lectures and hands-on lab sessions. This course provides a hands-on learning experience in programming and data science using iPython and JupyterLab. iPython is the interactive python kernel implemented in JupyterLab. Prerequisites: Prior programming background is NOT required. The course is intended for first-year WCOE students (aka engineering foundational) COVID-19 Important Guidelines: If Texas Tech University campus operations are required to change because of health concerns related to the COVID-19 pandemic, it is possible that this course will move to a fully online delivery format. Should that be necessary, students will be advised of technical and/or equipment requirements, including remote proctoring software. Policy on absences resulting from illness: We anticipate that some students may have extended absences. To avoid students feeling compelled to attend in-person class periods when having symptoms or feeling unwell, a standard policy is provided that holds students harmless for illness-related absences (see Section A below). A. Illness-Based Absence Policy (Face-to-Face Classes) If at any time during the semester you are ill, in the interest of your own health and safety as well as the health and safety of your instructors and classmates, you are encouraged not to attend face-to-face class meetings or events. Please review the steps outlined below that you should follow to ensure your absence for illness will be excused. These steps also apply to not participating in synchronous online class meetings if you feel too ill to do so and missing specified assignment due dates in asynchronous online classes because of illness. If you are ill and think the symptoms might be COVID-19-related: Call Student Health Services at 806.743.2848 or your health care provider. During after-hours and on weekends, contact TTU COVID-19 Helpline at TBD. Self-report as soon as possible using the Dean of Students COVID-19 webpage. This website has specific directions about how to upload documentation from a medical provider and what will happen if your illness renders you unable to participate in classes for more than one week. If your illness is determined to be COVID-19-related, all remaining documentation and communication will be handled through the Office of the Dean of Students, including notification of your instructors of the time you may be absent from and may return to classes. If your illness is determined not to be COVID-19-related, please follow steps 2.a-d below. If you are ill and can attribute your symptoms to something other than COVID-19: If your illness renders you unable to attend face-to-face classes, participate in synchronous online classes, or miss specified assignment due dates in asynchronous online classes, you are encouraged to contact either Student Health Services at 806.743.2848 or your health care provider. Note that Student Health Services and your own and other health care providers may arrange virtual visits. During the health provider visit, request a \u201creturn to school\u201d note. E-mail the instructor a picture of that note. Return to class by the next class period after the date indicated on your note. Following the steps outlined above helps to keep your instructors informed about your absences and ensures your absence or missing an assignment due date because of illness will be marked excused. You will still be responsible to complete within a week of returning to class any assignments, quizzes, or exams you miss because of illness. B. Illness-Based Absence Policy (Telepresence/On-Line Classes) Same as above with respect potential to infect others; go to a health care provider if you are ill. Telepresence courses are recorded and will be available on TTU MediaSite and/or YouTube (unlisted). Exercises, Quizzes, and Examinations are all administered by a Learning Management System (Blackboard) and students need to allow enough time to complete and upload their work. Due date adjustments/late submits on case-by-case basis; documentation required as in subsection A above. Course Sections Lesson time, days, and location: Section D04; CRN 64436; 1000-1120 T, TH ; Telepresence Lab Section D66; CRN 64441; 1130-1250 T, TH Section D01; CRN 63306; 1000-1120 T, TH ; Telepresence Lab Section D61; CRN 63744; 1130-1250 T, TH Course Instructor: Instructor: Theodore G. Cleveland, Ph.D., P.E., M. ASCE, F. EWRI Email: theodore.cleveland@ttu.edu (put ENGR 1330 in subject line for email related to this class) Office location: Telepresence ( Zoom ) Office hours: 1000-1100 M, 1600-1700 W or by appointment (meetings will be by Zoom call) Teaching Assistant: Teaching Assistant: Farhang Forghanparast, MSCE Email : Farhang.Forghanparast@ttu.edu Office location: Telepresence ( Zoom ) Office hours: 0900-1000 M; 1700-1800 W Textbook: Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0). Link: https://www.inferentialthinking.com/chapters/intro. Course Contents: Computational thinking for problem-solving: Logical problem solving, decomposition, pattern recognition, abstraction, representation, algorithm design, and generalization. Python Programming: Variables, constants, data types, data structures, strings, math operators boolean operators, expressions, program constructs, functions, looping, I/O files, modules, and database. Data science fundamentals: Experimental setup: Importing and formatting data sets, Displaying data, Data pre-processing. Introductory statistical analysis with Python: Elementary statistics, randomness, sampling, probability distributions, Confidence intervals, hypothesis testing, and A/B testing. Basic data analysis, visualization, and machine learning: Data pre-processing, Supervised/unsupervised learning, Performance evaluation metrics. Learning Outcomes: On completion of the course, students will have * Created Python programs employing computational thinking concepts to * Employed Python libraries relevant to data science. * Downloaded data files from selected public sources and analyzed content. * Created scripts to perform fundamental data analytics and basic visualization. ABET Student Outcomes Engineering: An ability to identify, formulate, and solve complex engineering problems by applying principles of engineering, science, and mathematics. An ability to acquire and apply new knowledge as needed, using appropriate learning strategies. Computer Science: Analyze a complex computing problem and to apply principles of computing and other relevant disciplines to identify solutions. Design, implement, and evaluate a computing-based solution to meet a given set of computing requirements in the context of the program\u2019s discipline. Resources/Tools Platforms for Python Programming (for your own computers) Anaconda platform (https://www.anaconda.com/): Anaconda distribution is an open-source Data Science Distribution Development Platform. It includes Python 3 with over 1,500 data science packages making it easy to manage libraries and dependencies. Available in Linux, Windows, and Mac OS X. Jupyter (https://jupyter.org/): JupyterLab is a web-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: Configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. note Anaconda for MacOS includes a JupyterLab instance, so a separate install is not required. Additional Modules for Python Programming Math module (https://docs.python.org/3/library/math.html): Gives access to the mathematical functions defined by the C standard e.g. factorial, gcd, exponential, logarithm. Operator module (https://docs.python.org/3/library/operator.html): Helps in exporting a set of efficient functions corresponding to the intrinsic operators of Python. For example, the operator add(x,y) is equivalent to the expression x+y. Python Modules for Data Science Scipy module (https://www.scipy.org/): A Python-based ecosystem of open-source software for mathematics, science, and engineering. Some of the core packages are: Numpy: Provides n-dimensional array package Scipy: Fundamental for scientific computing (e.g. linear algorithm, optimization) Matplotlib: Visualizations/2D plotting IPython: Enhanced interactive console <<= this is the kernel used in JupyterLab Pandas: Data structures and data analysis Scikit-learn module (https://scikit-learn.org/stable/): A library for machine learning in Python. It is a simple and efficient tool for predictive data analysis. It is built on NumPy, SciPy, and matplotlib modules. On-Line Options AWS Lightsail Instance (use Windows Server 2000 template; lowest resource provision tier; AWS RDP client, or download and install own RDP client) Then install Anaconda onto the AWS Instance Hardware Requirements Minimal, in fact this syllabus was created using a JupyterLab notebook (as a markdown processor) on a Raspberry Pi 4B, which technically cannot support a JupyterHub, but does. Your current laptop should be fine, or if you only have a chromebook, build an AWS instance. The college of engineering has specific laptop requirements for your other courses that are listed at https://www.depts.ttu.edu/coe/dean/engineeringitservices/buyingtherightcomputer.php Content Server Blackboard is used as the learning management system (LMS) for this class, and it uses web links to a content server at https://3.137.111.182/engr-1330-webroot/ The Blackboard links will generally go directly to a section in the webroot, but feel free to explore by going in the front door! Course Schedule Item Lesson Lab JupyterLab(Python Kernel) and Programming 21Jan2021 Lesson 0 Introduction to Computational Thinking with Data Science: - Computational thinking concepts - Python as a programming environment - Data science and practices - CCMR Approach Computing Environment set up: - Installing Anaconda (Win/MacOS/AWS) \u2013 Jupyter notebooks - Simple Examples 26Jan2021 Lesson 1 Programming Fundamentals: - Data types (int, float, string, bool) - Variables, operators, expressions, basic I/O - String functions and operations Introduction to Python - Data types (e.g. int, float, string, bool) - Expressions 28Jan2021 Lesson 2 Programming Fundamentals: - Data structures: Array, list, tuple, set, dictionary - Conditional statements Introduction to Python - Data structures - Conditional statements 2Feb2021 Lesson 3 Programming Fundamentals: - Loops - Flowcharts Introduction to Python - Loops 4Feb2021 Lesson 4 Programming Fundamentals: - Functions - Variable scope Introduction to Python - Functions - Variable scope 9Feb2021 Lesson 5 Programming Fundamentals: - Class and objects - File handling Introduction to Python - Class and objects - File handling Data Science External Modules 11Feb2021 Lesson 6 Data Representation and Operations: Python library: NumPy - Data representation: Arrays, vectors, matrices - Data operations: Indexing, math functions Exercises on NumPy 16Feb2021 Lesson 7 Data Query and Manipulation: Python Library: Pandas - Data frames: - Create, index, summarize statistics - fill and drop values - read/write to file Exercises on Pandas 18Feb2021 Lesson 8 Data Display: Python Libraries: Matplotlib - Graphing Conventions - Data Display for line charts, bar charts, box plot, scatter plot, and histograms Exercises on data display 23Feb2021 Lesson 9 Data Modeling: - Establishing causality - Randomness - Models as Preciction Machines Exercises on causality and simulation 25Feb2021 Review for Exam-1 (Lessons 0-9) Exam-1 - LMS administered Data Modeling: Statistical Approaches 2Mar2021 Lesson 10 Randomness and Probabilities: - Sampling - Empirical distributions Exercises on probabilities 4Mar2021 Lesson 11 Descriptive Statistics - Location/Center (mean, median,mode) - Dispersion/Spread (variance, standard deviation) - Asymmetry/Skew (Coefficient of Skewness) Descriptive Statistics 9Mar2021 Lesson 12 Distributions: - Normal, LogNormal - Gamma, Weibull - Extreme Value (Gumbell) Exercises on sampling 11Mar2021 Lesson 13 Probability Estimation Modeling - Ranking, order, plotting position - Distribution Fitting ; Method Of Moments; Maximum Likelihood Estimation Exercises 16Mar2021 Lesson 14 Hypothesis testing: - General concept - Assessing data models. Exercises on hypothesis testing 18Mar2021 Lesson 15 Hypothesis testing: -Comparing proportions - Type1 & Type2 errors - Attained significance (p-value) Exercises on hypothesis testing 23Mar2021 Lesson 16 Comparing two samples: A/B Testing Exercises on A/B testing 25Mar2021 Review for Exam-2 (Lessons 10-16) Exam-2 - LMS administered 30Mar2021 17. Confidence intervals Exercises Data Modeling: Regression (Model Fitting) Approaches 1Apr2021 18. Data Modeling: Regression Approach - Linear algebra of equation fitting Exercises 6Apr2021 19. Estimation Modeling by Regression: - Ordinary least squares (OLS) regression - Weighted least squares - Explanitory variables (features) - Response variable(s) exre 8Apr2021 20. Estimation Modeling by Regression: - Residuals - Performance metrics: Accuracy, error - Inference exercises 13Apr2021 21. Estimation Modeling by Regression: - Logistic Regression (a type of classification) Exercises on sample means Data Modeling: Machine Learning Approaches 15Apr2021 22. Data Modeling : The Machine Learning Approach: - Correlation - Training (a model fitting analog) - Confusion matrix, precision, recall, accuracy, F-score. - Making decisions Final projects selection - Project choices - Delivery schedule 20Apr2021 Lesson 23 Evaluation and Making Decisions: - Confusion matrix, precision, recall, accuracy, F-score. - Making decisions KNN with evaluation 22Apr2021 Lesson 24 Classification: - K Nearest Neighbor (KNN) More KNN Demonstration/Examples 27Apr2021 Review for Exam-3 (Lessons 17-24) Exam 3 - LMS administered 29Apr2021 Lesson 25 Classification: - Support Vector Machines (SVM) SVM Demonstration/Examples 4May2021 Lesson 26 Classification: - Artifical Neural Networks (ANN) ANN Demonstration 11May2021 Final Project Report and Link to Video Course Assessment and Grading Criteria: There will be three exams and one comprehensive final project for this course. An AI-supervised online community using the Packback Questions (https://www.packback.co) platform will be used for online discussion about class topics. The platform is the place to pose questions and answers and share codes and problems. Face-to-face sections do not use packback, and use a different scoring distribution In addition, lab notebooks, quizzes, and homework assignments also contribute to the final grade. Late assignments will not be scored. Grades will be based on the following components; weighting is approximate: Assessment Instrument Weight(%) Exam-1 10 Exam-2 10 Exam-3 15 Lab Notebooks & Homework 30 Quizzes 10 Packback 15 Final project 10 Overall total 100 Letter grades will be assigned using the following proportions: Normalized Score Range Letter Grade \u2265 90 A 80-89 B 70-79 C 55-69 D < 55 F Packback Questions Environment Participation is a requirement for this course, and the Packback Questions platform will be used for online discussion about class topics. Packback Questions is an online community where you can be fearlessly curious and ask open-ended questions to build on top of what we are covering in class and relate topics to real-world applications. Packback Requirements: Your participation on Packback will count toward 15% of your overall course grade. There will be a Weekly Sunday at 10:00PM CST deadline for submissions. In order to receive your points per week, you should submit the following per each deadline period: 1 open-ended Question per week with a minimum Curiosity Score of 30, each worth 33.33% of each assignment grade 2 Responses per week with a minimum Curiosity Score of 30, each worth 66.67% of each assignment grade Half credit will be provided for questions and responses that do not meet the minimum curiosity score. How to Register on Packback: An email invitation will be sent to you from help@packback.co prompting you to finish registration. If you don\u2019t receive an email (be sure to check your spam), you may register by following the instructions below: Create an account by navigating to https://questions.packback.co and clicking \u201cSign up for an Account\u201d Note: If you already have an account on Packback you can log in with your credentials. Then enter our class community\u2019s lookup key into the \u201cLooking to join a community you don't see here?\u201d section in Packback at the bottom of the homepage. Community Lookup Key: 1e3bb85a-ddb2-456a-a8fc-178ead55206d Follow the instructions on your screen to finish your registration. Packback will require a paid subscription (~$25). Refer to www.packback.co/product/pricing for more information. How to Get Help from the Packback Team: If you have any questions or concerns about Packback throughout the semester, please read their FAQ at help.packback.co. If you need more help, contact their customer support team directly at help@packback.co. For a brief introduction to Packback Questions and why we are using it in class, watch this video: vimeo.com/packback/Welcome-to-Packback-Questions Classroom Policy: The following activities are not allowed in the classroom: Texting or talking on the cellphone or other electronic devices, and reading non-course related materials. Telepresence (On-line) Courses Obviously electronic devices are vital; disrupting the conference is prohibited, please mute your microphone unless you have a question - consider typing your question into the chat window as well. Be aware of bandwidth issues and remember most lessons and laboratory sessions are recorded and posted on youtube. Recording, editing, and rendering takes awhile, so expect 24-36 hour delay before video is available. ADA Statement: Any student who, because of a disability, may require special arrangements in order to meet the course requirements should contact the instructor as soon as possible to make necessary arrangements. Students must present appropriate verification from Student Disability Services during the instructor's office hours. Please note that instructors are not allowed to provide classroom accommodation to a student until appropriate verification from Student Disability Services has been provided. For additional information, please contact Student Disability Services office in 335 West Hall or call 806.742.2405. Academic Integrity Statement: Academic integrity is taking responsibility for one\u2019s own class and/or course work, being individually accountable, and demonstrating intellectual honesty and ethical behavior. Academic integrity is a personal choice to abide by the standards of intellectual honesty and responsibility. Because education is a shared effort to achieve learning through the exchange of ideas, students, faculty, and staff have the collective responsibility to build mutual trust and respect. Ethical behavior and independent thought are essential for the highest level of academic achievement, which then must be measured. Academic achievement includes scholarship, teaching, and learning, all of which are shared endeavors. Grades are a device used to quantify the successful accumulation of knowledge through learning. Adhering to the standards of academic integrity ensures grades are earned honestly. Academic integrity is the foundation upon which students, faculty, and staff build their educational and professional careers. [Texas Tech University (\u201cUniversity\u201d) Quality Enhancement Plan, Academic Integrity Task Force, 2010]. Religious Holy Day Statement: \u201cReligious holy day\u201d means a holy day observed by a religion whose places of worship are exempt from property taxation under Texas Tax Code \u00a711.20. A student who intends to observe a religious holy day should make that intention known to the instructor prior to the absence. A student who is absent from classes for the observance of a religious holy day shall be allowed to take an examination or complete an assignment scheduled for that day within a reasonable time after the absence. A student who is excused may not be penalized for the absence; however, the instructor may respond appropriately if the student fails to complete the assignment satisfactorily. Ethical Conduct Policy: Cheating is prohibited, and the representation of the work of another person as your own will be grounds for receiving a failing grade in the course. DISCRIMINATION, HARASSMENT, AND SEXUAL VIOLENCE STATEMENT: Texas Tech University is committed to providing and strengthening an educational, working, and living environment where students, faculty, staff, and visitors are free from gender and/or sex discrimination of any kind. Sexual assault, discrimination, harassment, and other Title IX violations are not tolerated by the University. Report any incidents to the Office for Student Rights & Resolution, (806)-742-SAFE (7233) or file a report online at titleix.ttu.edu/students. Faculty and staff members at TTU are committed to connecting you to resources on campus. Some of these available resources are: TTU Student Counseling Center, 806- 742-3674, https://www.depts.ttu.edu/scc/(Provides confidential support on campus.) TTU 24-hour Crisis Helpline, 806-742-5555, (Assists students who are experiencing a mental health or interpersonal violence crisis. If you call the helpline, you will speak with a mental health counselor.) Voice of Hope Lubbock Rape Crisis Center, 806-763-7273, voiceofhopelubbock.org (24-hour hotline that provides support for survivors of sexual violence.) The Risk, Intervention, Safety and Education (RISE) Office, 806-742-2110, https://www.depts.ttu.edu/rise/ (Provides a range of resources and support options focused on prevention education and student wellness.) Texas Tech Police Department, 806-742- 3931,http://www.depts.ttu.edu/ttpd/ (To report criminal activity that occurs on or near Texas Tech campus.) CIVILITY IN THE CLASSROOM STATEMENT: Texas Tech University is a community of faculty, students, and staff that enjoys an expectation of cooperation, professionalism, and civility during the conduct of all forms of university business, including the conduct of student\u2013student and student\u2013faculty interactions in and out of the classroom. Further, the classroom is a setting in which an exchange of ideas and creative thinking should be encouraged and where intellectual growth and development are fostered. Students who disrupt this classroom mission by rude, sarcastic, threatening, abusive or obscene language and/or behavior will be subject to appropriate sanctions according to university policy. Likewise, faculty members are expected to maintain the highest standards of professionalism in all interactions with all constituents of the university. To ensure that you are fully engaged in class discussions and account team meetings during class time, you are expected to do the following: - Maintain the same level of civility and professionalism that would be expected in a face-to-face classroom setting. - Attend all classes regularly. - Log into the video conference on time and remain logged in for the duration of the class period. - Activate your camera so that you are visible to the instructor and other students in the class. If you have concerns about leaving your camera on (such as childcare obligations, privacy issues, or a particular circumstance during a class period), please talk to the instructor. - Refrain from engaging in non-class related activities during class time that create a distraction for other students in the class and/or limit your ability to engage in the course. Failure to meet these expectations may result in the following consequences: 1. Being counted as absent for the class meeting. 2. Not receiving credit for class participation for that class period. 3. Other consequences as stipulated in the syllabus, Texas Tech Code of Student Conduct, or other university policy. Repeated failure to meet expectations (e.g., attendance, participation in class, etc.), in addition to the above consequences, may result in the one or more of the following consequences: 1. Referral to the appropriate Associate Dean. 2. Academic penalty, ranging from a warning to failure of the course. (www.depts.ttu.edu/ethics/matadorchallenge/ethicalprinciples.php). LGBTQIA SUPPORT STATEMENT: I identify as an ally to the lesbian, gay, bisexual, transgender, queer, intersex, and asexual (LGBTQIA) community, and I am available to listen and support you in an affirming manner. I can assist in connecting you with resources on campus to address problems you may face pertaining to sexual orientation and/or gender identity that could interfere with your success at Texas Tech. Please note that additional resources are available through the Office of LGBTQIA within the Center for Campus Life, Student Union Building Room 201, www.lgbtqia.ttu.edu, 806.742.5433.\u201d Office of LGBTQIA, Student Union Building Room 201, www.lgbtqia.ttu.edu, 806.742.5433 Within the Center for Campus Life, the Office serves the Texas Tech community through facilitation and leadership of programming and advocacy efforts. This work is aimed at strengthening the lesbian, gay, bisexual, transgender, queer, intersex, and asexual (LGBTQIA) community and sustaining an inclusive campus that welcomes people of all sexual orientations, gender identities, and gender expressions.","title":"Syllabus"},{"location":"syllabus/syllabus/#engr-1330-computational-thinking-with-data-science","text":"","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"syllabus/syllabus/#course-description","text":"Introducion to Python programming, its relevant modules and libraries, and computational thinking for solving problems in Data Science. Data science approaches for importing, manipulating, and analyzing data. Modeling and visualizing real-world data sets in various science and engineering disciplines. 3 credit hours comprising of lectures and hands-on lab sessions. This course provides a hands-on learning experience in programming and data science using iPython and JupyterLab. iPython is the interactive python kernel implemented in JupyterLab.","title":"Course Description:"},{"location":"syllabus/syllabus/#prerequisites","text":"Prior programming background is NOT required. The course is intended for first-year WCOE students (aka engineering foundational)","title":"Prerequisites:"},{"location":"syllabus/syllabus/#covid-19-important-guidelines","text":"If Texas Tech University campus operations are required to change because of health concerns related to the COVID-19 pandemic, it is possible that this course will move to a fully online delivery format. Should that be necessary, students will be advised of technical and/or equipment requirements, including remote proctoring software. Policy on absences resulting from illness: We anticipate that some students may have extended absences. To avoid students feeling compelled to attend in-person class periods when having symptoms or feeling unwell, a standard policy is provided that holds students harmless for illness-related absences (see Section A below).","title":"COVID-19 Important Guidelines:"},{"location":"syllabus/syllabus/#a-illness-based-absence-policy-face-to-face-classes","text":"If at any time during the semester you are ill, in the interest of your own health and safety as well as the health and safety of your instructors and classmates, you are encouraged not to attend face-to-face class meetings or events. Please review the steps outlined below that you should follow to ensure your absence for illness will be excused. These steps also apply to not participating in synchronous online class meetings if you feel too ill to do so and missing specified assignment due dates in asynchronous online classes because of illness. If you are ill and think the symptoms might be COVID-19-related: Call Student Health Services at 806.743.2848 or your health care provider. During after-hours and on weekends, contact TTU COVID-19 Helpline at TBD. Self-report as soon as possible using the Dean of Students COVID-19 webpage. This website has specific directions about how to upload documentation from a medical provider and what will happen if your illness renders you unable to participate in classes for more than one week. If your illness is determined to be COVID-19-related, all remaining documentation and communication will be handled through the Office of the Dean of Students, including notification of your instructors of the time you may be absent from and may return to classes. If your illness is determined not to be COVID-19-related, please follow steps 2.a-d below. If you are ill and can attribute your symptoms to something other than COVID-19: If your illness renders you unable to attend face-to-face classes, participate in synchronous online classes, or miss specified assignment due dates in asynchronous online classes, you are encouraged to contact either Student Health Services at 806.743.2848 or your health care provider. Note that Student Health Services and your own and other health care providers may arrange virtual visits. During the health provider visit, request a \u201creturn to school\u201d note. E-mail the instructor a picture of that note. Return to class by the next class period after the date indicated on your note. Following the steps outlined above helps to keep your instructors informed about your absences and ensures your absence or missing an assignment due date because of illness will be marked excused. You will still be responsible to complete within a week of returning to class any assignments, quizzes, or exams you miss because of illness.","title":"A. Illness-Based Absence Policy (Face-to-Face Classes)"},{"location":"syllabus/syllabus/#b-illness-based-absence-policy-telepresenceon-line-classes","text":"Same as above with respect potential to infect others; go to a health care provider if you are ill. Telepresence courses are recorded and will be available on TTU MediaSite and/or YouTube (unlisted). Exercises, Quizzes, and Examinations are all administered by a Learning Management System (Blackboard) and students need to allow enough time to complete and upload their work. Due date adjustments/late submits on case-by-case basis; documentation required as in subsection A above.","title":"B. Illness-Based Absence Policy (Telepresence/On-Line Classes)"},{"location":"syllabus/syllabus/#course-sections","text":"Lesson time, days, and location: Section D04; CRN 64436; 1000-1120 T, TH ; Telepresence Lab Section D66; CRN 64441; 1130-1250 T, TH Section D01; CRN 63306; 1000-1120 T, TH ; Telepresence Lab Section D61; CRN 63744; 1130-1250 T, TH","title":"Course Sections"},{"location":"syllabus/syllabus/#course-instructor","text":"Instructor: Theodore G. Cleveland, Ph.D., P.E., M. ASCE, F. EWRI Email: theodore.cleveland@ttu.edu (put ENGR 1330 in subject line for email related to this class) Office location: Telepresence ( Zoom ) Office hours: 1000-1100 M, 1600-1700 W or by appointment (meetings will be by Zoom call)","title":"Course Instructor:"},{"location":"syllabus/syllabus/#teaching-assistant","text":"Teaching Assistant: Farhang Forghanparast, MSCE Email : Farhang.Forghanparast@ttu.edu Office location: Telepresence ( Zoom ) Office hours: 0900-1000 M; 1700-1800 W","title":"Teaching Assistant:"},{"location":"syllabus/syllabus/#textbook","text":"Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0). Link: https://www.inferentialthinking.com/chapters/intro.","title":"Textbook:"},{"location":"syllabus/syllabus/#course-contents","text":"Computational thinking for problem-solving: Logical problem solving, decomposition, pattern recognition, abstraction, representation, algorithm design, and generalization. Python Programming: Variables, constants, data types, data structures, strings, math operators boolean operators, expressions, program constructs, functions, looping, I/O files, modules, and database. Data science fundamentals: Experimental setup: Importing and formatting data sets, Displaying data, Data pre-processing. Introductory statistical analysis with Python: Elementary statistics, randomness, sampling, probability distributions, Confidence intervals, hypothesis testing, and A/B testing. Basic data analysis, visualization, and machine learning: Data pre-processing, Supervised/unsupervised learning, Performance evaluation metrics.","title":"Course Contents:"},{"location":"syllabus/syllabus/#learning-outcomes","text":"On completion of the course, students will have * Created Python programs employing computational thinking concepts to * Employed Python libraries relevant to data science. * Downloaded data files from selected public sources and analyzed content. * Created scripts to perform fundamental data analytics and basic visualization.","title":"Learning Outcomes:"},{"location":"syllabus/syllabus/#abet-student-outcomes","text":"Engineering: An ability to identify, formulate, and solve complex engineering problems by applying principles of engineering, science, and mathematics. An ability to acquire and apply new knowledge as needed, using appropriate learning strategies. Computer Science: Analyze a complex computing problem and to apply principles of computing and other relevant disciplines to identify solutions. Design, implement, and evaluate a computing-based solution to meet a given set of computing requirements in the context of the program\u2019s discipline.","title":"ABET Student Outcomes"},{"location":"syllabus/syllabus/#resourcestools","text":"","title":"Resources/Tools"},{"location":"syllabus/syllabus/#platforms-for-python-programming-for-your-own-computers","text":"Anaconda platform (https://www.anaconda.com/): Anaconda distribution is an open-source Data Science Distribution Development Platform. It includes Python 3 with over 1,500 data science packages making it easy to manage libraries and dependencies. Available in Linux, Windows, and Mac OS X. Jupyter (https://jupyter.org/): JupyterLab is a web-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible: Configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. note Anaconda for MacOS includes a JupyterLab instance, so a separate install is not required.","title":"Platforms for Python Programming (for your own computers)"},{"location":"syllabus/syllabus/#additional-modules-for-python-programming","text":"Math module (https://docs.python.org/3/library/math.html): Gives access to the mathematical functions defined by the C standard e.g. factorial, gcd, exponential, logarithm. Operator module (https://docs.python.org/3/library/operator.html): Helps in exporting a set of efficient functions corresponding to the intrinsic operators of Python. For example, the operator add(x,y) is equivalent to the expression x+y.","title":"Additional Modules for Python Programming"},{"location":"syllabus/syllabus/#python-modules-for-data-science","text":"Scipy module (https://www.scipy.org/): A Python-based ecosystem of open-source software for mathematics, science, and engineering. Some of the core packages are: Numpy: Provides n-dimensional array package Scipy: Fundamental for scientific computing (e.g. linear algorithm, optimization) Matplotlib: Visualizations/2D plotting IPython: Enhanced interactive console <<= this is the kernel used in JupyterLab Pandas: Data structures and data analysis Scikit-learn module (https://scikit-learn.org/stable/): A library for machine learning in Python. It is a simple and efficient tool for predictive data analysis. It is built on NumPy, SciPy, and matplotlib modules.","title":"Python Modules for Data Science"},{"location":"syllabus/syllabus/#on-line-options","text":"AWS Lightsail Instance (use Windows Server 2000 template; lowest resource provision tier; AWS RDP client, or download and install own RDP client) Then install Anaconda onto the AWS Instance","title":"On-Line Options"},{"location":"syllabus/syllabus/#hardware-requirements","text":"Minimal, in fact this syllabus was created using a JupyterLab notebook (as a markdown processor) on a Raspberry Pi 4B, which technically cannot support a JupyterHub, but does. Your current laptop should be fine, or if you only have a chromebook, build an AWS instance. The college of engineering has specific laptop requirements for your other courses that are listed at https://www.depts.ttu.edu/coe/dean/engineeringitservices/buyingtherightcomputer.php","title":"Hardware Requirements"},{"location":"syllabus/syllabus/#content-server","text":"Blackboard is used as the learning management system (LMS) for this class, and it uses web links to a content server at https://3.137.111.182/engr-1330-webroot/ The Blackboard links will generally go directly to a section in the webroot, but feel free to explore by going in the front door!","title":"Content Server"},{"location":"syllabus/syllabus/#course-schedule","text":"Item Lesson Lab JupyterLab(Python Kernel) and Programming 21Jan2021 Lesson 0 Introduction to Computational Thinking with Data Science: - Computational thinking concepts - Python as a programming environment - Data science and practices - CCMR Approach Computing Environment set up: - Installing Anaconda (Win/MacOS/AWS) \u2013 Jupyter notebooks - Simple Examples 26Jan2021 Lesson 1 Programming Fundamentals: - Data types (int, float, string, bool) - Variables, operators, expressions, basic I/O - String functions and operations Introduction to Python - Data types (e.g. int, float, string, bool) - Expressions 28Jan2021 Lesson 2 Programming Fundamentals: - Data structures: Array, list, tuple, set, dictionary - Conditional statements Introduction to Python - Data structures - Conditional statements 2Feb2021 Lesson 3 Programming Fundamentals: - Loops - Flowcharts Introduction to Python - Loops 4Feb2021 Lesson 4 Programming Fundamentals: - Functions - Variable scope Introduction to Python - Functions - Variable scope 9Feb2021 Lesson 5 Programming Fundamentals: - Class and objects - File handling Introduction to Python - Class and objects - File handling Data Science External Modules 11Feb2021 Lesson 6 Data Representation and Operations: Python library: NumPy - Data representation: Arrays, vectors, matrices - Data operations: Indexing, math functions Exercises on NumPy 16Feb2021 Lesson 7 Data Query and Manipulation: Python Library: Pandas - Data frames: - Create, index, summarize statistics - fill and drop values - read/write to file Exercises on Pandas 18Feb2021 Lesson 8 Data Display: Python Libraries: Matplotlib - Graphing Conventions - Data Display for line charts, bar charts, box plot, scatter plot, and histograms Exercises on data display 23Feb2021 Lesson 9 Data Modeling: - Establishing causality - Randomness - Models as Preciction Machines Exercises on causality and simulation 25Feb2021 Review for Exam-1 (Lessons 0-9) Exam-1 - LMS administered Data Modeling: Statistical Approaches 2Mar2021 Lesson 10 Randomness and Probabilities: - Sampling - Empirical distributions Exercises on probabilities 4Mar2021 Lesson 11 Descriptive Statistics - Location/Center (mean, median,mode) - Dispersion/Spread (variance, standard deviation) - Asymmetry/Skew (Coefficient of Skewness) Descriptive Statistics 9Mar2021 Lesson 12 Distributions: - Normal, LogNormal - Gamma, Weibull - Extreme Value (Gumbell) Exercises on sampling 11Mar2021 Lesson 13 Probability Estimation Modeling - Ranking, order, plotting position - Distribution Fitting ; Method Of Moments; Maximum Likelihood Estimation Exercises 16Mar2021 Lesson 14 Hypothesis testing: - General concept - Assessing data models. Exercises on hypothesis testing 18Mar2021 Lesson 15 Hypothesis testing: -Comparing proportions - Type1 & Type2 errors - Attained significance (p-value) Exercises on hypothesis testing 23Mar2021 Lesson 16 Comparing two samples: A/B Testing Exercises on A/B testing 25Mar2021 Review for Exam-2 (Lessons 10-16) Exam-2 - LMS administered 30Mar2021 17. Confidence intervals Exercises Data Modeling: Regression (Model Fitting) Approaches 1Apr2021 18. Data Modeling: Regression Approach - Linear algebra of equation fitting Exercises 6Apr2021 19. Estimation Modeling by Regression: - Ordinary least squares (OLS) regression - Weighted least squares - Explanitory variables (features) - Response variable(s) exre 8Apr2021 20. Estimation Modeling by Regression: - Residuals - Performance metrics: Accuracy, error - Inference exercises 13Apr2021 21. Estimation Modeling by Regression: - Logistic Regression (a type of classification) Exercises on sample means Data Modeling: Machine Learning Approaches 15Apr2021 22. Data Modeling : The Machine Learning Approach: - Correlation - Training (a model fitting analog) - Confusion matrix, precision, recall, accuracy, F-score. - Making decisions Final projects selection - Project choices - Delivery schedule 20Apr2021 Lesson 23 Evaluation and Making Decisions: - Confusion matrix, precision, recall, accuracy, F-score. - Making decisions KNN with evaluation 22Apr2021 Lesson 24 Classification: - K Nearest Neighbor (KNN) More KNN Demonstration/Examples 27Apr2021 Review for Exam-3 (Lessons 17-24) Exam 3 - LMS administered 29Apr2021 Lesson 25 Classification: - Support Vector Machines (SVM) SVM Demonstration/Examples 4May2021 Lesson 26 Classification: - Artifical Neural Networks (ANN) ANN Demonstration 11May2021 Final Project Report and Link to Video","title":"Course Schedule"},{"location":"syllabus/syllabus/#course-assessment-and-grading-criteria","text":"There will be three exams and one comprehensive final project for this course. An AI-supervised online community using the Packback Questions (https://www.packback.co) platform will be used for online discussion about class topics. The platform is the place to pose questions and answers and share codes and problems. Face-to-face sections do not use packback, and use a different scoring distribution In addition, lab notebooks, quizzes, and homework assignments also contribute to the final grade. Late assignments will not be scored. Grades will be based on the following components; weighting is approximate: Assessment Instrument Weight(%) Exam-1 10 Exam-2 10 Exam-3 15 Lab Notebooks & Homework 30 Quizzes 10 Packback 15 Final project 10 Overall total 100 Letter grades will be assigned using the following proportions: Normalized Score Range Letter Grade \u2265 90 A 80-89 B 70-79 C 55-69 D < 55 F","title":"Course Assessment and Grading Criteria:"},{"location":"syllabus/syllabus/#packback-questions-environment","text":"Participation is a requirement for this course, and the Packback Questions platform will be used for online discussion about class topics. Packback Questions is an online community where you can be fearlessly curious and ask open-ended questions to build on top of what we are covering in class and relate topics to real-world applications.","title":"Packback Questions Environment"},{"location":"syllabus/syllabus/#packback-requirements","text":"Your participation on Packback will count toward 15% of your overall course grade. There will be a Weekly Sunday at 10:00PM CST deadline for submissions. In order to receive your points per week, you should submit the following per each deadline period: 1 open-ended Question per week with a minimum Curiosity Score of 30, each worth 33.33% of each assignment grade 2 Responses per week with a minimum Curiosity Score of 30, each worth 66.67% of each assignment grade Half credit will be provided for questions and responses that do not meet the minimum curiosity score.","title":"Packback Requirements:"},{"location":"syllabus/syllabus/#how-to-register-on-packback","text":"An email invitation will be sent to you from help@packback.co prompting you to finish registration. If you don\u2019t receive an email (be sure to check your spam), you may register by following the instructions below: Create an account by navigating to https://questions.packback.co and clicking \u201cSign up for an Account\u201d Note: If you already have an account on Packback you can log in with your credentials. Then enter our class community\u2019s lookup key into the \u201cLooking to join a community you don't see here?\u201d section in Packback at the bottom of the homepage. Community Lookup Key: 1e3bb85a-ddb2-456a-a8fc-178ead55206d Follow the instructions on your screen to finish your registration. Packback will require a paid subscription (~$25). Refer to www.packback.co/product/pricing for more information. How to Get Help from the Packback Team: If you have any questions or concerns about Packback throughout the semester, please read their FAQ at help.packback.co. If you need more help, contact their customer support team directly at help@packback.co. For a brief introduction to Packback Questions and why we are using it in class, watch this video: vimeo.com/packback/Welcome-to-Packback-Questions","title":"How to Register on Packback:"},{"location":"syllabus/syllabus/#classroom-policy","text":"The following activities are not allowed in the classroom: Texting or talking on the cellphone or other electronic devices, and reading non-course related materials.","title":"Classroom Policy:"},{"location":"syllabus/syllabus/#telepresence-on-line-courses","text":"Obviously electronic devices are vital; disrupting the conference is prohibited, please mute your microphone unless you have a question - consider typing your question into the chat window as well. Be aware of bandwidth issues and remember most lessons and laboratory sessions are recorded and posted on youtube. Recording, editing, and rendering takes awhile, so expect 24-36 hour delay before video is available.","title":"Telepresence (On-line) Courses"},{"location":"syllabus/syllabus/#ada-statement","text":"Any student who, because of a disability, may require special arrangements in order to meet the course requirements should contact the instructor as soon as possible to make necessary arrangements. Students must present appropriate verification from Student Disability Services during the instructor's office hours. Please note that instructors are not allowed to provide classroom accommodation to a student until appropriate verification from Student Disability Services has been provided. For additional information, please contact Student Disability Services office in 335 West Hall or call 806.742.2405.","title":"ADA Statement:"},{"location":"syllabus/syllabus/#academic-integrity-statement","text":"Academic integrity is taking responsibility for one\u2019s own class and/or course work, being individually accountable, and demonstrating intellectual honesty and ethical behavior. Academic integrity is a personal choice to abide by the standards of intellectual honesty and responsibility. Because education is a shared effort to achieve learning through the exchange of ideas, students, faculty, and staff have the collective responsibility to build mutual trust and respect. Ethical behavior and independent thought are essential for the highest level of academic achievement, which then must be measured. Academic achievement includes scholarship, teaching, and learning, all of which are shared endeavors. Grades are a device used to quantify the successful accumulation of knowledge through learning. Adhering to the standards of academic integrity ensures grades are earned honestly. Academic integrity is the foundation upon which students, faculty, and staff build their educational and professional careers. [Texas Tech University (\u201cUniversity\u201d) Quality Enhancement Plan, Academic Integrity Task Force, 2010].","title":"Academic Integrity Statement:"},{"location":"syllabus/syllabus/#religious-holy-day-statement","text":"\u201cReligious holy day\u201d means a holy day observed by a religion whose places of worship are exempt from property taxation under Texas Tax Code \u00a711.20. A student who intends to observe a religious holy day should make that intention known to the instructor prior to the absence. A student who is absent from classes for the observance of a religious holy day shall be allowed to take an examination or complete an assignment scheduled for that day within a reasonable time after the absence. A student who is excused may not be penalized for the absence; however, the instructor may respond appropriately if the student fails to complete the assignment satisfactorily.","title":"Religious Holy Day Statement:"},{"location":"syllabus/syllabus/#ethical-conduct-policy","text":"Cheating is prohibited, and the representation of the work of another person as your own will be grounds for receiving a failing grade in the course. DISCRIMINATION, HARASSMENT, AND SEXUAL VIOLENCE STATEMENT: Texas Tech University is committed to providing and strengthening an educational, working, and living environment where students, faculty, staff, and visitors are free from gender and/or sex discrimination of any kind. Sexual assault, discrimination, harassment, and other Title IX violations are not tolerated by the University. Report any incidents to the Office for Student Rights & Resolution, (806)-742-SAFE (7233) or file a report online at titleix.ttu.edu/students. Faculty and staff members at TTU are committed to connecting you to resources on campus. Some of these available resources are: TTU Student Counseling Center, 806- 742-3674, https://www.depts.ttu.edu/scc/(Provides confidential support on campus.) TTU 24-hour Crisis Helpline, 806-742-5555, (Assists students who are experiencing a mental health or interpersonal violence crisis. If you call the helpline, you will speak with a mental health counselor.) Voice of Hope Lubbock Rape Crisis Center, 806-763-7273, voiceofhopelubbock.org (24-hour hotline that provides support for survivors of sexual violence.) The Risk, Intervention, Safety and Education (RISE) Office, 806-742-2110, https://www.depts.ttu.edu/rise/ (Provides a range of resources and support options focused on prevention education and student wellness.) Texas Tech Police Department, 806-742- 3931,http://www.depts.ttu.edu/ttpd/ (To report criminal activity that occurs on or near Texas Tech campus.) CIVILITY IN THE CLASSROOM STATEMENT: Texas Tech University is a community of faculty, students, and staff that enjoys an expectation of cooperation, professionalism, and civility during the conduct of all forms of university business, including the conduct of student\u2013student and student\u2013faculty interactions in and out of the classroom. Further, the classroom is a setting in which an exchange of ideas and creative thinking should be encouraged and where intellectual growth and development are fostered. Students who disrupt this classroom mission by rude, sarcastic, threatening, abusive or obscene language and/or behavior will be subject to appropriate sanctions according to university policy. Likewise, faculty members are expected to maintain the highest standards of professionalism in all interactions with all constituents of the university. To ensure that you are fully engaged in class discussions and account team meetings during class time, you are expected to do the following: - Maintain the same level of civility and professionalism that would be expected in a face-to-face classroom setting. - Attend all classes regularly. - Log into the video conference on time and remain logged in for the duration of the class period. - Activate your camera so that you are visible to the instructor and other students in the class. If you have concerns about leaving your camera on (such as childcare obligations, privacy issues, or a particular circumstance during a class period), please talk to the instructor. - Refrain from engaging in non-class related activities during class time that create a distraction for other students in the class and/or limit your ability to engage in the course. Failure to meet these expectations may result in the following consequences: 1. Being counted as absent for the class meeting. 2. Not receiving credit for class participation for that class period. 3. Other consequences as stipulated in the syllabus, Texas Tech Code of Student Conduct, or other university policy. Repeated failure to meet expectations (e.g., attendance, participation in class, etc.), in addition to the above consequences, may result in the one or more of the following consequences: 1. Referral to the appropriate Associate Dean. 2. Academic penalty, ranging from a warning to failure of the course. (www.depts.ttu.edu/ethics/matadorchallenge/ethicalprinciples.php). LGBTQIA SUPPORT STATEMENT: I identify as an ally to the lesbian, gay, bisexual, transgender, queer, intersex, and asexual (LGBTQIA) community, and I am available to listen and support you in an affirming manner. I can assist in connecting you with resources on campus to address problems you may face pertaining to sexual orientation and/or gender identity that could interfere with your success at Texas Tech. Please note that additional resources are available through the Office of LGBTQIA within the Center for Campus Life, Student Union Building Room 201, www.lgbtqia.ttu.edu, 806.742.5433.\u201d Office of LGBTQIA, Student Union Building Room 201, www.lgbtqia.ttu.edu, 806.742.5433 Within the Center for Campus Life, the Office serves the Texas Tech community through facilitation and leadership of programming and advocacy efforts. This work is aimed at strengthening the lesbian, gay, bisexual, transgender, queer, intersex, and asexual (LGBTQIA) community and sustaining an inclusive campus that welcomes people of all sexual orientations, gender identities, and gender expressions.","title":"Ethical Conduct Policy:"}]}