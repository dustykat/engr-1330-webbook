{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Computational Thinking and Data Science A WebBook to Accompany ENGR 1330 at TTU by Theodore G. Cleveland and Farhang Forghanparast with contributions from : Dinesh Sundaravadivelu Devarajan, Turgut Batuhan Baturalp (Batu), Tanja Karp, Long Nguyen, and Mona Rizvi Copyright \u00a9 2021 Author, The contents of this book are licensed for free consumption under the following license: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0 Introduction This on-line webbook is a collection of lessons, laboratory, and exercises contents for ENGR-1330 sections taught bt the first two authors; students in other sections are welcome to use this as a resource with proper attribution (check with your instructor regarding what they will consider acceptable) suggested citation: Theodore G. Cleveland, Farhang Forghanparast, Dinesh Sundaravadivelu Devarajan, Turgut Batuhan Baturalp (Batu), Tanja Karp, Long Nguyen, and Mona Rizvi. (2021) Computational Thinking and Data Science: A WebBook to Accompany ENGR 1330 at TTU , Whitacre College of Engineering, DOI (pending) https://3.137.111.182/engr-1330-webbook/ Document History This document is a living document and is updated frequently, Python is an ever evolving tool and stuff that works today will be constructively broken by the development team (python.org) in their quest for continuous improvement. Generally these changes occur in the packages (libraries, external modules) and primative python is quite stable. Administrator Notes The lead author built this webbook on a Raspberry Pi 4B (4GB) running Ubuntu 20.XX, an Apache Web Server, a JupyterHub (fully encrypted) with iPython extensions, R core, Latex, and MkDocs with extensions. The deployment hardware is an Amazon Web Services Virtual Private Server (Lightsail Instance) in the West Virginia Server Farm (typically the container is run on x86-64 Xeon hardware) Direct editing on the AWS server is possible, but the typesetting may not render correctly; additionally running some of the notebooks will use up the daily allocation of compute time on the server and the whole thing will hang up; remember to do all development and testing on the Raspberry PI or your laptop. The development server is hardware cloned weekly (it takes 12 hours, during which it is inaccessible) A working backup of this webbook is maintained at https://github.com/dustykat/engr-1330-webbook . About ENGR 1330 Webroot/Webbook The purpose of the webroot repository is to maintain a convienent back-up of course content for rapid migration across servers. The webroot is https://github.com/dustykat/engr-1330-webroot ; this webroot is specific to the lead author's server. This webbook is contained WITHIN the webroot in directory engr-1330-webbook and the .gitignore for the webroot repository contains this directory listing. However the book itself is stand alone, and can be copied directly into another webroot (and linked appropriately) Special Notes The structure is written to work on a web host, with hostname == atomickitty.ddns.net , if you clone to another server you will have the lovely task of changing the links. The string editor sed will become your friend! Materials herein come from many sources, in particular the Data8 repository from UC Berkeley. Sources in notebooks are at least cited by a URL. As the content is matured, proper citations are to be inserted. How to Use Clone the entire repository to /var/www/html/symlink_to_engr-1330-webbook. Have your main index point to this directory i.e. http://your-fqdn-server.org/symlink_to_engr-1330-webroot/ You can see working example at https://atomickitty.ddns.net/engr-1330-webroot/engr-1330-webbook/ctds-psuedocourse/site/ (You will have to set a browser exception to accept the self-signed certificate) Syncronization Notes: Sync with atomickitty.ddns.net:3.137.111.182 (AWS server -- primary and live website copy) Sync with 75.3.84.227:192.168.1.75/ (ATT server Raspberry Pi -- development website copy) Sync with 75.3.84.227:192.168.1.79/ (Macintosh -- developer copy) On-Line Book Author's Notes Inserting Code Fragments To insert a code fragment such as print('Hello World') simply indent in the source file used to generate the document print('hello world') These fragments can be cut-and-paste into a JupyterLab notebook. Inserting Images If the image is taken from a URL, use the following: ![image-name (a local tag)](url_to_image_source) Such as: ![image-name](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqn40YbupkMAzY63jYtA6auEmjRfCOvCd0FA&usqp=CAU) Which will render a black swan: If the image is local to the host replace the url with the path to the image. Inserting URL links This is a variation of images, but without the ! , such as [link-name-that-will-display](url_to_link_destimation) For example the code below will link to the black swan search results: [link-to-images-of-black-swans](https://www.google.com/search?q=images+of+black+swan&client=safari&rls=en&sxsrf=ALeKk03oIoQ387TWjJoKzX-D_b7o1to43Q:1613002985584&tbm=isch&source=iu&ictx=1&fir=L2P5MiS1ICLTxM%252CC6BDdJoXT9KcEM%252C_&vet=1&usg=AI4_-kTXrBMpj__xL5IkGCshrXTp04fX3w&sa=X&ved=2ahUKEwiCneivyODuAhVJBs0KHY88CaAQ9QF6BAgUEAE&biw=1447&bih=975#imgrc=i_lxoojURNE3XM) link-to-images-of-black-swans","title":"Home"},{"location":"#computational-thinking-and-data-science","text":"","title":" Computational Thinking and Data Science "},{"location":"#a-webbook-to-accompany-engr-1330-at-ttu","text":"by Theodore G. Cleveland and Farhang Forghanparast with contributions from : Dinesh Sundaravadivelu Devarajan, Turgut Batuhan Baturalp (Batu), Tanja Karp, Long Nguyen, and Mona Rizvi Copyright \u00a9 2021 Author, The contents of this book are licensed for free consumption under the following license: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0","title":"A WebBook to Accompany ENGR 1330 at TTU "},{"location":"#introduction","text":"This on-line webbook is a collection of lessons, laboratory, and exercises contents for ENGR-1330 sections taught bt the first two authors; students in other sections are welcome to use this as a resource with proper attribution (check with your instructor regarding what they will consider acceptable) suggested citation: Theodore G. Cleveland, Farhang Forghanparast, Dinesh Sundaravadivelu Devarajan, Turgut Batuhan Baturalp (Batu), Tanja Karp, Long Nguyen, and Mona Rizvi. (2021) Computational Thinking and Data Science: A WebBook to Accompany ENGR 1330 at TTU , Whitacre College of Engineering, DOI (pending) https://3.137.111.182/engr-1330-webbook/","title":"Introduction"},{"location":"#document-history","text":"This document is a living document and is updated frequently, Python is an ever evolving tool and stuff that works today will be constructively broken by the development team (python.org) in their quest for continuous improvement. Generally these changes occur in the packages (libraries, external modules) and primative python is quite stable.","title":"Document History"},{"location":"#administrator-notes","text":"The lead author built this webbook on a Raspberry Pi 4B (4GB) running Ubuntu 20.XX, an Apache Web Server, a JupyterHub (fully encrypted) with iPython extensions, R core, Latex, and MkDocs with extensions. The deployment hardware is an Amazon Web Services Virtual Private Server (Lightsail Instance) in the West Virginia Server Farm (typically the container is run on x86-64 Xeon hardware) Direct editing on the AWS server is possible, but the typesetting may not render correctly; additionally running some of the notebooks will use up the daily allocation of compute time on the server and the whole thing will hang up; remember to do all development and testing on the Raspberry PI or your laptop. The development server is hardware cloned weekly (it takes 12 hours, during which it is inaccessible) A working backup of this webbook is maintained at https://github.com/dustykat/engr-1330-webbook .","title":"Administrator Notes"},{"location":"#about-engr-1330-webrootwebbook","text":"The purpose of the webroot repository is to maintain a convienent back-up of course content for rapid migration across servers. The webroot is https://github.com/dustykat/engr-1330-webroot ; this webroot is specific to the lead author's server. This webbook is contained WITHIN the webroot in directory engr-1330-webbook and the .gitignore for the webroot repository contains this directory listing. However the book itself is stand alone, and can be copied directly into another webroot (and linked appropriately)","title":"About ENGR 1330 Webroot/Webbook"},{"location":"#special-notes","text":"The structure is written to work on a web host, with hostname == atomickitty.ddns.net , if you clone to another server you will have the lovely task of changing the links. The string editor sed will become your friend! Materials herein come from many sources, in particular the Data8 repository from UC Berkeley. Sources in notebooks are at least cited by a URL. As the content is matured, proper citations are to be inserted.","title":"Special Notes"},{"location":"#how-to-use","text":"Clone the entire repository to /var/www/html/symlink_to_engr-1330-webbook. Have your main index point to this directory i.e. http://your-fqdn-server.org/symlink_to_engr-1330-webroot/ You can see working example at https://atomickitty.ddns.net/engr-1330-webroot/engr-1330-webbook/ctds-psuedocourse/site/ (You will have to set a browser exception to accept the self-signed certificate)","title":"How to Use"},{"location":"#syncronization-notes","text":"Sync with atomickitty.ddns.net:3.137.111.182 (AWS server -- primary and live website copy) Sync with 75.3.84.227:192.168.1.75/ (ATT server Raspberry Pi -- development website copy) Sync with 75.3.84.227:192.168.1.79/ (Macintosh -- developer copy)","title":"Syncronization Notes:"},{"location":"#on-line-book-authors-notes","text":"Inserting Code Fragments To insert a code fragment such as print('Hello World') simply indent in the source file used to generate the document print('hello world') These fragments can be cut-and-paste into a JupyterLab notebook. Inserting Images If the image is taken from a URL, use the following: ![image-name (a local tag)](url_to_image_source) Such as: ![image-name](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqn40YbupkMAzY63jYtA6auEmjRfCOvCd0FA&usqp=CAU) Which will render a black swan: If the image is local to the host replace the url with the path to the image. Inserting URL links This is a variation of images, but without the ! , such as [link-name-that-will-display](url_to_link_destimation) For example the code below will link to the black swan search results: [link-to-images-of-black-swans](https://www.google.com/search?q=images+of+black+swan&client=safari&rls=en&sxsrf=ALeKk03oIoQ387TWjJoKzX-D_b7o1to43Q:1613002985584&tbm=isch&source=iu&ictx=1&fir=L2P5MiS1ICLTxM%252CC6BDdJoXT9KcEM%252C_&vet=1&usg=AI4_-kTXrBMpj__xL5IkGCshrXTp04fX3w&sa=X&ved=2ahUKEwiCneivyODuAhVJBs0KHY88CaAQ9QF6BAgUEAE&biw=1447&bih=975#imgrc=i_lxoojURNE3XM) link-to-images-of-black-swans","title":"On-Line Book Author's Notes"},{"location":"about/","text":"About ENGR 1330 Webbook The purpose of the webook/repository is to maintain a convienent back-up of course content for rapid migration across servers. Special Notes The structure is written to work on a web host, with hostname == atomickitty.ddns.net , if you clone to another server you will have the lovely task of changing the links. The string editor sed will become your friend! Materials herein come from many sources, in particular the Data8 repository from UC Berkeley. Sources in notebooks are at least cited by a URL. As the content is matured, proper citations are to be inserted. The 3-Readings directory contains copyrighted materials and should be exposed with care on a web server; generally no-one reads anymore, so its probably safe enought to protect using .htaccess simple uid:pwd approach. I use the materials during lectures to point out where I obtain various computational ideas. How to Use Clone the entire repository to /var/www/html/engr-1330-webbook. Have your main index point to this directory i.e. http://your-fqdn-server.org/engr-1330-webroot/ You can see working example at https://3.137.111.182/engr-1330-webbook/ (You will have to set a browser exception to accept the self-signed certificate) Syncronization Notes: Sync with 3.137.111.182/engr-1330-webbook/ (AWS server -- primary and live website copy) Sync with 75.3.84.227:192.168.1.75/ (Raspberry Pi -- developer and backup website copy) Sync with 75.3.84.227:192.168.1.79/ (Macintosh -- developer copy) About this document Put something here about the document, authors, copyright (GPL or MIT Open License) On-Line Book Author's Notes Inserting Code Fragments To insert a code fragment such as print('Hello World') simply indent in the source file used to generate the document print('hello world') These fragments can be cut-and-paste into a JupyterLab notebook. Inserting Images If the image is taken from a URL, use the following: ![image-name (a local tag)](url_to_image_source) Such as: ![image-name](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqn40YbupkMAzY63jYtA6auEmjRfCOvCd0FA&usqp=CAU) Which will render a black swan: If the image is local to the host replace the url with the path to the image. Inserting URL links This is a variation of images, but without the ! , such as [link-name-that-will-display](url_to_link_destimation) For example the code below will link to the black swan search results: [link-to-images-of-black-swans](https://www.google.com/search?q=images+of+black+swan&client=safari&rls=en&sxsrf=ALeKk03oIoQ387TWjJoKzX-D_b7o1to43Q:1613002985584&tbm=isch&source=iu&ictx=1&fir=L2P5MiS1ICLTxM%252CC6BDdJoXT9KcEM%252C_&vet=1&usg=AI4_-kTXrBMpj__xL5IkGCshrXTp04fX3w&sa=X&ved=2ahUKEwiCneivyODuAhVJBs0KHY88CaAQ9QF6BAgUEAE&biw=1447&bih=975#imgrc=i_lxoojURNE3XM) link-to-images-of-black-swans","title":"About"},{"location":"about/#about-engr-1330-webbook","text":"The purpose of the webook/repository is to maintain a convienent back-up of course content for rapid migration across servers.","title":"About ENGR 1330 Webbook"},{"location":"about/#special-notes","text":"The structure is written to work on a web host, with hostname == atomickitty.ddns.net , if you clone to another server you will have the lovely task of changing the links. The string editor sed will become your friend! Materials herein come from many sources, in particular the Data8 repository from UC Berkeley. Sources in notebooks are at least cited by a URL. As the content is matured, proper citations are to be inserted. The 3-Readings directory contains copyrighted materials and should be exposed with care on a web server; generally no-one reads anymore, so its probably safe enought to protect using .htaccess simple uid:pwd approach. I use the materials during lectures to point out where I obtain various computational ideas.","title":"Special Notes"},{"location":"about/#how-to-use","text":"Clone the entire repository to /var/www/html/engr-1330-webbook. Have your main index point to this directory i.e. http://your-fqdn-server.org/engr-1330-webroot/ You can see working example at https://3.137.111.182/engr-1330-webbook/ (You will have to set a browser exception to accept the self-signed certificate)","title":"How to Use"},{"location":"about/#syncronization-notes","text":"Sync with 3.137.111.182/engr-1330-webbook/ (AWS server -- primary and live website copy) Sync with 75.3.84.227:192.168.1.75/ (Raspberry Pi -- developer and backup website copy) Sync with 75.3.84.227:192.168.1.79/ (Macintosh -- developer copy)","title":"Syncronization Notes:"},{"location":"about/#about-this-document","text":"Put something here about the document, authors, copyright (GPL or MIT Open License)","title":"About this document"},{"location":"about/#on-line-book-authors-notes","text":"Inserting Code Fragments To insert a code fragment such as print('Hello World') simply indent in the source file used to generate the document print('hello world') These fragments can be cut-and-paste into a JupyterLab notebook. Inserting Images If the image is taken from a URL, use the following: ![image-name (a local tag)](url_to_image_source) Such as: ![image-name](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQqn40YbupkMAzY63jYtA6auEmjRfCOvCd0FA&usqp=CAU) Which will render a black swan: If the image is local to the host replace the url with the path to the image. Inserting URL links This is a variation of images, but without the ! , such as [link-name-that-will-display](url_to_link_destimation) For example the code below will link to the black swan search results: [link-to-images-of-black-swans](https://www.google.com/search?q=images+of+black+swan&client=safari&rls=en&sxsrf=ALeKk03oIoQ387TWjJoKzX-D_b7o1to43Q:1613002985584&tbm=isch&source=iu&ictx=1&fir=L2P5MiS1ICLTxM%252CC6BDdJoXT9KcEM%252C_&vet=1&usg=AI4_-kTXrBMpj__xL5IkGCshrXTp04fX3w&sa=X&ved=2ahUKEwiCneivyODuAhVJBs0KHY88CaAQ9QF6BAgUEAE&biw=1447&bih=975#imgrc=i_lxoojURNE3XM) link-to-images-of-black-swans","title":"On-Line Book Author's Notes"},{"location":"1-Lessons/Lesson00/lesson0/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: https://3.137.111.182/engr-1330-webroot/engr-1330-webbook/ctds-psuedocourse/docs/1-Lessons/Lesson00/lesson0.ipynb Introduction to Computational Thinking with Data Science: Copyright \u00a9 2021 Theodore G. Cleveland, Farhang Forghanparast, Dinesh Sundaravadivelu Devarajan, Turgut Batuhan Baturalp (Batu), Tanja Karp, Long Nguyen, and Mona Rizvi. The contents of this book are licensed for free consumption under the following license: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) . Last GitHub Commit Date: 13 July 2021 %%html <!-- Script Block to set tables to left alignment --> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} Computational Thinking Concepts Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. Much of what follows is borrowed from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2696102/ . Computational thinking is taking an approach to solving problems, designing systems and understanding human behaviour that draws on concepts fundamental to computing http://www.cs.cmu.edu/~15110-s13/Wing06-ct.pdf . Computational thinking is a kind of analytical thinking: It shares with mathematical thinking in the general ways in which we might approach solving a problem. It shares with engineering thinking in the general ways in which we might approach designing and evaluating a large, complex system that operates within the constraints of the real world. - It shares with scientific thinking in the general ways in which we might approach understanding computability, intelligence, the mind and human behaviour. The essence of computational thinking is abstraction and automation . In computing, we abstract notions beyond the physical dimensions of time and space. Our abstractions are extremely general because they are symbolic, where numeric abstractions are just a special case. CT Foundations CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation) Decomposition Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Examples include: Writing a paper: Introduction Body Conclusion Wide-viewed (Panorama) image: Taking multiple overlapped photos Stitch them Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it too will require assembly (system integration) to produce a desired solution. Abstraction Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve. Books in an online bookstore Important NOT important title Cover color ISBN Author\u2019s hometown Authors ... ... ... Algorithms Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. Image from https://www.newyorker.com/magazine/2021/01/18/whats-wrong-with-the-way-we-work?utm_source=pocket-newtab An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation. Algorithms are unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, can incorporate random input. System Integration (implementation) System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Data Science and Practice Data science is leveraging existing data sources, to create new ones as needed in order to extract meaningful information and actionable insights through business domain expertise, effective communication and results interpretation. Data science uses relevant statistical techniques, programming languages, software packages and libraries, and data infrastructure; The insights are used to drive business decisions and take actions intended to achieve business goals. Why is this important for engineers? Because engineering is a business! A list of typical skills https://elitedatascience.com/data-science-resources : Foundational Skills Programming and Data Manipulation Statistics and Probability Technical Skills Data Collection SQL Data Visualization Applied Machine Learning Business Skills Communication Creativity and Innovation Operations and Strategy Business Analytics Supplementary Skills Natural Language Processing Recommendation Systems Time Series Analysis Practice Projects Competitions Problem Solving Challenges Programming as a problem solving process The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method https://en.wikipedia.org/wiki/Scientific_method is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Generate and evaluate potential solutions Refine and implement a solution Verify and test the solution. For actual computational methods the protocol becomes: Explicitly state the problem Gather and state Input information, Governing equations or principles, and the required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example(s), then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4). Example 1 Problem Solving Process Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background. CCMR Approach A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts https://en.wikipedia.org/wiki/Scaffold_(programming) - a legitimate and valuable engineering activity. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html","title":"Computational Thinking and Data Science"},{"location":"1-Lessons/Lesson00/lesson0/#introduction-to-computational-thinking-with-data-science","text":"Copyright \u00a9 2021 Theodore G. Cleveland, Farhang Forghanparast, Dinesh Sundaravadivelu Devarajan, Turgut Batuhan Baturalp (Batu), Tanja Karp, Long Nguyen, and Mona Rizvi. The contents of this book are licensed for free consumption under the following license: Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) . Last GitHub Commit Date: 13 July 2021 %%html <!-- Script Block to set tables to left alignment --> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;}","title":"Introduction to Computational Thinking with Data Science:"},{"location":"1-Lessons/Lesson00/lesson0/#computational-thinking-concepts","text":"Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. Much of what follows is borrowed from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2696102/ . Computational thinking is taking an approach to solving problems, designing systems and understanding human behaviour that draws on concepts fundamental to computing http://www.cs.cmu.edu/~15110-s13/Wing06-ct.pdf . Computational thinking is a kind of analytical thinking: It shares with mathematical thinking in the general ways in which we might approach solving a problem. It shares with engineering thinking in the general ways in which we might approach designing and evaluating a large, complex system that operates within the constraints of the real world. - It shares with scientific thinking in the general ways in which we might approach understanding computability, intelligence, the mind and human behaviour. The essence of computational thinking is abstraction and automation . In computing, we abstract notions beyond the physical dimensions of time and space. Our abstractions are extremely general because they are symbolic, where numeric abstractions are just a special case.","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson00/lesson0/#ct-foundations","text":"CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation)","title":"CT Foundations"},{"location":"1-Lessons/Lesson00/lesson0/#decomposition","text":"Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Examples include: Writing a paper: Introduction Body Conclusion Wide-viewed (Panorama) image: Taking multiple overlapped photos Stitch them Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution.","title":"Decomposition"},{"location":"1-Lessons/Lesson00/lesson0/#pattern-recognition","text":"Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it too will require assembly (system integration) to produce a desired solution.","title":"Pattern Recognition"},{"location":"1-Lessons/Lesson00/lesson0/#abstraction","text":"Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve. Books in an online bookstore Important NOT important title Cover color ISBN Author\u2019s hometown Authors ... ... ...","title":"Abstraction"},{"location":"1-Lessons/Lesson00/lesson0/#algorithms","text":"Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. Image from https://www.newyorker.com/magazine/2021/01/18/whats-wrong-with-the-way-we-work?utm_source=pocket-newtab An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation. Algorithms are unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, can incorporate random input.","title":"Algorithms"},{"location":"1-Lessons/Lesson00/lesson0/#system-integration-implementation","text":"System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand.","title":"System Integration (implementation)"},{"location":"1-Lessons/Lesson00/lesson0/#data-science-and-practice","text":"Data science is leveraging existing data sources, to create new ones as needed in order to extract meaningful information and actionable insights through business domain expertise, effective communication and results interpretation. Data science uses relevant statistical techniques, programming languages, software packages and libraries, and data infrastructure; The insights are used to drive business decisions and take actions intended to achieve business goals. Why is this important for engineers? Because engineering is a business! A list of typical skills https://elitedatascience.com/data-science-resources : Foundational Skills Programming and Data Manipulation Statistics and Probability Technical Skills Data Collection SQL Data Visualization Applied Machine Learning Business Skills Communication Creativity and Innovation Operations and Strategy Business Analytics Supplementary Skills Natural Language Processing Recommendation Systems Time Series Analysis Practice Projects Competitions Problem Solving Challenges","title":"Data Science and Practice"},{"location":"1-Lessons/Lesson00/lesson0/#programming-as-a-problem-solving-process","text":"The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method https://en.wikipedia.org/wiki/Scientific_method is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Generate and evaluate potential solutions Refine and implement a solution Verify and test the solution. For actual computational methods the protocol becomes: Explicitly state the problem Gather and state Input information, Governing equations or principles, and the required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example(s), then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4).","title":"Programming as a problem solving process"},{"location":"1-Lessons/Lesson00/lesson0/#example-1-problem-solving-process","text":"Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background.","title":"Example 1 Problem Solving Process"},{"location":"1-Lessons/Lesson00/lesson0/#ccmr-approach","text":"A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts https://en.wikipedia.org/wiki/Scaffold_(programming) - a legitimate and valuable engineering activity.","title":"CCMR Approach"},{"location":"1-Lessons/Lesson00/lesson0/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html","title":"Readings"},{"location":"1-Lessons/Lesson01/lesson1/","text":"%%html <!-- Script Block to set tables to left alignment --> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} Download (right-click, save target as ...) this jupyterlab notebook from: https://3.137.111.182/engr-1330-webbook/1-Lessons/Lesson01/lesson1.ipynb ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 13 January 2021 Lesson 1 Problem Solving and Computational Thinking: CT concepts Programming as a problem solving process CCMR Approach Special Script Blocks In the lesson notebooks there will usually be two script blocks, identical to the ones below. The first block identifies the particular computer, the user, and the python kernel in use. The second block sets markdown tables to left edge when rendering. I usually put both blocks at the top of the notebook, just after some kind of title block, as done here. CT Foundations CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation) Decomposition Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Decomposition leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it also will require assembly (system integration) to produce a desired solution. Abstraction Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve. Algorithms Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation. System Integration (implementation) System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Programming as a problem solving process The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method (https://en.wikipedia.org/wiki/Scientific_method) is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Input information Governing equations or principles, and The required output information. Generate and evaluate potential solutions Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4). Example 1 Problem Solving Process Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background. CCMR Approach A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We can give this process a simple acronym CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much doing original programming as we are just scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /home/sensei/1330-textbook-webroot/docs/lesson0 /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Problem Solving with Computational Thinking"},{"location":"1-Lessons/Lesson01/lesson1/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 13 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson01/lesson1/#lesson-1-problem-solving-and-computational-thinking","text":"CT concepts Programming as a problem solving process CCMR Approach","title":"Lesson 1 Problem Solving and Computational Thinking:"},{"location":"1-Lessons/Lesson01/lesson1/#special-script-blocks","text":"In the lesson notebooks there will usually be two script blocks, identical to the ones below. The first block identifies the particular computer, the user, and the python kernel in use. The second block sets markdown tables to left edge when rendering. I usually put both blocks at the top of the notebook, just after some kind of title block, as done here.","title":"Special Script Blocks"},{"location":"1-Lessons/Lesson01/lesson1/#ct-foundations","text":"CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation)","title":"CT Foundations"},{"location":"1-Lessons/Lesson01/lesson1/#decomposition","text":"Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Decomposition leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution.","title":"Decomposition"},{"location":"1-Lessons/Lesson01/lesson1/#pattern-recognition","text":"Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it also will require assembly (system integration) to produce a desired solution.","title":"Pattern Recognition"},{"location":"1-Lessons/Lesson01/lesson1/#abstraction","text":"Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve.","title":"Abstraction"},{"location":"1-Lessons/Lesson01/lesson1/#algorithms","text":"Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation.","title":"Algorithms"},{"location":"1-Lessons/Lesson01/lesson1/#system-integration-implementation","text":"System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand.","title":"System Integration (implementation)"},{"location":"1-Lessons/Lesson01/lesson1/#programming-as-a-problem-solving-process","text":"The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method (https://en.wikipedia.org/wiki/Scientific_method) is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Input information Governing equations or principles, and The required output information. Generate and evaluate potential solutions Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4).","title":"Programming as a problem solving process"},{"location":"1-Lessons/Lesson01/lesson1/#example-1-problem-solving-process","text":"Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background.","title":"Example 1 Problem Solving Process"},{"location":"1-Lessons/Lesson01/lesson1/#ccmr-approach","text":"A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We can give this process a simple acronym CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much doing original programming as we are just scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity.","title":"CCMR Approach"},{"location":"1-Lessons/Lesson01/lesson1/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /home/sensei/1330-textbook-webroot/docs/lesson0 /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Readings"},{"location":"1-Lessons/Lesson02/lesson2/","text":"%%html <!-- Script Block to set tables to left alignment --> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 19 January 2021 Simple Arithmetic Computations: iPython, tokens, and structure Data types (int, float, string, bool) Variables, operators, expressions, basic I/O String functions and operations Programming Fundamentals Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. CT is a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Recall the 5 fundamental CT concepts are: Decomposition: the process of taking a complex problem and breaking it into more manageable sub-problems. Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition: finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method ( automation ) for each occurrence of the pattern. Abstraction : Determine important characteristics of the problem and use these characteristics to create a representation of the problem. Algorithms : Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. System Integration: the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Programming is (generally) writing code in a specific programming language to address a certain problem. In the above list it is largely contained within the algorithms concept. iPython The programming language we will use is Python (actually iPython). Python is an example of a high-level language; there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous: First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with just a few modifications. Low-level programs can run on only one kind of computer (chipset-specific for sure, in some cases hardware specific) and have to be rewritten to run on other processors. (e.g. x86-64 vs. arm7 vs. aarch64 vs. PowerPC ...) Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. The second type of syntax rule pertains to the structure of a statement specifically in the way the tokens are arranged. Tokens and Structure Consider the relativistic equation relating energy, mass, and the speed of light e = m \\cdot c^2 In this equation the tokens are e , m , c , = , \\cdot , and the structure is parsed from left to right as into the token named e place the result of the product of the contents of the tokens m and c^2 . Given that the speed of light is some universal constant, the only things that can change are the contents of m and the resulting change in e . In the above discourse, the tokens e , m , c are names for things that can have values -- we will call these variables (or constants as appropriate). The tokens = , \\cdot , and ~^2 are symbols for various arithmetic operations -- we will call these operators. The structure of the equation is specific -- we will call it a statement. When we attempt to write and execute python scripts - we will make various mistakes; these will generate warnings and errors, which we will repair to make a working program. Consider our equation: #clear all variables# Example Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-4-1c1f1fa5363a> in <module> 1 #clear all variables# Example ----> 2 Energy = Mass * SpeedOfLight**2 NameError: name 'Mass' is not defined Notice how the interpreter tells us that Mass is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-5-a4a52966e6df> in <module> 1 # Example 2 Mass = 1000000 ----> 3 Energy = Mass * SpeedOfLight**2 NameError: name 'SpeedOfLight' is not defined Notice how the interpreter now tells us that SpeedOfLight is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 Now the script ran without any reported errors, but we have not instructed the program on how to produce output. To keep the example simple we will just add a generic print statement. # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 print(\"Energy is:\", Energy, \"Newton meters\") Energy is: 89875517873681764000000 Newton meters Now lets examine our program. Identify the tokens that have values, Identify the tokens that are symbols of operations, identify the structure. Variables Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float). Naming Rules Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print , input , if , while , and for . There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables. Operators The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10. Arithmetic Operators In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x y Raises value in x by value in y. ( e.g. x y) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0 Data Type In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary Integer Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309 Real (Float) A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427 String(Alphanumeric) A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting. Changing Types A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens! 234 876.543 What is your name? Integer as float 234.0 Float as integer 876 Integer as string 234 Integer as hexadecimal 0xea Integer Type <class 'int'> Expressions Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15 Summary So far consider our story - a tool to help with problem solving is CT leading to an algorithm. The tool to implement the algorithm is the program and in our case JupyterLab running iPython interpreter for us. As a formal language we introduced: - tokens - structure From these two constructs we further introduced variables (a kind of token), data types (an abstraction, and arguably a decomposition), and expressions (a structure). We created simple scripts (with errors), examined the errors, corrected our script, and eventually got an answer. So we are well on our way in CT as it applies in Engineering. Programming as a problem solving process Recall the entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). Recall our suggested problem solving protocol: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Refine general solution for deployment (frequent use) Another protocol with the same goal is at https://3.137.111.182/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint/HowToBuildAProgram.html Notice the similarity! Example 2 Problem Solving Process Consider an engineering material problem where we wish to classify whether a material in loaded in the elastic or inelastic region as determined the stress (solid pressure) in a rod for some applied load. The yield stress is the classifier, and once the material yields (begins to fail) it will not carry any additional load (until ultimate failure, when it carries no load). Step 1. Compute the material stress under an applied load; determine if value exceedes yield stress, and report the loading condition Step 2. - Inputs: applied load, cross sectional area, yield stress - Governing equation: \\sigma = \\frac{P}{A} when \\frac{P}{A} is less than the yield stress, and is equal to the yield stress otherwise. - Outputs: The material stress \\sigma , and the classification elastic or inelastic. Step 3. Work a sample problem by-hand for testing the general solution. Assuming the yield stress is 1 million psi (units matter in an actual problem - kind of glossed over here) Applied Load (lbf) Cross Section Area (sq.in.) Stress (psi) Classification 10,000 1.0 10,000 Elastic 10,000 0.1 100,000 Elastic 100,000 0.1 1,000,000 Inelastic The stress requires us to read in the load value, read in the cross sectional area, divide the load by the area, and compare the result to the yield stress. If it exceeds the yield stress, then the actual stress is the yield stress, and the loading is inelastic, otherwise elastic \\sigma = \\frac{P}{A} If \\sigma >= \\text{Yield Stress Report Inelastic} Step 4. Develop a general solution (code) In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. We have not yet learned prompts to get input we simply direct assign values as below (and the conditional execution is the subject of a later lesson) In a simple JupyterLab script # Example 2 Problem Solving Process yield_stress = 1e6 applied_load = 1e5 cross_section = 0.1 computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",computed_stress) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",yield_stress) Inelastic Region: Stress = 1000000.0 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the inputs by user entry,and tidy the output by rounding to only two decimal places. A little CCMR from https://www.geeksforgeeks.org/taking-input-in-python/ gives us a way to deal with the inputs and typecasting. Some more CCMR from https://www.programiz.com/python-programming/methods/built-in/round gets us rounded out! # Example 2 Problem Solving Process yield_stress = float(input('Yield Stress (psi)')) applied_load = float(input('Applied Load (lbf)')) cross_section = float(input('Cross Section Area (sq.in.)')) computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",round(computed_stress,2)) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",round(yield_stress,2)) Yield Stress (psi) 1000000 Applied Load (lbf) 100000 Cross Section Area (sq.in.) 1 Elastic Region: Stress = 100000.0 So the simple task of computing the stress, is a bit more complex when decomposed, that it first appears, but illustrates a five step process (with a refinement step). CCMR Approach A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 3 https://www.inferentialthinking.com/chapters/03/programming-in-python.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python in One Day and Learn It Well. Python for Beginners with Hands-on Project. (Learn Coding Fast with Hands-On Project Book -- Kindle Edition by LCF Publishing (Author), Jamie Chan https://www.amazon.com/Python-2nd-Beginners-Hands-Project-ebook/dp/B071Z2Q6TQ/ref=sr_1_3?dchild=1&keywords=learn+python+in+a+day&qid=1611108340&sr=8-3 Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science) # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Simple Computation"},{"location":"1-Lessons/Lesson02/lesson2/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 19 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson02/lesson2/#simple-arithmetic-computations","text":"iPython, tokens, and structure Data types (int, float, string, bool) Variables, operators, expressions, basic I/O String functions and operations","title":"Simple Arithmetic Computations:"},{"location":"1-Lessons/Lesson02/lesson2/#programming-fundamentals","text":"Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. CT is a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Recall the 5 fundamental CT concepts are: Decomposition: the process of taking a complex problem and breaking it into more manageable sub-problems. Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition: finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method ( automation ) for each occurrence of the pattern. Abstraction : Determine important characteristics of the problem and use these characteristics to create a representation of the problem. Algorithms : Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. System Integration: the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Programming is (generally) writing code in a specific programming language to address a certain problem. In the above list it is largely contained within the algorithms concept.","title":"Programming Fundamentals"},{"location":"1-Lessons/Lesson02/lesson2/#ipython","text":"The programming language we will use is Python (actually iPython). Python is an example of a high-level language; there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous: First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with just a few modifications. Low-level programs can run on only one kind of computer (chipset-specific for sure, in some cases hardware specific) and have to be rewritten to run on other processors. (e.g. x86-64 vs. arm7 vs. aarch64 vs. PowerPC ...) Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. The second type of syntax rule pertains to the structure of a statement specifically in the way the tokens are arranged.","title":"iPython"},{"location":"1-Lessons/Lesson02/lesson2/#tokens-and-structure","text":"Consider the relativistic equation relating energy, mass, and the speed of light e = m \\cdot c^2 In this equation the tokens are e , m , c , = , \\cdot , and the structure is parsed from left to right as into the token named e place the result of the product of the contents of the tokens m and c^2 . Given that the speed of light is some universal constant, the only things that can change are the contents of m and the resulting change in e . In the above discourse, the tokens e , m , c are names for things that can have values -- we will call these variables (or constants as appropriate). The tokens = , \\cdot , and ~^2 are symbols for various arithmetic operations -- we will call these operators. The structure of the equation is specific -- we will call it a statement. When we attempt to write and execute python scripts - we will make various mistakes; these will generate warnings and errors, which we will repair to make a working program. Consider our equation: #clear all variables# Example Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-4-1c1f1fa5363a> in <module> 1 #clear all variables# Example ----> 2 Energy = Mass * SpeedOfLight**2 NameError: name 'Mass' is not defined Notice how the interpreter tells us that Mass is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 Energy = Mass * SpeedOfLight**2 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-5-a4a52966e6df> in <module> 1 # Example 2 Mass = 1000000 ----> 3 Energy = Mass * SpeedOfLight**2 NameError: name 'SpeedOfLight' is not defined Notice how the interpreter now tells us that SpeedOfLight is undefined - so a simple fix is to define it and try again # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 Now the script ran without any reported errors, but we have not instructed the program on how to produce output. To keep the example simple we will just add a generic print statement. # Example Mass = 1000000 #kilograms SpeedOfLight = 299792458 #meters per second Energy = Mass * SpeedOfLight**2 print(\"Energy is:\", Energy, \"Newton meters\") Energy is: 89875517873681764000000 Newton meters Now lets examine our program. Identify the tokens that have values, Identify the tokens that are symbols of operations, identify the structure.","title":"Tokens and Structure"},{"location":"1-Lessons/Lesson02/lesson2/#variables","text":"Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float).","title":"Variables"},{"location":"1-Lessons/Lesson02/lesson2/#naming-rules","text":"Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print , input , if , while , and for . There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables.","title":"Naming Rules"},{"location":"1-Lessons/Lesson02/lesson2/#operators","text":"The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10.","title":"Operators"},{"location":"1-Lessons/Lesson02/lesson2/#arithmetic-operators","text":"In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x y Raises value in x by value in y. ( e.g. x y) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0","title":"Arithmetic Operators"},{"location":"1-Lessons/Lesson02/lesson2/#data-type","text":"In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary","title":"Data Type"},{"location":"1-Lessons/Lesson02/lesson2/#integer","text":"Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309","title":"Integer"},{"location":"1-Lessons/Lesson02/lesson2/#real-float","text":"A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427","title":"Real (Float)"},{"location":"1-Lessons/Lesson02/lesson2/#stringalphanumeric","text":"A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting.","title":"String(Alphanumeric)"},{"location":"1-Lessons/Lesson02/lesson2/#changing-types","text":"A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens! 234 876.543 What is your name? Integer as float 234.0 Float as integer 876 Integer as string 234 Integer as hexadecimal 0xea Integer Type <class 'int'>","title":"Changing Types"},{"location":"1-Lessons/Lesson02/lesson2/#expressions","text":"Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15","title":"Expressions"},{"location":"1-Lessons/Lesson02/lesson2/#summary","text":"So far consider our story - a tool to help with problem solving is CT leading to an algorithm. The tool to implement the algorithm is the program and in our case JupyterLab running iPython interpreter for us. As a formal language we introduced: - tokens - structure From these two constructs we further introduced variables (a kind of token), data types (an abstraction, and arguably a decomposition), and expressions (a structure). We created simple scripts (with errors), examined the errors, corrected our script, and eventually got an answer. So we are well on our way in CT as it applies in Engineering.","title":"Summary"},{"location":"1-Lessons/Lesson02/lesson2/#programming-as-a-problem-solving-process","text":"Recall the entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). Recall our suggested problem solving protocol: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Refine general solution for deployment (frequent use) Another protocol with the same goal is at https://3.137.111.182/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint/HowToBuildAProgram.html Notice the similarity!","title":"Programming as a problem solving process"},{"location":"1-Lessons/Lesson02/lesson2/#example-2-problem-solving-process","text":"Consider an engineering material problem where we wish to classify whether a material in loaded in the elastic or inelastic region as determined the stress (solid pressure) in a rod for some applied load. The yield stress is the classifier, and once the material yields (begins to fail) it will not carry any additional load (until ultimate failure, when it carries no load). Step 1. Compute the material stress under an applied load; determine if value exceedes yield stress, and report the loading condition Step 2. - Inputs: applied load, cross sectional area, yield stress - Governing equation: \\sigma = \\frac{P}{A} when \\frac{P}{A} is less than the yield stress, and is equal to the yield stress otherwise. - Outputs: The material stress \\sigma , and the classification elastic or inelastic. Step 3. Work a sample problem by-hand for testing the general solution. Assuming the yield stress is 1 million psi (units matter in an actual problem - kind of glossed over here) Applied Load (lbf) Cross Section Area (sq.in.) Stress (psi) Classification 10,000 1.0 10,000 Elastic 10,000 0.1 100,000 Elastic 100,000 0.1 1,000,000 Inelastic The stress requires us to read in the load value, read in the cross sectional area, divide the load by the area, and compare the result to the yield stress. If it exceeds the yield stress, then the actual stress is the yield stress, and the loading is inelastic, otherwise elastic \\sigma = \\frac{P}{A} If \\sigma >= \\text{Yield Stress Report Inelastic} Step 4. Develop a general solution (code) In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. We have not yet learned prompts to get input we simply direct assign values as below (and the conditional execution is the subject of a later lesson) In a simple JupyterLab script # Example 2 Problem Solving Process yield_stress = 1e6 applied_load = 1e5 cross_section = 0.1 computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",computed_stress) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",yield_stress) Inelastic Region: Stress = 1000000.0 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the inputs by user entry,and tidy the output by rounding to only two decimal places. A little CCMR from https://www.geeksforgeeks.org/taking-input-in-python/ gives us a way to deal with the inputs and typecasting. Some more CCMR from https://www.programiz.com/python-programming/methods/built-in/round gets us rounded out! # Example 2 Problem Solving Process yield_stress = float(input('Yield Stress (psi)')) applied_load = float(input('Applied Load (lbf)')) cross_section = float(input('Cross Section Area (sq.in.)')) computed_stress = applied_load/cross_section if(computed_stress < yield_stress): print(\"Elastic Region: Stress = \",round(computed_stress,2)) elif(computed_stress >= yield_stress): print(\"Inelastic Region: Stress = \",round(yield_stress,2)) Yield Stress (psi) 1000000 Applied Load (lbf) 100000 Cross Section Area (sq.in.) 1 Elastic Region: Stress = 100000.0 So the simple task of computing the stress, is a bit more complex when decomposed, that it first appears, but illustrates a five step process (with a refinement step).","title":"Example 2 Problem Solving Process"},{"location":"1-Lessons/Lesson02/lesson2/#ccmr-approach","text":"A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity.","title":"CCMR Approach"},{"location":"1-Lessons/Lesson02/lesson2/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 3 https://www.inferentialthinking.com/chapters/03/programming-in-python.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python in One Day and Learn It Well. Python for Beginners with Hands-on Project. (Learn Coding Fast with Hands-On Project Book -- Kindle Edition by LCF Publishing (Author), Jamie Chan https://www.amazon.com/Python-2nd-Beginners-Hands-Project-ebook/dp/B071Z2Q6TQ/ref=sr_1_3?dchild=1&keywords=learn+python+in+a+day&qid=1611108340&sr=8-3 Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science) # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson02/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Readings"},{"location":"1-Lessons/Lesson03/lesson3/","text":"%%html <!-- Script Block to set tables to left alignment --> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 25 January 2021 Lesson 2 Data Structures and Conditional Statements: Data structures; lists, arrays, tuples, sets, dictionaries Name, index, contents; keys Conditional structures; logical compares, block and in-line if Objectives 1) Develop awareness of data structures available in Python to store and manipulate data - Implement arrays (lists), dictionaries, and tuples - Address contents of lists , dictionaries, and tuples 2) Develop awareness of decision making in Python - Implement decision making in Python using using if-then ... conditional statements Data Structures and Conditional Statements Computational thinking (CT) concepts involved are: Decomposition : Data interpretation, manipulation, and analysis of NumPy arrays Abstraction : Data structures; Arrays, lists, tuples, sets, and dictionaries Algorithms : Conditional statements What is a data structure? Data Structures are a specialized means of organizing and storing data in computers in such a way that we can perform operations on the stored data more efficiently. In our iPython world the structures are illustrated in the figure below Lists A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\\\ x_1= 11 \\\\ x_2= 5 \\\\ x_3= 9 \\\\ x_4= 13 \\\\ \\dots \\\\ x_N= 223 \\\\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO . A lot of other languages start at ONE. It's just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9. Arrays Arrays are lists that are used to store only elements of a specific data type - Ordered: Elements in an array can be indexed - Mutable: Elements in an array can be altered Data type that an array must hold is specified using the type code when it is created - \u2018f\u2019 for float - \u2018d\u2019 for double - \u2018i\u2019 for signed int - \u2018I\u2019 for unsigned int More types are listed below Type Code C Data Type Python Data Type Minimum Size in Bytes 'b' signed char int 1 'B' unsigned char int 1 'h' signed short int 2 'H' unsigned short int 2 'i' signed int int 2 'I' unsigned int int 2 'l' signed long int 4 'L' unsigned long int 4 'q' signed long long int 8 'Q' unsigned long long int 8 'f' float float 4 'd' double float 8 To use arrays, a library named \u2018array\u2019 must be imported import array Creating an array that contains signed integer numbers myarray = array.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 import array as arr #import using an alias so the calls dont look so funny myarray = arr.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 Lists: Can store elements of different data types; like arrays they are (arrays are lists, but lists are not quite arrays!) - Ordered: Elements in a list can be indexed - Mutable: Elements in a list can be altered - Mathematical operations must be applied to each element of the list Tuple - A special list A tuple is a special kind of list where the values cannot be changed after the list is created. Such a property is called immutable It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") Tuples are often created as output from packages and functions. Dictionary - A special list A dictionary is a special kind of list where the items are related data PAIRS . It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Dictionary properties - Unordered: Elements in a dictionary cannot be - Mutable elements: Elements in a dictionary can be altered - Immutable keys: Keys in a dictionary cannot be altered Sets - A special list Sets: Are used to store elements of different data types - Unordered: Elements in a set cannot be indexed - Mutable: Elements in a set can be altered - Non-repetition: Elements in a set are unique Elements of a set are enclosed in curly brackets { } - Creating sets that contains different data types - Sets cannot be nested What's the difference between a set and dictionary? From https://stackoverflow.com/questions/34370599/difference-between-dict-and-set-python \"Well, a set is like a dict with keys but no values, and they're both implemented using a hash table. But yes, it's a little annoying that the {} notation denotes an empty dict rather than an empty set , but that's a historical artifact.\" Conditional Statements Decision making via conditional statements is an important step in algorithm design; they control the flow of execution of a program. Conditional statements in Python include: if statement if....else statements if....elif....else statements Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Expressed in a flowchart a block if statement looks like: As psuedo code: if(condition is true): do stuff Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, but more recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs. Comparison The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal. Block if statement The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter. Inline if statement An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 Subpart 3 https://www.inferentialthinking.com/chapters/04/3/Comparison.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson03/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Data Structures and the MATH package"},{"location":"1-Lessons/Lesson03/lesson3/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 25 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson03/lesson3/#lesson-2-data-structures-and-conditional-statements","text":"Data structures; lists, arrays, tuples, sets, dictionaries Name, index, contents; keys Conditional structures; logical compares, block and in-line if","title":"Lesson 2 Data Structures and Conditional Statements:"},{"location":"1-Lessons/Lesson03/lesson3/#objectives","text":"1) Develop awareness of data structures available in Python to store and manipulate data - Implement arrays (lists), dictionaries, and tuples - Address contents of lists , dictionaries, and tuples 2) Develop awareness of decision making in Python - Implement decision making in Python using using if-then ... conditional statements","title":"Objectives"},{"location":"1-Lessons/Lesson03/lesson3/#data-structures-and-conditional-statements","text":"Computational thinking (CT) concepts involved are: Decomposition : Data interpretation, manipulation, and analysis of NumPy arrays Abstraction : Data structures; Arrays, lists, tuples, sets, and dictionaries Algorithms : Conditional statements","title":"Data Structures and Conditional Statements"},{"location":"1-Lessons/Lesson03/lesson3/#what-is-a-data-structure","text":"Data Structures are a specialized means of organizing and storing data in computers in such a way that we can perform operations on the stored data more efficiently. In our iPython world the structures are illustrated in the figure below","title":"What is a data structure?"},{"location":"1-Lessons/Lesson03/lesson3/#lists","text":"A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\\\ x_1= 11 \\\\ x_2= 5 \\\\ x_3= 9 \\\\ x_4= 13 \\\\ \\dots \\\\ x_N= 223 \\\\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO . A lot of other languages start at ONE. It's just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9.","title":"Lists"},{"location":"1-Lessons/Lesson03/lesson3/#arrays","text":"Arrays are lists that are used to store only elements of a specific data type - Ordered: Elements in an array can be indexed - Mutable: Elements in an array can be altered Data type that an array must hold is specified using the type code when it is created - \u2018f\u2019 for float - \u2018d\u2019 for double - \u2018i\u2019 for signed int - \u2018I\u2019 for unsigned int More types are listed below Type Code C Data Type Python Data Type Minimum Size in Bytes 'b' signed char int 1 'B' unsigned char int 1 'h' signed short int 2 'H' unsigned short int 2 'i' signed int int 2 'I' unsigned int int 2 'l' signed long int 4 'L' unsigned long int 4 'q' signed long long int 8 'Q' unsigned long long int 8 'f' float float 4 'd' double float 8 To use arrays, a library named \u2018array\u2019 must be imported import array Creating an array that contains signed integer numbers myarray = array.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 import array as arr #import using an alias so the calls dont look so funny myarray = arr.array('i', [1, 2, 4, 8, 16, 32]) myarray[0] #1-st element, 0-th position 1 Lists: Can store elements of different data types; like arrays they are (arrays are lists, but lists are not quite arrays!) - Ordered: Elements in a list can be indexed - Mutable: Elements in a list can be altered - Mathematical operations must be applied to each element of the list","title":"Arrays"},{"location":"1-Lessons/Lesson03/lesson3/#tuple-a-special-list","text":"A tuple is a special kind of list where the values cannot be changed after the list is created. Such a property is called immutable It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") Tuples are often created as output from packages and functions.","title":"Tuple - A special list"},{"location":"1-Lessons/Lesson03/lesson3/#dictionary-a-special-list","text":"A dictionary is a special kind of list where the items are related data PAIRS . It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Dictionary properties - Unordered: Elements in a dictionary cannot be - Mutable elements: Elements in a dictionary can be altered - Immutable keys: Keys in a dictionary cannot be altered","title":"Dictionary - A special list"},{"location":"1-Lessons/Lesson03/lesson3/#sets-a-special-list","text":"Sets: Are used to store elements of different data types - Unordered: Elements in a set cannot be indexed - Mutable: Elements in a set can be altered - Non-repetition: Elements in a set are unique Elements of a set are enclosed in curly brackets { } - Creating sets that contains different data types - Sets cannot be nested","title":"Sets - A special list"},{"location":"1-Lessons/Lesson03/lesson3/#whats-the-difference-between-a-set-and-dictionary","text":"From https://stackoverflow.com/questions/34370599/difference-between-dict-and-set-python \"Well, a set is like a dict with keys but no values, and they're both implemented using a hash table. But yes, it's a little annoying that the {} notation denotes an empty dict rather than an empty set , but that's a historical artifact.\"","title":"What's the difference between a set and dictionary?"},{"location":"1-Lessons/Lesson03/lesson3/#conditional-statements","text":"Decision making via conditional statements is an important step in algorithm design; they control the flow of execution of a program. Conditional statements in Python include: if statement if....else statements if....elif....else statements Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Expressed in a flowchart a block if statement looks like: As psuedo code: if(condition is true): do stuff Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, but more recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs.","title":"Conditional Statements"},{"location":"1-Lessons/Lesson03/lesson3/#comparison","text":"The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal.","title":"Comparison"},{"location":"1-Lessons/Lesson03/lesson3/#block-if-statement","text":"The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter.","title":"Block if statement"},{"location":"1-Lessons/Lesson03/lesson3/#inline-if-statement","text":"An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops.","title":"Inline if statement"},{"location":"1-Lessons/Lesson03/lesson3/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 Subpart 3 https://www.inferentialthinking.com/chapters/04/3/Comparison.html Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 4 https://www.inferentialthinking.com/chapters/04/Data_Types.html Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 compthink /home/compthink/engr-1330-webroot/1-Lessons/Lesson03/OriginalPowerpoint /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Readings"},{"location":"1-Lessons/Lesson04/lesson4/","text":"ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 31 January 2021 Lesson 4 Program Flow Control Structures for Repetition: Controlled repetition Structured FOR Loop Structured WHILE Loop Representing computational processes with flowcharts, a graphical abstraction Special Script Blocks %%html <!-- Script Block to set tables to left alignment --> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} Objectives 1) Develop awareness of loops, and their utility in automation. - To understand loop types available in Python. - To understand and implement loops in various examples and configurations. 2) Develop awareness of flowcharts as a tool for: - Post-development documentation - Pre-development program design Repetition and Loops Computational thinking (CT) concepts involved are: Decomposition : Break a problem down into smaller pieces; the body of tasks in one repetition of a loop represent decomposition of the entire sets of repeated activities Pattern Recognition : Finding similarities between things; the body of tasks in one repetition of a loop is the pattern, the indices and components that change are how we leverage reuse Abstraction : Pulling out specific differences to make one solution work for multiple problems Algorithms : A list of steps that you can follow to finish a task The action of doing something over and over again (repetition) is called a loop. Basically, Loops repeats a portion of code a finite number of times until a process is complete. Repetitive tasks are very common and essential in programming. They save time in coding, minimize coding errors, and leverage the speed of electronic computation. Loop Analogs If you think any mass manufacturing process, we apply the same process again and again. Even for something very simple such as preparing a peanut butter sandwich: Consider the flowchart in Figure 1, it represents a decomposition of sandwich assembly, but at a high level -- for instance, Gather Ingredients contains a lot of substeps that would need to be decomposed if fully automated assembly were to be accomplished; nevertheless lets stipulate that this flowchart will indeed construct a single sandwich. Figure 1 Supervisory Flowchart Sandwich Assembly (adapted from http://www.str-tn.org/subway_restaurant_training_manual.pdf) If we need to make 1000 peanut butter sandwichs we would then issue a directive to: 1) Implement sandwich assembly, repeat 999 times (repeat is the loop structure) (A serial structure, 1 sandwich artist, doing same job over and over again) OR 2) Implement 1000 sandwich assembly threads (A parallel structure, 1000 sandwich artists doing same job once) In general because we dont want to idle 999 sandwich artists, we would choose the serial structure, which frees 999 people to ask the existential question \"would you like fries with that?\" All cynicism aside, an automated process such as a loop, is typical in computational processing. Aside NVIDIA CUDA, and AMD OpenGL compilers can detect the structure above, and if there are enough GPU threads available , create the 1000 sandwich artists (1000 GPU threads), and run the process in parallel -- the actual workload is unchanged in a thermodynamic sense, but the apparent time (in human terms) spent in sandwich creation is a fraction of the serial approach. This parallelization is called unrolling the loop, and is a pretty common optimization step during compilation. This kind of programming is outside the scope of this class. Main attractiveness of loops is: - Leveraging pattern matching and automation - Code is more organized and shorter,because a loop is a sequence of instructions that is continually repeated until a certain condition is reached. There are 2 main types loops based on the repetition control condition; for loops and while loops. For Loop (Count controlled repetition structure) Count-controlled repetition is also called definite repetition because the number of repetitions is known before the loop begins executing. When we do not know in advance the number of times we want to execute a statement, we cannot use count-controlled repetition. In such an instance, we would use sentinel-controlled repetition. A count-controlled repetition will exit after running a certain number of times. The count is kept in a variable called an index or counter. When the index reaches a certain value (the loop bound) the loop will end. Count-controlled repetition requires control variable (or loop counter) initial value of the control variable increment (or decrement) by which the control variable is modified each iteration through the loop condition that tests for the final value of the control variable We can use both for and while loops, for count controlled repetition, but the for loop in combination with the range() function is more common. Structured FOR loop We have seen the for loop already, but we will formally introduce it here. The for loop executes a block of code repeatedly until the condition in the for statement is no longer true. Looping through an iterable An iterable is anything that can be looped over - typically a list, string, or tuple. The syntax for looping through an iterable is illustrated by an example. First a generic syntax for a in iterable: print(a) Notice our friends the colon : and the indentation. The range() function to create an iterable The range(begin,end,increment) function will create an iterable starting at a value of begin, in steps defined by increment ( begin += increment ), ending at end . So a generic syntax becomes for a in range(begin,end,increment): print(a) The examples that follow are count-controlled repetition (increment skip if greater) Example for loops # sum numbers from 1 to n howmany = int(input('Enter N')) accumulator = 0.0 for i in range(1,howmany+1,1): accumulator = accumulator + float(i) print( 'Sum from 1 to ',howmany, 'is %.3f' % accumulator ) Enter N 33 Sum from 1 to 33 is 561.000 # sum even numbers from 1 to n howmany = int(input('Enter N')) accumulator = 0.0 for i in range(1,howmany+1,1): if i%2 == 0: accumulator = accumulator + float(i) print( 'Sum of Evens from 1 to ',howmany, 'is %.3f' % accumulator ) Enter N 33 Sum of Evens from 1 to 33 is 272.000 howmany = int(input('Enter N')) linetoprint='' for i in range(1,howmany+1,1): linetoprint=linetoprint + '*' print(linetoprint) Enter N 33 * ** *** **** ***** ****** ******* ******** ********* ********** *********** ************ ************* ************** *************** **************** ***************** ****************** ******************* ******************** ********************* ********************** *********************** ************************ ************************* ************************** *************************** **************************** ***************************** ****************************** ******************************* ******************************** ********************************* Sentinel-controlled repetition. When loop control is based on the value of what we are processing, sentinel-controlled repetition is used. Sentinel-controlled repetition is also called indefinite repetition because it is not known in advance how many times the loop will be executed. It is a repetition procedure for solving a problem by using a sentinel value (also called a signal value, a dummy value or a flag value) to indicate \"end of process\". The sentinel value itself need not be a part of the processed data. One common example of using sentinel-controlled repetition is when we are processing data from a file and we do not know in advance when we would reach the end of the file. We can use both for and while loops, for Sentinel controlled repetition, but the while loop is more common. Structured WHILE loop The while loop repeats a block of instructions inside the loop while a condition remainsvtrue. First a generic syntax while condition is true: execute a execute b .... Notice our friends the colon : and the indentation again. Example while loops # sum numbers from 1 to n howmany = int(input('Enter N')) accumulator = 0.0 counter = 1 while counter <= howmany: accumulator = accumulator + float(counter) counter += 1 print( 'Sum from 1 to ',howmany, 'is %.3f' % accumulator ) Enter N 33 Sum from 1 to 33 is 561.000 # sum even numbers from 1 to n howmany = int(input('Enter N')) accumulator = 0.0 counter = 1 while counter <= howmany: if counter%2 == 0: accumulator = accumulator + float(counter) counter += 1 print( 'Sum of Evens 1 to ',howmany, 'is %.3f' % accumulator ) Enter N 33 Sum of Evens 1 to 33 is 272.000 howmany = int(input('Enter N')) linetoprint='' counter = 1 while counter <= howmany: linetoprint=linetoprint + '*' counter += 1 print(linetoprint) Enter N 33 * ** *** **** ***** ****** ******* ******** ********* ********** *********** ************ ************* ************** *************** **************** ***************** ****************** ******************* ******************** ********************* ********************** *********************** ************************ ************************* ************************** *************************** **************************** ***************************** ****************************** ******************************* ******************************** ********************************* Nested Repetition Nested repetition is when a control structure is placed inside of the body or main part of another control structure. break to exit out of a loop Sometimes you may want to exit the loop when a certain condition different from the counting condition is met. Perhaps you are looping through a list and want to exit when you find the first element in the list that matches some criterion. The break keyword is useful for such an operation. For example run the following program: # j = 0 for i in range(0,9,1): j += 2 print (\"i = \",i,\"j = \",j) if j == 6: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 j = 0 for i in range(0,5,1): j += 2 print( \"i = \",i,\"j = \",j) if j == 7: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 i = 3 j = 8 i = 4 j = 10 In the first case, the for loop only executes 3 times before the condition j == 6 is TRUE and the loop is exited. In the second case, j == 7 never happens so the loop completes all its anticipated traverses. In both cases an if statement was used within a for loop. Such \"mixed\" control structures are quite common (and pretty necessary). A while loop contained within a for loop, with several if statements would be very common and such a structure is called nested control. There is typically an upper limit to nesting but the limit is pretty large - easily in the hundreds. It depends on the language and the system architecture ; suffice to say it is not a practical limit except possibly for general-domain AI applications. We can also do mundane activities and leverage loops, arithmetic, and format codes to make useful tables like import math # package that contains cosine print(\" Cosines \") print(\" x \",\"|\",\" cos(x) \") print(\"--------|--------\") for i in range(0,157,1): x = float(i)*0.1 print(\"%.3f\" % x, \" |\", \" %.4f \" % math.cos(x)) # note the format code and the placeholder % and syntax Cosines x | cos(x) --------|-------- 0.000 | 1.0000 0.100 | 0.9950 0.200 | 0.9801 0.300 | 0.9553 0.400 | 0.9211 0.500 | 0.8776 0.600 | 0.8253 0.700 | 0.7648 0.800 | 0.6967 0.900 | 0.6216 1.000 | 0.5403 1.100 | 0.4536 1.200 | 0.3624 1.300 | 0.2675 1.400 | 0.1700 1.500 | 0.0707 1.600 | -0.0292 1.700 | -0.1288 1.800 | -0.2272 1.900 | -0.3233 2.000 | -0.4161 2.100 | -0.5048 2.200 | -0.5885 2.300 | -0.6663 2.400 | -0.7374 2.500 | -0.8011 2.600 | -0.8569 2.700 | -0.9041 2.800 | -0.9422 2.900 | -0.9710 3.000 | -0.9900 3.100 | -0.9991 3.200 | -0.9983 3.300 | -0.9875 3.400 | -0.9668 3.500 | -0.9365 3.600 | -0.8968 3.700 | -0.8481 3.800 | -0.7910 3.900 | -0.7259 4.000 | -0.6536 4.100 | -0.5748 4.200 | -0.4903 4.300 | -0.4008 4.400 | -0.3073 4.500 | -0.2108 4.600 | -0.1122 4.700 | -0.0124 4.800 | 0.0875 4.900 | 0.1865 5.000 | 0.2837 5.100 | 0.3780 5.200 | 0.4685 5.300 | 0.5544 5.400 | 0.6347 5.500 | 0.7087 5.600 | 0.7756 5.700 | 0.8347 5.800 | 0.8855 5.900 | 0.9275 6.000 | 0.9602 6.100 | 0.9833 6.200 | 0.9965 6.300 | 0.9999 6.400 | 0.9932 6.500 | 0.9766 6.600 | 0.9502 6.700 | 0.9144 6.800 | 0.8694 6.900 | 0.8157 7.000 | 0.7539 7.100 | 0.6845 7.200 | 0.6084 7.300 | 0.5261 7.400 | 0.4385 7.500 | 0.3466 7.600 | 0.2513 7.700 | 0.1534 7.800 | 0.0540 7.900 | -0.0460 8.000 | -0.1455 8.100 | -0.2435 8.200 | -0.3392 8.300 | -0.4314 8.400 | -0.5193 8.500 | -0.6020 8.600 | -0.6787 8.700 | -0.7486 8.800 | -0.8111 8.900 | -0.8654 9.000 | -0.9111 9.100 | -0.9477 9.200 | -0.9748 9.300 | -0.9922 9.400 | -0.9997 9.500 | -0.9972 9.600 | -0.9847 9.700 | -0.9624 9.800 | -0.9304 9.900 | -0.8892 10.000 | -0.8391 10.100 | -0.7806 10.200 | -0.7143 10.300 | -0.6408 10.400 | -0.5610 10.500 | -0.4755 10.600 | -0.3853 10.700 | -0.2913 10.800 | -0.1943 10.900 | -0.0954 11.000 | 0.0044 11.100 | 0.1042 11.200 | 0.2030 11.300 | 0.2997 11.400 | 0.3935 11.500 | 0.4833 11.600 | 0.5683 11.700 | 0.6476 11.800 | 0.7204 11.900 | 0.7861 12.000 | 0.8439 12.100 | 0.8932 12.200 | 0.9336 12.300 | 0.9647 12.400 | 0.9862 12.500 | 0.9978 12.600 | 0.9994 12.700 | 0.9911 12.800 | 0.9728 12.900 | 0.9449 13.000 | 0.9074 13.100 | 0.8610 13.200 | 0.8059 13.300 | 0.7427 13.400 | 0.6722 13.500 | 0.5949 13.600 | 0.5117 13.700 | 0.4234 13.800 | 0.3308 13.900 | 0.2349 14.000 | 0.1367 14.100 | 0.0372 14.200 | -0.0628 14.300 | -0.1621 14.400 | -0.2598 14.500 | -0.3549 14.600 | -0.4465 14.700 | -0.5336 14.800 | -0.6154 14.900 | -0.6910 15.000 | -0.7597 15.100 | -0.8208 15.200 | -0.8737 15.300 | -0.9179 15.400 | -0.9530 15.500 | -0.9785 15.600 | -0.9942 The continue statement The continue instruction skips the block of code after it is executed for that iteration. It is best illustrated by an example. j = 0 for i in range(0,5,1): j += 2 print (\"\\n i = \", i , \", j = \", j) #here the \\n is a newline command if j == 6: continue print(\" this message will be skipped over if j = 6 \") # still within the loop, so the skip is implemented i = 0 , j = 2 this message will be skipped over if j = 6 i = 1 , j = 4 this message will be skipped over if j = 6 i = 2 , j = 6 i = 3 , j = 8 this message will be skipped over if j = 6 i = 4 , j = 10 this message will be skipped over if j = 6 The try , except structure An important control structure (and a pretty cool one for error trapping) is the try , except statement. The statement controls how the program proceeds when an error occurs in an instruction. The structure is really useful to trap likely errors (divide by zero, wrong kind of input) yet let the program keep running or at least issue a meaningful message to the user. The syntax is: try: do something except: do something else if ``do something'' returns an error Here is a really simple, but hugely important example: #MyErrorTrap.py x = 12. y = 12. while y >= -120.: # sentinel controlled repetition try: print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) except: print (\"error divide by zero\") y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 error divide by zero x = 12.0 y = -1.0 x/y = -12.0 x = 12.0 y = -2.0 x/y = -6.0 x = 12.0 y = -3.0 x/y = -4.0 x = 12.0 y = -4.0 x/y = -3.0 x = 12.0 y = -5.0 x/y = -2.4 x = 12.0 y = -6.0 x/y = -2.0 x = 12.0 y = -7.0 x/y = -1.7142857142857142 x = 12.0 y = -8.0 x/y = -1.5 x = 12.0 y = -9.0 x/y = -1.3333333333333333 x = 12.0 y = -10.0 x/y = -1.2 x = 12.0 y = -11.0 x/y = -1.0909090909090908 x = 12.0 y = -12.0 x/y = -1.0 x = 12.0 y = -13.0 x/y = -0.9230769230769231 x = 12.0 y = -14.0 x/y = -0.8571428571428571 x = 12.0 y = -15.0 x/y = -0.8 x = 12.0 y = -16.0 x/y = -0.75 x = 12.0 y = -17.0 x/y = -0.7058823529411765 x = 12.0 y = -18.0 x/y = -0.6666666666666666 x = 12.0 y = -19.0 x/y = -0.631578947368421 x = 12.0 y = -20.0 x/y = -0.6 x = 12.0 y = -21.0 x/y = -0.5714285714285714 x = 12.0 y = -22.0 x/y = -0.5454545454545454 x = 12.0 y = -23.0 x/y = -0.5217391304347826 x = 12.0 y = -24.0 x/y = -0.5 x = 12.0 y = -25.0 x/y = -0.48 x = 12.0 y = -26.0 x/y = -0.46153846153846156 x = 12.0 y = -27.0 x/y = -0.4444444444444444 x = 12.0 y = -28.0 x/y = -0.42857142857142855 x = 12.0 y = -29.0 x/y = -0.41379310344827586 x = 12.0 y = -30.0 x/y = -0.4 x = 12.0 y = -31.0 x/y = -0.3870967741935484 x = 12.0 y = -32.0 x/y = -0.375 x = 12.0 y = -33.0 x/y = -0.36363636363636365 x = 12.0 y = -34.0 x/y = -0.35294117647058826 x = 12.0 y = -35.0 x/y = -0.34285714285714286 x = 12.0 y = -36.0 x/y = -0.3333333333333333 x = 12.0 y = -37.0 x/y = -0.32432432432432434 x = 12.0 y = -38.0 x/y = -0.3157894736842105 x = 12.0 y = -39.0 x/y = -0.3076923076923077 x = 12.0 y = -40.0 x/y = -0.3 x = 12.0 y = -41.0 x/y = -0.2926829268292683 x = 12.0 y = -42.0 x/y = -0.2857142857142857 x = 12.0 y = -43.0 x/y = -0.27906976744186046 x = 12.0 y = -44.0 x/y = -0.2727272727272727 x = 12.0 y = -45.0 x/y = -0.26666666666666666 x = 12.0 y = -46.0 x/y = -0.2608695652173913 x = 12.0 y = -47.0 x/y = -0.2553191489361702 x = 12.0 y = -48.0 x/y = -0.25 x = 12.0 y = -49.0 x/y = -0.24489795918367346 x = 12.0 y = -50.0 x/y = -0.24 x = 12.0 y = -51.0 x/y = -0.23529411764705882 x = 12.0 y = -52.0 x/y = -0.23076923076923078 x = 12.0 y = -53.0 x/y = -0.22641509433962265 x = 12.0 y = -54.0 x/y = -0.2222222222222222 x = 12.0 y = -55.0 x/y = -0.21818181818181817 x = 12.0 y = -56.0 x/y = -0.21428571428571427 x = 12.0 y = -57.0 x/y = -0.21052631578947367 x = 12.0 y = -58.0 x/y = -0.20689655172413793 x = 12.0 y = -59.0 x/y = -0.2033898305084746 x = 12.0 y = -60.0 x/y = -0.2 x = 12.0 y = -61.0 x/y = -0.19672131147540983 x = 12.0 y = -62.0 x/y = -0.1935483870967742 x = 12.0 y = -63.0 x/y = -0.19047619047619047 x = 12.0 y = -64.0 x/y = -0.1875 x = 12.0 y = -65.0 x/y = -0.18461538461538463 x = 12.0 y = -66.0 x/y = -0.18181818181818182 x = 12.0 y = -67.0 x/y = -0.1791044776119403 x = 12.0 y = -68.0 x/y = -0.17647058823529413 x = 12.0 y = -69.0 x/y = -0.17391304347826086 x = 12.0 y = -70.0 x/y = -0.17142857142857143 x = 12.0 y = -71.0 x/y = -0.16901408450704225 x = 12.0 y = -72.0 x/y = -0.16666666666666666 x = 12.0 y = -73.0 x/y = -0.1643835616438356 x = 12.0 y = -74.0 x/y = -0.16216216216216217 x = 12.0 y = -75.0 x/y = -0.16 x = 12.0 y = -76.0 x/y = -0.15789473684210525 x = 12.0 y = -77.0 x/y = -0.15584415584415584 x = 12.0 y = -78.0 x/y = -0.15384615384615385 x = 12.0 y = -79.0 x/y = -0.1518987341772152 x = 12.0 y = -80.0 x/y = -0.15 x = 12.0 y = -81.0 x/y = -0.14814814814814814 x = 12.0 y = -82.0 x/y = -0.14634146341463414 x = 12.0 y = -83.0 x/y = -0.14457831325301204 x = 12.0 y = -84.0 x/y = -0.14285714285714285 x = 12.0 y = -85.0 x/y = -0.1411764705882353 x = 12.0 y = -86.0 x/y = -0.13953488372093023 x = 12.0 y = -87.0 x/y = -0.13793103448275862 x = 12.0 y = -88.0 x/y = -0.13636363636363635 x = 12.0 y = -89.0 x/y = -0.1348314606741573 x = 12.0 y = -90.0 x/y = -0.13333333333333333 x = 12.0 y = -91.0 x/y = -0.13186813186813187 x = 12.0 y = -92.0 x/y = -0.13043478260869565 x = 12.0 y = -93.0 x/y = -0.12903225806451613 x = 12.0 y = -94.0 x/y = -0.1276595744680851 x = 12.0 y = -95.0 x/y = -0.12631578947368421 x = 12.0 y = -96.0 x/y = -0.125 x = 12.0 y = -97.0 x/y = -0.12371134020618557 x = 12.0 y = -98.0 x/y = -0.12244897959183673 x = 12.0 y = -99.0 x/y = -0.12121212121212122 x = 12.0 y = -100.0 x/y = -0.12 x = 12.0 y = -101.0 x/y = -0.1188118811881188 x = 12.0 y = -102.0 x/y = -0.11764705882352941 x = 12.0 y = -103.0 x/y = -0.11650485436893204 x = 12.0 y = -104.0 x/y = -0.11538461538461539 x = 12.0 y = -105.0 x/y = -0.11428571428571428 x = 12.0 y = -106.0 x/y = -0.11320754716981132 x = 12.0 y = -107.0 x/y = -0.11214953271028037 x = 12.0 y = -108.0 x/y = -0.1111111111111111 x = 12.0 y = -109.0 x/y = -0.11009174311926606 x = 12.0 y = -110.0 x/y = -0.10909090909090909 x = 12.0 y = -111.0 x/y = -0.10810810810810811 x = 12.0 y = -112.0 x/y = -0.10714285714285714 x = 12.0 y = -113.0 x/y = -0.10619469026548672 x = 12.0 y = -114.0 x/y = -0.10526315789473684 x = 12.0 y = -115.0 x/y = -0.10434782608695652 x = 12.0 y = -116.0 x/y = -0.10344827586206896 x = 12.0 y = -117.0 x/y = -0.10256410256410256 x = 12.0 y = -118.0 x/y = -0.1016949152542373 x = 12.0 y = -119.0 x/y = -0.10084033613445378 x = 12.0 y = -120.0 x/y = -0.1 So this silly code starts with x fixed at a value of 12, and y starting at 12 and decreasing by 1 until y equals -1. The code returns the ratio of x to y and at one point y is equal to zero and the division would be undefined. By trapping the error the code can issue us a measure and keep running. Modify the script as shown below,Run, and see what happens #NoErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) <ipython-input-23-82eeaceb9a12> in <module> 3 y = 12. 4 while y >= -12.: # sentinel controlled repetition ----> 5 print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) 6 y -= 1 ZeroDivisionError: float division by zero Flowcharts What is a Flowchart? A flowchart is a type of diagram that represents a workflow or process. A flowchart can also be defined as a diagrammatic representation of an algorithm, a step-by-step approach to solving a task. Figure 2 Repair Flowchart for a Lamp https://en.wikipedia.org/wiki/Flowchart The flowchart shows the steps as boxes of various kinds, and their order by connecting the boxes with arrows. This diagrammatic representation illustrates a solution model to a given problem. Flowcharts are used in analyzing, designing, documenting or managing a process or program in various fields. There is a symbol convention (a language) as depicted in Figure 2 below (from: https://en.wikipedia.org/wiki/Flowchart) Figure 1 Flowchart Symbols https://en.wikipedia.org/wiki/Flowchart IBM engineers implemented programming flowcharts based upon Goldstine and von Neumann's unpublished report, \"Planning and coding of problems for an electronic computing instrument, Part II, Volume 1\" (1947), which is reproduced in von Neumann's collected works. The flowchart became a popular tool for describing computer algorithms, but its popularity decreased in the 1970s, when interactive computer terminals and third-generation programming languages became common tools for computer programming, since algorithms can be expressed more concisely as source code in such languages. Often pseudo-code is used, which uses the common idioms of such languages without strictly adhering to the details of a particular one. Nowadays flowcharts are still used for describing computer algorithms.[9] Modern techniques such as UML activity diagrams and Drakon-charts can be considered to be extensions of the flowchart. Nearly all flowcharts focus on on some kind of control, rather than on the particular flow itself! While quaint today, they are an effective way to document processes in a program and visualize structures. We recomend you get in the habit of making rudimentary flowcharts, at least at the supervisory level (the sandwich chart above) How are they useful? (paraphrased from https://www.breezetree.com/articles/top-reasons-to-flowchart) Sometimes it's more effective to visualize something graphically that it is to describe it with words. That is the essence of what flowcharts do for you. Flowcharts explain a process clearly through symbols and text. Moreover, flowcharts give you the gist of the process flow in a single glance. The following are some of the more salient reasons to use flowcharts. Process Documentation / Training Materials Another common use for flowcharts is to create process documentation. Although this reason overlaps with regulatory and quality management requirements (below), many non-regulated businesses use flowcharts for their documentation as well. These can range in form from high-level procedures to low-level, detailed work instructions. You may think that this applies mainly to large organizations, but small companies can greatly benefit from flowcharting their processes as well. Small enterprises need to be nimble and organized. Standardizing their processes is a great way to achieve this. In fact, the popular entrepreneurial book The E-Myth Revisited: Why Most Small Businesses Don't Work and What to Do About It by Michael Gerber is based on the fact that small businesses are more likely to succeed if they treat their operations like a franchise. in a nutshell, this means standardizing and documenting their business processes. There's no better way to do that than with flowcharts, right? Training materials are often created using flowcharts because they're visually stimulating and easy to understand. A nicely laid out flowchart will gain and hold the reader's attention when a block of text will often fail. Workflow Management and Continuous Improvement Workflows don't manage themselves. To ensure that you are meeting your customers' needs, you need to take control of your business processes. The first step to workflow management is to define the current state of your processes by creating an \"As-Is Flowchart\". That allows you to analyze your processes for waste and inefficiency. After you have identified areas for process improvement, you can then craft new flowcharts to document the leaner processes. Programming Information technology played a big influence on the use and spread of flowcharts in the 20th century. While Dr. W. Edwards Deming was advocating their use in quality management, professionals in the data processing world were using them to flesh out their programming logic. Flowcharts were a mainstay of procedural programming, however, and with the advent of object oriented programming and various modeling tools, the use of flowcharts for programming is no longer as commonplace as it once was. That said, even with in the scope of object oriented programming, complex program logic can be modeled effectively using a flowchart. Moreover, diagramming the user's experience as they navigate through a program is a valuable prerequisite prior to designing the user interface. So flowcharts still have their place in the world of programming. Troubleshooting Guides Most of us have come across a troubleshooting flowchart at one time or another. These are usually in the form of Decision Trees that progressively narrow the range of possible solutions based on a series of criteria. The effectiveness of these types of flowcharts depends on how neatly the range of problems and solutions can fit into a simple True/False diagnosis model. A well done troubleshooting flowcharts can cut the problem solving time greatly. Regulatory and Quality Management Requirements Your business processes may be subject to regulatory requirements such as Sarbanes-Oxley (SOX), which requires that your accounting procedures be clearly defined and documented. An easy way to do this is to create accounting flowcharts for all your accounting processes. Similarly, many organizations fall under certification requirements for quality management systems - such as ISO 9000, TS 16949, or one of the many others. In such environments, flowcharts are not only useful but in certain clauses they are actually mandated. Sorting (Important Flow Control Cases) Advanced/Optional Topic A frequent task in data science, engineering, etc. is the seemingly mundane task of sorting or ordering things. Here we explore a couple of simple sorting algorithms, just to show some of the thoughts that go into such a task, then will ultimately resort to the internal sorting routines built into Python. Sorting is frequently necessary when data are being handled; for example in integration and differentiation the data are usually presented to the various algorithms in ascending or descending order (at least on the x-axis). One may have tables of numbers, representing one or more explanatory variables, and one or more responses. At times we may need to arrange these tables in an order dictated by one or another of these various variables. Alternatively we may want to nd the median value or upper quartile of such a list { this task requires sorting. When sorting, one can also carry along operations to maintain correspondence with other lists (for lack of better name lets call this sort-and-carry). Tasks that fall under the broad category of sorting are: - Sort ; rearrange an array of numbers into numerical order (ascending or descending). - Sort and carry along ; rearrange an array of numbers into numerical order while per- forming the same rearrangement of one or more additional arrays so that the correspon- dence between elements in all arrays is maintained (the sets of arrays are essentially a relational database { so that each record (row) maintains the cross-record ( elds; columns) relationship). - Index ; given an array, prepare an index table that is a table of pointers that indicates which number array element comes rst in numerical order, which is second, and so on. - Rank ; given an array, prepare a rank table that tells the numerical rank of an array element. The task of sorting N elements requires on the order of K \\cdot Nlog2N operations. The algorithm inventor tries to make K as small as possible (understanding that K = 0 is practically impossible). Three useful sorting algorithms are: 1. Straight insertion sort; 2. Heapsort sort; and 3. Quicksort sort. The choice of method depends on the size of the list that needs to be sorted. If the list is short (perhaps N < 50 elements) then straight insertion is fast enough, concise, and simple to program. For a long list ( N > 1000 elements) Quicksort is faster, but achieves the speed by use of extra memory. Heapsort is also good for large lists, and is an in-place routine. Python lists have a built-in sort() method that modifies the list in-place and a sorted() built-in function that builds a new sorted list from an iterable. So when sorting needs to be done, you should use the built-in tools. However, because it is a useful programming construct, the three sorting algorithms are presented as Python primitive codes. Bubble Sort The bubble sort is a place to start despite it's relative slowness. It is a pretty reviled algorithm (read the Wikipedia entry), but it is the algorithm that a naive programmer might cobble together in a hurry, and despite its shortcomings (it's really slow and inefficient), it is robust. Here is a description of the sorting task as described by Christian and Griffths (2016) (pg. 65): \"Imagine you want to alphabetize your unsorted collection of books. A natural approach would be just to scan across the shelf looking for out- of-order pairs - Wallace followed by Pynchon, for instance - and flipping them around. Put Pynchon ahead of Wallace, then continue your scan, looping around to the beginning of the shelf each time you reach the end. When you make a complete pass without finding any more out-of-order pairs on the entire shelf, then you know the job is done. This process is a Bubble Sort, and it lands us in quadratic time. There are n books out of order, and each scan through the shelf can move each one at most one position. (We spot a tiny problem, make a tiny fix.) So in the worst case, where the shelf is perfectly backward, at least one book will need to be moved n positions. Thus a maximum of n passes through n books, which gives us O(n2) in the worst case. For instance, it means that sorting five shelves of books will take not five times as long as sorting a single shelf, but twenty-five times as long.\" Converting the word description into Python is fairly simple. We will have a vector of n numbers (we use a vector because its easy to step through the different positions), and we will scan through the vector once (and essentially find the smallest thing), and put it into the first position. Then we scan again from the second position and find the smallest thing remaining, and put it into the second position, and so on until the last scan which should have the remaining largest thing. If we desire a decreasing order, simply change the sense of the comparison. The algorithm defines an array and then sorts by repeated passes through the array. The program (outside of the sorting algorithm) is really quite simple. * Load contents into an array to be sorted. * Echo (print) the array (so we can verify the data are loaded as anticipated). * Loads the sorting function (the two loops) * Sort the array, put the results back into the array (an in-place sort). * Report the results. #array = [7,11,5,8,9,13,66,99,223] #array = [7,11,5] array=[1003 ,3.2 ,55.5 , -0.0001 , -6 ,666.6 ,102] howMany = len(array) print(\"Item Count = : \",howMany) print(\"Unsorted List : \", array) # insertion sort for irow in range(0, howMany-1) : for jrow in range(0,(howMany-1-irow)) : if array[jrow]> array[jrow+1] : swap = array[jrow] array[jrow]=array[jrow+1] array[jrow+1]=swap else: continue #results print(\"Sorted List : \", array, end =\"\") In the script we see that the program (near the bottom of the file) assigns the values to the vector named array and the initial order of the array is {1003, 3.2, 55.5,-0.0001,-6, 666.6, 102} . The smallest value in the example is -6 and it appears in the 5-th position, not the 1-st as it should. The first pass through the array will move the largest value, 1003, in sequence to the right until it occupies the last position. Repeated passes through the array move the remaining largest values to the right until the array is ordered. One can consider the values of the array at each scan of the array as a series of transformations (irow-th scan) -- in practical cases we don't necessarily care about the intermediate values, but here because the size is manageable and we are trying to get our feet wet with algorithms, we can look at the values. The sequence of results (transformations) after each pass through the array is shown in the following list: 1. Initial value: [1003; 3,2; 55,5;-0,0001;-6; 666,6; 102]. 2. First pass: [3,2; 55,5;-0,0001;-6; 666,6; 102; 1003]. 3. Second pass: [3,2;-0,0001;-6; 55,5; 102; 666,6; 1003]. 4. Third pass: [-0,0001;-6; 3,2; 55,5; 102; 666,6; 1003]. 5. Fourth pass: [-6;-0,0001; 3,2; 55,5; 102; 666,6; 1003]. 6. Fifth pass: [-6;-0,0001; 3,2; 55,5; 102; 666,6; 1003]. Sorted, fast scan. 7. Sixth pass: [-6;-0,0001; 3,2; 55,5; 102; 666,6; 1003]. Sorted, fast scan. We could probably add additional code to break from the scans when we have a single pass with no exchanges (like the last two scans) -- while meaningless in this example, for larger collections of things, being able to break out when the sorting is complete is a nice feature. Insertion Sort The next type of sorting would be to select one item and locate it either left or right of an adjacent item based on its size { like sorting a deck of cards, or perhaps a better description { again using the bookshelf analog from Christian and Griffths (2016) (pg. 65) You might take a different tack -- pulling all the books off the shelf and putting them back in place one by one. You'd put the ffrst book in the middle of the shelf, then take the second and compare it to the first, inserting it either to the right or to the left. Picking up the third book, you'd run through the books on the shelf from left to right until you found the right spot to tuck it in. Repeating this process, gradually all of the books would end up sorted on the shelf and you'd be done. Computer scientists call this, appropriately enough, Insertion Sort. The good news is that it's arguably even more intuitive than Bubble Sort and doesn't have quite the bad reputation. The bad news is that it's not actually that much faster. You still have to do one insertion for each book. And each insertion still involves moving past about half the books on the shelf, on average, to find the correct place. Although in practice Insertion Sort does run a bit faster than Bubble Sort, again we land squarely, if you will, in quadratic time. Sorting anything more than a single bookshelf is still an unwieldy prospect.\" Listing 8 is an R implementation of a straight insertion sort. The script is quite compact, and I used indentation and extra line spacing to keep track of the scoping delimiters. The sort works as follows, take the an element of the array (start with 2 and work to the right) and put it into a temporary location (called swap in my script). Then compare locations to the left of swap. If smaller, then break from the loop, exchange values, otherwise the values are currently ordered. Repeat (starting at the next element) , when all elements have been traversed the resulting vector is sorted. Here are the transformations for each pass through the outer loop: Straight Insertion The straight insertion sort is the algorithm a card player would use to sort cards. Pick out the second card and put it into order with respect to the first; then pick the third card and insert it into sequence with the first two; continue until the last card is picked out and inserted. Once the last card is sequenced, the result is a sorted deck (list). Python implementation of such an algorithm is: #array = [7,11,5,8,9,13,66,99,223] array = [7,11,5] howMany = len(array) print(\"Item Count = : \",howMany) print(\"Unsorted List : \", array, end =\"\") # insertion sort for i in range(1, len(array)): # Traverse through 1 to len(arr) key = array[i] # Move elements of arr[0..i-1], that are # greater than key, to one position ahead # of their current position j = i-1 while j >=0 and key < array[j] : array[j+1] = array[j] j -= 1 array[j+1] = key #results print(\"Sorted List : \", array, end =\"\") Probably useful to put into a functional structure: # Function to do insertion sort def insertionSort(array): # Traverse through 1 to len(arr) for i in range(1, len(array)): key = array[i] # Move elements of arr[0..i-1], that are # greater than key, to one position ahead # of their current position j = i-1 while j >=0 and key < array[j] : array[j+1] = array[j] j -= 1 array[j+1] = key return(array) array = [7,11,5,8,9,13,66,99,223] print(\"Unsorted List : \", array) insertionSort(array) print(\"Sorted List : \", array, end =\"\") Merge Sort A practical extension of these slow sorts is called the Merge Sort. It is an incredibly useful method. One simply breaks up the items into smaller arrays, sorts those arrays - then merges the sub-arrays into larger arrays (now already sorted), and nally merges the last two arrays into the nal, single, sorted array. Here is a better description, again from Christian and Griffths (2016): \" ... information processing began in the US censuses of the nineteenth century, with the development, by Herman Hollerith and later by IBM, of physical punch-card sorting devices. In 1936, IBM began producing a line of machines called \\collators\" that could merge two separately ordered stacks of cards into one. As long as the two stacks were themselves sorted, the procedure of merging them into a single sorted stack was incredibly straightforward and took linear time: simply compare the two top cards to each other, move the smaller of them to the new stack you're creating, and repeat until finished. The program that John von Neumann wrote in 1945 to demonstrate the power of the stored-program computer took the idea of collating to its beautiful and ultimate conclusion. Sorting two cards is simple: just put the smaller one on top. And given a pair of two-card stacks, both of them sorted, you can easily collate them into an ordered stack of four. Repeating this trick a few times, you'd build bigger and bigger stacks, each one of them already sorted. Soon enough, you could collate yourself a perfectly sorted full deck - with a final climactic merge, like a riffe shuffle's order- creating twin, producing the desired result. This approach is known today as Merge Sort, one of the legendary algorithms in computer science.\" There are several other variants related to Merge Sort; Quicksort and Heapsort being close relatives; # Python program for implementation of MergeSort # https://www.geeksforgeeks.org/merge-sort/ # This code is contributed by Mayank Khanna def mergeSort(arr): if len(arr) >1: mid = len(arr)//2 # Finding the mid of the array L = arr[:mid] # Dividing the array elements R = arr[mid:] # into 2 halves mergeSort(L) # Sorting the first half mergeSort(R) # Sorting the second half i = j = k = 0 # Copy data to temp arrays L[] and R[] while i < len(L) and j < len(R): if L[i] < R[j]: arr[k] = L[i] i+= 1 else: arr[k] = R[j] j+= 1 k+= 1 # Checking if any element was left while i < len(L): arr[k] = L[i] i+= 1 k+= 1 while j < len(R): arr[k] = R[j] j+= 1 k+= 1 # Code to print the list def printList(arr): for i in range(len(arr)): print(arr[i], end =\" \") print() # driver code to test the above code #if __name__ == '__main__': arr=[1003 ,3.2 ,55.5 , -0.0001 , -6 ,666.6 ,102] #arr = [12, 11, 13, 5, 6, 7] print (\"Given array is\", end =\"\\n\") printList(arr) mergeSort(arr) print(\"Sorted array is: \", end =\"\\n\") printList(arr) Heapsort Need narrative here # Python program for implementation of heap Sort # To heapify subtree rooted at index i. # n is size of heap def heapify(arr, n, i): largest = i # Initialize largest as root l = 2 * i + 1 # left = 2*i + 1 r = 2 * i + 2 # right = 2*i + 2 # See if left child of root exists and is # greater than root if l < n and arr[i] < arr[l]: largest = l # See if right child of root exists and is # greater than root if r < n and arr[largest] < arr[r]: largest = r # Change root, if needed if largest != i: arr[i],arr[largest] = arr[largest],arr[i] # swap # Heapify the root. heapify(arr, n, largest) # The main function to sort an array of given size def heapSort(arr): n = len(arr) # Build a maxheap. # Since last parent will be at ((n//2)-1) we can start at that location. for i in range(n // 2 - 1, -1, -1): heapify(arr, n, i) # One by one extract elements for i in range(n-1, 0, -1): arr[i], arr[0] = arr[0], arr[i] # swap heapify(arr, i, 0) # Driver code to test above arr=[1003 ,3.2 ,55.5 , -0.0001 , -6 ,666.6 ,102] #arr = [ 12, 11, 13, 5, 6, 7] heapSort(arr) n = len(arr) print (\"Sorted array is\") for i in range(n): print (\"%d\" %arr[i]), # This code is contributed by Mohit Kumra Lexicographical Sorting Need narrative here # Python program to sort the words in lexicographical # order def sortLexo(my_string): # Split the my_string till where space is found. words = my_string.split() # sort() will sort the strings. words.sort() # Iterate i through 'words' to print the words # in alphabetical manner. for i in words: print( i ) # Driver code if __name__ == '__main__': my_string = \"hello this is example how to sort \" \\ \"the word in alphabetical manner\" # Calling function sortLexo(my_string) I conclude the section on sorting with one more quoted section from Christian and Griffiths (2016) about the value for sorting - which is already relevant to a lot of data science: \"The poster child for the advantages of sorting would be an Internet search engine like Google. It seems staggering to think that Google can take the search phrase you typed in and scour the entire Internet for it in less than half a second. Well, it can't - but it doesn't need to. If you're Google, you are almost certain that (a) your data will be searched, (b) it will be searched not just once but repeatedly, and (c) the time needed to sort is somehow less valuable\" than the time needed to search. (Here, sorting is done by machines ahead of time, before the results are needed, and searching is done by users for whom time is of the essence.) All of these factors point in favor of tremendous up-front sorting, which is indeed what Google and its fellow search engines do.\" References Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapters 3-6 https://www.inferentialthinking.com/chapters/03/programming-in-python.html Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. Brian Christian and Tom Griffiths (2016) ALGORITHMS TO LIVE BY: The Computer Science of Human Decisions Henry Holt and Co. (https://www.amazon.com/Algorithms-Live-Computer-Science-Decisions/dp/1627790365) import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info)","title":"Program Control Structures"},{"location":"1-Lessons/Lesson04/lesson4/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 31 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson04/lesson4/#lesson-4-program-flow-control-structures-for-repetition","text":"Controlled repetition Structured FOR Loop Structured WHILE Loop Representing computational processes with flowcharts, a graphical abstraction","title":"Lesson 4 Program Flow Control Structures for Repetition:"},{"location":"1-Lessons/Lesson04/lesson4/#special-script-blocks","text":"%%html <!-- Script Block to set tables to left alignment --> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;}","title":"Special Script Blocks"},{"location":"1-Lessons/Lesson04/lesson4/#objectives","text":"1) Develop awareness of loops, and their utility in automation. - To understand loop types available in Python. - To understand and implement loops in various examples and configurations. 2) Develop awareness of flowcharts as a tool for: - Post-development documentation - Pre-development program design","title":"Objectives"},{"location":"1-Lessons/Lesson04/lesson4/#repetition-and-loops","text":"Computational thinking (CT) concepts involved are: Decomposition : Break a problem down into smaller pieces; the body of tasks in one repetition of a loop represent decomposition of the entire sets of repeated activities Pattern Recognition : Finding similarities between things; the body of tasks in one repetition of a loop is the pattern, the indices and components that change are how we leverage reuse Abstraction : Pulling out specific differences to make one solution work for multiple problems Algorithms : A list of steps that you can follow to finish a task The action of doing something over and over again (repetition) is called a loop. Basically, Loops repeats a portion of code a finite number of times until a process is complete. Repetitive tasks are very common and essential in programming. They save time in coding, minimize coding errors, and leverage the speed of electronic computation.","title":"Repetition and Loops"},{"location":"1-Lessons/Lesson04/lesson4/#loop-analogs","text":"If you think any mass manufacturing process, we apply the same process again and again. Even for something very simple such as preparing a peanut butter sandwich: Consider the flowchart in Figure 1, it represents a decomposition of sandwich assembly, but at a high level -- for instance, Gather Ingredients contains a lot of substeps that would need to be decomposed if fully automated assembly were to be accomplished; nevertheless lets stipulate that this flowchart will indeed construct a single sandwich. Figure 1 Supervisory Flowchart Sandwich Assembly (adapted from http://www.str-tn.org/subway_restaurant_training_manual.pdf) If we need to make 1000 peanut butter sandwichs we would then issue a directive to: 1) Implement sandwich assembly, repeat 999 times (repeat is the loop structure) (A serial structure, 1 sandwich artist, doing same job over and over again) OR 2) Implement 1000 sandwich assembly threads (A parallel structure, 1000 sandwich artists doing same job once) In general because we dont want to idle 999 sandwich artists, we would choose the serial structure, which frees 999 people to ask the existential question \"would you like fries with that?\" All cynicism aside, an automated process such as a loop, is typical in computational processing. Aside NVIDIA CUDA, and AMD OpenGL compilers can detect the structure above, and if there are enough GPU threads available , create the 1000 sandwich artists (1000 GPU threads), and run the process in parallel -- the actual workload is unchanged in a thermodynamic sense, but the apparent time (in human terms) spent in sandwich creation is a fraction of the serial approach. This parallelization is called unrolling the loop, and is a pretty common optimization step during compilation. This kind of programming is outside the scope of this class. Main attractiveness of loops is: - Leveraging pattern matching and automation - Code is more organized and shorter,because a loop is a sequence of instructions that is continually repeated until a certain condition is reached. There are 2 main types loops based on the repetition control condition; for loops and while loops.","title":"Loop Analogs"},{"location":"1-Lessons/Lesson04/lesson4/#for-loop-count-controlled-repetition-structure","text":"Count-controlled repetition is also called definite repetition because the number of repetitions is known before the loop begins executing. When we do not know in advance the number of times we want to execute a statement, we cannot use count-controlled repetition. In such an instance, we would use sentinel-controlled repetition. A count-controlled repetition will exit after running a certain number of times. The count is kept in a variable called an index or counter. When the index reaches a certain value (the loop bound) the loop will end. Count-controlled repetition requires control variable (or loop counter) initial value of the control variable increment (or decrement) by which the control variable is modified each iteration through the loop condition that tests for the final value of the control variable We can use both for and while loops, for count controlled repetition, but the for loop in combination with the range() function is more common.","title":"For Loop (Count controlled repetition structure)"},{"location":"1-Lessons/Lesson04/lesson4/#structured-for-loop","text":"We have seen the for loop already, but we will formally introduce it here. The for loop executes a block of code repeatedly until the condition in the for statement is no longer true.","title":"Structured FOR loop"},{"location":"1-Lessons/Lesson04/lesson4/#looping-through-an-iterable","text":"An iterable is anything that can be looped over - typically a list, string, or tuple. The syntax for looping through an iterable is illustrated by an example. First a generic syntax for a in iterable: print(a) Notice our friends the colon : and the indentation.","title":"Looping through an iterable"},{"location":"1-Lessons/Lesson04/lesson4/#the-range-function-to-create-an-iterable","text":"The range(begin,end,increment) function will create an iterable starting at a value of begin, in steps defined by increment ( begin += increment ), ending at end . So a generic syntax becomes for a in range(begin,end,increment): print(a) The examples that follow are count-controlled repetition (increment skip if greater)","title":"The range() function to create an iterable"},{"location":"1-Lessons/Lesson04/lesson4/#example-for-loops","text":"# sum numbers from 1 to n howmany = int(input('Enter N')) accumulator = 0.0 for i in range(1,howmany+1,1): accumulator = accumulator + float(i) print( 'Sum from 1 to ',howmany, 'is %.3f' % accumulator ) Enter N 33 Sum from 1 to 33 is 561.000 # sum even numbers from 1 to n howmany = int(input('Enter N')) accumulator = 0.0 for i in range(1,howmany+1,1): if i%2 == 0: accumulator = accumulator + float(i) print( 'Sum of Evens from 1 to ',howmany, 'is %.3f' % accumulator ) Enter N 33 Sum of Evens from 1 to 33 is 272.000 howmany = int(input('Enter N')) linetoprint='' for i in range(1,howmany+1,1): linetoprint=linetoprint + '*' print(linetoprint) Enter N 33 * ** *** **** ***** ****** ******* ******** ********* ********** *********** ************ ************* ************** *************** **************** ***************** ****************** ******************* ******************** ********************* ********************** *********************** ************************ ************************* ************************** *************************** **************************** ***************************** ****************************** ******************************* ******************************** *********************************","title":"Example for loops"},{"location":"1-Lessons/Lesson04/lesson4/#sentinel-controlled-repetition","text":"When loop control is based on the value of what we are processing, sentinel-controlled repetition is used. Sentinel-controlled repetition is also called indefinite repetition because it is not known in advance how many times the loop will be executed. It is a repetition procedure for solving a problem by using a sentinel value (also called a signal value, a dummy value or a flag value) to indicate \"end of process\". The sentinel value itself need not be a part of the processed data. One common example of using sentinel-controlled repetition is when we are processing data from a file and we do not know in advance when we would reach the end of the file. We can use both for and while loops, for Sentinel controlled repetition, but the while loop is more common.","title":"Sentinel-controlled repetition."},{"location":"1-Lessons/Lesson04/lesson4/#structured-while-loop","text":"The while loop repeats a block of instructions inside the loop while a condition remainsvtrue. First a generic syntax while condition is true: execute a execute b .... Notice our friends the colon : and the indentation again.","title":"Structured WHILE loop"},{"location":"1-Lessons/Lesson04/lesson4/#example-while-loops","text":"# sum numbers from 1 to n howmany = int(input('Enter N')) accumulator = 0.0 counter = 1 while counter <= howmany: accumulator = accumulator + float(counter) counter += 1 print( 'Sum from 1 to ',howmany, 'is %.3f' % accumulator ) Enter N 33 Sum from 1 to 33 is 561.000 # sum even numbers from 1 to n howmany = int(input('Enter N')) accumulator = 0.0 counter = 1 while counter <= howmany: if counter%2 == 0: accumulator = accumulator + float(counter) counter += 1 print( 'Sum of Evens 1 to ',howmany, 'is %.3f' % accumulator ) Enter N 33 Sum of Evens 1 to 33 is 272.000 howmany = int(input('Enter N')) linetoprint='' counter = 1 while counter <= howmany: linetoprint=linetoprint + '*' counter += 1 print(linetoprint) Enter N 33 * ** *** **** ***** ****** ******* ******** ********* ********** *********** ************ ************* ************** *************** **************** ***************** ****************** ******************* ******************** ********************* ********************** *********************** ************************ ************************* ************************** *************************** **************************** ***************************** ****************************** ******************************* ******************************** *********************************","title":"Example while loops"},{"location":"1-Lessons/Lesson04/lesson4/#nested-repetition","text":"Nested repetition is when a control structure is placed inside of the body or main part of another control structure.","title":"Nested Repetition"},{"location":"1-Lessons/Lesson04/lesson4/#break-to-exit-out-of-a-loop","text":"Sometimes you may want to exit the loop when a certain condition different from the counting condition is met. Perhaps you are looping through a list and want to exit when you find the first element in the list that matches some criterion. The break keyword is useful for such an operation. For example run the following program: # j = 0 for i in range(0,9,1): j += 2 print (\"i = \",i,\"j = \",j) if j == 6: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 j = 0 for i in range(0,5,1): j += 2 print( \"i = \",i,\"j = \",j) if j == 7: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 i = 3 j = 8 i = 4 j = 10 In the first case, the for loop only executes 3 times before the condition j == 6 is TRUE and the loop is exited. In the second case, j == 7 never happens so the loop completes all its anticipated traverses. In both cases an if statement was used within a for loop. Such \"mixed\" control structures are quite common (and pretty necessary). A while loop contained within a for loop, with several if statements would be very common and such a structure is called nested control. There is typically an upper limit to nesting but the limit is pretty large - easily in the hundreds. It depends on the language and the system architecture ; suffice to say it is not a practical limit except possibly for general-domain AI applications. We can also do mundane activities and leverage loops, arithmetic, and format codes to make useful tables like import math # package that contains cosine print(\" Cosines \") print(\" x \",\"|\",\" cos(x) \") print(\"--------|--------\") for i in range(0,157,1): x = float(i)*0.1 print(\"%.3f\" % x, \" |\", \" %.4f \" % math.cos(x)) # note the format code and the placeholder % and syntax Cosines x | cos(x) --------|-------- 0.000 | 1.0000 0.100 | 0.9950 0.200 | 0.9801 0.300 | 0.9553 0.400 | 0.9211 0.500 | 0.8776 0.600 | 0.8253 0.700 | 0.7648 0.800 | 0.6967 0.900 | 0.6216 1.000 | 0.5403 1.100 | 0.4536 1.200 | 0.3624 1.300 | 0.2675 1.400 | 0.1700 1.500 | 0.0707 1.600 | -0.0292 1.700 | -0.1288 1.800 | -0.2272 1.900 | -0.3233 2.000 | -0.4161 2.100 | -0.5048 2.200 | -0.5885 2.300 | -0.6663 2.400 | -0.7374 2.500 | -0.8011 2.600 | -0.8569 2.700 | -0.9041 2.800 | -0.9422 2.900 | -0.9710 3.000 | -0.9900 3.100 | -0.9991 3.200 | -0.9983 3.300 | -0.9875 3.400 | -0.9668 3.500 | -0.9365 3.600 | -0.8968 3.700 | -0.8481 3.800 | -0.7910 3.900 | -0.7259 4.000 | -0.6536 4.100 | -0.5748 4.200 | -0.4903 4.300 | -0.4008 4.400 | -0.3073 4.500 | -0.2108 4.600 | -0.1122 4.700 | -0.0124 4.800 | 0.0875 4.900 | 0.1865 5.000 | 0.2837 5.100 | 0.3780 5.200 | 0.4685 5.300 | 0.5544 5.400 | 0.6347 5.500 | 0.7087 5.600 | 0.7756 5.700 | 0.8347 5.800 | 0.8855 5.900 | 0.9275 6.000 | 0.9602 6.100 | 0.9833 6.200 | 0.9965 6.300 | 0.9999 6.400 | 0.9932 6.500 | 0.9766 6.600 | 0.9502 6.700 | 0.9144 6.800 | 0.8694 6.900 | 0.8157 7.000 | 0.7539 7.100 | 0.6845 7.200 | 0.6084 7.300 | 0.5261 7.400 | 0.4385 7.500 | 0.3466 7.600 | 0.2513 7.700 | 0.1534 7.800 | 0.0540 7.900 | -0.0460 8.000 | -0.1455 8.100 | -0.2435 8.200 | -0.3392 8.300 | -0.4314 8.400 | -0.5193 8.500 | -0.6020 8.600 | -0.6787 8.700 | -0.7486 8.800 | -0.8111 8.900 | -0.8654 9.000 | -0.9111 9.100 | -0.9477 9.200 | -0.9748 9.300 | -0.9922 9.400 | -0.9997 9.500 | -0.9972 9.600 | -0.9847 9.700 | -0.9624 9.800 | -0.9304 9.900 | -0.8892 10.000 | -0.8391 10.100 | -0.7806 10.200 | -0.7143 10.300 | -0.6408 10.400 | -0.5610 10.500 | -0.4755 10.600 | -0.3853 10.700 | -0.2913 10.800 | -0.1943 10.900 | -0.0954 11.000 | 0.0044 11.100 | 0.1042 11.200 | 0.2030 11.300 | 0.2997 11.400 | 0.3935 11.500 | 0.4833 11.600 | 0.5683 11.700 | 0.6476 11.800 | 0.7204 11.900 | 0.7861 12.000 | 0.8439 12.100 | 0.8932 12.200 | 0.9336 12.300 | 0.9647 12.400 | 0.9862 12.500 | 0.9978 12.600 | 0.9994 12.700 | 0.9911 12.800 | 0.9728 12.900 | 0.9449 13.000 | 0.9074 13.100 | 0.8610 13.200 | 0.8059 13.300 | 0.7427 13.400 | 0.6722 13.500 | 0.5949 13.600 | 0.5117 13.700 | 0.4234 13.800 | 0.3308 13.900 | 0.2349 14.000 | 0.1367 14.100 | 0.0372 14.200 | -0.0628 14.300 | -0.1621 14.400 | -0.2598 14.500 | -0.3549 14.600 | -0.4465 14.700 | -0.5336 14.800 | -0.6154 14.900 | -0.6910 15.000 | -0.7597 15.100 | -0.8208 15.200 | -0.8737 15.300 | -0.9179 15.400 | -0.9530 15.500 | -0.9785 15.600 | -0.9942","title":"break to exit out of a loop"},{"location":"1-Lessons/Lesson04/lesson4/#the-continue-statement","text":"The continue instruction skips the block of code after it is executed for that iteration. It is best illustrated by an example. j = 0 for i in range(0,5,1): j += 2 print (\"\\n i = \", i , \", j = \", j) #here the \\n is a newline command if j == 6: continue print(\" this message will be skipped over if j = 6 \") # still within the loop, so the skip is implemented i = 0 , j = 2 this message will be skipped over if j = 6 i = 1 , j = 4 this message will be skipped over if j = 6 i = 2 , j = 6 i = 3 , j = 8 this message will be skipped over if j = 6 i = 4 , j = 10 this message will be skipped over if j = 6","title":"The continue statement"},{"location":"1-Lessons/Lesson04/lesson4/#the-try-except-structure","text":"An important control structure (and a pretty cool one for error trapping) is the try , except statement. The statement controls how the program proceeds when an error occurs in an instruction. The structure is really useful to trap likely errors (divide by zero, wrong kind of input) yet let the program keep running or at least issue a meaningful message to the user. The syntax is: try: do something except: do something else if ``do something'' returns an error Here is a really simple, but hugely important example: #MyErrorTrap.py x = 12. y = 12. while y >= -120.: # sentinel controlled repetition try: print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) except: print (\"error divide by zero\") y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 error divide by zero x = 12.0 y = -1.0 x/y = -12.0 x = 12.0 y = -2.0 x/y = -6.0 x = 12.0 y = -3.0 x/y = -4.0 x = 12.0 y = -4.0 x/y = -3.0 x = 12.0 y = -5.0 x/y = -2.4 x = 12.0 y = -6.0 x/y = -2.0 x = 12.0 y = -7.0 x/y = -1.7142857142857142 x = 12.0 y = -8.0 x/y = -1.5 x = 12.0 y = -9.0 x/y = -1.3333333333333333 x = 12.0 y = -10.0 x/y = -1.2 x = 12.0 y = -11.0 x/y = -1.0909090909090908 x = 12.0 y = -12.0 x/y = -1.0 x = 12.0 y = -13.0 x/y = -0.9230769230769231 x = 12.0 y = -14.0 x/y = -0.8571428571428571 x = 12.0 y = -15.0 x/y = -0.8 x = 12.0 y = -16.0 x/y = -0.75 x = 12.0 y = -17.0 x/y = -0.7058823529411765 x = 12.0 y = -18.0 x/y = -0.6666666666666666 x = 12.0 y = -19.0 x/y = -0.631578947368421 x = 12.0 y = -20.0 x/y = -0.6 x = 12.0 y = -21.0 x/y = -0.5714285714285714 x = 12.0 y = -22.0 x/y = -0.5454545454545454 x = 12.0 y = -23.0 x/y = -0.5217391304347826 x = 12.0 y = -24.0 x/y = -0.5 x = 12.0 y = -25.0 x/y = -0.48 x = 12.0 y = -26.0 x/y = -0.46153846153846156 x = 12.0 y = -27.0 x/y = -0.4444444444444444 x = 12.0 y = -28.0 x/y = -0.42857142857142855 x = 12.0 y = -29.0 x/y = -0.41379310344827586 x = 12.0 y = -30.0 x/y = -0.4 x = 12.0 y = -31.0 x/y = -0.3870967741935484 x = 12.0 y = -32.0 x/y = -0.375 x = 12.0 y = -33.0 x/y = -0.36363636363636365 x = 12.0 y = -34.0 x/y = -0.35294117647058826 x = 12.0 y = -35.0 x/y = -0.34285714285714286 x = 12.0 y = -36.0 x/y = -0.3333333333333333 x = 12.0 y = -37.0 x/y = -0.32432432432432434 x = 12.0 y = -38.0 x/y = -0.3157894736842105 x = 12.0 y = -39.0 x/y = -0.3076923076923077 x = 12.0 y = -40.0 x/y = -0.3 x = 12.0 y = -41.0 x/y = -0.2926829268292683 x = 12.0 y = -42.0 x/y = -0.2857142857142857 x = 12.0 y = -43.0 x/y = -0.27906976744186046 x = 12.0 y = -44.0 x/y = -0.2727272727272727 x = 12.0 y = -45.0 x/y = -0.26666666666666666 x = 12.0 y = -46.0 x/y = -0.2608695652173913 x = 12.0 y = -47.0 x/y = -0.2553191489361702 x = 12.0 y = -48.0 x/y = -0.25 x = 12.0 y = -49.0 x/y = -0.24489795918367346 x = 12.0 y = -50.0 x/y = -0.24 x = 12.0 y = -51.0 x/y = -0.23529411764705882 x = 12.0 y = -52.0 x/y = -0.23076923076923078 x = 12.0 y = -53.0 x/y = -0.22641509433962265 x = 12.0 y = -54.0 x/y = -0.2222222222222222 x = 12.0 y = -55.0 x/y = -0.21818181818181817 x = 12.0 y = -56.0 x/y = -0.21428571428571427 x = 12.0 y = -57.0 x/y = -0.21052631578947367 x = 12.0 y = -58.0 x/y = -0.20689655172413793 x = 12.0 y = -59.0 x/y = -0.2033898305084746 x = 12.0 y = -60.0 x/y = -0.2 x = 12.0 y = -61.0 x/y = -0.19672131147540983 x = 12.0 y = -62.0 x/y = -0.1935483870967742 x = 12.0 y = -63.0 x/y = -0.19047619047619047 x = 12.0 y = -64.0 x/y = -0.1875 x = 12.0 y = -65.0 x/y = -0.18461538461538463 x = 12.0 y = -66.0 x/y = -0.18181818181818182 x = 12.0 y = -67.0 x/y = -0.1791044776119403 x = 12.0 y = -68.0 x/y = -0.17647058823529413 x = 12.0 y = -69.0 x/y = -0.17391304347826086 x = 12.0 y = -70.0 x/y = -0.17142857142857143 x = 12.0 y = -71.0 x/y = -0.16901408450704225 x = 12.0 y = -72.0 x/y = -0.16666666666666666 x = 12.0 y = -73.0 x/y = -0.1643835616438356 x = 12.0 y = -74.0 x/y = -0.16216216216216217 x = 12.0 y = -75.0 x/y = -0.16 x = 12.0 y = -76.0 x/y = -0.15789473684210525 x = 12.0 y = -77.0 x/y = -0.15584415584415584 x = 12.0 y = -78.0 x/y = -0.15384615384615385 x = 12.0 y = -79.0 x/y = -0.1518987341772152 x = 12.0 y = -80.0 x/y = -0.15 x = 12.0 y = -81.0 x/y = -0.14814814814814814 x = 12.0 y = -82.0 x/y = -0.14634146341463414 x = 12.0 y = -83.0 x/y = -0.14457831325301204 x = 12.0 y = -84.0 x/y = -0.14285714285714285 x = 12.0 y = -85.0 x/y = -0.1411764705882353 x = 12.0 y = -86.0 x/y = -0.13953488372093023 x = 12.0 y = -87.0 x/y = -0.13793103448275862 x = 12.0 y = -88.0 x/y = -0.13636363636363635 x = 12.0 y = -89.0 x/y = -0.1348314606741573 x = 12.0 y = -90.0 x/y = -0.13333333333333333 x = 12.0 y = -91.0 x/y = -0.13186813186813187 x = 12.0 y = -92.0 x/y = -0.13043478260869565 x = 12.0 y = -93.0 x/y = -0.12903225806451613 x = 12.0 y = -94.0 x/y = -0.1276595744680851 x = 12.0 y = -95.0 x/y = -0.12631578947368421 x = 12.0 y = -96.0 x/y = -0.125 x = 12.0 y = -97.0 x/y = -0.12371134020618557 x = 12.0 y = -98.0 x/y = -0.12244897959183673 x = 12.0 y = -99.0 x/y = -0.12121212121212122 x = 12.0 y = -100.0 x/y = -0.12 x = 12.0 y = -101.0 x/y = -0.1188118811881188 x = 12.0 y = -102.0 x/y = -0.11764705882352941 x = 12.0 y = -103.0 x/y = -0.11650485436893204 x = 12.0 y = -104.0 x/y = -0.11538461538461539 x = 12.0 y = -105.0 x/y = -0.11428571428571428 x = 12.0 y = -106.0 x/y = -0.11320754716981132 x = 12.0 y = -107.0 x/y = -0.11214953271028037 x = 12.0 y = -108.0 x/y = -0.1111111111111111 x = 12.0 y = -109.0 x/y = -0.11009174311926606 x = 12.0 y = -110.0 x/y = -0.10909090909090909 x = 12.0 y = -111.0 x/y = -0.10810810810810811 x = 12.0 y = -112.0 x/y = -0.10714285714285714 x = 12.0 y = -113.0 x/y = -0.10619469026548672 x = 12.0 y = -114.0 x/y = -0.10526315789473684 x = 12.0 y = -115.0 x/y = -0.10434782608695652 x = 12.0 y = -116.0 x/y = -0.10344827586206896 x = 12.0 y = -117.0 x/y = -0.10256410256410256 x = 12.0 y = -118.0 x/y = -0.1016949152542373 x = 12.0 y = -119.0 x/y = -0.10084033613445378 x = 12.0 y = -120.0 x/y = -0.1 So this silly code starts with x fixed at a value of 12, and y starting at 12 and decreasing by 1 until y equals -1. The code returns the ratio of x to y and at one point y is equal to zero and the division would be undefined. By trapping the error the code can issue us a measure and keep running. Modify the script as shown below,Run, and see what happens #NoErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) <ipython-input-23-82eeaceb9a12> in <module> 3 y = 12. 4 while y >= -12.: # sentinel controlled repetition ----> 5 print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) 6 y -= 1 ZeroDivisionError: float division by zero","title":"The try, except structure"},{"location":"1-Lessons/Lesson04/lesson4/#flowcharts","text":"","title":"Flowcharts"},{"location":"1-Lessons/Lesson04/lesson4/#what-is-a-flowchart","text":"A flowchart is a type of diagram that represents a workflow or process. A flowchart can also be defined as a diagrammatic representation of an algorithm, a step-by-step approach to solving a task. Figure 2 Repair Flowchart for a Lamp https://en.wikipedia.org/wiki/Flowchart The flowchart shows the steps as boxes of various kinds, and their order by connecting the boxes with arrows. This diagrammatic representation illustrates a solution model to a given problem. Flowcharts are used in analyzing, designing, documenting or managing a process or program in various fields. There is a symbol convention (a language) as depicted in Figure 2 below (from: https://en.wikipedia.org/wiki/Flowchart) Figure 1 Flowchart Symbols https://en.wikipedia.org/wiki/Flowchart IBM engineers implemented programming flowcharts based upon Goldstine and von Neumann's unpublished report, \"Planning and coding of problems for an electronic computing instrument, Part II, Volume 1\" (1947), which is reproduced in von Neumann's collected works. The flowchart became a popular tool for describing computer algorithms, but its popularity decreased in the 1970s, when interactive computer terminals and third-generation programming languages became common tools for computer programming, since algorithms can be expressed more concisely as source code in such languages. Often pseudo-code is used, which uses the common idioms of such languages without strictly adhering to the details of a particular one. Nowadays flowcharts are still used for describing computer algorithms.[9] Modern techniques such as UML activity diagrams and Drakon-charts can be considered to be extensions of the flowchart. Nearly all flowcharts focus on on some kind of control, rather than on the particular flow itself! While quaint today, they are an effective way to document processes in a program and visualize structures. We recomend you get in the habit of making rudimentary flowcharts, at least at the supervisory level (the sandwich chart above)","title":"What is a Flowchart?"},{"location":"1-Lessons/Lesson04/lesson4/#how-are-they-useful","text":"(paraphrased from https://www.breezetree.com/articles/top-reasons-to-flowchart) Sometimes it's more effective to visualize something graphically that it is to describe it with words. That is the essence of what flowcharts do for you. Flowcharts explain a process clearly through symbols and text. Moreover, flowcharts give you the gist of the process flow in a single glance. The following are some of the more salient reasons to use flowcharts. Process Documentation / Training Materials Another common use for flowcharts is to create process documentation. Although this reason overlaps with regulatory and quality management requirements (below), many non-regulated businesses use flowcharts for their documentation as well. These can range in form from high-level procedures to low-level, detailed work instructions. You may think that this applies mainly to large organizations, but small companies can greatly benefit from flowcharting their processes as well. Small enterprises need to be nimble and organized. Standardizing their processes is a great way to achieve this. In fact, the popular entrepreneurial book The E-Myth Revisited: Why Most Small Businesses Don't Work and What to Do About It by Michael Gerber is based on the fact that small businesses are more likely to succeed if they treat their operations like a franchise. in a nutshell, this means standardizing and documenting their business processes. There's no better way to do that than with flowcharts, right? Training materials are often created using flowcharts because they're visually stimulating and easy to understand. A nicely laid out flowchart will gain and hold the reader's attention when a block of text will often fail. Workflow Management and Continuous Improvement Workflows don't manage themselves. To ensure that you are meeting your customers' needs, you need to take control of your business processes. The first step to workflow management is to define the current state of your processes by creating an \"As-Is Flowchart\". That allows you to analyze your processes for waste and inefficiency. After you have identified areas for process improvement, you can then craft new flowcharts to document the leaner processes. Programming Information technology played a big influence on the use and spread of flowcharts in the 20th century. While Dr. W. Edwards Deming was advocating their use in quality management, professionals in the data processing world were using them to flesh out their programming logic. Flowcharts were a mainstay of procedural programming, however, and with the advent of object oriented programming and various modeling tools, the use of flowcharts for programming is no longer as commonplace as it once was. That said, even with in the scope of object oriented programming, complex program logic can be modeled effectively using a flowchart. Moreover, diagramming the user's experience as they navigate through a program is a valuable prerequisite prior to designing the user interface. So flowcharts still have their place in the world of programming. Troubleshooting Guides Most of us have come across a troubleshooting flowchart at one time or another. These are usually in the form of Decision Trees that progressively narrow the range of possible solutions based on a series of criteria. The effectiveness of these types of flowcharts depends on how neatly the range of problems and solutions can fit into a simple True/False diagnosis model. A well done troubleshooting flowcharts can cut the problem solving time greatly. Regulatory and Quality Management Requirements Your business processes may be subject to regulatory requirements such as Sarbanes-Oxley (SOX), which requires that your accounting procedures be clearly defined and documented. An easy way to do this is to create accounting flowcharts for all your accounting processes. Similarly, many organizations fall under certification requirements for quality management systems - such as ISO 9000, TS 16949, or one of the many others. In such environments, flowcharts are not only useful but in certain clauses they are actually mandated.","title":"How are they useful?"},{"location":"1-Lessons/Lesson04/lesson4/#sorting-important-flow-control-cases","text":"Advanced/Optional Topic A frequent task in data science, engineering, etc. is the seemingly mundane task of sorting or ordering things. Here we explore a couple of simple sorting algorithms, just to show some of the thoughts that go into such a task, then will ultimately resort to the internal sorting routines built into Python. Sorting is frequently necessary when data are being handled; for example in integration and differentiation the data are usually presented to the various algorithms in ascending or descending order (at least on the x-axis). One may have tables of numbers, representing one or more explanatory variables, and one or more responses. At times we may need to arrange these tables in an order dictated by one or another of these various variables. Alternatively we may want to nd the median value or upper quartile of such a list { this task requires sorting. When sorting, one can also carry along operations to maintain correspondence with other lists (for lack of better name lets call this sort-and-carry). Tasks that fall under the broad category of sorting are: - Sort ; rearrange an array of numbers into numerical order (ascending or descending). - Sort and carry along ; rearrange an array of numbers into numerical order while per- forming the same rearrangement of one or more additional arrays so that the correspon- dence between elements in all arrays is maintained (the sets of arrays are essentially a relational database { so that each record (row) maintains the cross-record ( elds; columns) relationship). - Index ; given an array, prepare an index table that is a table of pointers that indicates which number array element comes rst in numerical order, which is second, and so on. - Rank ; given an array, prepare a rank table that tells the numerical rank of an array element. The task of sorting N elements requires on the order of K \\cdot Nlog2N operations. The algorithm inventor tries to make K as small as possible (understanding that K = 0 is practically impossible). Three useful sorting algorithms are: 1. Straight insertion sort; 2. Heapsort sort; and 3. Quicksort sort. The choice of method depends on the size of the list that needs to be sorted. If the list is short (perhaps N < 50 elements) then straight insertion is fast enough, concise, and simple to program. For a long list ( N > 1000 elements) Quicksort is faster, but achieves the speed by use of extra memory. Heapsort is also good for large lists, and is an in-place routine. Python lists have a built-in sort() method that modifies the list in-place and a sorted() built-in function that builds a new sorted list from an iterable. So when sorting needs to be done, you should use the built-in tools. However, because it is a useful programming construct, the three sorting algorithms are presented as Python primitive codes.","title":"Sorting (Important Flow Control Cases)"},{"location":"1-Lessons/Lesson04/lesson4/#bubble-sort","text":"The bubble sort is a place to start despite it's relative slowness. It is a pretty reviled algorithm (read the Wikipedia entry), but it is the algorithm that a naive programmer might cobble together in a hurry, and despite its shortcomings (it's really slow and inefficient), it is robust. Here is a description of the sorting task as described by Christian and Griffths (2016) (pg. 65): \"Imagine you want to alphabetize your unsorted collection of books. A natural approach would be just to scan across the shelf looking for out- of-order pairs - Wallace followed by Pynchon, for instance - and flipping them around. Put Pynchon ahead of Wallace, then continue your scan, looping around to the beginning of the shelf each time you reach the end. When you make a complete pass without finding any more out-of-order pairs on the entire shelf, then you know the job is done. This process is a Bubble Sort, and it lands us in quadratic time. There are n books out of order, and each scan through the shelf can move each one at most one position. (We spot a tiny problem, make a tiny fix.) So in the worst case, where the shelf is perfectly backward, at least one book will need to be moved n positions. Thus a maximum of n passes through n books, which gives us O(n2) in the worst case. For instance, it means that sorting five shelves of books will take not five times as long as sorting a single shelf, but twenty-five times as long.\" Converting the word description into Python is fairly simple. We will have a vector of n numbers (we use a vector because its easy to step through the different positions), and we will scan through the vector once (and essentially find the smallest thing), and put it into the first position. Then we scan again from the second position and find the smallest thing remaining, and put it into the second position, and so on until the last scan which should have the remaining largest thing. If we desire a decreasing order, simply change the sense of the comparison. The algorithm defines an array and then sorts by repeated passes through the array. The program (outside of the sorting algorithm) is really quite simple. * Load contents into an array to be sorted. * Echo (print) the array (so we can verify the data are loaded as anticipated). * Loads the sorting function (the two loops) * Sort the array, put the results back into the array (an in-place sort). * Report the results. #array = [7,11,5,8,9,13,66,99,223] #array = [7,11,5] array=[1003 ,3.2 ,55.5 , -0.0001 , -6 ,666.6 ,102] howMany = len(array) print(\"Item Count = : \",howMany) print(\"Unsorted List : \", array) # insertion sort for irow in range(0, howMany-1) : for jrow in range(0,(howMany-1-irow)) : if array[jrow]> array[jrow+1] : swap = array[jrow] array[jrow]=array[jrow+1] array[jrow+1]=swap else: continue #results print(\"Sorted List : \", array, end =\"\") In the script we see that the program (near the bottom of the file) assigns the values to the vector named array and the initial order of the array is {1003, 3.2, 55.5,-0.0001,-6, 666.6, 102} . The smallest value in the example is -6 and it appears in the 5-th position, not the 1-st as it should. The first pass through the array will move the largest value, 1003, in sequence to the right until it occupies the last position. Repeated passes through the array move the remaining largest values to the right until the array is ordered. One can consider the values of the array at each scan of the array as a series of transformations (irow-th scan) -- in practical cases we don't necessarily care about the intermediate values, but here because the size is manageable and we are trying to get our feet wet with algorithms, we can look at the values. The sequence of results (transformations) after each pass through the array is shown in the following list: 1. Initial value: [1003; 3,2; 55,5;-0,0001;-6; 666,6; 102]. 2. First pass: [3,2; 55,5;-0,0001;-6; 666,6; 102; 1003]. 3. Second pass: [3,2;-0,0001;-6; 55,5; 102; 666,6; 1003]. 4. Third pass: [-0,0001;-6; 3,2; 55,5; 102; 666,6; 1003]. 5. Fourth pass: [-6;-0,0001; 3,2; 55,5; 102; 666,6; 1003]. 6. Fifth pass: [-6;-0,0001; 3,2; 55,5; 102; 666,6; 1003]. Sorted, fast scan. 7. Sixth pass: [-6;-0,0001; 3,2; 55,5; 102; 666,6; 1003]. Sorted, fast scan. We could probably add additional code to break from the scans when we have a single pass with no exchanges (like the last two scans) -- while meaningless in this example, for larger collections of things, being able to break out when the sorting is complete is a nice feature.","title":"Bubble Sort"},{"location":"1-Lessons/Lesson04/lesson4/#insertion-sort","text":"The next type of sorting would be to select one item and locate it either left or right of an adjacent item based on its size { like sorting a deck of cards, or perhaps a better description { again using the bookshelf analog from Christian and Griffths (2016) (pg. 65) You might take a different tack -- pulling all the books off the shelf and putting them back in place one by one. You'd put the ffrst book in the middle of the shelf, then take the second and compare it to the first, inserting it either to the right or to the left. Picking up the third book, you'd run through the books on the shelf from left to right until you found the right spot to tuck it in. Repeating this process, gradually all of the books would end up sorted on the shelf and you'd be done. Computer scientists call this, appropriately enough, Insertion Sort. The good news is that it's arguably even more intuitive than Bubble Sort and doesn't have quite the bad reputation. The bad news is that it's not actually that much faster. You still have to do one insertion for each book. And each insertion still involves moving past about half the books on the shelf, on average, to find the correct place. Although in practice Insertion Sort does run a bit faster than Bubble Sort, again we land squarely, if you will, in quadratic time. Sorting anything more than a single bookshelf is still an unwieldy prospect.\" Listing 8 is an R implementation of a straight insertion sort. The script is quite compact, and I used indentation and extra line spacing to keep track of the scoping delimiters. The sort works as follows, take the an element of the array (start with 2 and work to the right) and put it into a temporary location (called swap in my script). Then compare locations to the left of swap. If smaller, then break from the loop, exchange values, otherwise the values are currently ordered. Repeat (starting at the next element) , when all elements have been traversed the resulting vector is sorted. Here are the transformations for each pass through the outer loop:","title":"Insertion Sort"},{"location":"1-Lessons/Lesson04/lesson4/#straight-insertion","text":"The straight insertion sort is the algorithm a card player would use to sort cards. Pick out the second card and put it into order with respect to the first; then pick the third card and insert it into sequence with the first two; continue until the last card is picked out and inserted. Once the last card is sequenced, the result is a sorted deck (list). Python implementation of such an algorithm is: #array = [7,11,5,8,9,13,66,99,223] array = [7,11,5] howMany = len(array) print(\"Item Count = : \",howMany) print(\"Unsorted List : \", array, end =\"\") # insertion sort for i in range(1, len(array)): # Traverse through 1 to len(arr) key = array[i] # Move elements of arr[0..i-1], that are # greater than key, to one position ahead # of their current position j = i-1 while j >=0 and key < array[j] : array[j+1] = array[j] j -= 1 array[j+1] = key #results print(\"Sorted List : \", array, end =\"\") Probably useful to put into a functional structure: # Function to do insertion sort def insertionSort(array): # Traverse through 1 to len(arr) for i in range(1, len(array)): key = array[i] # Move elements of arr[0..i-1], that are # greater than key, to one position ahead # of their current position j = i-1 while j >=0 and key < array[j] : array[j+1] = array[j] j -= 1 array[j+1] = key return(array) array = [7,11,5,8,9,13,66,99,223] print(\"Unsorted List : \", array) insertionSort(array) print(\"Sorted List : \", array, end =\"\")","title":"Straight Insertion"},{"location":"1-Lessons/Lesson04/lesson4/#merge-sort","text":"A practical extension of these slow sorts is called the Merge Sort. It is an incredibly useful method. One simply breaks up the items into smaller arrays, sorts those arrays - then merges the sub-arrays into larger arrays (now already sorted), and nally merges the last two arrays into the nal, single, sorted array. Here is a better description, again from Christian and Griffths (2016): \" ... information processing began in the US censuses of the nineteenth century, with the development, by Herman Hollerith and later by IBM, of physical punch-card sorting devices. In 1936, IBM began producing a line of machines called \\collators\" that could merge two separately ordered stacks of cards into one. As long as the two stacks were themselves sorted, the procedure of merging them into a single sorted stack was incredibly straightforward and took linear time: simply compare the two top cards to each other, move the smaller of them to the new stack you're creating, and repeat until finished. The program that John von Neumann wrote in 1945 to demonstrate the power of the stored-program computer took the idea of collating to its beautiful and ultimate conclusion. Sorting two cards is simple: just put the smaller one on top. And given a pair of two-card stacks, both of them sorted, you can easily collate them into an ordered stack of four. Repeating this trick a few times, you'd build bigger and bigger stacks, each one of them already sorted. Soon enough, you could collate yourself a perfectly sorted full deck - with a final climactic merge, like a riffe shuffle's order- creating twin, producing the desired result. This approach is known today as Merge Sort, one of the legendary algorithms in computer science.\" There are several other variants related to Merge Sort; Quicksort and Heapsort being close relatives; # Python program for implementation of MergeSort # https://www.geeksforgeeks.org/merge-sort/ # This code is contributed by Mayank Khanna def mergeSort(arr): if len(arr) >1: mid = len(arr)//2 # Finding the mid of the array L = arr[:mid] # Dividing the array elements R = arr[mid:] # into 2 halves mergeSort(L) # Sorting the first half mergeSort(R) # Sorting the second half i = j = k = 0 # Copy data to temp arrays L[] and R[] while i < len(L) and j < len(R): if L[i] < R[j]: arr[k] = L[i] i+= 1 else: arr[k] = R[j] j+= 1 k+= 1 # Checking if any element was left while i < len(L): arr[k] = L[i] i+= 1 k+= 1 while j < len(R): arr[k] = R[j] j+= 1 k+= 1 # Code to print the list def printList(arr): for i in range(len(arr)): print(arr[i], end =\" \") print() # driver code to test the above code #if __name__ == '__main__': arr=[1003 ,3.2 ,55.5 , -0.0001 , -6 ,666.6 ,102] #arr = [12, 11, 13, 5, 6, 7] print (\"Given array is\", end =\"\\n\") printList(arr) mergeSort(arr) print(\"Sorted array is: \", end =\"\\n\") printList(arr)","title":"Merge Sort"},{"location":"1-Lessons/Lesson04/lesson4/#heapsort","text":"Need narrative here # Python program for implementation of heap Sort # To heapify subtree rooted at index i. # n is size of heap def heapify(arr, n, i): largest = i # Initialize largest as root l = 2 * i + 1 # left = 2*i + 1 r = 2 * i + 2 # right = 2*i + 2 # See if left child of root exists and is # greater than root if l < n and arr[i] < arr[l]: largest = l # See if right child of root exists and is # greater than root if r < n and arr[largest] < arr[r]: largest = r # Change root, if needed if largest != i: arr[i],arr[largest] = arr[largest],arr[i] # swap # Heapify the root. heapify(arr, n, largest) # The main function to sort an array of given size def heapSort(arr): n = len(arr) # Build a maxheap. # Since last parent will be at ((n//2)-1) we can start at that location. for i in range(n // 2 - 1, -1, -1): heapify(arr, n, i) # One by one extract elements for i in range(n-1, 0, -1): arr[i], arr[0] = arr[0], arr[i] # swap heapify(arr, i, 0) # Driver code to test above arr=[1003 ,3.2 ,55.5 , -0.0001 , -6 ,666.6 ,102] #arr = [ 12, 11, 13, 5, 6, 7] heapSort(arr) n = len(arr) print (\"Sorted array is\") for i in range(n): print (\"%d\" %arr[i]), # This code is contributed by Mohit Kumra","title":"Heapsort"},{"location":"1-Lessons/Lesson04/lesson4/#lexicographical-sorting","text":"Need narrative here # Python program to sort the words in lexicographical # order def sortLexo(my_string): # Split the my_string till where space is found. words = my_string.split() # sort() will sort the strings. words.sort() # Iterate i through 'words' to print the words # in alphabetical manner. for i in words: print( i ) # Driver code if __name__ == '__main__': my_string = \"hello this is example how to sort \" \\ \"the word in alphabetical manner\" # Calling function sortLexo(my_string) I conclude the section on sorting with one more quoted section from Christian and Griffiths (2016) about the value for sorting - which is already relevant to a lot of data science: \"The poster child for the advantages of sorting would be an Internet search engine like Google. It seems staggering to think that Google can take the search phrase you typed in and scour the entire Internet for it in less than half a second. Well, it can't - but it doesn't need to. If you're Google, you are almost certain that (a) your data will be searched, (b) it will be searched not just once but repeatedly, and (c) the time needed to sort is somehow less valuable\" than the time needed to search. (Here, sorting is done by machines ahead of time, before the results are needed, and searching is done by users for whom time is of the essence.) All of these factors point in favor of tremendous up-front sorting, which is indeed what Google and its fellow search engines do.\"","title":"Lexicographical Sorting"},{"location":"1-Lessons/Lesson04/lesson4/#references","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapters 3-6 https://www.inferentialthinking.com/chapters/03/programming-in-python.html Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. Brian Christian and Tom Griffiths (2016) ALGORITHMS TO LIVE BY: The Computer Science of Human Decisions Henry Holt and Co. (https://www.amazon.com/Algorithms-Live-Computer-Science-Decisions/dp/1627790365) import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info)","title":"References"},{"location":"1-Lessons/Lesson07/lesson6/","text":"ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 31 January 2021 Lesson 6 Classes, Objects, and File Handling: Classes and Objects Files Create (new), Open (existing) Read from .... Write to ... Close (save) Delete Special Script Blocks %%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} Objectives To understand the use of classes and objects to do effective coding in Python To understand the basic idea of how to manipulate the data in a file using file handling options in Python Classes and Objects In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated. When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class. Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state. Class definitions, like function definitions (def statements) must be executed before they have any effect. (You could conceivably place a class definition in a branch of an if statement, or inside a function.) In practice, the statements inside a class definition will usually be function definitions, but other statements are allowed, and sometimes useful \u2014 we\u2019ll come back to this later. The function definitions inside a class normally have a peculiar form of argument list, dictated by the calling conventions for methods \u2014 again, this is explained later. When a class definition is entered, a new namespace is created, and used as the local scope \u2014 thus, all assignments to local variables go into this new namespace. In particular, function definitions bind the name of the new function here. When a class definition is left normally (via the end), a class object is created. This is basically a wrapper around the contents of the namespace created by the class definition; we\u2019ll learn more about class objects in the next section. The original local scope (the one in effect just before the class definition was entered) is reinstated, and the class object is bound here to the class name given in the class definition header (ClassName in the example). What is an object? An object is simply a collection of data (variables) and methods (functions) that act on those data. Similarly, a class is a blueprint for that object. We can think of class as a sketch (prototype) of a house. It contains all the details about the floors, doors, windows etc. Based on these descriptions we build the house. House is the object. As many houses can be made from a house's blueprint, we can create many objects from a class. An object is also called an instance of a class and the process of creating this object is called instantiation Learn more at 1. https://docs.python.org/3/tutorial/classes.html 2. https://en.wikipedia.org/wiki/Class_(computer_programming) An Example: Write a class named ' Tax ' to calculate the state tax (in dollars) of Employees at Texas Tech University based on their annual salary. The state tax is 16% if the annual salary is below 80,000 dollars and 22% if the salary is more than 80,000 dollars. Employee Annual salary (dollars) Bob 150,000 Mary 78,000 John 55,000 Danny 175,000 Notes: Use docstrings to describe the purpose of the class. Use if....else conditional statements within the method of the class to choose the relevant tax % based on the annual salary. Create an object for employee and display the output as shown below. Bob's tax amount (in dollars): AMOUNT Mary's tax amount (in dollars): AMOUNT John's tax amount (in dollars): AMOUNT Danny's tax amount (in dollars): AMOUNT class Tax: \"\"\"This class calculates the tax amount based on the annual salary and the state tax %\"\"\" def __init__(self, salary): # here is the instantiation constructor self.salary = salary def taxamount(self): # here is a method (function) that can operate on the class once created if self.salary < 80000: return self.salary*(16/100) else: return self.salary*(22/100) bob = Tax(150000) dir(bob) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] bob = Tax(150000) # objects constructed using Tax class mary = Tax(78000) john = Tax(55000) danny = Tax(175000) dir(Tax) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'taxamount'] dir(mary) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] print(\"bobz salary \", bob.salary ) print(\"Bob's tax amount (in dollars):\", bob.taxamount() ) print(\"Mary's tax amount (in dollars):\", mary.taxamount()) print(\"John's tax amount (in dollars):\", john.taxamount()) print(\"Danny's tax amount (in dollars):\", danny.taxamount()) bobz salary 150000 Bob's tax amount (in dollars): 33000.0 Mary's tax amount (in dollars): 12480.0 John's tax amount (in dollars): 8800.0 Danny's tax amount (in dollars): 38500.0 Numbers, strings, lists, and dictionaries are all objects that are instances of a parent class print(type(0)) <class 'int'> print(type(\"\")) <class 'str'> print(type([1, 2, 3, 4])) <class 'list'> To get more information about the built-in classes and objects, use dir( ) and help( ) functions print(dir(int)) print(help(\"\")) User-defined classes: Defining docstrings class Dog: \"\"\"This class enables the dog to say its name and age in dog years\"\"\" def __init__(self, name, years): \"\"\"This function contains all the necessary attributes\"\"\" self.name = name self.years = years self.dog_age = years*9 def sound(self): \"\"\"This function enables the dog to speak\"\"\" print(\"woof! I am {} and I am {} dog years old! woof!\".format(self.name, self.dog_age)) fudge = Dog(\"Fudge\", 2) maple = Dog(\"Maple\", 1.5) fudge.sound() maple.sound() woof! I am Fudge and I am 18 dog years old! woof! woof! I am Maple and I am 13.5 dog years old! woof! help(Dog) Help on class Dog in module __main__: class Dog(builtins.object) | Dog(name, years) | | This class enables the dog to say its name and age in dog years | | Methods defined here: | | __init__(self, name, years) | This function contains all the necessary attributes | | sound(self) | This function enables the dog to speak | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) Files and Filesystems Background A computer file is a computer resource for recording data discretely (not in the secretive context, but specifically somewhere on a piece of hardware) in a computer storage device. Just as words can be written to paper, so can information be written to a computer file. Files can be edited and transferred through the internet on that particular computer system. There are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once. By using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times. Typically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access. File system In computing, a file system or filesystem, controls how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stops and the next begins. By separating the data into pieces and giving each piece a name, the data is isolated and identified. Taking its name from the way paper-based data management system is named, each group of data is called a \u201cfile\u201d. The structure and logic rules used to manage the groups of data and their names is called a \u201cfile system\u201d. Path A path, the general form of the name of a file or directory, specifies a unique location in a file system. A path points to a file system location by following the directory tree hierarchy expressed in a string of characters in which path components, separated by a delimiting character, represent each directory. The delimiting character is most commonly the slash (\u201d/\u201d), the backslash character (\u201d\\\u201d), or colon (\u201d:\u201d), though some operating systems may use a different delimiter. Paths are used extensively in computer science to represent the directory/file relationships common in modern operating systems, and are essential in the construction of Uniform Resource Locators (URLs). Resources can be represented by either absolute or relative paths. As an example consider the following two files: /Users/theodore/MyGit/@atomickitty/hurri-sensors/.git/Guest.conf /etc/apache2/users/Guest.conf They both have the same file name, but are located on different paths. Failure to provide the path when addressing the file can be a problem. Another way to interpret is that the two unique files actually have different names, and only part of those names is common (Guest.conf) The two names above (including the path) are called fully qualified filenames (or absolute names), a relative path (usually relative to the file or program of interest depends on where in the directory structure the file lives. If we are currently in the .git directory (the first file) the path to the file is just the filename. We have experienced path issues with dependencies on .png files - in general your JupyterLab notebooks on CoCalc can only look at the local directory which is why we have to copy files into the directory for things to work. File Types Text Files. Text files are regular files that contain information readable by the user. This information is stored in ASCII. You can display and print these files. The lines of a text file must not contain NULL characters, and none can exceed a prescribed (by architecture) length, including the new-line character. The term text file does not prevent the inclusion of control or other nonprintable characters (other than NUL). Therefore, standard utilities that list text files as inputs or outputs are either able to process the special characters gracefully or they explicitly describe their limitations within their individual sections. Binary Files. Binary files are regular files that contain information readable by the computer. Binary files may be executable files that instruct the system to accomplish a job. Commands and programs are stored in executable, binary files. Special compiling programs translate ASCII text into binary code. The only difference between text and binary files is that text files have lines of less than some length, with no NULL characters, each terminated by a new-line character. Directory Files. Directory files contain information the system needs to access all types of files, but they do not contain the actual file data. As a result, directories occupy less space than a regular file and give the file system structure flexibility and depth. Each directory entry represents either a file or a subdirectory. Each entry contains the name of the file and the file's index node reference number (i-node). The i-node points to the unique index node assigned to the file. The i-node describes the location of the data associated with the file. Directories are created and controlled by a separate set of commands. File Manipulation For this lesson we examine just a handfull of file manipulations which are quite useful. Files can be \"created\",\"read\",\"updated\", or \"deleted\" (CRUD). Example: Create a file, write to it. Below is an example of creating a file that does not yet exist. The script is a bit pendandic on purpose. First will use some system commands to view the contents of the local directory import sys # on a Mac/Linux ! rm -rf myfirstfile.txt # delete file if it exists ! pwd # list name of working directory, note it includes path, so it is an absolute path # on Winderz #! del myfirstfile.txt # delete file if it exists #! %pwd # list name of working directory, note it includes path, so it is an absolute path /home/sensei/1330-textbook-webroot/docs/lesson6 # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 752 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt # create file example externalfile = open(\"myfirstfile.txt\",'w') # create connection to file, set to write (w), file does not need to exist mymessage = 'message in a bottle' #some object to write, in this case a string externalfile.write(mymessage)# write the contents of mymessage to the file externalfile.close() # close the file connection At this point our new file should exist, lets list the directory and see if that is so # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 756 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 19 Feb 17 17:35 myfirstfile.txt -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt Sure enough, its there, we will use a bash command cat to look at the contents of the file. ! cat myfirstfile.txt # Mac/Linux # ! type myfirstfile.txt # Winderz message in a bottle Example: Read from an existing file. We will continue using the file we just made, and read from it the example is below # read file example externalfile = open(\"myfirstfile.txt\",'r') # create connection to file, set to read (r), file must exist silly_string = externalfile.read() # read the contents externalfile.close() # close the file connection print(silly_string) message in a bottle Example: Update a file. This example continues with our same file, but we will now add contents without destroying existing contents. The keyword is append externalfile = open(\"myfirstfile.txt\",'a') # create connection to file, set to append (a), file does not need to exist externalfile.write('\\n') # adds a newline character what_to_add = 'I love rock-and-roll, put another dime in the jukebox baby ... \\n' externalfile.write(what_to_add) # add a string including the linefeed what_to_add = '... the waiting is the hardest part \\n' externalfile.write(what_to_add) # add a string including the linefeed mylist = [1,2,3,4,5] # a list of numbers what_to_add = ','.join(map(repr, mylist)) + \"\\n\" # one way to write the list externalfile.write(what_to_add) what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" # another way to write the list externalfile.write(what_to_add) externalfile.close() As before we can examine the contents using a shell command sent from the notebook. ! cat myfirstfile.txt # ! type myfirstfile.txt # Winderz message in a bottle I love rock-and-roll, put another dime in the jukebox baby ... ... the waiting is the hardest part 1,2,3,4,5 1,2,3,4,5 Example: Delete a file Delete can be done by a system call as we did above to clear the local directory In a JupyterLab notebook, we can either use import sys ! rm -rf myfirstfile.txt # delete file if it exists or import os os.remove(\"myfirstfile.txt\") they both have same effect, both equally dangerous to your filesystem. Learn more about CRUD with text files at https://www.guru99.com/reading-and-writing-files-in-python.html Learn more about file delete at https://www.dummies.com/programming/python/how-to-delete-a-file-in-python/ import os file2kill = \"myfirstfile.txt\" try: os.remove(file2kill) # file must exist or will generate an exception except: pass # example of using pass to improve readability print(file2kill, \" missing or deleted !\") myfirstfile.txt missing or deleted ! A little discussion on the part where we wrote numbers what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" Here are descriptions of the two functions map and repr map(function, iterable, ...) Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list. repr(object) Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse quotes). It is sometimes useful to be able to access this operation as an ordinary function. For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() , otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. A class can control what this function returns for its instances by defining a repr() method. What they do in this script is important. The statement: what_to_add = \u2019,\u2019.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" is building a string that will be comprised of elements of mylist[0:len(mylist)]. The repr() function gets these elements as they are represented in the computer, the delimiter a comma is added using the join method in Python, and because everything is now a string the ... + \"\\n\" puts a linefeed character at the end of the string so the output will start a new line the next time something is written. Example create a text file, name it \"MyFavoriteQuotation\" . Write your favorite quotation in the file. Read the file. Add this string to it in a new line : \"And that's something I wish I had said...\" Show the final outcome. # create the \"My Favorite Quotation\" file: externalfile = open(\"MyFavoriteQuotation.txt\",'w') # create connection to file, set to write (w) myquotation = 'The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.' #My choice: quotation from Pulp Fiction externalfile.write(myquotation)# write the contents of mymessage to the file externalfile.close() # close the file connection #Let's read the file ! cat MyFavoriteQuotation.txt # Let's add the string externalfile = open(\"MyFavoriteQuotation.txt\",'a') #create connection to file, set to append (a) externalfile.write('\\n') # adds a newline character what_to_add = \"And that's something I wish I had said ... \\n\" externalfile.write(what_to_add) externalfile.close() #Let's read the file one last time ! cat MyFavoriteQuotation.txt # ! type MyFavoriteQuotation # Winderz The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you. And that's something I wish I had said ... References Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty compthink /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Computational Linear Algebra using NUMPY"},{"location":"1-Lessons/Lesson07/lesson6/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 31 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson07/lesson6/#lesson-6-classes-objects-and-file-handling","text":"Classes and Objects Files Create (new), Open (existing) Read from .... Write to ... Close (save) Delete","title":"Lesson 6 Classes, Objects, and File Handling:"},{"location":"1-Lessons/Lesson07/lesson6/#special-script-blocks","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;}","title":"Special Script Blocks"},{"location":"1-Lessons/Lesson07/lesson6/#objectives","text":"To understand the use of classes and objects to do effective coding in Python To understand the basic idea of how to manipulate the data in a file using file handling options in Python","title":"Objectives"},{"location":"1-Lessons/Lesson07/lesson6/#classes-and-objects","text":"In object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated. When an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class. Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state. Class definitions, like function definitions (def statements) must be executed before they have any effect. (You could conceivably place a class definition in a branch of an if statement, or inside a function.) In practice, the statements inside a class definition will usually be function definitions, but other statements are allowed, and sometimes useful \u2014 we\u2019ll come back to this later. The function definitions inside a class normally have a peculiar form of argument list, dictated by the calling conventions for methods \u2014 again, this is explained later. When a class definition is entered, a new namespace is created, and used as the local scope \u2014 thus, all assignments to local variables go into this new namespace. In particular, function definitions bind the name of the new function here. When a class definition is left normally (via the end), a class object is created. This is basically a wrapper around the contents of the namespace created by the class definition; we\u2019ll learn more about class objects in the next section. The original local scope (the one in effect just before the class definition was entered) is reinstated, and the class object is bound here to the class name given in the class definition header (ClassName in the example).","title":"Classes and Objects"},{"location":"1-Lessons/Lesson07/lesson6/#what-is-an-object","text":"An object is simply a collection of data (variables) and methods (functions) that act on those data. Similarly, a class is a blueprint for that object. We can think of class as a sketch (prototype) of a house. It contains all the details about the floors, doors, windows etc. Based on these descriptions we build the house. House is the object. As many houses can be made from a house's blueprint, we can create many objects from a class. An object is also called an instance of a class and the process of creating this object is called instantiation Learn more at 1. https://docs.python.org/3/tutorial/classes.html 2. https://en.wikipedia.org/wiki/Class_(computer_programming)","title":"What is an object?"},{"location":"1-Lessons/Lesson07/lesson6/#an-example","text":"Write a class named ' Tax ' to calculate the state tax (in dollars) of Employees at Texas Tech University based on their annual salary. The state tax is 16% if the annual salary is below 80,000 dollars and 22% if the salary is more than 80,000 dollars. Employee Annual salary (dollars) Bob 150,000 Mary 78,000 John 55,000 Danny 175,000","title":"An Example:"},{"location":"1-Lessons/Lesson07/lesson6/#notes","text":"Use docstrings to describe the purpose of the class. Use if....else conditional statements within the method of the class to choose the relevant tax % based on the annual salary. Create an object for employee and display the output as shown below. Bob's tax amount (in dollars): AMOUNT Mary's tax amount (in dollars): AMOUNT John's tax amount (in dollars): AMOUNT Danny's tax amount (in dollars): AMOUNT class Tax: \"\"\"This class calculates the tax amount based on the annual salary and the state tax %\"\"\" def __init__(self, salary): # here is the instantiation constructor self.salary = salary def taxamount(self): # here is a method (function) that can operate on the class once created if self.salary < 80000: return self.salary*(16/100) else: return self.salary*(22/100) bob = Tax(150000) dir(bob) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] bob = Tax(150000) # objects constructed using Tax class mary = Tax(78000) john = Tax(55000) danny = Tax(175000) dir(Tax) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'taxamount'] dir(mary) ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'salary', 'taxamount'] print(\"bobz salary \", bob.salary ) print(\"Bob's tax amount (in dollars):\", bob.taxamount() ) print(\"Mary's tax amount (in dollars):\", mary.taxamount()) print(\"John's tax amount (in dollars):\", john.taxamount()) print(\"Danny's tax amount (in dollars):\", danny.taxamount()) bobz salary 150000 Bob's tax amount (in dollars): 33000.0 Mary's tax amount (in dollars): 12480.0 John's tax amount (in dollars): 8800.0 Danny's tax amount (in dollars): 38500.0 Numbers, strings, lists, and dictionaries are all objects that are instances of a parent class print(type(0)) <class 'int'> print(type(\"\")) <class 'str'> print(type([1, 2, 3, 4])) <class 'list'> To get more information about the built-in classes and objects, use dir( ) and help( ) functions print(dir(int)) print(help(\"\")) User-defined classes: Defining docstrings class Dog: \"\"\"This class enables the dog to say its name and age in dog years\"\"\" def __init__(self, name, years): \"\"\"This function contains all the necessary attributes\"\"\" self.name = name self.years = years self.dog_age = years*9 def sound(self): \"\"\"This function enables the dog to speak\"\"\" print(\"woof! I am {} and I am {} dog years old! woof!\".format(self.name, self.dog_age)) fudge = Dog(\"Fudge\", 2) maple = Dog(\"Maple\", 1.5) fudge.sound() maple.sound() woof! I am Fudge and I am 18 dog years old! woof! woof! I am Maple and I am 13.5 dog years old! woof! help(Dog) Help on class Dog in module __main__: class Dog(builtins.object) | Dog(name, years) | | This class enables the dog to say its name and age in dog years | | Methods defined here: | | __init__(self, name, years) | This function contains all the necessary attributes | | sound(self) | This function enables the dog to speak | | ---------------------------------------------------------------------- | Data descriptors defined here: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined)","title":"Notes:"},{"location":"1-Lessons/Lesson07/lesson6/#files-and-filesystems","text":"","title":"Files and Filesystems"},{"location":"1-Lessons/Lesson07/lesson6/#background","text":"A computer file is a computer resource for recording data discretely (not in the secretive context, but specifically somewhere on a piece of hardware) in a computer storage device. Just as words can be written to paper, so can information be written to a computer file. Files can be edited and transferred through the internet on that particular computer system. There are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once. By using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times. Typically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access.","title":"Background"},{"location":"1-Lessons/Lesson07/lesson6/#file-system","text":"In computing, a file system or filesystem, controls how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stops and the next begins. By separating the data into pieces and giving each piece a name, the data is isolated and identified. Taking its name from the way paper-based data management system is named, each group of data is called a \u201cfile\u201d. The structure and logic rules used to manage the groups of data and their names is called a \u201cfile system\u201d.","title":"File system"},{"location":"1-Lessons/Lesson07/lesson6/#path","text":"A path, the general form of the name of a file or directory, specifies a unique location in a file system. A path points to a file system location by following the directory tree hierarchy expressed in a string of characters in which path components, separated by a delimiting character, represent each directory. The delimiting character is most commonly the slash (\u201d/\u201d), the backslash character (\u201d\\\u201d), or colon (\u201d:\u201d), though some operating systems may use a different delimiter. Paths are used extensively in computer science to represent the directory/file relationships common in modern operating systems, and are essential in the construction of Uniform Resource Locators (URLs). Resources can be represented by either absolute or relative paths. As an example consider the following two files: /Users/theodore/MyGit/@atomickitty/hurri-sensors/.git/Guest.conf /etc/apache2/users/Guest.conf They both have the same file name, but are located on different paths. Failure to provide the path when addressing the file can be a problem. Another way to interpret is that the two unique files actually have different names, and only part of those names is common (Guest.conf) The two names above (including the path) are called fully qualified filenames (or absolute names), a relative path (usually relative to the file or program of interest depends on where in the directory structure the file lives. If we are currently in the .git directory (the first file) the path to the file is just the filename. We have experienced path issues with dependencies on .png files - in general your JupyterLab notebooks on CoCalc can only look at the local directory which is why we have to copy files into the directory for things to work.","title":"Path"},{"location":"1-Lessons/Lesson07/lesson6/#file-types","text":"Text Files. Text files are regular files that contain information readable by the user. This information is stored in ASCII. You can display and print these files. The lines of a text file must not contain NULL characters, and none can exceed a prescribed (by architecture) length, including the new-line character. The term text file does not prevent the inclusion of control or other nonprintable characters (other than NUL). Therefore, standard utilities that list text files as inputs or outputs are either able to process the special characters gracefully or they explicitly describe their limitations within their individual sections. Binary Files. Binary files are regular files that contain information readable by the computer. Binary files may be executable files that instruct the system to accomplish a job. Commands and programs are stored in executable, binary files. Special compiling programs translate ASCII text into binary code. The only difference between text and binary files is that text files have lines of less than some length, with no NULL characters, each terminated by a new-line character. Directory Files. Directory files contain information the system needs to access all types of files, but they do not contain the actual file data. As a result, directories occupy less space than a regular file and give the file system structure flexibility and depth. Each directory entry represents either a file or a subdirectory. Each entry contains the name of the file and the file's index node reference number (i-node). The i-node points to the unique index node assigned to the file. The i-node describes the location of the data associated with the file. Directories are created and controlled by a separate set of commands.","title":"File Types"},{"location":"1-Lessons/Lesson07/lesson6/#file-manipulation","text":"For this lesson we examine just a handfull of file manipulations which are quite useful. Files can be \"created\",\"read\",\"updated\", or \"deleted\" (CRUD).","title":"File Manipulation"},{"location":"1-Lessons/Lesson07/lesson6/#example-create-a-file-write-to-it","text":"Below is an example of creating a file that does not yet exist. The script is a bit pendandic on purpose. First will use some system commands to view the contents of the local directory import sys # on a Mac/Linux ! rm -rf myfirstfile.txt # delete file if it exists ! pwd # list name of working directory, note it includes path, so it is an absolute path # on Winderz #! del myfirstfile.txt # delete file if it exists #! %pwd # list name of working directory, note it includes path, so it is an absolute path /home/sensei/1330-textbook-webroot/docs/lesson6 # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 752 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt # create file example externalfile = open(\"myfirstfile.txt\",'w') # create connection to file, set to write (w), file does not need to exist mymessage = 'message in a bottle' #some object to write, in this case a string externalfile.write(mymessage)# write the contents of mymessage to the file externalfile.close() # close the file connection At this point our new file should exist, lets list the directory and see if that is so # on a Mac/Linux ! ls -l # list contents of working directory # on Winderz #! dir # list contents of working directory total 756 -rw-rw-r-- 1 sensei sensei 9353 Feb 17 17:31 ClassObjects_FileHandling_LabSession.ipynb -rw-rw-r-- 1 sensei sensei 38589 Feb 17 17:31 ClassObjects_FileHandling_LectureSession.ipynb -rw-rw-r-- 1 sensei sensei 627474 Feb 17 17:31 ENGR-1330-Lesson6-Dev.html -rw-rw-r-- 1 sensei sensei 36134 Feb 17 17:31 ENGR-1330-Lesson6-Dev.ipynb -rw-rw-r-- 1 sensei sensei 524 Feb 17 17:31 ReadingFile.txt -rw-rw-r-- 1 sensei sensei 36342 Feb 17 17:34 lesson6.ipynb -rw-rw-r-- 1 sensei sensei 19 Feb 17 17:35 myfirstfile.txt -rw-rw-r-- 1 sensei sensei 153 Feb 17 17:31 sample-Copy1.txt -rw-rw-r-- 1 sensei sensei 111 Feb 17 17:31 sample.txt Sure enough, its there, we will use a bash command cat to look at the contents of the file. ! cat myfirstfile.txt # Mac/Linux # ! type myfirstfile.txt # Winderz message in a bottle","title":"Example: Create a file, write to it."},{"location":"1-Lessons/Lesson07/lesson6/#example-read-from-an-existing-file","text":"We will continue using the file we just made, and read from it the example is below # read file example externalfile = open(\"myfirstfile.txt\",'r') # create connection to file, set to read (r), file must exist silly_string = externalfile.read() # read the contents externalfile.close() # close the file connection print(silly_string) message in a bottle","title":"Example: Read from an existing file."},{"location":"1-Lessons/Lesson07/lesson6/#example-update-a-file","text":"This example continues with our same file, but we will now add contents without destroying existing contents. The keyword is append externalfile = open(\"myfirstfile.txt\",'a') # create connection to file, set to append (a), file does not need to exist externalfile.write('\\n') # adds a newline character what_to_add = 'I love rock-and-roll, put another dime in the jukebox baby ... \\n' externalfile.write(what_to_add) # add a string including the linefeed what_to_add = '... the waiting is the hardest part \\n' externalfile.write(what_to_add) # add a string including the linefeed mylist = [1,2,3,4,5] # a list of numbers what_to_add = ','.join(map(repr, mylist)) + \"\\n\" # one way to write the list externalfile.write(what_to_add) what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" # another way to write the list externalfile.write(what_to_add) externalfile.close() As before we can examine the contents using a shell command sent from the notebook. ! cat myfirstfile.txt # ! type myfirstfile.txt # Winderz message in a bottle I love rock-and-roll, put another dime in the jukebox baby ... ... the waiting is the hardest part 1,2,3,4,5 1,2,3,4,5","title":"Example: Update a file."},{"location":"1-Lessons/Lesson07/lesson6/#example-delete-a-file","text":"Delete can be done by a system call as we did above to clear the local directory In a JupyterLab notebook, we can either use import sys ! rm -rf myfirstfile.txt # delete file if it exists or import os os.remove(\"myfirstfile.txt\") they both have same effect, both equally dangerous to your filesystem. Learn more about CRUD with text files at https://www.guru99.com/reading-and-writing-files-in-python.html Learn more about file delete at https://www.dummies.com/programming/python/how-to-delete-a-file-in-python/ import os file2kill = \"myfirstfile.txt\" try: os.remove(file2kill) # file must exist or will generate an exception except: pass # example of using pass to improve readability print(file2kill, \" missing or deleted !\") myfirstfile.txt missing or deleted ! A little discussion on the part where we wrote numbers what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" Here are descriptions of the two functions map and repr map(function, iterable, ...) Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list. repr(object) Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse quotes). It is sometimes useful to be able to access this operation as an ordinary function. For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() , otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. A class can control what this function returns for its instances by defining a repr() method. What they do in this script is important. The statement: what_to_add = \u2019,\u2019.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" is building a string that will be comprised of elements of mylist[0:len(mylist)]. The repr() function gets these elements as they are represented in the computer, the delimiter a comma is added using the join method in Python, and because everything is now a string the ... + \"\\n\" puts a linefeed character at the end of the string so the output will start a new line the next time something is written.","title":"Example: Delete a file"},{"location":"1-Lessons/Lesson07/lesson6/#example","text":"create a text file, name it \"MyFavoriteQuotation\" . Write your favorite quotation in the file. Read the file. Add this string to it in a new line : \"And that's something I wish I had said...\" Show the final outcome. # create the \"My Favorite Quotation\" file: externalfile = open(\"MyFavoriteQuotation.txt\",'w') # create connection to file, set to write (w) myquotation = 'The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.' #My choice: quotation from Pulp Fiction externalfile.write(myquotation)# write the contents of mymessage to the file externalfile.close() # close the file connection #Let's read the file ! cat MyFavoriteQuotation.txt # Let's add the string externalfile = open(\"MyFavoriteQuotation.txt\",'a') #create connection to file, set to append (a) externalfile.write('\\n') # adds a newline character what_to_add = \"And that's something I wish I had said ... \\n\" externalfile.write(what_to_add) externalfile.close() #Let's read the file one last time ! cat MyFavoriteQuotation.txt # ! type MyFavoriteQuotation # Winderz The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you. And that's something I wish I had said ...","title":"Example"},{"location":"1-Lessons/Lesson07/lesson6/#references","text":"Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty compthink /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"References"},{"location":"1-Lessons/Lesson08/lesson7/","text":"ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 14 February 2021 Lesson 7 The Pandas module About Pandas How to install Anaconda JupyterHub/Lab (on Linux) JupyterHub/Lab (on MacOS) JupyterHub/Lab (on Windoze) The Dataframe Primatives Using Pandas Create, Modify, Delete datagrames Slice Dataframes Conditional Selection Synthetic Programming (Symbolic Function Application) Files Access Files from a remote Web Server Get file contents Get the actual file Adaptations for encrypted servers (future semester) Special Script Blocks %%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} Objectives To understand the dataframe abstraction as implemented in the Pandas library(module). To be able to access and manipulate data within a dataframe To be able to obtain basic statistical measures of data within a dataframe Read/Write from/to files MS Excel-type files (.xls,.xlsx,.csv) (LibreOffice files use the MS .xml standard) Ordinary ASCII (.txt) files Access files directly from a URL (advanced concept) Using a wget-type function Using a curl-type function Using API keys (future versions) Pandas: Pandas is the core library for dataframe manipulation in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is derived from the term \u2018Panel Data\u2019. If you are curious about Pandas, this cheat sheet is recommended: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf Data Structure The Primary data structure is called a dataframe. It is an abstraction where data are represented as a 2-dimensional mutable and heterogenous tabular data structure; much like a Worksheet in MS Excel. The structure itself is popular among statisticians and data scientists and business executives. According to the marketing department \"Pandas Provides rich data structures and functions designed to make working with data fast, easy, and expressive. It is useful in data manipulation, cleaning, and analysis; Pandas excels in performance and productivity \" The Dataframe A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. Like MS Excel we can query the dataframe to find the contents of a particular cell using its row name and column name , or operate on entire rows and columns To use pandas, we need to import the module. Computational Thinking Concepts The CT concepts expressed within Pandas include: Decomposition : Data interpretation, manipulation, and analysis of Pandas dataframes is an act of decomposition -- although the dataframes can be quite complex. Abstraction : The dataframe is a data representation abstraction that allows for placeholder operations, later substituted with specific contents for a problem; enhances reuse and readability. We leverage the principle of algebraic replacement using these abstractions. Algorithms : Data interpretation, manipulation, and analysis of dataframes are generally implemented as part of a supervisory algorithm. Module Set-Up In principle, Pandas should be available in a default Anaconda install - You should not have to do any extra installation steps to install the library in Python - You do have to import the library in your scripts How to check - Simply open a code cell and run import pandas if the notebook does not protest (i.e. pink block of error), the youis good to go. import pandas If you do get an error, that means that you will have to install using conda or pip ; you are on-your-own here! On the content server the process is: Open a new terminal from the launcher Change to root user su then enter the root password sudo -H /opt/jupyterhib/bin/python3 -m pip install pandas Wait until the install is complete; for security, user compthink is not in the sudo group Verify the install by trying to execute import pandas as above. The process above will be similar on a Macintosh, or Windows if you did not use an Anaconda distribution. Best is to have a sucessful anaconda install, or go to the GoodJobUntilMyOrgansGetHarvested . If you have to do this kind of install, you will have to do some reading, some references I find useful are: 1. https://jupyterlab.readthedocs.io/en/stable/user/extensions.html 2. https://www.pugetsystems.com/labs/hpc/Note-How-To-Install-JupyterHub-on-a-Local-Server-1673/#InstallJupyterHub 3. https://jupyterhub.readthedocs.io/en/stable/installation-guide-hard.html (This is the approach on the content server which has a functioning JupyterHub) Dataframe-type Structure using primative python First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. import numpy mytabular = numpy.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 19, 21, 22, 81] ['B', 94, 75, 66, 44] ['C', 70, 56, 47, 63] ['D', 56, 80, 39, 39] ['E', 66, 78, 39, 15] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 70, 56, 47, 63] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 21 75 56 80 78 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 47 Now we shall create a proper dataframe We will now do the same using pandas mydf = pandas.DataFrame(numpy.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 We can also turn our table into a dataframe, notice how the constructor adds header row and index column mydf1 = pandas.DataFrame(mytable) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 19 21 22 81 2 B 94 75 66 44 3 C 70 56 47 63 4 D 56 80 39 39 5 E 66 78 39 15 To get proper behavior, we can just reuse our original objects mydf2 = pandas.DataFrame(mytabular,myrowname,mycolname) mydf2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 19 21 22 81 B 94 75 66 44 C 70 56 47 63 D 56 80 39 39 E 66 78 39 15 Why are mydf and mydf2 different? Getting the shape of dataframes The shape method, which is available after the dataframe is constructed, will return the row and column rank (count) of a dataframe. mydf.shape (5, 4) mydf1.shape (6, 5) mydf2.shape (5, 4) Appending new columns To append a column simply assign a value to a new column name to the dataframe mydf['new']= 'NA' mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA Appending new rows This is sometimes a bit trickier but here is one way: - create a copy of a row, give it a new name. - concatenate it back into the dataframe. newrow = mydf.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pandas.concat([mydf,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA X 80 67 4 24 NA Removing Rows and Columns To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 # or just use drop with axis specify newtable.drop('X', axis=0, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 Indexing We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 20 B 8 D 83 E 67 Name: X, dtype: int64 newtable[['X','W']] #Selecing a multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 20 97 B 8 74 D 83 92 E 67 80 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 80 X 67 Y 4 Z 24 Name: E, dtype: int64 newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 newtable.loc[['E','D','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 80 67 4 24 D 92 83 90 28 B 74 8 7 99 newtable.loc[['B','E','D'],['X','Y']] #Selecting elements via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 8 7 E 67 4 D 83 90 Conditional Selection mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? mydf[mydf['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? mydf[mydf['col2']==mydf['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object Descriptor Functions #Creating a dataframe from a dictionary mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach head method Returns the first few rows, useful to infer structure #Returns only the first five rows mydf.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit info method Returns the data model (data column count, names, data types) #Info about the dataframe mydf.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes describe method Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe mydf.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000 Counting and Sum methods There are also methods for counts and sums by specific columns mydf['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) mydf['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values mydf['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) mydf['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64 Using functions in dataframes - symbolic apply The power of Pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. This employs principles of pattern matching , abstraction , and algorithm development ; a holy trinity of Computational Thinning. It's somewhat complicated but quite handy, best shown by an example: def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(mydf) print('Apply the times2 function to col2') mydf['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64 Sorts mydf.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit mydf.sort_values('col3', ascending = True) #Lexiographic sort .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 1 2 555 apple 6 7 222 banana 2 3 666 grape 4 5 666 jackfruit 3 4 444 mango 0 1 444 orange 7 8 222 peach 5 6 111 watermelon Aggregating (Grouping Values) dataframe contents #Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } mydf1 = pandas.DataFrame(data) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' mydf1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' mydf1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27 Filtering out missing values Filtering and cleaning are often used to describe the process where data that does not support a narrative is removed ;typically for maintenance of profit applications, if the data are actually missing that is common situation where cleaning is justified. #Creating a dataframe from a dictionary df = pandas.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach Reading a File into a Dataframe Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) # xlsx reads deprecated here is a hack using openpyxl readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 Writing a dataframe to file #Creating and writing to a .csv file readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pandas.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pandas.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', index = False, engine='openpyxl') readfileexcel = pandas.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 Downloading files from websites (optional) This section shows how to get files from a remote computer. There are several ways to get the files, most importantly you need the FQDN to the file. Method 1: Get data from a file on a remote server (unencrypted) This section shows how to obtain data files from public URLs. Prerequesites: You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://... Web Developer Notes If you want to distribute files (web developers) the files need to be in the server webroot, but can be deep into the heirarchial structure. Here we will do an example with a file that contains topographic data in XYZ format, without header information. The first few lines of the remote file look like: 74.90959724 93.21251922 0 75.17907367 64.40278759 0 94.9935575 93.07951286 0 95.26234119 64.60091165 0 54.04976655 64.21159095 0 54.52914363 35.06934342 0 75.44993558 34.93079513 0 75.09317373 5.462959114 0 74.87357468 10.43130083 0 74.86249082 15.72938748 0 And importantly it is tab delimited. The module to manipulate url in python is called urllib Google search to learn more, here we are using only a small component without exception trapping. #Step 1: import needed modules to interact with the internet from urllib.request import urlopen # import a method that will connect to a url and read file contents import pandas #import pandas This next code fragment sets a string called remote_url ; it is just a variable, name can be anything that honors python naming rules. Then the urllib function urlopen with read and decode methods is employed, the result is stored in an object named elevationXYZ #Step 2: make the connection to the remote file (actually its implementing \"bash curl -O http://fqdn/path ...\") remote_url = 'http://www.rtfmps.com/share_files/pip-corner-sumps.txt' # elevationXYZ = urlopen(remote_url).read().decode().split() # Gets the file contents as a single vector, comma delimited, file is not retained locally At this point the object exists as a single vector with hundreds of elements. We now need to structure the content. Here using python primatives, and knowing how the data are supposed to look, we prepare variables to recieve the structured results #Step 3 Python primatives to structure the data, or use fancy modules (probably easy in numpy) howmany = len(elevationXYZ) # how long is the vector? nrow = int(howmany/3) xyz = [[0 for j in range(3)] for j in range(nrow)] # null space to receive data define columnX Now that everything is ready, we can extract from the object the values we want into xyz #Step4 Now will build xyz as a matrix with 3 columns index = 0 for irow in range(0,nrow): xyz[irow][0]=float(elevationXYZ[index]) xyz[irow][1]=float(elevationXYZ[index+1]) xyz[irow][2]=float(elevationXYZ[index+2]) index += 3 #increment the index xyz is now a 3-column float array and can now probably be treated as a data frame. Here we use a pandas method to build the dataframe. df = pandas.DataFrame(xyz) Get some info, yep three columns (ordered triples to be precise!) df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 774 entries, 0 to 773 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 774 non-null float64 1 1 774 non-null float64 2 2 774 non-null float64 dtypes: float64(3) memory usage: 18.3 KB And some summary statistics (meaningless for these data), but now have taken data from the internet and prepared it for analysis. df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 count 774.000000 774.000000 774.000000 mean 52.064621 48.770060 2.364341 std 30.883400 32.886277 1.497413 min -2.113554 -11.360960 0.000000 25% 25.640786 21.809579 2.000000 50% 55.795821 49.059950 2.000000 75% 76.752290 75.015933 4.000000 max 111.726727 115.123931 4.000000 And lets look at the first few rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 74.909597 93.212519 0.0 1 75.179074 64.402788 0.0 2 94.993557 93.079513 0.0 3 95.262341 64.600912 0.0 4 54.049767 64.211591 0.0 Method 2: Get the actual file from a remote web server (unencrypted) You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://... We will need a module to interface with the remote server, in this example lets use something different than urllib . Here we will use requests , so first we load the module import requests # Module to process http/https requests Now we will generate a GET request to the remote http server. I chose to do so using a variable to store the remote URL so I can reuse code in future projects. The GET request (an http/https method) is generated with the requests method get and assigned to an object named rget -- the name is arbitrary. Next we extract the file from the rget object and write it to a local file with the name of the remote file - esentially automating the download process. Then we import the pandas module. remote_url=\"http://54.243.252.9/engr-1330-psuedo-course/MyJupyterNotebooks/42-DataScience-EvaporationAnalysis/all_quads_gross_evaporation.csv\" # set the url rget = requests.get(remote_url, allow_redirects=True) # get the remote resource, follow imbedded links open('all_quads_gross_evaporation.csv','wb').write(rget.content) # extract from the remote the contents, assign to a local file same name import pandas as pd # Module to process dataframes (not absolutely needed but somewhat easier than using primatives, and gives graphing tools) # verify file exists ! pwd ! ls -la /home/sensei/1330-textbook-webroot/docs/lesson7 total 1412 drwxrwxr-x 3 sensei sensei 4096 Feb 16 20:57 . drwxr-xr-x 10 sensei sensei 4096 Feb 16 20:30 .. drwxrwxr-x 2 sensei sensei 4096 Feb 16 20:53 .ipynb_checkpoints -rw-rw-r-- 1 sensei sensei 21150 Feb 15 15:58 01-table-dataframe.png -rw-rw-r-- 1 sensei sensei 51 Feb 15 15:58 CSV_ReadingFile.csv -rw-rw-r-- 1 sensei sensei 55 Feb 16 20:59 CSV_WritingFile1.csv -rw-rw-r-- 1 sensei sensei 46 Feb 16 20:59 CSV_WritingFile2.csv -rw-rw-r-- 1 sensei sensei 693687 Feb 15 15:58 ENGR-1330-Lesson8-Dev.html -rw-rw-r-- 1 sensei sensei 166938 Feb 15 15:58 ENGR-1330-Lesson8-Dev.ipynb -rw-rw-r-- 1 sensei sensei 5508 Feb 15 15:58 Excel_ReadingFile.xlsx -rw-rw-r-- 1 sensei sensei 5041 Feb 16 20:59 Excel_WritingFile.xlsx -rw-rw-r-- 1 sensei sensei 363498 Feb 16 20:59 all_quads_gross_evaporation.csv -rw-rw-r-- 1 sensei sensei 108222 Feb 16 20:57 lesson7.ipynb -rw-rw-r-- 1 sensei sensei 40566 Feb 15 15:58 output_126_1.png Now we can read the file contents and check its structure, before proceeding. evapdf = pd.read_csv(\"all_quads_gross_evaporation.csv\",parse_dates=[\"YYYY-MM\"]) # Read the file as a .CSV assign to a dataframe evapdf evapdf.head() # check structure .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } YYYY-MM 104 105 106 107 108 204 205 206 207 ... 911 912 1008 1009 1010 1011 1108 1109 1110 1210 0 1954-01-01 1.80 1.80 2.02 2.24 2.24 2.34 1.89 1.80 1.99 ... 1.42 1.30 2.50 2.42 1.94 1.29 2.59 2.49 2.22 2.27 1 1954-02-01 4.27 4.27 4.13 3.98 3.90 4.18 4.26 4.27 4.26 ... 2.59 2.51 4.71 4.30 3.84 2.50 5.07 4.62 4.05 4.18 2 1954-03-01 4.98 4.98 4.62 4.25 4.20 5.01 4.98 4.98 4.68 ... 3.21 3.21 6.21 6.06 5.02 3.21 6.32 6.20 5.68 5.70 3 1954-04-01 6.09 5.94 5.94 6.07 5.27 6.31 5.98 5.89 5.72 ... 3.83 3.54 6.45 6.25 4.92 3.54 6.59 6.44 5.88 5.95 4 1954-05-01 5.41 5.09 5.14 4.40 3.61 5.57 4.56 4.47 4.18 ... 3.48 3.97 7.92 8.13 6.31 3.99 7.75 7.98 7.40 7.40 5 rows \u00d7 93 columns Structure looks like a spreadsheet as expected; lets plot the time series for cell '911' evapdf.plot.line(x='YYYY-MM',y='911') # Plot quadrant 911 evaporation time series <AxesSubplot:xlabel='YYYY-MM'> Method 3: Get the actual file from an encrypted server This section is saved for future semesters References Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Database Query and Manipulation using PANDAS"},{"location":"1-Lessons/Lesson08/lesson7/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 14 February 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson08/lesson7/#lesson-7-the-pandas-module","text":"About Pandas How to install Anaconda JupyterHub/Lab (on Linux) JupyterHub/Lab (on MacOS) JupyterHub/Lab (on Windoze) The Dataframe Primatives Using Pandas Create, Modify, Delete datagrames Slice Dataframes Conditional Selection Synthetic Programming (Symbolic Function Application) Files Access Files from a remote Web Server Get file contents Get the actual file Adaptations for encrypted servers (future semester)","title":"Lesson 7 The Pandas module"},{"location":"1-Lessons/Lesson08/lesson7/#special-script-blocks","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;}","title":"Special Script Blocks"},{"location":"1-Lessons/Lesson08/lesson7/#objectives","text":"To understand the dataframe abstraction as implemented in the Pandas library(module). To be able to access and manipulate data within a dataframe To be able to obtain basic statistical measures of data within a dataframe Read/Write from/to files MS Excel-type files (.xls,.xlsx,.csv) (LibreOffice files use the MS .xml standard) Ordinary ASCII (.txt) files Access files directly from a URL (advanced concept) Using a wget-type function Using a curl-type function Using API keys (future versions)","title":"Objectives"},{"location":"1-Lessons/Lesson08/lesson7/#pandas","text":"Pandas is the core library for dataframe manipulation in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is derived from the term \u2018Panel Data\u2019. If you are curious about Pandas, this cheat sheet is recommended: https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf","title":"Pandas:"},{"location":"1-Lessons/Lesson08/lesson7/#data-structure","text":"The Primary data structure is called a dataframe. It is an abstraction where data are represented as a 2-dimensional mutable and heterogenous tabular data structure; much like a Worksheet in MS Excel. The structure itself is popular among statisticians and data scientists and business executives. According to the marketing department \"Pandas Provides rich data structures and functions designed to make working with data fast, easy, and expressive. It is useful in data manipulation, cleaning, and analysis; Pandas excels in performance and productivity \"","title":"Data Structure"},{"location":"1-Lessons/Lesson08/lesson7/#the-dataframe","text":"A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. Like MS Excel we can query the dataframe to find the contents of a particular cell using its row name and column name , or operate on entire rows and columns To use pandas, we need to import the module.","title":"The Dataframe"},{"location":"1-Lessons/Lesson08/lesson7/#computational-thinking-concepts","text":"The CT concepts expressed within Pandas include: Decomposition : Data interpretation, manipulation, and analysis of Pandas dataframes is an act of decomposition -- although the dataframes can be quite complex. Abstraction : The dataframe is a data representation abstraction that allows for placeholder operations, later substituted with specific contents for a problem; enhances reuse and readability. We leverage the principle of algebraic replacement using these abstractions. Algorithms : Data interpretation, manipulation, and analysis of dataframes are generally implemented as part of a supervisory algorithm.","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson08/lesson7/#module-set-up","text":"In principle, Pandas should be available in a default Anaconda install - You should not have to do any extra installation steps to install the library in Python - You do have to import the library in your scripts How to check - Simply open a code cell and run import pandas if the notebook does not protest (i.e. pink block of error), the youis good to go. import pandas If you do get an error, that means that you will have to install using conda or pip ; you are on-your-own here! On the content server the process is: Open a new terminal from the launcher Change to root user su then enter the root password sudo -H /opt/jupyterhib/bin/python3 -m pip install pandas Wait until the install is complete; for security, user compthink is not in the sudo group Verify the install by trying to execute import pandas as above. The process above will be similar on a Macintosh, or Windows if you did not use an Anaconda distribution. Best is to have a sucessful anaconda install, or go to the GoodJobUntilMyOrgansGetHarvested . If you have to do this kind of install, you will have to do some reading, some references I find useful are: 1. https://jupyterlab.readthedocs.io/en/stable/user/extensions.html 2. https://www.pugetsystems.com/labs/hpc/Note-How-To-Install-JupyterHub-on-a-Local-Server-1673/#InstallJupyterHub 3. https://jupyterhub.readthedocs.io/en/stable/installation-guide-hard.html (This is the approach on the content server which has a functioning JupyterHub)","title":"Module Set-Up"},{"location":"1-Lessons/Lesson08/lesson7/#dataframe-type-structure-using-primative-python","text":"First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. import numpy mytabular = numpy.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 19, 21, 22, 81] ['B', 94, 75, 66, 44] ['C', 70, 56, 47, 63] ['D', 56, 80, 39, 39] ['E', 66, 78, 39, 15] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 70, 56, 47, 63] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 21 75 56 80 78 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 47","title":"Dataframe-type Structure using primative python"},{"location":"1-Lessons/Lesson08/lesson7/#now-we-shall-create-a-proper-dataframe","text":"We will now do the same using pandas mydf = pandas.DataFrame(numpy.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 We can also turn our table into a dataframe, notice how the constructor adds header row and index column mydf1 = pandas.DataFrame(mytable) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 19 21 22 81 2 B 94 75 66 44 3 C 70 56 47 63 4 D 56 80 39 39 5 E 66 78 39 15 To get proper behavior, we can just reuse our original objects mydf2 = pandas.DataFrame(mytabular,myrowname,mycolname) mydf2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 19 21 22 81 B 94 75 66 44 C 70 56 47 63 D 56 80 39 39 E 66 78 39 15 Why are mydf and mydf2 different?","title":"Now we shall create a proper dataframe"},{"location":"1-Lessons/Lesson08/lesson7/#getting-the-shape-of-dataframes","text":"The shape method, which is available after the dataframe is constructed, will return the row and column rank (count) of a dataframe. mydf.shape (5, 4) mydf1.shape (6, 5) mydf2.shape (5, 4)","title":"Getting the shape of dataframes"},{"location":"1-Lessons/Lesson08/lesson7/#appending-new-columns","text":"To append a column simply assign a value to a new column name to the dataframe mydf['new']= 'NA' mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA","title":"Appending new columns"},{"location":"1-Lessons/Lesson08/lesson7/#appending-new-rows","text":"This is sometimes a bit trickier but here is one way: - create a copy of a row, give it a new name. - concatenate it back into the dataframe. newrow = mydf.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pandas.concat([mydf,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 97 20 61 35 NA B 74 8 7 99 NA C 75 67 52 82 NA D 92 83 90 28 NA E 80 67 4 24 NA X 80 67 4 24 NA","title":"Appending new rows"},{"location":"1-Lessons/Lesson08/lesson7/#removing-rows-and-columns","text":"To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 C 75 67 52 82 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 X 80 67 4 24 # or just use drop with axis specify newtable.drop('X', axis=0, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24","title":"Removing Rows and Columns"},{"location":"1-Lessons/Lesson08/lesson7/#indexing","text":"We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 20 B 8 D 83 E 67 Name: X, dtype: int64 newtable[['X','W']] #Selecing a multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 20 97 B 8 74 D 83 92 E 67 80 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 80 X 67 Y 4 Z 24 Name: E, dtype: int64 newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 97 20 61 35 B 74 8 7 99 D 92 83 90 28 E 80 67 4 24 newtable.loc[['E','D','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 80 67 4 24 D 92 83 90 28 B 74 8 7 99 newtable.loc[['B','E','D'],['X','Y']] #Selecting elements via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 8 7 E 67 4 D 83 90","title":"Indexing"},{"location":"1-Lessons/Lesson08/lesson7/#conditional-selection","text":"mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? mydf[mydf['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? mydf[mydf['col2']==mydf['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object","title":"Conditional Selection"},{"location":"1-Lessons/Lesson08/lesson7/#descriptor-functions","text":"#Creating a dataframe from a dictionary mydf = pandas.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) mydf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach","title":"Descriptor Functions"},{"location":"1-Lessons/Lesson08/lesson7/#head-method","text":"Returns the first few rows, useful to infer structure #Returns only the first five rows mydf.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit","title":"head method"},{"location":"1-Lessons/Lesson08/lesson7/#info-method","text":"Returns the data model (data column count, names, data types) #Info about the dataframe mydf.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes","title":"info method"},{"location":"1-Lessons/Lesson08/lesson7/#describe-method","text":"Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe mydf.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000","title":"describe method"},{"location":"1-Lessons/Lesson08/lesson7/#counting-and-sum-methods","text":"There are also methods for counts and sums by specific columns mydf['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) mydf['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values mydf['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) mydf['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64","title":"Counting and Sum methods"},{"location":"1-Lessons/Lesson08/lesson7/#using-functions-in-dataframes-symbolic-apply","text":"The power of Pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. This employs principles of pattern matching , abstraction , and algorithm development ; a holy trinity of Computational Thinning. It's somewhat complicated but quite handy, best shown by an example: def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(mydf) print('Apply the times2 function to col2') mydf['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64","title":"Using functions in dataframes - symbolic apply"},{"location":"1-Lessons/Lesson08/lesson7/#sorts","text":"mydf.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit mydf.sort_values('col3', ascending = True) #Lexiographic sort .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 1 2 555 apple 6 7 222 banana 2 3 666 grape 4 5 666 jackfruit 3 4 444 mango 0 1 444 orange 7 8 222 peach 5 6 111 watermelon","title":"Sorts"},{"location":"1-Lessons/Lesson08/lesson7/#aggregating-grouping-values-dataframe-contents","text":"#Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } mydf1 = pandas.DataFrame(data) mydf1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' mydf1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' mydf1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27","title":"Aggregating (Grouping Values) dataframe contents"},{"location":"1-Lessons/Lesson08/lesson7/#filtering-out-missing-values","text":"Filtering and cleaning are often used to describe the process where data that does not support a narrative is removed ;typically for maintenance of profit applications, if the data are actually missing that is common situation where cleaning is justified. #Creating a dataframe from a dictionary df = pandas.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach","title":"Filtering out missing values"},{"location":"1-Lessons/Lesson08/lesson7/#reading-a-file-into-a-dataframe","text":"Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) # xlsx reads deprecated here is a hack using openpyxl readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15","title":"Reading a File into a Dataframe"},{"location":"1-Lessons/Lesson08/lesson7/#writing-a-dataframe-to-file","text":"#Creating and writing to a .csv file readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pandas.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pandas.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pandas.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pandas.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', index = False, engine='openpyxl') readfileexcel = pandas.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15","title":"Writing a dataframe to file"},{"location":"1-Lessons/Lesson08/lesson7/#downloading-files-from-websites-optional","text":"This section shows how to get files from a remote computer. There are several ways to get the files, most importantly you need the FQDN to the file.","title":"Downloading files from websites (optional)"},{"location":"1-Lessons/Lesson08/lesson7/#method-1-get-data-from-a-file-on-a-remote-server-unencrypted","text":"This section shows how to obtain data files from public URLs. Prerequesites: You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://...","title":"Method 1: Get data from a file on a remote server (unencrypted)"},{"location":"1-Lessons/Lesson08/lesson7/#web-developer-notes","text":"If you want to distribute files (web developers) the files need to be in the server webroot, but can be deep into the heirarchial structure. Here we will do an example with a file that contains topographic data in XYZ format, without header information. The first few lines of the remote file look like: 74.90959724 93.21251922 0 75.17907367 64.40278759 0 94.9935575 93.07951286 0 95.26234119 64.60091165 0 54.04976655 64.21159095 0 54.52914363 35.06934342 0 75.44993558 34.93079513 0 75.09317373 5.462959114 0 74.87357468 10.43130083 0 74.86249082 15.72938748 0 And importantly it is tab delimited. The module to manipulate url in python is called urllib Google search to learn more, here we are using only a small component without exception trapping. #Step 1: import needed modules to interact with the internet from urllib.request import urlopen # import a method that will connect to a url and read file contents import pandas #import pandas This next code fragment sets a string called remote_url ; it is just a variable, name can be anything that honors python naming rules. Then the urllib function urlopen with read and decode methods is employed, the result is stored in an object named elevationXYZ #Step 2: make the connection to the remote file (actually its implementing \"bash curl -O http://fqdn/path ...\") remote_url = 'http://www.rtfmps.com/share_files/pip-corner-sumps.txt' # elevationXYZ = urlopen(remote_url).read().decode().split() # Gets the file contents as a single vector, comma delimited, file is not retained locally At this point the object exists as a single vector with hundreds of elements. We now need to structure the content. Here using python primatives, and knowing how the data are supposed to look, we prepare variables to recieve the structured results #Step 3 Python primatives to structure the data, or use fancy modules (probably easy in numpy) howmany = len(elevationXYZ) # how long is the vector? nrow = int(howmany/3) xyz = [[0 for j in range(3)] for j in range(nrow)] # null space to receive data define columnX Now that everything is ready, we can extract from the object the values we want into xyz #Step4 Now will build xyz as a matrix with 3 columns index = 0 for irow in range(0,nrow): xyz[irow][0]=float(elevationXYZ[index]) xyz[irow][1]=float(elevationXYZ[index+1]) xyz[irow][2]=float(elevationXYZ[index+2]) index += 3 #increment the index xyz is now a 3-column float array and can now probably be treated as a data frame. Here we use a pandas method to build the dataframe. df = pandas.DataFrame(xyz) Get some info, yep three columns (ordered triples to be precise!) df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 774 entries, 0 to 773 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 0 774 non-null float64 1 1 774 non-null float64 2 2 774 non-null float64 dtypes: float64(3) memory usage: 18.3 KB And some summary statistics (meaningless for these data), but now have taken data from the internet and prepared it for analysis. df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 count 774.000000 774.000000 774.000000 mean 52.064621 48.770060 2.364341 std 30.883400 32.886277 1.497413 min -2.113554 -11.360960 0.000000 25% 25.640786 21.809579 2.000000 50% 55.795821 49.059950 2.000000 75% 76.752290 75.015933 4.000000 max 111.726727 115.123931 4.000000 And lets look at the first few rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 74.909597 93.212519 0.0 1 75.179074 64.402788 0.0 2 94.993557 93.079513 0.0 3 95.262341 64.600912 0.0 4 54.049767 64.211591 0.0","title":"Web Developer Notes"},{"location":"1-Lessons/Lesson08/lesson7/#method-2-get-the-actual-file-from-a-remote-web-server-unencrypted","text":"You know the FQDN to the file it will be in structure of \"http://server-name/.../filename.ext\" The server is running ordinary (unencrypted) web services, i.e. http://... We will need a module to interface with the remote server, in this example lets use something different than urllib . Here we will use requests , so first we load the module import requests # Module to process http/https requests Now we will generate a GET request to the remote http server. I chose to do so using a variable to store the remote URL so I can reuse code in future projects. The GET request (an http/https method) is generated with the requests method get and assigned to an object named rget -- the name is arbitrary. Next we extract the file from the rget object and write it to a local file with the name of the remote file - esentially automating the download process. Then we import the pandas module. remote_url=\"http://54.243.252.9/engr-1330-psuedo-course/MyJupyterNotebooks/42-DataScience-EvaporationAnalysis/all_quads_gross_evaporation.csv\" # set the url rget = requests.get(remote_url, allow_redirects=True) # get the remote resource, follow imbedded links open('all_quads_gross_evaporation.csv','wb').write(rget.content) # extract from the remote the contents, assign to a local file same name import pandas as pd # Module to process dataframes (not absolutely needed but somewhat easier than using primatives, and gives graphing tools) # verify file exists ! pwd ! ls -la /home/sensei/1330-textbook-webroot/docs/lesson7 total 1412 drwxrwxr-x 3 sensei sensei 4096 Feb 16 20:57 . drwxr-xr-x 10 sensei sensei 4096 Feb 16 20:30 .. drwxrwxr-x 2 sensei sensei 4096 Feb 16 20:53 .ipynb_checkpoints -rw-rw-r-- 1 sensei sensei 21150 Feb 15 15:58 01-table-dataframe.png -rw-rw-r-- 1 sensei sensei 51 Feb 15 15:58 CSV_ReadingFile.csv -rw-rw-r-- 1 sensei sensei 55 Feb 16 20:59 CSV_WritingFile1.csv -rw-rw-r-- 1 sensei sensei 46 Feb 16 20:59 CSV_WritingFile2.csv -rw-rw-r-- 1 sensei sensei 693687 Feb 15 15:58 ENGR-1330-Lesson8-Dev.html -rw-rw-r-- 1 sensei sensei 166938 Feb 15 15:58 ENGR-1330-Lesson8-Dev.ipynb -rw-rw-r-- 1 sensei sensei 5508 Feb 15 15:58 Excel_ReadingFile.xlsx -rw-rw-r-- 1 sensei sensei 5041 Feb 16 20:59 Excel_WritingFile.xlsx -rw-rw-r-- 1 sensei sensei 363498 Feb 16 20:59 all_quads_gross_evaporation.csv -rw-rw-r-- 1 sensei sensei 108222 Feb 16 20:57 lesson7.ipynb -rw-rw-r-- 1 sensei sensei 40566 Feb 15 15:58 output_126_1.png Now we can read the file contents and check its structure, before proceeding. evapdf = pd.read_csv(\"all_quads_gross_evaporation.csv\",parse_dates=[\"YYYY-MM\"]) # Read the file as a .CSV assign to a dataframe evapdf evapdf.head() # check structure .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } YYYY-MM 104 105 106 107 108 204 205 206 207 ... 911 912 1008 1009 1010 1011 1108 1109 1110 1210 0 1954-01-01 1.80 1.80 2.02 2.24 2.24 2.34 1.89 1.80 1.99 ... 1.42 1.30 2.50 2.42 1.94 1.29 2.59 2.49 2.22 2.27 1 1954-02-01 4.27 4.27 4.13 3.98 3.90 4.18 4.26 4.27 4.26 ... 2.59 2.51 4.71 4.30 3.84 2.50 5.07 4.62 4.05 4.18 2 1954-03-01 4.98 4.98 4.62 4.25 4.20 5.01 4.98 4.98 4.68 ... 3.21 3.21 6.21 6.06 5.02 3.21 6.32 6.20 5.68 5.70 3 1954-04-01 6.09 5.94 5.94 6.07 5.27 6.31 5.98 5.89 5.72 ... 3.83 3.54 6.45 6.25 4.92 3.54 6.59 6.44 5.88 5.95 4 1954-05-01 5.41 5.09 5.14 4.40 3.61 5.57 4.56 4.47 4.18 ... 3.48 3.97 7.92 8.13 6.31 3.99 7.75 7.98 7.40 7.40 5 rows \u00d7 93 columns Structure looks like a spreadsheet as expected; lets plot the time series for cell '911' evapdf.plot.line(x='YYYY-MM',y='911') # Plot quadrant 911 evaporation time series <AxesSubplot:xlabel='YYYY-MM'>","title":"Method 2: Get the actual file from a remote web server (unencrypted)"},{"location":"1-Lessons/Lesson08/lesson7/#method-3-get-the-actual-file-from-an-encrypted-server","text":"This section is saved for future semesters","title":"Method 3: Get the actual file from an encrypted server"},{"location":"1-Lessons/Lesson08/lesson7/#references","text":"Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"References"},{"location":"1-Lessons/Lesson09/lesson8/","text":"ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 18 February 2021 Lesson 8 Visual Display of Data This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, box plot, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib Graphic Standards for Plots Parts of a Plot Building Plots using matplotlib external package Objectives Define the ordinate, abscissa, independent and dependent variables Identify the parts of a proper plot Define how to plot experimental data and theoretical data About matplotlib Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis). Computational thinking (CT) concepts involved are: Decomposition : Break a problem down into smaller pieces; separating plotting from other parts of analysis simplifies maintenace of scripts Abstraction : Pulling out specific differences to make one solution work for multiple problems; wrappers around generic plot calls enhances reuse Algorithms : A list of steps that you can follow to finish a task; Often the last step and most important to make professional graphics to justify the expense (of paying you to do engineering) to the client. Graphics Conventions for Plots Terminology: Ordinate, Abscissa, Dependent and Independent Variables A few terms are used in describing plots: - Abscissa \u2013 the horizontal axis on a plot (the left-right axis) - Ordinate \u2013 the vertical axis on a plot (the up-down axis) A few terms in describing data models - Independent Variable (Explainatory, Predictor, Feature, ...) \u2013 a variable that can be controlled/manipulated in an experiment or theoretical analysis - Dependent Variable (Response, Prediction, ...) \u2013 the variable that measured/observed as a function of the independent variable Plotting convention in most cases assigns explainatory variables to the horizontal axis (e.g. Independent variable is plotted on the Abscissa) and the response variable(s) to the vertical axis (e.g. Dependent Variable is plotted on the Ordinate) Conventions for Proper Plots Include a title OR a caption with a brief description of the plot Label both axes clearly Include the variable name, the variable, and the unit in each label If possible, select increments for both the x and y axes that provide for easy interpolation Include gridlines Show experimental measurements as symbols Show model (theoretical) relationships as lines Use portrait orientation when making your plot Make the plot large enough to be easily read If more than one experimental dataset is plotted Use different shapes for each dataset Use different colors for each dataset Include a legend defining the datasets Background Data are not always numerical. Data can be music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) and other types. Categorical data is a type where you can place individual components into a category: For example visualize a freezer where a business stores ice cream, the product is categorized by flavor, each carton is a component. - The individual components are cartons of ice-cream, and the category is the flavor in the carton Bar Graphs Bar charts (graphs) are useful tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! type(flavors) list myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio <Figure size 720x360 with 0 Axes> # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='magenta', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='green', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now lets deconstruct the script a bit: ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! This part of the code creates a dictionary object, keys are the flavors, values are the carton counts (not the best way, but good for our learning needs). Next we import the python plotting library from matplotlib and name it plt to keep the script a bit easier to read. Next we use the list method to create two lists from the dictionary, flavors and cartons . Keep this in mind plotting is usually done on lists, so we need to prepare the structures properly. The next statement myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio Uses the figure class in pyplot from matplotlib to make a figure object named myfigure, the plot is built into this object. Every call to a method in plt adds content to myfigure until we send the instruction to render the plot ( plt.show() ) The next portion of the script builds the plot: plt.bar(flavors, cartons, color ='orange', width = 0.4) # Build a bar chart, plot series flavor on x-axis, plot series carton on y-axis. Make the bars orange, set bar width (units unspecified) plt.xlabel(\"Flavors\") # Label the x-axis as Flavors plt.ylabel(\"No. of Cartons in Stock\") # Label the x-axis as Flavors plt.title(\"Current Ice Cream in Storage\") # Title for the whole plot This last statement renders the plot to the graphics device (probably localhost in the web browser) plt.show() Now lets add another set of categories to the plot and see what happens ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beastcount = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='gray', width = 0.4) plt.bar(animals, beastcount, color ='brown', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now suppose we want horizontal bars we can search pyplot for such a thing. If one types horizontal bar chart into the pyplot search engine there is a link that leads to: Which has the right look! If we examine the script there is a method called barh so lets try that. ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beasts = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.barh(flavors, cartons, color ='orange') plt.barh(animals, beasts, color ='green') plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.barh(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:ylabel='Flavor'> import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show() Line Charts A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application. Example Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() time = [0,1.0,4.0,5.0,6.0,2.0,3.0] speed = [0,3,20,30,45.6,7,12] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='green', marker='o',linewidth=1) # basic line plot plt.show() # Estimate acceleration (naive) dvdt = (max(speed) - min(speed))/(max(time)-min(time)) plottitle = 'Average acceleration %.3f' % (dvdt) + ' m/sec/sec' seriesnames = ['Data','Model'] modely = [min(speed),max(speed)] modelx = [min(time),max(time)] mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=0) # basic line plot plt.plot(modelx, modely, c='blue',linewidth=1) # basic line plot plt.xlabel('Time (sec)') plt.ylabel('Speed (m/sec)') plt.legend(seriesnames) plt.title(plottitle) plt.show() Scatter Plots A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot # Example 1. A data file containing heights of fathers, mothers, and sons is to be examined df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists daddy = df['father'] ; mommy = df['mother'] ; baby = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red' , label='Father') # one plot series plt.scatter(baby, mommy, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\") df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> Histograms Quilting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson. To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\" import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Gross Gross (Adjusted) Year count 2.000000e+02 2.000000e+02 200.000000 mean 2.216196e+08 5.041983e+08 1986.620000 std 1.441574e+08 2.159814e+08 20.493548 min 9.183673e+06 3.222619e+08 1921.000000 25% 1.087824e+08 3.677804e+08 1973.000000 50% 2.001273e+08 4.388570e+08 1990.000000 75% 3.069535e+08 5.512131e+08 2003.250000 max 9.067234e+08 1.757788e+09 2015.000000 References Constructing Horizontal Bar Charts (matplotlib.org documentation) https://matplotlib.org/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html?highlight=horizontal%20bar%20chart How to make a bar chart https://www.geeksforgeeks.org/bar-plot-in-matplotlib/","title":"Visual Display of Data using MATPLOTLIB"},{"location":"1-Lessons/Lesson09/lesson8/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 18 February 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson09/lesson8/#lesson-8-visual-display-of-data","text":"This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, box plot, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib Graphic Standards for Plots Parts of a Plot Building Plots using matplotlib external package","title":"Lesson 8 Visual Display of Data"},{"location":"1-Lessons/Lesson09/lesson8/#objectives","text":"Define the ordinate, abscissa, independent and dependent variables Identify the parts of a proper plot Define how to plot experimental data and theoretical data","title":"Objectives"},{"location":"1-Lessons/Lesson09/lesson8/#about-matplotlib","text":"Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis). Computational thinking (CT) concepts involved are: Decomposition : Break a problem down into smaller pieces; separating plotting from other parts of analysis simplifies maintenace of scripts Abstraction : Pulling out specific differences to make one solution work for multiple problems; wrappers around generic plot calls enhances reuse Algorithms : A list of steps that you can follow to finish a task; Often the last step and most important to make professional graphics to justify the expense (of paying you to do engineering) to the client.","title":"About matplotlib"},{"location":"1-Lessons/Lesson09/lesson8/#graphics-conventions-for-plots","text":"","title":"Graphics Conventions for Plots"},{"location":"1-Lessons/Lesson09/lesson8/#terminology-ordinate-abscissa-dependent-and-independent-variables","text":"A few terms are used in describing plots: - Abscissa \u2013 the horizontal axis on a plot (the left-right axis) - Ordinate \u2013 the vertical axis on a plot (the up-down axis) A few terms in describing data models - Independent Variable (Explainatory, Predictor, Feature, ...) \u2013 a variable that can be controlled/manipulated in an experiment or theoretical analysis - Dependent Variable (Response, Prediction, ...) \u2013 the variable that measured/observed as a function of the independent variable Plotting convention in most cases assigns explainatory variables to the horizontal axis (e.g. Independent variable is plotted on the Abscissa) and the response variable(s) to the vertical axis (e.g. Dependent Variable is plotted on the Ordinate)","title":"Terminology: Ordinate, Abscissa, Dependent and Independent Variables"},{"location":"1-Lessons/Lesson09/lesson8/#conventions-for-proper-plots","text":"Include a title OR a caption with a brief description of the plot Label both axes clearly Include the variable name, the variable, and the unit in each label If possible, select increments for both the x and y axes that provide for easy interpolation Include gridlines Show experimental measurements as symbols Show model (theoretical) relationships as lines Use portrait orientation when making your plot Make the plot large enough to be easily read If more than one experimental dataset is plotted Use different shapes for each dataset Use different colors for each dataset Include a legend defining the datasets","title":"Conventions for Proper Plots"},{"location":"1-Lessons/Lesson09/lesson8/#background","text":"Data are not always numerical. Data can be music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) and other types. Categorical data is a type where you can place individual components into a category: For example visualize a freezer where a business stores ice cream, the product is categorized by flavor, each carton is a component. - The individual components are cartons of ice-cream, and the category is the flavor in the carton","title":"Background"},{"location":"1-Lessons/Lesson09/lesson8/#bar-graphs","text":"Bar charts (graphs) are useful tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! type(flavors) list myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio <Figure size 720x360 with 0 Axes> # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='magenta', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='green', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now lets deconstruct the script a bit: ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! This part of the code creates a dictionary object, keys are the flavors, values are the carton counts (not the best way, but good for our learning needs). Next we import the python plotting library from matplotlib and name it plt to keep the script a bit easier to read. Next we use the list method to create two lists from the dictionary, flavors and cartons . Keep this in mind plotting is usually done on lists, so we need to prepare the structures properly. The next statement myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio Uses the figure class in pyplot from matplotlib to make a figure object named myfigure, the plot is built into this object. Every call to a method in plt adds content to myfigure until we send the instruction to render the plot ( plt.show() ) The next portion of the script builds the plot: plt.bar(flavors, cartons, color ='orange', width = 0.4) # Build a bar chart, plot series flavor on x-axis, plot series carton on y-axis. Make the bars orange, set bar width (units unspecified) plt.xlabel(\"Flavors\") # Label the x-axis as Flavors plt.ylabel(\"No. of Cartons in Stock\") # Label the x-axis as Flavors plt.title(\"Current Ice Cream in Storage\") # Title for the whole plot This last statement renders the plot to the graphics device (probably localhost in the web browser) plt.show() Now lets add another set of categories to the plot and see what happens ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beastcount = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='gray', width = 0.4) plt.bar(animals, beastcount, color ='brown', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now suppose we want horizontal bars we can search pyplot for such a thing. If one types horizontal bar chart into the pyplot search engine there is a link that leads to: Which has the right look! If we examine the script there is a method called barh so lets try that. ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model eaters = {'Cats':6, 'Dogs':5, 'Ferrets':19} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! animals = list(eaters.keys()) beasts = list(eaters.values()) myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.barh(flavors, cartons, color ='orange') plt.barh(animals, beasts, color ='green') plt.xlabel(\"Flavors\") plt.ylabel(\"Counts: Cartons and Beasts\") plt.title(\"Current Ice Cream in Storage\") plt.show() Now using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.barh(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:ylabel='Flavor'> import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show()","title":"Bar Graphs"},{"location":"1-Lessons/Lesson09/lesson8/#line-charts","text":"A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application. Example Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() time = [0,1.0,4.0,5.0,6.0,2.0,3.0] speed = [0,3,20,30,45.6,7,12] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='green', marker='o',linewidth=1) # basic line plot plt.show() # Estimate acceleration (naive) dvdt = (max(speed) - min(speed))/(max(time)-min(time)) plottitle = 'Average acceleration %.3f' % (dvdt) + ' m/sec/sec' seriesnames = ['Data','Model'] modely = [min(speed),max(speed)] modelx = [min(time),max(time)] mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=0) # basic line plot plt.plot(modelx, modely, c='blue',linewidth=1) # basic line plot plt.xlabel('Time (sec)') plt.ylabel('Speed (m/sec)') plt.legend(seriesnames) plt.title(plottitle) plt.show()","title":"Line Charts"},{"location":"1-Lessons/Lesson09/lesson8/#scatter-plots","text":"A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot # Example 1. A data file containing heights of fathers, mothers, and sons is to be examined df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists daddy = df['father'] ; mommy = df['mother'] ; baby = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(baby, daddy, c='red' , label='Father') # one plot series plt.scatter(baby, mommy, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\") df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'>","title":"Scatter Plots"},{"location":"1-Lessons/Lesson09/lesson8/#histograms","text":"Quilting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson. To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\" import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Gross Gross (Adjusted) Year count 2.000000e+02 2.000000e+02 200.000000 mean 2.216196e+08 5.041983e+08 1986.620000 std 1.441574e+08 2.159814e+08 20.493548 min 9.183673e+06 3.222619e+08 1921.000000 25% 1.087824e+08 3.677804e+08 1973.000000 50% 2.001273e+08 4.388570e+08 1990.000000 75% 3.069535e+08 5.512131e+08 2003.250000 max 9.067234e+08 1.757788e+09 2015.000000","title":"Histograms"},{"location":"1-Lessons/Lesson09/lesson8/#references","text":"Constructing Horizontal Bar Charts (matplotlib.org documentation) https://matplotlib.org/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html?highlight=horizontal%20bar%20chart How to make a bar chart https://www.geeksforgeeks.org/bar-plot-in-matplotlib/","title":"References"},{"location":"1-Lessons/Lesson10/lesson10/","text":"# Environment Check -- Deactivate on a working host import sys print(sys.executable) print(sys.version) print(sys.version_info) /opt/jupyterhub/bin/python3 3.8.5 (default, Jan 27 2021, 15:41:15) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Copyright \u00a9 DATE Author, all rights reserved Transporting Wind Turbine Blades Consider the transport of Turbine Blades, they are rather long, and often the transport compaines have to navigate difficult turns as in Figure 1. Consider route planning using a simple case: Two intersecting mountain road cuts (think of vertical walls) meet at an angle of 123 ^o , as shown in Figure 2. The East-West road is 7 feet wide, while the Southwest-Northeast road is 9 feet wide. What is the longest blade that can negotiate the turn? You can neglect the blade thickness (think of it as a line segment) and cannot tip it to make it through the corner. Build a tool (solution script) that can allow for general use where the angle A is variable as are the road widths. Analysis Visualize the turbine blade in sucessive positions as we transport it around the corner; there will be some critical position where each end touch the road cut walls while a point on the blade touches the corner of the intersection. If we analyze the various triangles formed by the turbine blade we can express the lengths in terms of the widths and angles. For the part of the blade on the East-West portion we obtain: l_1 = \\frac{w_2}{sin(B)} For the part of the blade on the Southeast-Northwest portion we obtain: l_2 = \\frac{w_1}{sin(C)} The angles are related as: B = \\pi - A - C And the turbine total length is: l = l_1 + l_2 = \\frac{w_2}{sin(B)} + \\frac{w_1}{sin(C)} Substitute our expression for B , and we have everything in terms on road widths, and intersection angle: l = \\frac{w_2}{sin(\\pi - A - C)} + \\frac{w_1}{sin(C)} Now we want to find the smallest l as a function of C , the necessary condition for such a minimum is \\frac{dl}{dC}=0 which by application of Calculus produces: \\frac{dl}{dC}=\\frac{w_2 cos(\\pi - A - C)}{sin^2(\\pi - A - C)} - \\frac{w_1 cos(C)}{sin^2(C)}=0 where A , w_1 , and w_2 are known, now we have to find a solution to this equation. and once we have found the value of C that satisfies \\frac{dl}{dC}=0 we can recover the length from l = \\frac{w_2}{sin(\\pi - A - C)} + \\frac{w_1}{sin(C)} Lets consider some methods to find the length: Plot the function, find the value from the plot Apply a search method (grid searching) Newton's method (with finite difference approximations to derivative) # Plot the function, find the value from the plot ## forward define prototype functions # Our plotting function import matplotlib.pyplot as plt def makeAplot(listx1,listy1,strlablx,strlably,strtitle): mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(listx1,listy1, c='red', marker='o',linewidth=1) # basic data plot plt.xlabel(strlablx) plt.ylabel(strlably) plt.title(strtitle) plt.show() # prototype dl/dC function import math def func(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 =w2*math.cos(math.pi - A - x) denom1 = math.sin(math.pi - A - x)**2 numer2 = w1*math.cos(x) denom2 = math.sin(x)**2 func = numer1/denom1 - numer2/denom2 return(func) # prototype length function def blade_length(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 = w2 denom1 = math.sin(math.pi - A - x) numer2 = w1 denom2 = math.sin(x) blade_length = numer1/denom1 + numer2/denom2 return(blade_length) # Plot the function c_angle = [] # empty list to populate f_value = [] # empty list to populate # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet stepsize = 0.01 # increments of angle (degrees) total_steps = int(Cm/stepsize) tolerance = 5 # Only save a result close to the zero for i in range(1,total_steps): test_angle = float(i)*math.pi/total_steps test_value = func(test_angle) if abs(test_value) <= tolerance: c_angle.append((test_angle*180)/math.pi) # angle in degrees f_value.append(test_value) # value of dl/dC function # print((test_angle*180)/math.pi,test_value) # activate to examine values makeAplot(c_angle,f_value,'strlablx','strlably','strtitle') # Now get the length from our best guess from plot; trial and error to refine guess visual_angle = 43.888 # based on eyeball fit! print('Maximum blade length for C angle of ',round(visual_angle,1),' degrees is ',round(blade_length(visual_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(visual_angle*math.pi/180.),3)) Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.002 Grid Search Describe method then show example # Apply a search method # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet tolerance = 0.01 # define close enough to zero stepsize = 0.001 # increments of angle (degrees) total_steps = int(Cm/stepsize) best_size = 1e9 for i in range(1,total_steps): test_angle = float(i)*math.pi/total_steps test_value = func(test_angle) if abs(test_value) <= tolerance: # are we close to zero? if abs(test_value) < best_size: # are better than last close to zero? best_size = abs(test_value) best_angle = test_angle # print(float(i),test_value,best_size) else: pass #print(((best_angle*180)/math.pi),best_size) # Now report values print('Searched ',total_steps,' interference angles in steps of ',stepsize,' degrees of arc') print(' Lower C angle ',round(float(1)*math.pi/total_steps,3),'degrees of arc') #print(' Blade Length to Fit Intersection is ',round(blade_length(float(1)*math.pi/total_steps),3),' feet') print(' Upper C angle ',round(Cm,3), 'degrees of arc') print('Best C angle ',round(((best_angle*180)/math.pi),3), 'degrees of arc') print('Longest Blade Length to Fit Intersection is ',round(blade_length(best_angle),3),' feet') Searched 89900 interference angles in steps of 0.001 degrees of arc Lower C angle 0.0 degrees of arc Upper C angle 89.9 degrees of arc Best C angle 43.889 degrees of arc Longest Blade Length to Fit Intersection is 50.93 feet Single Variable Newtons Method Newton's method is an iterative technique that can produce good estimates of solutions to implicit equations. The method is employed by rewriting the equation in the form f(x) = 0 , then successively manipulating guesses for x until the function evaluates to a value close enough to zero for the modeler to accept.In the turbine blade case the \\frac{dl}{dC}=0 function is already in the correct form. Background The figure above is a graph of some function whose intercept with the x-axis is unknown. The goal of Newton's method is to find this intersection (root) from a realistic first guess. Suppose the first guess is x1 , shown on the figure as the right-most specific value of x . The value of the function at this location is f(x1) . Because x1 is supposed to be a root the difference from the value zero represents an error in the estimate. Newton's method simply provides a recipe for corrections to this error. Provided x1 is not near a minimum or maximum (slope of the function is not zero) then a better estimate of the root can be obtained by extending a tangent line from x1, f(x1) to the x-axis . The intersection of this line with the axis represents a better estimate of the root. This new estimate is x2 . A formula for x2 can be derived from the geometry of the triangle x2,f(x1),x1 . Recall from calculus that the tangent to a function at a particular point is the first derivative of the function. Therefore, from the geometry of the triangle and the definition of tangent we can write, \\begin{equation} tan(\\theta)=\\frac{df}{dx}\\Biggr\\vert_{x_1} = \\frac{f(x_1)}{x_1 - x_2} \\end{equation} Solving the equation for x 2 results in a formula that expresses x2 in terms of the first guess plus a correction term. \\begin{equation} x_2=x_1 - \\frac{f(x_1)}{\\frac{df}{dx}\\vert_{x_1}} \\end{equation} The second term on the right hand side is the correction term to the estimate on the right hand side. Once x2 is calculated we can repeat the formula substituting x2 for x1 and x3 for x2 in the formula. Repeated application usually leads to one of three outcomes: 1. a root; 2. divergence to +/- \\inf ; or 3. cycling. These three outcomes are discussed below in various subsections along with some remedies. The generalized formula is \\begin{equation} x_{k+1}=x_{k} - \\frac{ f(x_{k}) }{ \\frac{df}{dx}\\rvert_{x_k} } \\label{eqn:NewtonFormula} \\end{equation} If the derivative is evaluated using analytical derivatives the method is called Newton's method, if approximations to the derivative are used, it is called a quasi-Newton method. Newton's Method --- Using analytical derivatives This subsection is an example in Python of implementing Newton's method with analytical derivatives. The recipe itself is: Write the function in proper form, and code it into a computer. Write the derivative in proper form and code it into a computer. Make an initial guess of the solution (0 and 1 are always convenient guesses). Evaluate the function, evaluate the derivative, calculate their ratio. Subtract the ratio from the current guess and save the result as the update. Test for stopping: Did the update stay the same value? Yes, then stop, probably have a solution. Is the function nearly zero? Yes, then stop we probably have a solution. Have we tried too many updates? Yes, then stop the process is probably cycling, stop. If stopping is indicated proceed to next step, otherwise proceed back to step 4. Stopping indicated, report last update as the result (or report failure to find solution), and related information about the status of the numerical method. The following example illustrates these step as well as an ipython implementation of Newton's method. Suppose we wish to find a root (value of x ) that satisfies: \\begin{equation} f(x) = e^x - 10 cos(x) -100 \\end{equation} Then we will need to code it into a script. Here is a code fragment that will generate the prototype function # import built in function for e^x, cosine from math import exp, cos, sin # Define the function def func(x): func = exp(x) - 10*cos(x) - 100 #using the name as the temp var return func Notice in the code fragment we import three built-in functions from the Python math package, specifically \\exp() , \\sin() , and \\cos () . The next step is to code the derivative. In this case the derivative is \\begin{equation} \\frac{df}{dx}\\vert{(x)} = e^x + 10 \\sin(x) \\end{equation} and the prototype function is coded as def dfdx(x): dfdx = exp(x) + 10*sin(x) return dfdx Next we will need script to read in an initial guess, and ask us how many trials we will use to try to find a solution, as well as how close to zero we should be before we declare victory. # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 2 Enter iteration maximum 9 Enter a solution tolerance (e.g. 1e-06) 1e-6 The use of HowSmall is called a zero tolerance. We will use the same numerical value for two tolerance tests. Also notice how we are using error traps to force numeric input. Probably overkill for this example, but because we already wrote the try-except code in an earlier lesson, might as well reuse the code. Professional codes do a lot of error checking before launching into the actual processing - especially if the processing part is time consuming, its worth the time to check for obvious errors before running for a few hours then at some point failing because of an input value error that was predictable. Now back to the tolerance tests. The first test is to determine if the update has changed or not. If it has not, we may not have a correct answer, but there is no point continuing because the update is unlikely to move further. The test is something like \\begin{equation} \\text{IF}~\\lvert x_{k+1} - x_{k} \\rvert < \\text{Tol.~ THEN Exit and Report Results} \\end{equation} The second test is if the function value is close to zero. The structure of the test is similar, just an different argument. The second test is something like \\begin{equation} \\text{IF}~\\lvert f(x_{k+1}) \\rvert < \\text{Tol.~ THEN Exit and Report Results} \\end{equation} One can see from the nature of the two tests that a programmer might want to make the tolerance values different. This modification is left as a reader exercise. Checking for maximum iterations is relatively easy, we just include code that checks for normal exit the loop. Here is code fragment that implements the method, makes the various tests, and reports results. # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Update not changing \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") Update not changing Function value = 1.4210854715202004e-14 Root value = 4.593209147284144 End of NewtonMethod.py Now we simply connect the three fragments, and we would have a working Python script that implements Newton's method for the example equation. The example is specific to the particular function provided, but the programmer could move the two functions func and dfdx into a user specified module, and then load that module in the program to make it even more generic. The next section will use such an approach to illustrate the ability to build a generalized Newton method and only have to program the function itself Newton's Method --- Using finite-differences to estimate the derivative A practical difficulty in using Newton's method is determining the value of the derivative in cases where differentiation is difficult. In these cases we can replace the derivative by a finite difference equation and then proceed as in Newton's method. Recall from calculus that the derivative was defined as the limit of the difference quotient: \\begin{equation} \\frac{df}{dx}\\vert_{x} = \\lim_{\\Delta x \\rightarrow 0}\\frac{f(x + \\Delta x) - f(x) }{\\Delta x} \\end{equation} A good approximation to the derivative should be possible by using this formula with a small, but non-zero value for \\Delta x . \\begin{equation} \\frac{df}{dx}\\vert_{x} \\approx \\frac{f(x + \\Delta x) - f(x) }{\\Delta x} \\end{equation} When one replaces the derivative with the difference formula the root finding method the resulting update formula is \\begin{equation} x_{k+1}=x_k - \\frac{f(x_k) \\Delta x}{f(x_k + \\Delta x)-f(x_k)} \\end{equation} This root-finding method is called a quasi-Newton method. Here is the code fragment that we change by commenting out the analytical derivative and replacing it with a first-order finite difference approximation of the derivative. The numerical value 1e-06 is called the step size ( \\Delta x ) and should be an input value (rather than built-in to the code as shown here) like the tolerance test values, and be passed to the function as another argument. # reset the notebook %reset -f # import built in function for e^x, cosine from math import exp, cos, sin # Define the function def func(x): func = exp(x) - 10*cos(x) - 100 #using the name as the temp var return func def dfdx(x): # dfdx = exp(x) + 10*sin(x) dfdx = (func(x + 1e-06) - func(x) )/ (1e-06) return (dfdx) # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 1 Enter iteration maximum 10 Enter a solution tolerance (e.g. 1e-06) 1e-6 # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Update not changing \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") Iteration Limit Reached Function value = 0.00017750521082859905 Root value = 4.593211144371335 End of NewtonMethod.py Pretty much the same result, but now we dont have to determine the analytical derivative. Turbine Example using Newton's Method All we have to do is redefine the various functions, copy from above (because we reset the notebook twice, we lost these objects) # reset the notebook %reset -f # prototype dl/dC function import math def func(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 =w2*math.cos(math.pi - A - x) denom1 = math.sin(math.pi - A - x)**2 numer2 = w1*math.cos(x) denom2 = math.sin(x)**2 func = numer1/denom1 - numer2/denom2 return(func) # prototype length function def blade_length(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 = w2 denom1 = math.sin(math.pi - A - x) numer2 = w1 denom2 = math.sin(x) blade_length = numer1/denom1 + numer2/denom2 return(blade_length) # prototype finite difference approximation to derivative def dfdx(x): dfdx = (func(x + 1e-09) - func(x) )/ (1e-09) return (dfdx) # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 30 Enter iteration maximum 12 Enter a solution tolerance (e.g. 1e-06) 1e-6 # need to convert to radians xnow = xnow*math.pi/180. # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) print(xnow,func(xnow),xnew,func(xnew)) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Iteration\",i,\" Update not changing \\n\",) print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") 0.5235987755982988 -46.15910654595476 0.6916206270990959 -11.659686294276273 0.6916206270990959 -11.659686294276273 0.7630538719365358 -0.4517499985484186 0.7630538719365358 -0.4517499985484186 0.7660021307306737 -0.00012395061385106487 0.7660021307306737 -0.00012395061385106487 0.7660029400921291 -2.327027459614328e-12 Iteration 3 Update not changing Function value = -2.327027459614328e-12 Root value = 0.7660029400921291 End of NewtonMethod.py # need to convert back to degrees newton_angle = xnew*180/math.pi print(\"C angle \",newton_angle) C angle 43.88873556189143 # Now get the length from our best guess from plot #newton_angle = 43.8887 # based on newton method fit! print('Maximum blade length for C angle of ',round(newton_angle,1),' degrees is ',round(blade_length(newton_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(newton_angle*math.pi/180.),3)) Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.0 # Newton's method using scipy import scipy.optimize myguess = 30 myguess = myguess*math.pi/180. newton_angle = scipy.optimize.newton(func, myguess)*180/math.pi print(\"Using scipy the C angle is \",newton_angle) print('Maximum blade length for C angle of ',round(newton_angle,1),' degrees is ',round(blade_length(newton_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(newton_angle*math.pi/180.),3)) Using scipy the C angle is 43.888735561892304 Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.0","title":"Implicit Equations"},{"location":"1-Lessons/Lesson10/lesson10/#transporting-wind-turbine-blades","text":"Consider the transport of Turbine Blades, they are rather long, and often the transport compaines have to navigate difficult turns as in Figure 1. Consider route planning using a simple case: Two intersecting mountain road cuts (think of vertical walls) meet at an angle of 123 ^o , as shown in Figure 2. The East-West road is 7 feet wide, while the Southwest-Northeast road is 9 feet wide. What is the longest blade that can negotiate the turn? You can neglect the blade thickness (think of it as a line segment) and cannot tip it to make it through the corner. Build a tool (solution script) that can allow for general use where the angle A is variable as are the road widths.","title":"Transporting Wind Turbine Blades"},{"location":"1-Lessons/Lesson10/lesson10/#analysis","text":"Visualize the turbine blade in sucessive positions as we transport it around the corner; there will be some critical position where each end touch the road cut walls while a point on the blade touches the corner of the intersection. If we analyze the various triangles formed by the turbine blade we can express the lengths in terms of the widths and angles. For the part of the blade on the East-West portion we obtain: l_1 = \\frac{w_2}{sin(B)} For the part of the blade on the Southeast-Northwest portion we obtain: l_2 = \\frac{w_1}{sin(C)} The angles are related as: B = \\pi - A - C And the turbine total length is: l = l_1 + l_2 = \\frac{w_2}{sin(B)} + \\frac{w_1}{sin(C)} Substitute our expression for B , and we have everything in terms on road widths, and intersection angle: l = \\frac{w_2}{sin(\\pi - A - C)} + \\frac{w_1}{sin(C)} Now we want to find the smallest l as a function of C , the necessary condition for such a minimum is \\frac{dl}{dC}=0 which by application of Calculus produces: \\frac{dl}{dC}=\\frac{w_2 cos(\\pi - A - C)}{sin^2(\\pi - A - C)} - \\frac{w_1 cos(C)}{sin^2(C)}=0 where A , w_1 , and w_2 are known, now we have to find a solution to this equation. and once we have found the value of C that satisfies \\frac{dl}{dC}=0 we can recover the length from l = \\frac{w_2}{sin(\\pi - A - C)} + \\frac{w_1}{sin(C)} Lets consider some methods to find the length: Plot the function, find the value from the plot Apply a search method (grid searching) Newton's method (with finite difference approximations to derivative) # Plot the function, find the value from the plot ## forward define prototype functions # Our plotting function import matplotlib.pyplot as plt def makeAplot(listx1,listy1,strlablx,strlably,strtitle): mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(listx1,listy1, c='red', marker='o',linewidth=1) # basic data plot plt.xlabel(strlablx) plt.ylabel(strlably) plt.title(strtitle) plt.show() # prototype dl/dC function import math def func(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 =w2*math.cos(math.pi - A - x) denom1 = math.sin(math.pi - A - x)**2 numer2 = w1*math.cos(x) denom2 = math.sin(x)**2 func = numer1/denom1 - numer2/denom2 return(func) # prototype length function def blade_length(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 = w2 denom1 = math.sin(math.pi - A - x) numer2 = w1 denom2 = math.sin(x) blade_length = numer1/denom1 + numer2/denom2 return(blade_length) # Plot the function c_angle = [] # empty list to populate f_value = [] # empty list to populate # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet stepsize = 0.01 # increments of angle (degrees) total_steps = int(Cm/stepsize) tolerance = 5 # Only save a result close to the zero for i in range(1,total_steps): test_angle = float(i)*math.pi/total_steps test_value = func(test_angle) if abs(test_value) <= tolerance: c_angle.append((test_angle*180)/math.pi) # angle in degrees f_value.append(test_value) # value of dl/dC function # print((test_angle*180)/math.pi,test_value) # activate to examine values makeAplot(c_angle,f_value,'strlablx','strlably','strtitle') # Now get the length from our best guess from plot; trial and error to refine guess visual_angle = 43.888 # based on eyeball fit! print('Maximum blade length for C angle of ',round(visual_angle,1),' degrees is ',round(blade_length(visual_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(visual_angle*math.pi/180.),3)) Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.002","title":"Analysis"},{"location":"1-Lessons/Lesson10/lesson10/#grid-search","text":"Describe method then show example # Apply a search method # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet tolerance = 0.01 # define close enough to zero stepsize = 0.001 # increments of angle (degrees) total_steps = int(Cm/stepsize) best_size = 1e9 for i in range(1,total_steps): test_angle = float(i)*math.pi/total_steps test_value = func(test_angle) if abs(test_value) <= tolerance: # are we close to zero? if abs(test_value) < best_size: # are better than last close to zero? best_size = abs(test_value) best_angle = test_angle # print(float(i),test_value,best_size) else: pass #print(((best_angle*180)/math.pi),best_size) # Now report values print('Searched ',total_steps,' interference angles in steps of ',stepsize,' degrees of arc') print(' Lower C angle ',round(float(1)*math.pi/total_steps,3),'degrees of arc') #print(' Blade Length to Fit Intersection is ',round(blade_length(float(1)*math.pi/total_steps),3),' feet') print(' Upper C angle ',round(Cm,3), 'degrees of arc') print('Best C angle ',round(((best_angle*180)/math.pi),3), 'degrees of arc') print('Longest Blade Length to Fit Intersection is ',round(blade_length(best_angle),3),' feet') Searched 89900 interference angles in steps of 0.001 degrees of arc Lower C angle 0.0 degrees of arc Upper C angle 89.9 degrees of arc Best C angle 43.889 degrees of arc Longest Blade Length to Fit Intersection is 50.93 feet","title":"Grid Search"},{"location":"1-Lessons/Lesson10/lesson10/#single-variable-newtons-method","text":"Newton's method is an iterative technique that can produce good estimates of solutions to implicit equations. The method is employed by rewriting the equation in the form f(x) = 0 , then successively manipulating guesses for x until the function evaluates to a value close enough to zero for the modeler to accept.In the turbine blade case the \\frac{dl}{dC}=0 function is already in the correct form.","title":"Single Variable Newtons Method"},{"location":"1-Lessons/Lesson10/lesson10/#background","text":"The figure above is a graph of some function whose intercept with the x-axis is unknown. The goal of Newton's method is to find this intersection (root) from a realistic first guess. Suppose the first guess is x1 , shown on the figure as the right-most specific value of x . The value of the function at this location is f(x1) . Because x1 is supposed to be a root the difference from the value zero represents an error in the estimate. Newton's method simply provides a recipe for corrections to this error. Provided x1 is not near a minimum or maximum (slope of the function is not zero) then a better estimate of the root can be obtained by extending a tangent line from x1, f(x1) to the x-axis . The intersection of this line with the axis represents a better estimate of the root. This new estimate is x2 . A formula for x2 can be derived from the geometry of the triangle x2,f(x1),x1 . Recall from calculus that the tangent to a function at a particular point is the first derivative of the function. Therefore, from the geometry of the triangle and the definition of tangent we can write, \\begin{equation} tan(\\theta)=\\frac{df}{dx}\\Biggr\\vert_{x_1} = \\frac{f(x_1)}{x_1 - x_2} \\end{equation} Solving the equation for x 2 results in a formula that expresses x2 in terms of the first guess plus a correction term. \\begin{equation} x_2=x_1 - \\frac{f(x_1)}{\\frac{df}{dx}\\vert_{x_1}} \\end{equation} The second term on the right hand side is the correction term to the estimate on the right hand side. Once x2 is calculated we can repeat the formula substituting x2 for x1 and x3 for x2 in the formula. Repeated application usually leads to one of three outcomes: 1. a root; 2. divergence to +/- \\inf ; or 3. cycling. These three outcomes are discussed below in various subsections along with some remedies. The generalized formula is \\begin{equation} x_{k+1}=x_{k} - \\frac{ f(x_{k}) }{ \\frac{df}{dx}\\rvert_{x_k} } \\label{eqn:NewtonFormula} \\end{equation} If the derivative is evaluated using analytical derivatives the method is called Newton's method, if approximations to the derivative are used, it is called a quasi-Newton method.","title":"Background"},{"location":"1-Lessons/Lesson10/lesson10/#newtons-method-using-analytical-derivatives","text":"This subsection is an example in Python of implementing Newton's method with analytical derivatives. The recipe itself is: Write the function in proper form, and code it into a computer. Write the derivative in proper form and code it into a computer. Make an initial guess of the solution (0 and 1 are always convenient guesses). Evaluate the function, evaluate the derivative, calculate their ratio. Subtract the ratio from the current guess and save the result as the update. Test for stopping: Did the update stay the same value? Yes, then stop, probably have a solution. Is the function nearly zero? Yes, then stop we probably have a solution. Have we tried too many updates? Yes, then stop the process is probably cycling, stop. If stopping is indicated proceed to next step, otherwise proceed back to step 4. Stopping indicated, report last update as the result (or report failure to find solution), and related information about the status of the numerical method. The following example illustrates these step as well as an ipython implementation of Newton's method. Suppose we wish to find a root (value of x ) that satisfies: \\begin{equation} f(x) = e^x - 10 cos(x) -100 \\end{equation} Then we will need to code it into a script. Here is a code fragment that will generate the prototype function # import built in function for e^x, cosine from math import exp, cos, sin # Define the function def func(x): func = exp(x) - 10*cos(x) - 100 #using the name as the temp var return func Notice in the code fragment we import three built-in functions from the Python math package, specifically \\exp() , \\sin() , and \\cos () . The next step is to code the derivative. In this case the derivative is \\begin{equation} \\frac{df}{dx}\\vert{(x)} = e^x + 10 \\sin(x) \\end{equation} and the prototype function is coded as def dfdx(x): dfdx = exp(x) + 10*sin(x) return dfdx Next we will need script to read in an initial guess, and ask us how many trials we will use to try to find a solution, as well as how close to zero we should be before we declare victory. # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 2 Enter iteration maximum 9 Enter a solution tolerance (e.g. 1e-06) 1e-6 The use of HowSmall is called a zero tolerance. We will use the same numerical value for two tolerance tests. Also notice how we are using error traps to force numeric input. Probably overkill for this example, but because we already wrote the try-except code in an earlier lesson, might as well reuse the code. Professional codes do a lot of error checking before launching into the actual processing - especially if the processing part is time consuming, its worth the time to check for obvious errors before running for a few hours then at some point failing because of an input value error that was predictable. Now back to the tolerance tests. The first test is to determine if the update has changed or not. If it has not, we may not have a correct answer, but there is no point continuing because the update is unlikely to move further. The test is something like \\begin{equation} \\text{IF}~\\lvert x_{k+1} - x_{k} \\rvert < \\text{Tol.~ THEN Exit and Report Results} \\end{equation} The second test is if the function value is close to zero. The structure of the test is similar, just an different argument. The second test is something like \\begin{equation} \\text{IF}~\\lvert f(x_{k+1}) \\rvert < \\text{Tol.~ THEN Exit and Report Results} \\end{equation} One can see from the nature of the two tests that a programmer might want to make the tolerance values different. This modification is left as a reader exercise. Checking for maximum iterations is relatively easy, we just include code that checks for normal exit the loop. Here is code fragment that implements the method, makes the various tests, and reports results. # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Update not changing \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") Update not changing Function value = 1.4210854715202004e-14 Root value = 4.593209147284144 End of NewtonMethod.py Now we simply connect the three fragments, and we would have a working Python script that implements Newton's method for the example equation. The example is specific to the particular function provided, but the programmer could move the two functions func and dfdx into a user specified module, and then load that module in the program to make it even more generic. The next section will use such an approach to illustrate the ability to build a generalized Newton method and only have to program the function itself","title":"Newton's Method --- Using analytical derivatives"},{"location":"1-Lessons/Lesson10/lesson10/#newtons-method-using-finite-differences-to-estimate-the-derivative","text":"A practical difficulty in using Newton's method is determining the value of the derivative in cases where differentiation is difficult. In these cases we can replace the derivative by a finite difference equation and then proceed as in Newton's method. Recall from calculus that the derivative was defined as the limit of the difference quotient: \\begin{equation} \\frac{df}{dx}\\vert_{x} = \\lim_{\\Delta x \\rightarrow 0}\\frac{f(x + \\Delta x) - f(x) }{\\Delta x} \\end{equation} A good approximation to the derivative should be possible by using this formula with a small, but non-zero value for \\Delta x . \\begin{equation} \\frac{df}{dx}\\vert_{x} \\approx \\frac{f(x + \\Delta x) - f(x) }{\\Delta x} \\end{equation} When one replaces the derivative with the difference formula the root finding method the resulting update formula is \\begin{equation} x_{k+1}=x_k - \\frac{f(x_k) \\Delta x}{f(x_k + \\Delta x)-f(x_k)} \\end{equation} This root-finding method is called a quasi-Newton method. Here is the code fragment that we change by commenting out the analytical derivative and replacing it with a first-order finite difference approximation of the derivative. The numerical value 1e-06 is called the step size ( \\Delta x ) and should be an input value (rather than built-in to the code as shown here) like the tolerance test values, and be passed to the function as another argument. # reset the notebook %reset -f # import built in function for e^x, cosine from math import exp, cos, sin # Define the function def func(x): func = exp(x) - 10*cos(x) - 100 #using the name as the temp var return func def dfdx(x): # dfdx = exp(x) + 10*sin(x) dfdx = (func(x + 1e-06) - func(x) )/ (1e-06) return (dfdx) # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 1 Enter iteration maximum 10 Enter a solution tolerance (e.g. 1e-06) 1e-6 # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Update not changing \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") Iteration Limit Reached Function value = 0.00017750521082859905 Root value = 4.593211144371335 End of NewtonMethod.py Pretty much the same result, but now we dont have to determine the analytical derivative.","title":"Newton's Method --- Using finite-differences to estimate the derivative"},{"location":"1-Lessons/Lesson10/lesson10/#turbine-example-using-newtons-method","text":"All we have to do is redefine the various functions, copy from above (because we reset the notebook twice, we lost these objects) # reset the notebook %reset -f # prototype dl/dC function import math def func(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 =w2*math.cos(math.pi - A - x) denom1 = math.sin(math.pi - A - x)**2 numer2 = w1*math.cos(x) denom2 = math.sin(x)**2 func = numer1/denom1 - numer2/denom2 return(func) # prototype length function def blade_length(x): # x is angle C in radians global w1,w2,A # w1,w2 are road cut widths, A is angle in radians; GLOBAL DEFINE numer1 = w2 denom1 = math.sin(math.pi - A - x) numer2 = w1 denom2 = math.sin(x) blade_length = numer1/denom1 + numer2/denom2 return(blade_length) # prototype finite difference approximation to derivative def dfdx(x): dfdx = (func(x + 1e-09) - func(x) )/ (1e-09) return (dfdx) # set problem constants A = 90.1 # intersection angle in degrees Cm = 180 - A # biggest C angle in degrees A = A * (1/180.00)*math.pi # intersection angle in radians w1 = 17.0 #road cut width in feet w2 = 19.0 #road cut width in feet # Now for the Newton Method Implementation # Get initial guess, use a simple error trap yes=0 while yes == 0: xnow = input(\"Enter an initial guess for Newton method \\n\") try: xnow = float(xnow) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get number trials, use a simple error trap yes=0 while yes == 0: HowMany = input(\"Enter iteration maximum \\n\") try: HowMany = int(HowMany) yes =1 except: print (\"Value should be numeric, try again \\n\") # Get stopping criterion yes=0 while yes == 0: HowSmall = input(\"Enter a solution tolerance (e.g. 1e-06) \\n\") try: HowSmall= float(HowSmall) yes =1 except: print (\"Value should be numeric, try again \\n\") Enter an initial guess for Newton method 30 Enter iteration maximum 12 Enter a solution tolerance (e.g. 1e-06) 1e-6 # need to convert to radians xnow = xnow*math.pi/180. # now we begin the process count = 0 for i in range(0,HowMany,1): xnew = xnow - func(xnow)/dfdx(xnow) print(xnow,func(xnow),xnew,func(xnew)) # stopping criteria -- update not changing if abs(xnew - xnow) < HowSmall: print (\"Iteration\",i,\" Update not changing \\n\",) print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # stopping criteria -- function close to zero if abs( func(xnew) ) < HowSmall: print (\"Function value close to zero \\n\") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) break else: xnow = xnew count = count +1 continue # next step, then have either broken from the loop or iteration counted out if count == HowMany: print(\" Iteration Limit Reached \") print(\"Function value =\",func(xnew)) print(\" Root value =\",xnew) print(\"End of NewtonMethod.py \") 0.5235987755982988 -46.15910654595476 0.6916206270990959 -11.659686294276273 0.6916206270990959 -11.659686294276273 0.7630538719365358 -0.4517499985484186 0.7630538719365358 -0.4517499985484186 0.7660021307306737 -0.00012395061385106487 0.7660021307306737 -0.00012395061385106487 0.7660029400921291 -2.327027459614328e-12 Iteration 3 Update not changing Function value = -2.327027459614328e-12 Root value = 0.7660029400921291 End of NewtonMethod.py # need to convert back to degrees newton_angle = xnew*180/math.pi print(\"C angle \",newton_angle) C angle 43.88873556189143 # Now get the length from our best guess from plot #newton_angle = 43.8887 # based on newton method fit! print('Maximum blade length for C angle of ',round(newton_angle,1),' degrees is ',round(blade_length(newton_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(newton_angle*math.pi/180.),3)) Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.0 # Newton's method using scipy import scipy.optimize myguess = 30 myguess = myguess*math.pi/180. newton_angle = scipy.optimize.newton(func, myguess)*180/math.pi print(\"Using scipy the C angle is \",newton_angle) print('Maximum blade length for C angle of ',round(newton_angle,1),' degrees is ',round(blade_length(newton_angle*math.pi/180.),3),' feet') print(' dl/dC function target is zero; current value is ',round(func(newton_angle*math.pi/180.),3)) Using scipy the C angle is 43.888735561892304 Maximum blade length for C angle of 43.9 degrees is 50.93 feet dl/dC function target is zero; current value is -0.0","title":"Turbine Example using Newton's Method"},{"location":"1-Lessons/Lesson10/old_src/lesson9/","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 18 February 2021 Lesson 9 Data Modeling: Statistical Approach This lesson covers concepts related to modeling data - it is the start of several lessons on the subject. The ultimate goal is to explain observed behavior with a model (like \\textbf{F} = m\\textbf{a} ) so that the model can be used to predict behavior. If we are predicting between existing observations, that's interpolation and is relatively straightforward. If we are predicting beyond existing observations, that's called extrapolation and is less straightforward. To get started we will examine the concepts of causality (cause => effect) and correlation, and the use of simulation to generate probability estimates. Objectives To understand the fundamental concepts involved in causality; and the difference between cause and correlation. To understand the fundamental concepts involved in iteration. To understand the fundamental concepts involved in simulation Computational Thinking Concepts The CT concepts include: Algorithm Design => Causality, Iteration, Simulation System Integration => Iteration, Simulation Correlation and Causality What is causality? (A long winded psuedo definition!) Causality is the relationship between causes and effects. The notion of causality does not have a uniform definition in the sciences, and is studied using philosophy and statistics. From the perspective of physics, it is generally believed that causality cannot occur between an effect and an event that is not in the back (past) light cone of said effect. Similarly, a cause could not have an effect outside its front (future) light cone. Here are some recent articles regarding Closed Time Loops, that explains causal consistency. The second paper is by an undergraduate student! https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.125.040605 https://iopscience.iop.org/article/10.1088/1361-6382/aba4bc Both to some extent theoretically support our popular notion of time travel (aka Dr. Who) without pesky paradoxes; someone with creative writing juices, could have a good science fiction career using these papers as a starting thesis! In classical physics, an effect cannot occur before its cause. In Einstein's theory of special relativity, causality means that an effect can not occur from a cause that is not in the back (past) light cone of that event. Similarly, a cause cannot have an effect outside its front (future) light cone. These restrictions are consistent with the assumption that causal influences cannot travel faster than the speed of light and/or backwards in time. In quantum field theory, observables of events with a spacelike relationship, \"elsewhere\", have to commute, so the order of observations or measurements of such observables do not impact each other. Causality in this context should not be confused with Newton's second law, which is related to the conservation of momentum, and is a consequence of the spatial homogeneity of physical laws. The word causality in this context means that all effects must have specific causes. Another requirement, at least valid at the level of human experience, is that cause and effect be mediated across space and time (requirement of contiguity). This requirement has been very influential in the past, in the first place as a result of direct observation of causal processes (like pushing a cart), in the second place as a problematic aspect of Newton's theory of gravitation (attraction of the earth by the sun by means of action at a distance) replacing mechanistic proposals like Descartes' vortex theory; in the third place as an incentive to develop dynamic field theories (e.g., Maxwell's electrodynamics and Einstein's general theory of relativity) restoring contiguity in the transmission of influences in a more successful way than in Descartes' theory. Yada yada bla bla bla ... Correlation (Causality's mimic!) The literary (as in writing!) formulation of causality is a \"why?, because ...\" structure (sort of like if=>then) The answer to a because question, should be the \"cause.\" Many authors use \"since\" to imply cause, but it is incorrect grammar - since answers the question of when? Think \"CAUSE\" => \"EFFECT\" Correlation doesn\u2019t mean cause (although it is a really good predictor of the crap we all buy - its why Amazon is sucessfull) Consider the chart below The correlation between money spent on pets and the number of lawyers is quite good (nearly perfect), so does having pets cause lawyers? Of course not, the general social economic conditions that improve general wealth, and create sufficient disposable income to have pets (here we mean companion animals, not food on the hoof) also creates conditions for laywers to proliferate, hence a good correlation. Nice video : Correlation and Causation https://www.youtube.com/watch?v=1Sa2v7kVEc0 Quoting from http://water.usgs.gov/pubs/twri/twri4a3/ Concentrations of atrazine and nitrate in shallow groundwaters are measured in wells over a several county area. For each sample, the concentration of one is plotted versus the concentration of the other. As atrazine concentrations increase, so do nitrate. How might the strength of this association be measured and summarized? Streams draining the Sierra Nevada mountains in California usually receive less precipitation in November than in other months. Has the amount of November precipitation significantly changed over the last 70 years, showing a gradual change in the climate of the area? How might this be tested? The above situations require a measure of the strength of association between two continuous variables, such as between two chemical concentrations, or between amount of precipitation and time. How do they co-vary? One class of measures are called correlation coefficients. Also important is how the significance of that association can be tested for, to determine whether the observed pattern differs from what is expected due entirely to chance. Whenever a correlation coefficient is calculated, the data should be plotted on a scatterplot. No single numerical measure can substitute for the visual insight gained from a plot. Many different patterns can produce the same correlation coefficient, and similar strengths of relationships can produce differing coefficients, depending on the curvature of the relationship. Implications Most research questions attempt to explain cause and effect. - In experimental research, the relationship is constructed and the experiment is somewhat of a failure if none of the presumed causal (causal == explainatory) variables influence the response (response == effect) - In a data science experimental context, causality may be impossible to establish, however correlations can be established and exploited. In data science, many studies involve observations on a group of individuals, a factor of interest called a treatment (explainatory variable, predictor variable, predictor feature ...), and an outcome (response, effect, state, predicted value ...) measured on each individual. The presumptive establishment of causality takes place in two stages. First, an association is observed. Any relation between the treatment and the outcome is called an association (we can measure the strength of the association using correlation coefficients!). Second, A more careful analysis is used to establish causality. a. One approach would be to control all variables other than the suspected (explainatory) variables, which for any meaningful process is essentially impossible. b. Another approach is to establish randomized control studies: 1. Start with a sample from a population (e.g. volunteers to test Covid 19 vaccines) 2. Randomly assign members to either a. Control group b. Treatment group 3. Expose the two groups identically, except the control group recieves a false (null) treatment 4. Compare the responses of the two groups, if they are same, there exists no evidence that the treatment variable CAUSES a response These concepts can be extended with some ingenuity to engineered systems and natural systems. Consider Data Science Questions: - Does going to school cause flu? - Does flu cause school attendance? - Does going to school contribute to the spread of flu? - Does the spread of flu contribute to the school attendance? - Are there other variables that affects both? a. These are called \u201cconfounding factors\u201d or \u201clurking variables\u201d. b. Cold weather?, more indoor time?, more interaction? Confounding Factors An underlying difference between the two groups (other than the treatment) is called a confounding factor, because it might confound you (that is, mess you up) when you try to reach a conclusion. For example, Cold weather in the previous example. Confounding also occurs when explainatory variables are correlated to another, for instance flood flows are well correlated to drainage area, main channel length, mean annual precipitation, main channel slope, and elevation. However main channel length is itself strongly correlated to drainage area, so much so as to be nearly useless as an explainatory variable when drainage area is retained in a data model. It would be a \"confounding variable\" in this context. Randomization To establish presumptive causality in our data science experiments, we need randomization tools. We can use Python to make psuedo-random choices. There are built-in functions in numpy library under random submodule. The choice function randomly picks one item from an array. The syntax is np.random.choice(array_name) , where array_name is the name of the array from which to make the choice.\u200b #Making Random Choice from an Array (or list) import numpy as np two_groups = np.array(['treatment', 'control']) np.random.choice(two_groups,1) # mylist = ['treatment', 'control'] # this works too # np.random.choice(mylist) array(['treatment'], dtype='<U9') The difference of this function from others that we learned so far, is that it doesn\u2019t give the same result every time. We can roll a dice using this function by randomly selecting from an array from 1 to 6. my_die = np.array(['one', 'two','three', 'four','five', 'six']) np.random.choice(my_die) 'six' # now a bunch of rolls print('roll #1 ',np.random.choice(my_die) ) print('roll #2 ',np.random.choice(my_die) ) print('roll #3 ',np.random.choice(my_die) ) print('roll #4 ',np.random.choice(my_die) ) print('roll #5 ',np.random.choice(my_die) ) print('roll #6 ',np.random.choice(my_die) ) roll #1 four roll #2 four roll #3 four roll #4 five roll #5 six roll #6 one # or multiple rolls, single call myDiceRolls = np.random.choice(my_die,6) print(myDiceRolls) ['six' 'two' 'two' 'six' 'one' 'five'] 'six' We might need to repeat a process multiple times to reach better results or cover more results. Let\u2019s create a game with following rules: If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. my_wallet = 1 # start with 1 dollars def place_a_bet(wallet): print(\"Place your bet!\") if wallet == 0: print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Single play print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) print(\"Amount in my account =:\",my_wallet) Amount in my account =: 1 Place your bet! Roll the die! You Lose, Bummer! Amount in my account =: 0 A more automated solution is to use a for statement to loop over the contents of a sequence. Each result is called iteration. Here we use a for statement in a more realistic way: we print the results of betting five times on the die as described earlier. This process is called simulating the results of five bets. We use the word simulating to remind ourselves that we are not physically rolling dice and exchanging money but using Python to mimic the process. # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' my_wallet = 10 how_many_throws = 1 for i in range(how_many_throws): print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) print(\"After \",i+1,\" plays\") print(CRED + \"Amount in my account =:\",my_wallet,CEND) print(\"_______________________\") Amount in my account =: 10 Place your bet! Roll the die! You win a dollar! After 1 plays \u001b[91mAmount in my account =: 11 \u001b[0m _______________________ Simulation of multiple gamblers/multiple visits to the Casino https://www.inferentialthinking.com/chapters/09/3/Simulation.html outcomes = np.array([]) #null array to store outcomes # redefine functions to suppress output def place_a_bet(wallet): # print(\"Place your bet!\") if wallet == 0: # print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" # print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: #print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: #print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: #print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' how_many_simulations = 100000 for j in range(how_many_simulations): my_wallet = 1 how_many_throws = 30 for i in range(how_many_throws): # print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) # print(\"After \",i+1,\" plays\") # print(CRED + \"Amount in my account =:\",my_wallet,CEND) # print(\"_______________________\") outcomes = np.append(outcomes,my_wallet) # build a histogram chart - outcomes is an array import matplotlib.pyplot as plt from scipy.stats import gamma #ax.hist(r, density=True, histtype='stepfilled', alpha=0.2) plt.hist(outcomes, density=True, bins = 20) plt.xlabel(\"Dollars in Gamer's Wallet\") plt.ylabel('Relative Frequency') #### just a data model, gamma distribution ############## # code below adapted from https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html a = 5 # bit of trial and error x = np.linspace(gamma.ppf(0.001, a),gamma.ppf(0.999, a), 1000) plt.plot(x, gamma.pdf(x, a, loc=-1.25, scale=1),'r-', lw=5, alpha=1.0, label='gamma pdf') ######################################################### # Render the plot plt.show() #print(\"Expected value of wallet (mean) =: \",outcomes.mean()) import pandas as pd df = pd.DataFrame(outcomes) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 count 100000.000000 mean 1.632990 std 1.651133 min 0.000000 25% 0.000000 50% 1.000000 75% 2.000000 max 14.000000 Simulation Simulation is the process of using a computer to mimic a real experiment or process. In this class, those experiments will almost invariably involve chance. To summarize from: https://www.inferentialthinking.com/chapters/09/3/Simulation.html Step 1: What to Simulate: Specify the quantity you want to simulate. For example, you might decide that you want to simulate the outcomes of tosses of a coin. Step 2: Simulating One Value: Figure out how to simulate one value of the quantity you specified in Step 1. (usually turn into a function for readability) Step 3: Number of Repetitions: Decide how many times you want to simulate the quantity. You will have to repeat Step 2 that many times. Step 4: Coding the Simulation: Put it all together in code. Step 5: Interpret the results (plots, Simulation Example Should I change my choice? Based on Monty Hall example from https://youtu.be/Xp6V_lO1ZKA But we already have a small car! (Also watch https://www.youtube.com/watch?v=6Ewq_ytHA7g to learn significance of the small car!) Consider The gist of the game is that a contestent chooses a door, the host reveals one of the unselected doors and offers the contestant a chance to change their choice. Should the contestant stick with her initial choice, or switch to the other door? That is the Monty Hall problem. Using classical probability theory it is straightforward to show that: The chance that the car is behind the originally chosen door is 1/3. After Monty opens the door with the goat, the chance distribution changes. If the contestant switches the decision, he/she doubles the chance. Suppose we have harder situations, can we use this simple problem to learn how to ask complex questions? import numpy as np import pandas as pd import matplotlib.pyplot as plt def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 1', 'Goat 2', 'Car'] Goat 1 Goat 2 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining how_many_games = 10000 for i in np.arange(how_many_games): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Goat 2 Goat 1 Car 1 Goat 1 Goat 2 Car 2 Goat 1 Goat 2 Car 3 Goat 2 Goat 1 Car 4 Goat 2 Goat 1 Car ... ... ... ... 9995 Car Goat 2 Goat 1 9996 Car Goat 1 Goat 2 9997 Car Goat 2 Goat 1 9998 Car Goat 2 Goat 1 9999 Goat 1 Goat 2 Car 10000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show() Interpret Results According to the plot, it is beneficial for the players to switch doors because the initial chance for being correct is only 1/3 Does changing doors have a CAUSAL effect on outcome? ## Various Examples Defect Chances A sample of four electronic components is taken from the output of a production line. The probabilities of the various outcomes are calculated to be: Pr [0 defectives] = 0.6561, Pr [1 defective] = 0.2916, Pr [2 defectives] = 0.0486, Pr [3 defectives] = 0.0036, Pr [4 defectives] = 0.0001. What is the probability of at least one defective? #Method-1 pr_atleast1 = 1-0.6561 print(pr_atleast1) 0.3439 #Method-2 pr_atleast1 = 0.2916+0.0483+0.0036+0.0001 print(pr_atleast1) 0.3436 Common is a Birthday? A class of engineering students consists of 45 people. What is the probability that no two students have birthdays on the same day, not considering the year of birth? To simplify the calculation, assume that there are 365 days in the year and that births are equally likely on all of them. Then what is the probability that some members of the class have birthdays on the same day? Also, vary the number of students in the class from 2 to 200 to see its effect on the probability values. #A student in the class states his birthday. So the probability that he/she has the birthday on that date is 1 pr_first = 1 print(pr_first) 1 #Probability that the second student has different birthday than the first student is 364/365 pr_second = 364/365 print(pr_second) 0.9972602739726028 #Probability that the third student has different birthday than the first and the second students is 363/365 pr_third = 363/365 print(pr_third) 0.9945205479452055 #Probability that the fourth student has different birthday than the first, the second, and the third students is 362/365 pr_fourth = 362/365 print(pr_fourth) 0.9917808219178083 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-1: Looping over a list student_ids = list(range(2,46,1)) pr_nosame = 1 for i in student_ids: pr_nosame = pr_nosame*((365-i+1)/365) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) 0.05902410053422507 0.940975899465775 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop student_ids = np.arange(2,46,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame = np.prod(pr_eachstudent) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-19-e397c0f6a5ec> in <module> 7 #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop 8 ----> 9 student_ids = np.arange(2,46,1) 10 11 pr_eachstudent = ((365-student_ids+1)/365) NameError: name 'np' is not defined #Simulation: Getting the probability for different numbers of total students in the class total_students = np.arange(2,201,1) pr_nosame = [] pr_same = [] for i in total_students: student_ids = np.arange(2,i,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame_total = np.prod(pr_eachstudent) pr_nosame.append(pr_nosame_total) pr_same.append(1 - pr_nosame_total) #Creating a dataframe with columns - number of students and probability import pandas as pd final_data = {'Number of students': total_students, 'Probability': pr_same} df = pd.DataFrame(final_data) print(df) #Creating a scatter plot between number of students and probability that at least a pair of students have the same birthday import matplotlib.pyplot as plt plt.scatter(total_students, pr_same, color = 'blue') plt.xlabel('No. of students in the class') plt.ylabel('P [same birthday]') plt.title('Effect of sample size on the chance of success') Making Hole (and money!) An oil company is bidding for the rights to drill a well in field A and a well in field B. The probability it will drill a well in field A is 40%. If it does, the probability the well will be successful is 45%. The probability it will drill a well in field B is 30%. If it does, the probability the well will be successful is 55%. Calculate each of the following probabilities: a) What is the probability of a successful well in field A? pr_successA = 0.40*0.45 pr_successA 0.18000000000000002 b) What is the probability of a successful well in field B? pr_successB = 0.30*0.55 pr_successB 0.165 c) What is the probability of both a successful well in field A and a successful well in field B? pr_successAB = pr_successA*pr_successB pr_successAB 0.029700000000000004 d) What is the probability of at least one successful well in the two fields together? pr_onesuccess = pr_successA + pr_successB - pr_successAB pr_onesuccess 0.3153 e) What is the probability of no successful well in field A? pr_nosuccessA = (1-0.4)+(0.4*0.55) pr_nosuccessA 0.8200000000000001 f) What is the probability of no successful well in field B? pr_nosuccessB = (1-0.3)+(0.3*0.45) pr_nosuccessB 0.835 g) What is the probability of no successful well in the two fields together? pr_nosuccessAB = 1 - pr_onesuccess pr_nosuccessAB 0.6847 h) What is the probability of exactly one successful well in the two fields together? pr_exactonesuccess = (0.18*0.835)+(0.165*0.82) pr_exactonesuccess 0.28559999999999997 References Ford, Martin. 2009 The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future (p. 107). Acculant Publishing. Kindle Edition. Computational and Inferential Thinking: The Foundations of Data Science. By Ani Adhikari and John DeNero, with Contributions by David Wagner and Henry Milner. Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0). https://www.inferentialthinking.com/chapters/09/Randomness.html # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) ! pwd atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) /home/sensei/1330-textbook-webroot/docs/lesson9","title":"Lesson9"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 18 February 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#lesson-9-data-modeling-statistical-approach","text":"This lesson covers concepts related to modeling data - it is the start of several lessons on the subject. The ultimate goal is to explain observed behavior with a model (like \\textbf{F} = m\\textbf{a} ) so that the model can be used to predict behavior. If we are predicting between existing observations, that's interpolation and is relatively straightforward. If we are predicting beyond existing observations, that's called extrapolation and is less straightforward. To get started we will examine the concepts of causality (cause => effect) and correlation, and the use of simulation to generate probability estimates.","title":"Lesson 9 Data Modeling: Statistical Approach"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#objectives","text":"To understand the fundamental concepts involved in causality; and the difference between cause and correlation. To understand the fundamental concepts involved in iteration. To understand the fundamental concepts involved in simulation","title":"Objectives"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#computational-thinking-concepts","text":"The CT concepts include: Algorithm Design => Causality, Iteration, Simulation System Integration => Iteration, Simulation","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#correlation-and-causality","text":"","title":"Correlation and Causality"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#what-is-causality-a-long-winded-psuedo-definition","text":"Causality is the relationship between causes and effects. The notion of causality does not have a uniform definition in the sciences, and is studied using philosophy and statistics. From the perspective of physics, it is generally believed that causality cannot occur between an effect and an event that is not in the back (past) light cone of said effect. Similarly, a cause could not have an effect outside its front (future) light cone. Here are some recent articles regarding Closed Time Loops, that explains causal consistency. The second paper is by an undergraduate student! https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.125.040605 https://iopscience.iop.org/article/10.1088/1361-6382/aba4bc Both to some extent theoretically support our popular notion of time travel (aka Dr. Who) without pesky paradoxes; someone with creative writing juices, could have a good science fiction career using these papers as a starting thesis! In classical physics, an effect cannot occur before its cause. In Einstein's theory of special relativity, causality means that an effect can not occur from a cause that is not in the back (past) light cone of that event. Similarly, a cause cannot have an effect outside its front (future) light cone. These restrictions are consistent with the assumption that causal influences cannot travel faster than the speed of light and/or backwards in time. In quantum field theory, observables of events with a spacelike relationship, \"elsewhere\", have to commute, so the order of observations or measurements of such observables do not impact each other. Causality in this context should not be confused with Newton's second law, which is related to the conservation of momentum, and is a consequence of the spatial homogeneity of physical laws. The word causality in this context means that all effects must have specific causes. Another requirement, at least valid at the level of human experience, is that cause and effect be mediated across space and time (requirement of contiguity). This requirement has been very influential in the past, in the first place as a result of direct observation of causal processes (like pushing a cart), in the second place as a problematic aspect of Newton's theory of gravitation (attraction of the earth by the sun by means of action at a distance) replacing mechanistic proposals like Descartes' vortex theory; in the third place as an incentive to develop dynamic field theories (e.g., Maxwell's electrodynamics and Einstein's general theory of relativity) restoring contiguity in the transmission of influences in a more successful way than in Descartes' theory. Yada yada bla bla bla ...","title":"What is causality? (A long winded psuedo definition!)"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#correlation-causalitys-mimic","text":"The literary (as in writing!) formulation of causality is a \"why?, because ...\" structure (sort of like if=>then) The answer to a because question, should be the \"cause.\" Many authors use \"since\" to imply cause, but it is incorrect grammar - since answers the question of when? Think \"CAUSE\" => \"EFFECT\" Correlation doesn\u2019t mean cause (although it is a really good predictor of the crap we all buy - its why Amazon is sucessfull) Consider the chart below The correlation between money spent on pets and the number of lawyers is quite good (nearly perfect), so does having pets cause lawyers? Of course not, the general social economic conditions that improve general wealth, and create sufficient disposable income to have pets (here we mean companion animals, not food on the hoof) also creates conditions for laywers to proliferate, hence a good correlation. Nice video : Correlation and Causation https://www.youtube.com/watch?v=1Sa2v7kVEc0 Quoting from http://water.usgs.gov/pubs/twri/twri4a3/ Concentrations of atrazine and nitrate in shallow groundwaters are measured in wells over a several county area. For each sample, the concentration of one is plotted versus the concentration of the other. As atrazine concentrations increase, so do nitrate. How might the strength of this association be measured and summarized? Streams draining the Sierra Nevada mountains in California usually receive less precipitation in November than in other months. Has the amount of November precipitation significantly changed over the last 70 years, showing a gradual change in the climate of the area? How might this be tested? The above situations require a measure of the strength of association between two continuous variables, such as between two chemical concentrations, or between amount of precipitation and time. How do they co-vary? One class of measures are called correlation coefficients. Also important is how the significance of that association can be tested for, to determine whether the observed pattern differs from what is expected due entirely to chance. Whenever a correlation coefficient is calculated, the data should be plotted on a scatterplot. No single numerical measure can substitute for the visual insight gained from a plot. Many different patterns can produce the same correlation coefficient, and similar strengths of relationships can produce differing coefficients, depending on the curvature of the relationship.","title":"Correlation (Causality's mimic!)"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#implications","text":"Most research questions attempt to explain cause and effect. - In experimental research, the relationship is constructed and the experiment is somewhat of a failure if none of the presumed causal (causal == explainatory) variables influence the response (response == effect) - In a data science experimental context, causality may be impossible to establish, however correlations can be established and exploited. In data science, many studies involve observations on a group of individuals, a factor of interest called a treatment (explainatory variable, predictor variable, predictor feature ...), and an outcome (response, effect, state, predicted value ...) measured on each individual. The presumptive establishment of causality takes place in two stages. First, an association is observed. Any relation between the treatment and the outcome is called an association (we can measure the strength of the association using correlation coefficients!). Second, A more careful analysis is used to establish causality. a. One approach would be to control all variables other than the suspected (explainatory) variables, which for any meaningful process is essentially impossible. b. Another approach is to establish randomized control studies: 1. Start with a sample from a population (e.g. volunteers to test Covid 19 vaccines) 2. Randomly assign members to either a. Control group b. Treatment group 3. Expose the two groups identically, except the control group recieves a false (null) treatment 4. Compare the responses of the two groups, if they are same, there exists no evidence that the treatment variable CAUSES a response These concepts can be extended with some ingenuity to engineered systems and natural systems. Consider Data Science Questions: - Does going to school cause flu? - Does flu cause school attendance? - Does going to school contribute to the spread of flu? - Does the spread of flu contribute to the school attendance? - Are there other variables that affects both? a. These are called \u201cconfounding factors\u201d or \u201clurking variables\u201d. b. Cold weather?, more indoor time?, more interaction?","title":"Implications"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#confounding-factors","text":"An underlying difference between the two groups (other than the treatment) is called a confounding factor, because it might confound you (that is, mess you up) when you try to reach a conclusion. For example, Cold weather in the previous example. Confounding also occurs when explainatory variables are correlated to another, for instance flood flows are well correlated to drainage area, main channel length, mean annual precipitation, main channel slope, and elevation. However main channel length is itself strongly correlated to drainage area, so much so as to be nearly useless as an explainatory variable when drainage area is retained in a data model. It would be a \"confounding variable\" in this context.","title":"Confounding Factors"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#randomization","text":"To establish presumptive causality in our data science experiments, we need randomization tools. We can use Python to make psuedo-random choices. There are built-in functions in numpy library under random submodule. The choice function randomly picks one item from an array. The syntax is np.random.choice(array_name) , where array_name is the name of the array from which to make the choice.\u200b #Making Random Choice from an Array (or list) import numpy as np two_groups = np.array(['treatment', 'control']) np.random.choice(two_groups,1) # mylist = ['treatment', 'control'] # this works too # np.random.choice(mylist) array(['treatment'], dtype='<U9') The difference of this function from others that we learned so far, is that it doesn\u2019t give the same result every time. We can roll a dice using this function by randomly selecting from an array from 1 to 6. my_die = np.array(['one', 'two','three', 'four','five', 'six']) np.random.choice(my_die) 'six' # now a bunch of rolls print('roll #1 ',np.random.choice(my_die) ) print('roll #2 ',np.random.choice(my_die) ) print('roll #3 ',np.random.choice(my_die) ) print('roll #4 ',np.random.choice(my_die) ) print('roll #5 ',np.random.choice(my_die) ) print('roll #6 ',np.random.choice(my_die) ) roll #1 four roll #2 four roll #3 four roll #4 five roll #5 six roll #6 one # or multiple rolls, single call myDiceRolls = np.random.choice(my_die,6) print(myDiceRolls) ['six' 'two' 'two' 'six' 'one' 'five'] 'six' We might need to repeat a process multiple times to reach better results or cover more results. Let\u2019s create a game with following rules: If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. my_wallet = 1 # start with 1 dollars def place_a_bet(wallet): print(\"Place your bet!\") if wallet == 0: print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Single play print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) print(\"Amount in my account =:\",my_wallet) Amount in my account =: 1 Place your bet! Roll the die! You Lose, Bummer! Amount in my account =: 0 A more automated solution is to use a for statement to loop over the contents of a sequence. Each result is called iteration. Here we use a for statement in a more realistic way: we print the results of betting five times on the die as described earlier. This process is called simulating the results of five bets. We use the word simulating to remind ourselves that we are not physically rolling dice and exchanging money but using Python to mimic the process. # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' my_wallet = 10 how_many_throws = 1 for i in range(how_many_throws): print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) print(\"After \",i+1,\" plays\") print(CRED + \"Amount in my account =:\",my_wallet,CEND) print(\"_______________________\") Amount in my account =: 10 Place your bet! Roll the die! You win a dollar! After 1 plays \u001b[91mAmount in my account =: 11 \u001b[0m _______________________","title":"Randomization"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#simulation-of-multiple-gamblersmultiple-visits-to-the-casino","text":"https://www.inferentialthinking.com/chapters/09/3/Simulation.html outcomes = np.array([]) #null array to store outcomes # redefine functions to suppress output def place_a_bet(wallet): # print(\"Place your bet!\") if wallet == 0: # print(\"You have no money, get out of my Casino!\") return(wallet) else: wallet = wallet - 1 return(wallet) def make_a_roll(wallet): \"\"\"Returns my net gain on one bet\"\"\" # print(\"Roll the die!\") x = np.random.choice(np.arange(1, 7)) # roll a die once and record the number of spots if x <= 2: #print(\"You Lose, Bummer!\") return(wallet) # lose the bet elif x <= 4: #print(\"You Draw, Take your bet back.\") wallet = wallet+1 return(wallet) # draw, get bet back elif x <= 6: #print(\"You win a dollar!\") wallet = wallet+2 return (wallet) # win, get bet back and win a dollar! # Some printing tricks CRED = '\\033[91m' CEND = '\\033[0m' how_many_simulations = 100000 for j in range(how_many_simulations): my_wallet = 1 how_many_throws = 30 for i in range(how_many_throws): # print(\"Amount in my account =:\",my_wallet) my_wallet = place_a_bet(my_wallet) my_wallet = make_a_roll(my_wallet) #print(CRED + \"Error, does not compute!\" + CEND) # print(\"After \",i+1,\" plays\") # print(CRED + \"Amount in my account =:\",my_wallet,CEND) # print(\"_______________________\") outcomes = np.append(outcomes,my_wallet) # build a histogram chart - outcomes is an array import matplotlib.pyplot as plt from scipy.stats import gamma #ax.hist(r, density=True, histtype='stepfilled', alpha=0.2) plt.hist(outcomes, density=True, bins = 20) plt.xlabel(\"Dollars in Gamer's Wallet\") plt.ylabel('Relative Frequency') #### just a data model, gamma distribution ############## # code below adapted from https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html a = 5 # bit of trial and error x = np.linspace(gamma.ppf(0.001, a),gamma.ppf(0.999, a), 1000) plt.plot(x, gamma.pdf(x, a, loc=-1.25, scale=1),'r-', lw=5, alpha=1.0, label='gamma pdf') ######################################################### # Render the plot plt.show() #print(\"Expected value of wallet (mean) =: \",outcomes.mean()) import pandas as pd df = pd.DataFrame(outcomes) df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 count 100000.000000 mean 1.632990 std 1.651133 min 0.000000 25% 0.000000 50% 1.000000 75% 2.000000 max 14.000000","title":"Simulation of multiple gamblers/multiple visits to the Casino"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#simulation","text":"Simulation is the process of using a computer to mimic a real experiment or process. In this class, those experiments will almost invariably involve chance. To summarize from: https://www.inferentialthinking.com/chapters/09/3/Simulation.html Step 1: What to Simulate: Specify the quantity you want to simulate. For example, you might decide that you want to simulate the outcomes of tosses of a coin. Step 2: Simulating One Value: Figure out how to simulate one value of the quantity you specified in Step 1. (usually turn into a function for readability) Step 3: Number of Repetitions: Decide how many times you want to simulate the quantity. You will have to repeat Step 2 that many times. Step 4: Coding the Simulation: Put it all together in code. Step 5: Interpret the results (plots,","title":"Simulation"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#simulation-example","text":"Should I change my choice? Based on Monty Hall example from https://youtu.be/Xp6V_lO1ZKA But we already have a small car! (Also watch https://www.youtube.com/watch?v=6Ewq_ytHA7g to learn significance of the small car!) Consider The gist of the game is that a contestent chooses a door, the host reveals one of the unselected doors and offers the contestant a chance to change their choice. Should the contestant stick with her initial choice, or switch to the other door? That is the Monty Hall problem. Using classical probability theory it is straightforward to show that: The chance that the car is behind the originally chosen door is 1/3. After Monty opens the door with the goat, the chance distribution changes. If the contestant switches the decision, he/she doubles the chance. Suppose we have harder situations, can we use this simple problem to learn how to ask complex questions? import numpy as np import pandas as pd import matplotlib.pyplot as plt def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 1', 'Goat 2', 'Car'] Goat 1 Goat 2 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining how_many_games = 10000 for i in np.arange(how_many_games): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Goat 2 Goat 1 Car 1 Goat 1 Goat 2 Car 2 Goat 1 Goat 2 Car 3 Goat 2 Goat 1 Car 4 Goat 2 Goat 1 Car ... ... ... ... 9995 Car Goat 2 Goat 1 9996 Car Goat 1 Goat 2 9997 Car Goat 2 Goat 1 9998 Car Goat 2 Goat 1 9999 Goat 1 Goat 2 Car 10000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show()","title":"Simulation Example"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#interpret-results","text":"According to the plot, it is beneficial for the players to switch doors because the initial chance for being correct is only 1/3 Does changing doors have a CAUSAL effect on outcome? ## Various Examples","title":"Interpret Results"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#defect-chances","text":"A sample of four electronic components is taken from the output of a production line. The probabilities of the various outcomes are calculated to be: Pr [0 defectives] = 0.6561, Pr [1 defective] = 0.2916, Pr [2 defectives] = 0.0486, Pr [3 defectives] = 0.0036, Pr [4 defectives] = 0.0001. What is the probability of at least one defective? #Method-1 pr_atleast1 = 1-0.6561 print(pr_atleast1) 0.3439 #Method-2 pr_atleast1 = 0.2916+0.0483+0.0036+0.0001 print(pr_atleast1) 0.3436","title":"Defect Chances"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#common-is-a-birthday","text":"A class of engineering students consists of 45 people. What is the probability that no two students have birthdays on the same day, not considering the year of birth? To simplify the calculation, assume that there are 365 days in the year and that births are equally likely on all of them. Then what is the probability that some members of the class have birthdays on the same day? Also, vary the number of students in the class from 2 to 200 to see its effect on the probability values. #A student in the class states his birthday. So the probability that he/she has the birthday on that date is 1 pr_first = 1 print(pr_first) 1 #Probability that the second student has different birthday than the first student is 364/365 pr_second = 364/365 print(pr_second) 0.9972602739726028 #Probability that the third student has different birthday than the first and the second students is 363/365 pr_third = 363/365 print(pr_third) 0.9945205479452055 #Probability that the fourth student has different birthday than the first, the second, and the third students is 362/365 pr_fourth = 362/365 print(pr_fourth) 0.9917808219178083 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-1: Looping over a list student_ids = list(range(2,46,1)) pr_nosame = 1 for i in student_ids: pr_nosame = pr_nosame*((365-i+1)/365) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) 0.05902410053422507 0.940975899465775 #Probability that none of the 45 students have the same birthday in the class will then be -- # P[no same birthdays] = (1)*(364/365)*(363/365)*(362/365)*........*((365-i+1)/365)*........*((365-45+1)/365) #How will you generalize this? #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop student_ids = np.arange(2,46,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame = np.prod(pr_eachstudent) print(pr_nosame) #Probability that at least one pair out of the 45 students have the same birthday in the class will then be -- # P[same birthday] = 1 - P[no same birthday] pr_same = 1 - pr_nosame print(pr_same) --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-19-e397c0f6a5ec> in <module> 7 #Method-2: Using NumPy array instead of a list so that we can avoid writing a for loop 8 ----> 9 student_ids = np.arange(2,46,1) 10 11 pr_eachstudent = ((365-student_ids+1)/365) NameError: name 'np' is not defined #Simulation: Getting the probability for different numbers of total students in the class total_students = np.arange(2,201,1) pr_nosame = [] pr_same = [] for i in total_students: student_ids = np.arange(2,i,1) pr_eachstudent = ((365-student_ids+1)/365) pr_nosame_total = np.prod(pr_eachstudent) pr_nosame.append(pr_nosame_total) pr_same.append(1 - pr_nosame_total) #Creating a dataframe with columns - number of students and probability import pandas as pd final_data = {'Number of students': total_students, 'Probability': pr_same} df = pd.DataFrame(final_data) print(df) #Creating a scatter plot between number of students and probability that at least a pair of students have the same birthday import matplotlib.pyplot as plt plt.scatter(total_students, pr_same, color = 'blue') plt.xlabel('No. of students in the class') plt.ylabel('P [same birthday]') plt.title('Effect of sample size on the chance of success')","title":"Common is a Birthday?"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#making-hole-and-money","text":"An oil company is bidding for the rights to drill a well in field A and a well in field B. The probability it will drill a well in field A is 40%. If it does, the probability the well will be successful is 45%. The probability it will drill a well in field B is 30%. If it does, the probability the well will be successful is 55%. Calculate each of the following probabilities: a) What is the probability of a successful well in field A? pr_successA = 0.40*0.45 pr_successA 0.18000000000000002 b) What is the probability of a successful well in field B? pr_successB = 0.30*0.55 pr_successB 0.165 c) What is the probability of both a successful well in field A and a successful well in field B? pr_successAB = pr_successA*pr_successB pr_successAB 0.029700000000000004 d) What is the probability of at least one successful well in the two fields together? pr_onesuccess = pr_successA + pr_successB - pr_successAB pr_onesuccess 0.3153 e) What is the probability of no successful well in field A? pr_nosuccessA = (1-0.4)+(0.4*0.55) pr_nosuccessA 0.8200000000000001 f) What is the probability of no successful well in field B? pr_nosuccessB = (1-0.3)+(0.3*0.45) pr_nosuccessB 0.835 g) What is the probability of no successful well in the two fields together? pr_nosuccessAB = 1 - pr_onesuccess pr_nosuccessAB 0.6847 h) What is the probability of exactly one successful well in the two fields together? pr_exactonesuccess = (0.18*0.835)+(0.165*0.82) pr_exactonesuccess 0.28559999999999997","title":"Making Hole (and money!)"},{"location":"1-Lessons/Lesson10/old_src/lesson9/#references","text":"Ford, Martin. 2009 The Lights in the Tunnel: Automation, Accelerating Technology and the Economy of the Future (p. 107). Acculant Publishing. Kindle Edition. Computational and Inferential Thinking: The Foundations of Data Science. By Ani Adhikari and John DeNero, with Contributions by David Wagner and Henry Milner. Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0). https://www.inferentialthinking.com/chapters/09/Randomness.html # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) ! pwd atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) /home/sensei/1330-textbook-webroot/docs/lesson9","title":"References"},{"location":"1-Lessons/Lesson11/lesson11/","text":"%%html table {margin-left: 0 !important;} Interpolation, Integration, Differentiation of Functions and Tabular Data Interpolation The Starship rocket in the figure below sends a lot of telemetry data to both on-board and off-board (ground-based) control computers. Suppose telemetry is received every 1/10 of a second, providing the altitude (position) of the craft, something like the figure below. How can one estimate the altitude at intermediate times (between the 1/10 of a second \"true\" values)? The problem is a type of interpolation problem similar to calculating water density from tables for intermediate values by assuming a straight line passed between the two values from the table. However it may not be appropriate to assume that the altitudes are linear with time. The special challenge comes when we want to estimate intermediate values when there is a maximum or minimum in the tabular structure, and we will have to process many records for different cases. The classical approach to such a problem is to fit a polynomial to the tabular results and interrogate the resulting polynomial to obtain estimates of the intermediate values. This prediction engine (the polynomial) is required to return the exact value at a observation location (in our case a 1/10 second interval). This requirement is quite distinct from other types of prediction engines we will study. Lagrangian Interpolation Polynomial interpolation is the method of determining a polynomial that fits a set of given points. There are several approaches to polynomial interpolation, of which one of the most well known is the Lagrangian method. The Lagrangian polynomial https://en.wikipedia.org/wiki/Lagrange_polynomial is the polynomial of order n-1 , where n is he number of tabular data pairs we wish to interpolate. Suppose we have a table of data (or telemetry sent back from our rocket), of x- and f(x) -values: x~ ~f(x) x_1 f_1 x_2 f_2 x_3 f_3 x_4 f_4 The highest order polynomial that can be passed through these four data pairs is a cubic. A Lagrangian form for such a cubic is P_3(x) = f(x_1)\\frac{(x-x_2)(x-x_3)(x-x_4)}{(x_1-x_2)(x_1-x_3)(x_1-x_4)} + f(x_2)\\frac{(x-x_1)(x-x_3)(x-x_4)}{(x_2-x_1)(x_2-x_3)(x_2-x_4)} + f(x_3)\\frac{(x-x_1)(x-x_2)(x-x_4)}{(x_3-x_1)(x_3-x_2)(x_3-x_4)} +f(x_4)\\frac{(x-x_1)(x-x_2)(x-x_3)}{(x_4-x_1)(x_4-x_2)(x_4-x_3)} Notice that it is constructed of four terms, each of which is a cubic in x ; hence the sum is a cubic also. The pattern of each term is to form the numerator as a product of differences of the form (x-x_i) , omitting one x_i in each term, the ommitted term is used in the denominator as a replacement for x in each position in the numerator. In each term, the difference factor is multiplied by the value f_i corresponding to the x_i ommitted in the numerator. The Lagragian polynomial for other degrees of interpolating polynomials employs this same pattern of forming a sum of polynomials of the desired degree. Of importance is that the polynomial is intended to be used for interpolation, that is the value we seek P(x^*) assumes we will supply x^* in the range [x_1 , x_4] . Going outside this range is called extrapolation, and interpolator-type prediction engines are the wromg tool! Example 1 Consider the three observations below, estimate (predict) the value for $f(2.3). x~ ~f(x) 1.1 10.6 1.7 15.2 3.0 20.3 The Lagrangian form of the highest order of polynomial that can pass through the 3 data pairs is the quadratic: P_2(x) = f(x_1)\\frac{(x-x_2)(x-x_3)}{(x_1-x_2)(x_1-x_3)} + f(x_2)\\frac{(x-x_1)(x-x_3)}{(x_2-x_1)(x_2-x_3)} + f(x_3)\\frac{(x-x_1)(x-x_2)}{(x_3-x_1)(x_3-x_2)} Once the denominators are completed, it is relatively straightforward to compute the estimate (prediction), in this case P_2(x) = (10.6)\\frac{(x-1.7)(x-3.0)}{(1.1-1.7)(1.1-3.0)} + (15.2)\\frac{(x-1.1)(x-3.0)}{(1.7-1.1)(1.7-3.0)} + (20.3)\\frac{(x-1.1)(x-1.7)}{(3.0-1.1)(3.0-1.7)} At x=2.3 the result is P_2(2.3)=18.38 . Naturally, we want to use Computational Thinking principles, to pattern match and generalize the arithmetic as below. def lagint(xlist,ylist,xpred): # lagrangian interpolation of order len(xlist)-1 # lagint = 0.0 # ypred is an accumulator, and will be output norder = len(xlist) for i in range(norder): term = ylist[i] # build up terms of polynomial for j in range(norder): if (i != j): term = term * (xpred-xlist[j])/(xlist[i]-xlist[j]) # pass # may not need this expression lagint = lagint + term # print(i,j) #debugging expression return(lagint) xtable = [1.1,1.7,3.0] ytable = [10.6,15.2,20.3] xwant = 2.3 print(round(lagint(xtable,ytable,xwant),2)) 18.38 Example 2 This example is copied from https://rstudio-pubs-static.s3.amazonaws.com/286315_f00cf07beb3945d2a0260d6eaecb5d36.html In the original source the author plots the resulting function, we can do the same here. First the observation set: x~ ~f(x) 0 7 2 11 3 28 4 63 Next we will plot the interpolating polynomial from x=0 to x=4 in steps of 0.1 xtable = [0.1,0.3,0.5,0.7,0.9,1.1,1.3] ytable = [0.003,0.067,0.148,0.248,0.370,0.518,0.697] xwant = 0.3 print(lagint(xtable,ytable,xwant)) 0.067 # Observations xtable = [0,2,3,4] ytable = [7,11,28,63] # xpred = [] # empty list to store results for plotting ypred = [] # empty list to store results for plotting # step_size = 0.10 # step size how_many = int((xtable[len(xtable)-1])/step_size) # build the predictions for i in range(how_many+1): xpred.append(float(i)*step_size) ypred.append(lagint(xtable,ytable,float(i)*step_size)) #print(lagint(xtable,ytable,xwant)) import matplotlib.pyplot # the python plotting library myfigure = matplotlib.pyplot.figure(figsize = (6,6)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(xtable, ytable ,color ='blue') # The observations as points matplotlib.pyplot.plot(xpred, ypred, color ='red') # the polynomial matplotlib.pyplot.xlabel(\"Input Value\") matplotlib.pyplot.ylabel(\"Function Value\") mytitle = \"Interpolating Polynomial Fit to Observations\\n \" mytitle += \"Blue Markers are Observations \" + \"\\n\" mytitle += \"Red Curve is Fitted Polynomial \"+ \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Exercises In a radiation-induced polymerization study, a gamma source was used to give measured doses of radiation. The dosage varied with position in the apparatus, with the following data being recorded: Position from emitter (inches) Dosage Rate, 10^5 rads/hr 0 1.90 0.5 2.39 1.0 2.71 1.5 2.98 2.0 3.20 3.0 3.20 3.5 2.98 4.0 2.74 For some reason, the reading at 2.5 inches was not reported, but the value of radiation at that distance is needed; estimate the dosage level at 2.5 inches using an interpolation-type prediction engine. Plot the interpolating function as well as the observations. # Observations position = [0,0.5,1.0,1.5,2.0,3.0,3.5,4.0] dosage = [1.90,2.39,2.71,2.98,3.20,3.20,2.98,2.74] # xpred = [] # empty list to store results for plotting ypred = [] # empty list to store results for plotting # step_size = 0.10 # step size how_many = int((position[len(position)-1])/step_size) # build the predictions for i in range(how_many+1): xpred.append(float(i)*step_size) ypred.append(lagint(position,dosage,float(i)*step_size)) #print(lagint(xtable,ytable,xwant)) myfigure = matplotlib.pyplot.figure(figsize = (6,6)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(position,dosage ,color ='blue') # The observations as points matplotlib.pyplot.plot(xpred, ypred, color ='red') # the polynomial matplotlib.pyplot.xlabel(\"Distance\") matplotlib.pyplot.ylabel(\"Dosage\") mytitle = \"Interpolating Polynomial Fit to Observations\\n \" mytitle += \"Blue Markers are Observations \" + \"\\n\" mytitle += \"Red Curve is Fitted Polynomial \"+ \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() lagint(position,dosage,2.5) Interpolation References Lagrangian Interpolation (using R packages) https://rstudio-pubs-static.s3.amazonaws.com/286315_f00cf07beb3945d2a0260d6eaecb5d36.html Lagrangian Interpolation (Video) https://www.youtube.com/watch?v=_zK_KhHW6og Lagrange Polynomials https://en.wikipedia.org/wiki/Lagrange_polynomial Gerald, C.F., and Wheatley, P. O., 1984. Applied Numerical Analysis. 3rd Ed. Addison Wesley, Inc. , pp. 171-210. Westerink, J.J. 2018. CE 30125 Computational Methods, Department of Civil and Environmental Engineering and Earth Sciences University of Notre Dame, Notre Dame IN 46556 https://coast.nd.edu/jjwteach/www/www/30125/pdfnotes/lecture3_6v13.pdf Integration of Functions At this point we have enough Python to consider doing some useful computations. We will start with numerical integration because it is useful and only requires count-controlled repetition and single subscript lists. Background Numerical integration is the numerical approximation of \\begin{equation} I = \\int_a^b f(x)dx \\end{equation} Consider the problem of determining the shaded area under the curve y = f(x) from x = a to x = b , as depicted in the figure below, and suppose that analytical integration is not feasible. The function may be known in tabular form from experimental measurements or it may be known in an analytical form. The function is taken to be continuous within the interval a < x < b . We may divide the area into n vertical panels, each of width \\Delta x = (b - a)/n , and then add the areas of all strips to obtain A~\\approx \\int ydx . A representative panel of area A_i is shown with darker shading in the figure. Three useful numerical approximations are listed in the following sections. The approximations differ in how the function is represented by the panels --- in all cases the function is approximated by known polynomial models between the panel end points. In each case the greater the number of strips, and correspondingly smaller value of \\Delta x , the more accurate the approximation. Typically, one can begin with a relatively small number of panels and increase the number until the resulting area approximation stops changing. Rectangular Panels The figure below is a schematic of a rectangular panels. The figure is assuming the function structure is known and can be evaluated at an arbitrary location in the \\Delta x dimension. Each panel is treated as a rectangle, as shown by the representative panel whose height y_m is chosen visually so that the small cross-hatched areas are as nearly equal as possible. Thus, we form the sum \\sum y_m of the effective heights and multiply by \\Delta x . For a function known in analytical form, a value for y_m equal to that of the function at the midpoint x_i + \\Delta x /2 may be calculated and used in the summation. For tabulated functions, we have to choose to either take y_m as the value at the left endpoint or right endpoint. This limitation is often quite handy when we are trying to integrate a function that is integrable, but undefined on one endpoint. Lets try some examples in Python. Find the area under the curve y= x\\sqrt{1+x^2} from x = 0 to x = 2 . First lets read in the value for the lowerlimit, we will do some limited error checks to be sure user enters a number, but won't check that the number is non-negative. # RectangularPanels.py # Numerical Integration print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Program finds area under curve y = x * sqrt(1+x) Enter a lower bound x_low 2 Verify that value is indeed what we entered print(x_low) 2.0 Now do the same for the upper limit, notice how we are using the yes variable. We set a \"fail\" value, and demand input until we get \"success\". The structure used here is called a try -- exception structure and is very common in programming. Error checking is really important so that garbled input does not hang things up. yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Enter an upper bound x_high 4 Again verify! print(x_high) 4.0 Now use the try - exception structure to input how many panels we wish to use. Notice you can enter a negative value which will ultimately break things. Also observe this value is an integer. yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Enter how many panels 5 Again verify! print(how_many) 5 Now we can actually perform the integration by evaluating the function at the panel half-widths. In this example we are using primitive arithmetic, so the \\sqrt{} is accomplished by exponentation, the syntax is c = a ** b is the operation c = a^b . The integration uses an accumulator, which is a memory location where subsquent results are added (accumulated) back into the accumulator. This structure is so common that there are alternate, compact syntax to perform this task, here it is all out in the open. The counting loop where we evaluate the function at different x values, starts at 1 and ends at how_many+1 because python for loops use an increment skip if equal structure. When the value in range equals how_many the for loop exits ( break is implied.) A loop control structure starting from 0 is shown in the code as a comment line. Simply uncomment this line, and comment the line just below to have the structure typical in python scripts. In the start from 1 case, we want to evaluate at the last value of how_many . # OK we should have the three things we need for evaluating the integral delta_x = (x_high - x_low)/float(how_many) # compute panel width xx = x_low + delta_x/2 # initial value for x ### OK THIS IS THE ACTUAL INTEGRATOR PART ### accumulated_area = 0.0 # initial value in an accumulator #for i in range(0,how_many,1): #note we are counting from 0 for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * ( (1+xx**2)**(0.5) ) ) * delta_x xx = xx + delta_x ### AND WE ARE DONE INTEGRATING ############# Finally, we want to report our result print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) # the backslash \\ # \" to x = ..... lets us use multiple lines # the \\n is a \"newline\" character Area under curve y = x * sqrt(1+x) from x = 2.0 to x = 4.0 is approximately: 19.610958667237167 The code implements rudimentary error checking -- it forces us to enter numeric values for the lower and upper values of x as well as the number of panels to use. It does not check for undefined ranges and such, but you should get the idea -- notice that a large fraction of the entire program is error trapping; this devotion to error trapping is typical for professional programs where you are going to distribute executable modules and not expect the end user to be a programmer. Using the math package The actual computations are done rather crudely -- there is a math package that would give us the ability to compute the square root as a function call rather than exponentiation to a real values exponent. That is illustrated below # RectangularPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator xx = x_low + delta_x/2 # initial value for x for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * sqrt(1+xx**2) ) * delta_x xx = xx + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) Program finds area under curve y = x * sqrt(1+x) Enter a lower bound x_low 0 Enter an upper bound x_high 2 Enter how many panels 6 Area under curve y = x * sqrt(1+x) from x = 0.0 to x = 2.0 is approximately: 3.3793974379024605 Trapezoidal Panels The trapezoidal panels are approximated as shown in the figure below. The area A_i is the average height (y_i + y_{i+1} )/2 times \\Delta x . Adding the areas gives the area approximation as tabulated. For the example with the curvature shown, the approximation will be on the low side. For the reverse curvature, the approximation will be on the high side. The trapezoidal approximation is commonly used with tabulated values. The script below illustrates the trapezoidal method for approximating an integral. In the example, the left and right panel endpoints in x are set as separate variables x_{left} and x_{right} and incremented by \\Delta x as we step through the count-controlled repetition to accumulate the area. The corresponding y values are computed within the loop and averaged, then multiplied by \\Delta x and added to the accumulator. Finally the x values are incremented --- for grins, we used the += operator on the accumulator # TrapezoidalPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_right = x_left + delta_x # initial value for x_right edge panel for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left* sqrt(1+x_left**2) ) y_right = ( x_right* sqrt(1+x_right**2) ) accumulated_area += + (1./2.) * ( y_left + y_right ) * delta_x x_left += delta_x x_right += delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) Program finds area under curve y = x * sqrt(1+x) Enter a lower bound x_low 0 Enter an upper bound x_high 8 Enter how many panels 9 Area under curve y = x * sqrt(1+x) from x = 0.0 to x = 8.0 is approximately: 175.33954986737925 Parabolic Panels Parabolic panels approximate the shape of the panel with a parabola. The area between the chord and the curve (neglected in the trapezoidal solution) may be accounted for by approximating the function with a parabola passing through the points defined by three successive values of y . This area may be calculated from the geometry of the parabola and added to the trapezoidal area of the pair of strips to give the area \\Delta A of the pair as illustrated. Adding all of the \\Delta A s produces the tabulation shown, which is known as Simpson's rule. To use Simpson's rule, the number n of strips must be even. The same example as presented for rectangular panels is repeated, except using parabolic panels. The code is changed yet again because we will evaluate at each end of the panel as well as at an intermediate value. # ParabolicPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_middle = x_left + delta_x # initial value for x_middle edge panel x_right = x_middle + delta_x # initial value for x_right edge panel how_many = int(how_many/2) # using 2 panels every step, so 1/2 many steps -- force integer result for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left * sqrt(1+ x_left**2) ) y_middle = ( x_middle * sqrt(1+ x_middle**2) ) y_right = ( x_right * sqrt(1+ x_right**2) ) accumulated_area = accumulated_area + \\ (1./3.) * ( y_left + 4.* y_middle + y_right ) * delta_x x_left = x_left + 2*delta_x x_middle = x_left + delta_x x_right = x_middle + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) Program finds area under curve y = x * sqrt(1+x) Enter a lower bound x_low 0 Enter an upper bound x_high 1 Enter how many panels 4 Area under curve y = x * sqrt(1+x) from x = 0.0 to x = 1.0 is approximately: 0.6094186631272838 If we study all the forms of the numerical method we observe that the numerical integration method is really the sum of function values at specific locations in the interval of interest, with each value multiplied by a specific weight. In this development the weights were based on polynomials, but other method use different weighting functions. An extremely important method is called gaussian quadrature. This method is valuable because one can approximate convolution integrals quite effectively using quadrature routines, while the number of function evaluations for a polynomial based approximation could be hopeless. When the function values are tabular, we are going to have to accept the rectangular (with adaptations) and trapezoidal as our best tools to approximate an integral because we don't have any really effective way to evaluate the function between the tabulated values. Integration of Tabular Data This section is going to work with tabular data -- different from function evaluation, but similar. To be really useful, we need to learn how to read data from a file; manually entering tabular data is really time consuming, error prone, and just plain idiotic. So in this chapter we will learn how to read data from a file into a list, then we can process the list as if it were a function and integrate its contents. Reading from a file --- open, read, close files First, lets consider a file named MyFile.txt . The extension is important so that the Shell does not think it is a Python script. The contents of MyFile.txt are: 1 1 2 4 3 9 4 16 5 25 The code fragment below, will let us look at the file (already existing in our local directory) import subprocess # lets us run \"shell\" commands and recover stdio stream usefull_cat_call = subprocess.run([\"cat\",\"MyFile.txt\"], stdout=subprocess.PIPE, text=True) # this is the call to run the bash command \"cat MyFile.txt\" which will display the contents of the file if it exists. print(usefull_cat_call.stdout) 1 1 2 4 3 9 4 16 5 25 Now that we know that the file exists,to read the contents into a Python script we have to do the following: Open a connection to the file --- this is a concept common to all languages, it might be called something different, but the program needs to somehow know the location and name of the file. Read the contents into an object --- we have a lot of control on how this gets done, for the time being we won't exercise much control yet. When you do substantial programs, you will depend on the control of the reads (and writes). Disconnect the file --- this too is common to all languages. Its a really easy step to forget. Not a big deal if the program ends as planned but terrible if there is a error in the program and the connection is still open. Usually nothing bad happens, but with an open connection it is possible for the file to get damaged. If that file represents millions of customers credit card numbers, that's kind of a problem, and time to go work on your resume, or get your passport collection out and choose a country without extradition. The code fragment below performs these three tasks and prints the things we read Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read the five lines line1 = Afile.readline() line2 = Afile.readline() line3 = Afile.readline() line4 = Afile.readline() line5 = Afile.readline() Afile.close() # disconnect from the file # echo the input print(line1,end=\"\") print(line2,end=\"\") print(line3,end=\"\") print(line4,end=\"\") print(line5,end=\"\") 1 1 2 4 3 9 4 16 5 25 Read into a list A far more useful and elegant way to read from a file is to use a for loop. The attribute line within a file is an iterable, hence construction the loop is pretty straightforward. A script fragment below does the same thing as the example above, but uses a for loop to accomplish stepping through the file. Additionally, I have added a counter to keep track of how many lines were read --- in a lot of engineering programs, the number of things read becomes important later in a program, hence it is usually a good idea to capture the count when the data are first read. First lets work out if we can automatically detect the end of the file. So this script just reads and prints the attribute line from object Afile . Notice how the print statement is changed, to suppress the extra line feed. Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) Now we will add a list to receive the input, here it reads the file above as a string into a list xy , then splits that list and places the contents into two other lists, x and y . The script has several parts to discuss. First, the destination variables (lists) must be created -- I used the null list concept here because I don't know how big the list is until I read the list. Next I used the .append() method which operates on the xy list. The arguments of the method [str(n) for n in line.strip().split()] tells the program that the elements are to be interpreted as a string, and to split (split) the line into sub-strings based on a null delimiter (whitespace), and to remove all the whitespace (strip) characters. Once the line is split, the strings are appended into the xy list. The xy list is printed to show that it is a list of 5 things, each thing being a string comprised of two sets of characters separated by a comma. xy is a list of strings. The next section of the code then uses the pair function within another .append() method to break the character sets in each element of xy into two parts x and y . Lastly during the pair operation, the code also converts the data into real values (float) and then prints the data in two columns. This seems like a lot of work, but we could easily get this code to be super reliable, then save it as a function and never have to write it again. That too comes later -- suffice to say for now we can read a file, parse its contents into two lists x and y . Thus we are now able to integrate tabular data. xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) The list is: [['1', '1'], ['2', '4'], ['3', '9'], ['4', '16'], ['5', '25']] x = 1.0 y = 1.0 x = 2.0 y = 4.0 x = 3.0 y = 9.0 x = 4.0 y = 16.0 x = 5.0 y = 25.0 Integrating the Tabular Data Suppose instead of a function we only have tabulations and wist to estimate the area under the curve represented by the tabular values. Then our integration rules from the prior chapter still work more or less, except the rectangular panels will have to be shifted to either the left edge or right edge of a panel (where the tabulation exists). Lets just examine an example. Suppose some measurement technology produced a table of related values. The excitation variable is x and f(x) is the response. x f(x) 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 To integrate this table using the trapezoidal method is straightforward. We will modify our earlier code to read the table (which we put into a file), and compute the integral. # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 File has 9 records (lines) The list is: [['1.0', '1.543'], ['1.1', '1.668'], ['1.2', '1.811'], ['1.3', '1.971'], ['1.4', '2.151'], ['1.5', '2.352'], ['1.6', '2.577'], ['1.7', '2.828'], ['1.8', '3.107']] x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Cool, it seems to work -- now tidy the code a bit by suppressing extra outputs # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: ##print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nRecords read =: \",how_many_lines) ##print(\"The list is: \",end=\"\") ##print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result Records read =: 9 x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Realistically the only other simple integration method for tabular data is the rectangular rule, either using the left edge of a panel or the right edge of a panel (and I suppose you could do both and average the result which would be the trapezoidal method). Exercises 1) Approximate \\int_0^2 f(x) dx from the tabulation in the Table below: x f(x) 0.00 1.0000 0.12 0.8869 0.53 0.5886 0.87 0.4190 1.08 0.3396 1.43 0.2393 2.00 0.1353 # 2) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{9.0} cosh(x) dx from the tabulation above. 3) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{4.2} cosh(x) dx from the tabulation above. Briefly explain how you handle starting and stopping the integration from values that are intermediate and are tabulated. # 4) (Advanced) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{4.0} cosh(x) dx from the tabulation above. Explain how handled working with values that fall between tabulated values. References https://rstudio-pubs-static.s3.amazonaws.com/286315_f00cf07beb3945d2a0260d6eaecb5d36.html Lagrangian Interpolation (Video) https://www.youtube.com/watch?v=_zK_KhHW6og Lagrange Polynomials https://en.wikipedia.org/wiki/Lagrange_polynomial","title":"Interpolation and Integration"},{"location":"1-Lessons/Lesson11/lesson11/#interpolation-integration-differentiation-of-functions-and-tabular-data","text":"","title":"Interpolation, Integration, Differentiation of Functions and Tabular Data"},{"location":"1-Lessons/Lesson11/lesson11/#interpolation","text":"The Starship rocket in the figure below sends a lot of telemetry data to both on-board and off-board (ground-based) control computers. Suppose telemetry is received every 1/10 of a second, providing the altitude (position) of the craft, something like the figure below. How can one estimate the altitude at intermediate times (between the 1/10 of a second \"true\" values)? The problem is a type of interpolation problem similar to calculating water density from tables for intermediate values by assuming a straight line passed between the two values from the table. However it may not be appropriate to assume that the altitudes are linear with time. The special challenge comes when we want to estimate intermediate values when there is a maximum or minimum in the tabular structure, and we will have to process many records for different cases. The classical approach to such a problem is to fit a polynomial to the tabular results and interrogate the resulting polynomial to obtain estimates of the intermediate values. This prediction engine (the polynomial) is required to return the exact value at a observation location (in our case a 1/10 second interval). This requirement is quite distinct from other types of prediction engines we will study.","title":"Interpolation"},{"location":"1-Lessons/Lesson11/lesson11/#lagrangian-interpolation","text":"Polynomial interpolation is the method of determining a polynomial that fits a set of given points. There are several approaches to polynomial interpolation, of which one of the most well known is the Lagrangian method. The Lagrangian polynomial https://en.wikipedia.org/wiki/Lagrange_polynomial is the polynomial of order n-1 , where n is he number of tabular data pairs we wish to interpolate. Suppose we have a table of data (or telemetry sent back from our rocket), of x- and f(x) -values: x~ ~f(x) x_1 f_1 x_2 f_2 x_3 f_3 x_4 f_4 The highest order polynomial that can be passed through these four data pairs is a cubic. A Lagrangian form for such a cubic is P_3(x) = f(x_1)\\frac{(x-x_2)(x-x_3)(x-x_4)}{(x_1-x_2)(x_1-x_3)(x_1-x_4)} + f(x_2)\\frac{(x-x_1)(x-x_3)(x-x_4)}{(x_2-x_1)(x_2-x_3)(x_2-x_4)} + f(x_3)\\frac{(x-x_1)(x-x_2)(x-x_4)}{(x_3-x_1)(x_3-x_2)(x_3-x_4)} +f(x_4)\\frac{(x-x_1)(x-x_2)(x-x_3)}{(x_4-x_1)(x_4-x_2)(x_4-x_3)} Notice that it is constructed of four terms, each of which is a cubic in x ; hence the sum is a cubic also. The pattern of each term is to form the numerator as a product of differences of the form (x-x_i) , omitting one x_i in each term, the ommitted term is used in the denominator as a replacement for x in each position in the numerator. In each term, the difference factor is multiplied by the value f_i corresponding to the x_i ommitted in the numerator. The Lagragian polynomial for other degrees of interpolating polynomials employs this same pattern of forming a sum of polynomials of the desired degree. Of importance is that the polynomial is intended to be used for interpolation, that is the value we seek P(x^*) assumes we will supply x^* in the range [x_1 , x_4] . Going outside this range is called extrapolation, and interpolator-type prediction engines are the wromg tool!","title":"Lagrangian Interpolation"},{"location":"1-Lessons/Lesson11/lesson11/#example-1","text":"Consider the three observations below, estimate (predict) the value for $f(2.3). x~ ~f(x) 1.1 10.6 1.7 15.2 3.0 20.3 The Lagrangian form of the highest order of polynomial that can pass through the 3 data pairs is the quadratic: P_2(x) = f(x_1)\\frac{(x-x_2)(x-x_3)}{(x_1-x_2)(x_1-x_3)} + f(x_2)\\frac{(x-x_1)(x-x_3)}{(x_2-x_1)(x_2-x_3)} + f(x_3)\\frac{(x-x_1)(x-x_2)}{(x_3-x_1)(x_3-x_2)} Once the denominators are completed, it is relatively straightforward to compute the estimate (prediction), in this case P_2(x) = (10.6)\\frac{(x-1.7)(x-3.0)}{(1.1-1.7)(1.1-3.0)} + (15.2)\\frac{(x-1.1)(x-3.0)}{(1.7-1.1)(1.7-3.0)} + (20.3)\\frac{(x-1.1)(x-1.7)}{(3.0-1.1)(3.0-1.7)} At x=2.3 the result is P_2(2.3)=18.38 . Naturally, we want to use Computational Thinking principles, to pattern match and generalize the arithmetic as below. def lagint(xlist,ylist,xpred): # lagrangian interpolation of order len(xlist)-1 # lagint = 0.0 # ypred is an accumulator, and will be output norder = len(xlist) for i in range(norder): term = ylist[i] # build up terms of polynomial for j in range(norder): if (i != j): term = term * (xpred-xlist[j])/(xlist[i]-xlist[j]) # pass # may not need this expression lagint = lagint + term # print(i,j) #debugging expression return(lagint) xtable = [1.1,1.7,3.0] ytable = [10.6,15.2,20.3] xwant = 2.3 print(round(lagint(xtable,ytable,xwant),2)) 18.38","title":"Example 1"},{"location":"1-Lessons/Lesson11/lesson11/#example-2","text":"This example is copied from https://rstudio-pubs-static.s3.amazonaws.com/286315_f00cf07beb3945d2a0260d6eaecb5d36.html In the original source the author plots the resulting function, we can do the same here. First the observation set: x~ ~f(x) 0 7 2 11 3 28 4 63 Next we will plot the interpolating polynomial from x=0 to x=4 in steps of 0.1 xtable = [0.1,0.3,0.5,0.7,0.9,1.1,1.3] ytable = [0.003,0.067,0.148,0.248,0.370,0.518,0.697] xwant = 0.3 print(lagint(xtable,ytable,xwant)) 0.067 # Observations xtable = [0,2,3,4] ytable = [7,11,28,63] # xpred = [] # empty list to store results for plotting ypred = [] # empty list to store results for plotting # step_size = 0.10 # step size how_many = int((xtable[len(xtable)-1])/step_size) # build the predictions for i in range(how_many+1): xpred.append(float(i)*step_size) ypred.append(lagint(xtable,ytable,float(i)*step_size)) #print(lagint(xtable,ytable,xwant)) import matplotlib.pyplot # the python plotting library myfigure = matplotlib.pyplot.figure(figsize = (6,6)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(xtable, ytable ,color ='blue') # The observations as points matplotlib.pyplot.plot(xpred, ypred, color ='red') # the polynomial matplotlib.pyplot.xlabel(\"Input Value\") matplotlib.pyplot.ylabel(\"Function Value\") mytitle = \"Interpolating Polynomial Fit to Observations\\n \" mytitle += \"Blue Markers are Observations \" + \"\\n\" mytitle += \"Red Curve is Fitted Polynomial \"+ \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show()","title":"Example 2"},{"location":"1-Lessons/Lesson11/lesson11/#exercises","text":"In a radiation-induced polymerization study, a gamma source was used to give measured doses of radiation. The dosage varied with position in the apparatus, with the following data being recorded: Position from emitter (inches) Dosage Rate, 10^5 rads/hr 0 1.90 0.5 2.39 1.0 2.71 1.5 2.98 2.0 3.20 3.0 3.20 3.5 2.98 4.0 2.74 For some reason, the reading at 2.5 inches was not reported, but the value of radiation at that distance is needed; estimate the dosage level at 2.5 inches using an interpolation-type prediction engine. Plot the interpolating function as well as the observations. # Observations position = [0,0.5,1.0,1.5,2.0,3.0,3.5,4.0] dosage = [1.90,2.39,2.71,2.98,3.20,3.20,2.98,2.74] # xpred = [] # empty list to store results for plotting ypred = [] # empty list to store results for plotting # step_size = 0.10 # step size how_many = int((position[len(position)-1])/step_size) # build the predictions for i in range(how_many+1): xpred.append(float(i)*step_size) ypred.append(lagint(position,dosage,float(i)*step_size)) #print(lagint(xtable,ytable,xwant)) myfigure = matplotlib.pyplot.figure(figsize = (6,6)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(position,dosage ,color ='blue') # The observations as points matplotlib.pyplot.plot(xpred, ypred, color ='red') # the polynomial matplotlib.pyplot.xlabel(\"Distance\") matplotlib.pyplot.ylabel(\"Dosage\") mytitle = \"Interpolating Polynomial Fit to Observations\\n \" mytitle += \"Blue Markers are Observations \" + \"\\n\" mytitle += \"Red Curve is Fitted Polynomial \"+ \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() lagint(position,dosage,2.5)","title":"Exercises"},{"location":"1-Lessons/Lesson11/lesson11/#interpolation-references","text":"Lagrangian Interpolation (using R packages) https://rstudio-pubs-static.s3.amazonaws.com/286315_f00cf07beb3945d2a0260d6eaecb5d36.html Lagrangian Interpolation (Video) https://www.youtube.com/watch?v=_zK_KhHW6og Lagrange Polynomials https://en.wikipedia.org/wiki/Lagrange_polynomial Gerald, C.F., and Wheatley, P. O., 1984. Applied Numerical Analysis. 3rd Ed. Addison Wesley, Inc. , pp. 171-210. Westerink, J.J. 2018. CE 30125 Computational Methods, Department of Civil and Environmental Engineering and Earth Sciences University of Notre Dame, Notre Dame IN 46556 https://coast.nd.edu/jjwteach/www/www/30125/pdfnotes/lecture3_6v13.pdf","title":"Interpolation References"},{"location":"1-Lessons/Lesson11/lesson11/#integration-of-functions","text":"At this point we have enough Python to consider doing some useful computations. We will start with numerical integration because it is useful and only requires count-controlled repetition and single subscript lists.","title":"Integration of Functions"},{"location":"1-Lessons/Lesson11/lesson11/#background","text":"Numerical integration is the numerical approximation of \\begin{equation} I = \\int_a^b f(x)dx \\end{equation} Consider the problem of determining the shaded area under the curve y = f(x) from x = a to x = b , as depicted in the figure below, and suppose that analytical integration is not feasible. The function may be known in tabular form from experimental measurements or it may be known in an analytical form. The function is taken to be continuous within the interval a < x < b . We may divide the area into n vertical panels, each of width \\Delta x = (b - a)/n , and then add the areas of all strips to obtain A~\\approx \\int ydx . A representative panel of area A_i is shown with darker shading in the figure. Three useful numerical approximations are listed in the following sections. The approximations differ in how the function is represented by the panels --- in all cases the function is approximated by known polynomial models between the panel end points. In each case the greater the number of strips, and correspondingly smaller value of \\Delta x , the more accurate the approximation. Typically, one can begin with a relatively small number of panels and increase the number until the resulting area approximation stops changing.","title":"Background"},{"location":"1-Lessons/Lesson11/lesson11/#rectangular-panels","text":"The figure below is a schematic of a rectangular panels. The figure is assuming the function structure is known and can be evaluated at an arbitrary location in the \\Delta x dimension. Each panel is treated as a rectangle, as shown by the representative panel whose height y_m is chosen visually so that the small cross-hatched areas are as nearly equal as possible. Thus, we form the sum \\sum y_m of the effective heights and multiply by \\Delta x . For a function known in analytical form, a value for y_m equal to that of the function at the midpoint x_i + \\Delta x /2 may be calculated and used in the summation. For tabulated functions, we have to choose to either take y_m as the value at the left endpoint or right endpoint. This limitation is often quite handy when we are trying to integrate a function that is integrable, but undefined on one endpoint. Lets try some examples in Python. Find the area under the curve y= x\\sqrt{1+x^2} from x = 0 to x = 2 . First lets read in the value for the lowerlimit, we will do some limited error checks to be sure user enters a number, but won't check that the number is non-negative. # RectangularPanels.py # Numerical Integration print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Program finds area under curve y = x * sqrt(1+x) Enter a lower bound x_low 2 Verify that value is indeed what we entered print(x_low) 2.0 Now do the same for the upper limit, notice how we are using the yes variable. We set a \"fail\" value, and demand input until we get \"success\". The structure used here is called a try -- exception structure and is very common in programming. Error checking is really important so that garbled input does not hang things up. yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Enter an upper bound x_high 4 Again verify! print(x_high) 4.0 Now use the try - exception structure to input how many panels we wish to use. Notice you can enter a negative value which will ultimately break things. Also observe this value is an integer. yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Enter how many panels 5 Again verify! print(how_many) 5 Now we can actually perform the integration by evaluating the function at the panel half-widths. In this example we are using primitive arithmetic, so the \\sqrt{} is accomplished by exponentation, the syntax is c = a ** b is the operation c = a^b . The integration uses an accumulator, which is a memory location where subsquent results are added (accumulated) back into the accumulator. This structure is so common that there are alternate, compact syntax to perform this task, here it is all out in the open. The counting loop where we evaluate the function at different x values, starts at 1 and ends at how_many+1 because python for loops use an increment skip if equal structure. When the value in range equals how_many the for loop exits ( break is implied.) A loop control structure starting from 0 is shown in the code as a comment line. Simply uncomment this line, and comment the line just below to have the structure typical in python scripts. In the start from 1 case, we want to evaluate at the last value of how_many . # OK we should have the three things we need for evaluating the integral delta_x = (x_high - x_low)/float(how_many) # compute panel width xx = x_low + delta_x/2 # initial value for x ### OK THIS IS THE ACTUAL INTEGRATOR PART ### accumulated_area = 0.0 # initial value in an accumulator #for i in range(0,how_many,1): #note we are counting from 0 for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * ( (1+xx**2)**(0.5) ) ) * delta_x xx = xx + delta_x ### AND WE ARE DONE INTEGRATING ############# Finally, we want to report our result print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) # the backslash \\ # \" to x = ..... lets us use multiple lines # the \\n is a \"newline\" character Area under curve y = x * sqrt(1+x) from x = 2.0 to x = 4.0 is approximately: 19.610958667237167 The code implements rudimentary error checking -- it forces us to enter numeric values for the lower and upper values of x as well as the number of panels to use. It does not check for undefined ranges and such, but you should get the idea -- notice that a large fraction of the entire program is error trapping; this devotion to error trapping is typical for professional programs where you are going to distribute executable modules and not expect the end user to be a programmer.","title":"Rectangular Panels"},{"location":"1-Lessons/Lesson11/lesson11/#using-the-math-package","text":"The actual computations are done rather crudely -- there is a math package that would give us the ability to compute the square root as a function call rather than exponentiation to a real values exponent. That is illustrated below # RectangularPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator xx = x_low + delta_x/2 # initial value for x for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * sqrt(1+xx**2) ) * delta_x xx = xx + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) Program finds area under curve y = x * sqrt(1+x) Enter a lower bound x_low 0 Enter an upper bound x_high 2 Enter how many panels 6 Area under curve y = x * sqrt(1+x) from x = 0.0 to x = 2.0 is approximately: 3.3793974379024605","title":"Using the math package"},{"location":"1-Lessons/Lesson11/lesson11/#trapezoidal-panels","text":"The trapezoidal panels are approximated as shown in the figure below. The area A_i is the average height (y_i + y_{i+1} )/2 times \\Delta x . Adding the areas gives the area approximation as tabulated. For the example with the curvature shown, the approximation will be on the low side. For the reverse curvature, the approximation will be on the high side. The trapezoidal approximation is commonly used with tabulated values. The script below illustrates the trapezoidal method for approximating an integral. In the example, the left and right panel endpoints in x are set as separate variables x_{left} and x_{right} and incremented by \\Delta x as we step through the count-controlled repetition to accumulate the area. The corresponding y values are computed within the loop and averaged, then multiplied by \\Delta x and added to the accumulator. Finally the x values are incremented --- for grins, we used the += operator on the accumulator # TrapezoidalPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_right = x_left + delta_x # initial value for x_right edge panel for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left* sqrt(1+x_left**2) ) y_right = ( x_right* sqrt(1+x_right**2) ) accumulated_area += + (1./2.) * ( y_left + y_right ) * delta_x x_left += delta_x x_right += delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) Program finds area under curve y = x * sqrt(1+x) Enter a lower bound x_low 0 Enter an upper bound x_high 8 Enter how many panels 9 Area under curve y = x * sqrt(1+x) from x = 0.0 to x = 8.0 is approximately: 175.33954986737925","title":"Trapezoidal Panels"},{"location":"1-Lessons/Lesson11/lesson11/#parabolic-panels","text":"Parabolic panels approximate the shape of the panel with a parabola. The area between the chord and the curve (neglected in the trapezoidal solution) may be accounted for by approximating the function with a parabola passing through the points defined by three successive values of y . This area may be calculated from the geometry of the parabola and added to the trapezoidal area of the pair of strips to give the area \\Delta A of the pair as illustrated. Adding all of the \\Delta A s produces the tabulation shown, which is known as Simpson's rule. To use Simpson's rule, the number n of strips must be even. The same example as presented for rectangular panels is repeated, except using parabolic panels. The code is changed yet again because we will evaluate at each end of the panel as well as at an intermediate value. # ParabolicPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_middle = x_left + delta_x # initial value for x_middle edge panel x_right = x_middle + delta_x # initial value for x_right edge panel how_many = int(how_many/2) # using 2 panels every step, so 1/2 many steps -- force integer result for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left * sqrt(1+ x_left**2) ) y_middle = ( x_middle * sqrt(1+ x_middle**2) ) y_right = ( x_right * sqrt(1+ x_right**2) ) accumulated_area = accumulated_area + \\ (1./3.) * ( y_left + 4.* y_middle + y_right ) * delta_x x_left = x_left + 2*delta_x x_middle = x_left + delta_x x_right = x_middle + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) Program finds area under curve y = x * sqrt(1+x) Enter a lower bound x_low 0 Enter an upper bound x_high 1 Enter how many panels 4 Area under curve y = x * sqrt(1+x) from x = 0.0 to x = 1.0 is approximately: 0.6094186631272838 If we study all the forms of the numerical method we observe that the numerical integration method is really the sum of function values at specific locations in the interval of interest, with each value multiplied by a specific weight. In this development the weights were based on polynomials, but other method use different weighting functions. An extremely important method is called gaussian quadrature. This method is valuable because one can approximate convolution integrals quite effectively using quadrature routines, while the number of function evaluations for a polynomial based approximation could be hopeless. When the function values are tabular, we are going to have to accept the rectangular (with adaptations) and trapezoidal as our best tools to approximate an integral because we don't have any really effective way to evaluate the function between the tabulated values.","title":"Parabolic Panels"},{"location":"1-Lessons/Lesson11/lesson11/#integration-of-tabular-data","text":"This section is going to work with tabular data -- different from function evaluation, but similar. To be really useful, we need to learn how to read data from a file; manually entering tabular data is really time consuming, error prone, and just plain idiotic. So in this chapter we will learn how to read data from a file into a list, then we can process the list as if it were a function and integrate its contents.","title":"Integration of Tabular Data"},{"location":"1-Lessons/Lesson11/lesson11/#reading-from-a-file-open-read-close-files","text":"First, lets consider a file named MyFile.txt . The extension is important so that the Shell does not think it is a Python script. The contents of MyFile.txt are: 1 1 2 4 3 9 4 16 5 25 The code fragment below, will let us look at the file (already existing in our local directory) import subprocess # lets us run \"shell\" commands and recover stdio stream usefull_cat_call = subprocess.run([\"cat\",\"MyFile.txt\"], stdout=subprocess.PIPE, text=True) # this is the call to run the bash command \"cat MyFile.txt\" which will display the contents of the file if it exists. print(usefull_cat_call.stdout) 1 1 2 4 3 9 4 16 5 25 Now that we know that the file exists,to read the contents into a Python script we have to do the following: Open a connection to the file --- this is a concept common to all languages, it might be called something different, but the program needs to somehow know the location and name of the file. Read the contents into an object --- we have a lot of control on how this gets done, for the time being we won't exercise much control yet. When you do substantial programs, you will depend on the control of the reads (and writes). Disconnect the file --- this too is common to all languages. Its a really easy step to forget. Not a big deal if the program ends as planned but terrible if there is a error in the program and the connection is still open. Usually nothing bad happens, but with an open connection it is possible for the file to get damaged. If that file represents millions of customers credit card numbers, that's kind of a problem, and time to go work on your resume, or get your passport collection out and choose a country without extradition. The code fragment below performs these three tasks and prints the things we read Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read the five lines line1 = Afile.readline() line2 = Afile.readline() line3 = Afile.readline() line4 = Afile.readline() line5 = Afile.readline() Afile.close() # disconnect from the file # echo the input print(line1,end=\"\") print(line2,end=\"\") print(line3,end=\"\") print(line4,end=\"\") print(line5,end=\"\") 1 1 2 4 3 9 4 16 5 25","title":"Reading from a file --- open, read, close files"},{"location":"1-Lessons/Lesson11/lesson11/#read-into-a-list","text":"A far more useful and elegant way to read from a file is to use a for loop. The attribute line within a file is an iterable, hence construction the loop is pretty straightforward. A script fragment below does the same thing as the example above, but uses a for loop to accomplish stepping through the file. Additionally, I have added a counter to keep track of how many lines were read --- in a lot of engineering programs, the number of things read becomes important later in a program, hence it is usually a good idea to capture the count when the data are first read. First lets work out if we can automatically detect the end of the file. So this script just reads and prints the attribute line from object Afile . Notice how the print statement is changed, to suppress the extra line feed. Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) Now we will add a list to receive the input, here it reads the file above as a string into a list xy , then splits that list and places the contents into two other lists, x and y . The script has several parts to discuss. First, the destination variables (lists) must be created -- I used the null list concept here because I don't know how big the list is until I read the list. Next I used the .append() method which operates on the xy list. The arguments of the method [str(n) for n in line.strip().split()] tells the program that the elements are to be interpreted as a string, and to split (split) the line into sub-strings based on a null delimiter (whitespace), and to remove all the whitespace (strip) characters. Once the line is split, the strings are appended into the xy list. The xy list is printed to show that it is a list of 5 things, each thing being a string comprised of two sets of characters separated by a comma. xy is a list of strings. The next section of the code then uses the pair function within another .append() method to break the character sets in each element of xy into two parts x and y . Lastly during the pair operation, the code also converts the data into real values (float) and then prints the data in two columns. This seems like a lot of work, but we could easily get this code to be super reliable, then save it as a function and never have to write it again. That too comes later -- suffice to say for now we can read a file, parse its contents into two lists x and y . Thus we are now able to integrate tabular data. xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) The list is: [['1', '1'], ['2', '4'], ['3', '9'], ['4', '16'], ['5', '25']] x = 1.0 y = 1.0 x = 2.0 y = 4.0 x = 3.0 y = 9.0 x = 4.0 y = 16.0 x = 5.0 y = 25.0","title":"Read into a list"},{"location":"1-Lessons/Lesson11/lesson11/#integrating-the-tabular-data","text":"Suppose instead of a function we only have tabulations and wist to estimate the area under the curve represented by the tabular values. Then our integration rules from the prior chapter still work more or less, except the rectangular panels will have to be shifted to either the left edge or right edge of a panel (where the tabulation exists). Lets just examine an example. Suppose some measurement technology produced a table of related values. The excitation variable is x and f(x) is the response. x f(x) 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 To integrate this table using the trapezoidal method is straightforward. We will modify our earlier code to read the table (which we put into a file), and compute the integral. # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 File has 9 records (lines) The list is: [['1.0', '1.543'], ['1.1', '1.668'], ['1.2', '1.811'], ['1.3', '1.971'], ['1.4', '2.151'], ['1.5', '2.352'], ['1.6', '2.577'], ['1.7', '2.828'], ['1.8', '3.107']] x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Cool, it seems to work -- now tidy the code a bit by suppressing extra outputs # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: ##print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nRecords read =: \",how_many_lines) ##print(\"The list is: \",end=\"\") ##print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result Records read =: 9 x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Realistically the only other simple integration method for tabular data is the rectangular rule, either using the left edge of a panel or the right edge of a panel (and I suppose you could do both and average the result which would be the trapezoidal method).","title":"Integrating the Tabular Data"},{"location":"1-Lessons/Lesson11/lesson11/#exercises_1","text":"1) Approximate \\int_0^2 f(x) dx from the tabulation in the Table below: x f(x) 0.00 1.0000 0.12 0.8869 0.53 0.5886 0.87 0.4190 1.08 0.3396 1.43 0.2393 2.00 0.1353 # 2) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{9.0} cosh(x) dx from the tabulation above.","title":"Exercises"},{"location":"1-Lessons/Lesson11/lesson11/#references","text":"https://rstudio-pubs-static.s3.amazonaws.com/286315_f00cf07beb3945d2a0260d6eaecb5d36.html Lagrangian Interpolation (Video) https://www.youtube.com/watch?v=_zK_KhHW6og Lagrange Polynomials https://en.wikipedia.org/wiki/Lagrange_polynomial","title":"References"},{"location":"1-Lessons/Lesson11/old_src/lesson10/","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 25 February 2021 Lesson 10 Data Modeling: Randomness and Probability This lesson introduces probability as the chance of occurance of some event. Concepts of sampling and empirical distributions are introduced. Objectives To be able to find probabilities of enumerable (discrete) events. To be able to approximate probabilities of enumerable and/or continuous events. Explain the concepts of sample, population, and probabilities Computing probability: single events, both events, at least event. Computational Thinking Concepts The CT concepts include: Decomposition => Reduce complex observations into concept of an event Abstraction => Outcome == an event, and its likelihood Pattern Matching => Fliping coins, rolling die == map to an event space; apply gambling principles System Integration => Iteration, Simulation Randomness and Probabilities The textbook discusses randomness at: https://www.inferentialthinking.com/chapters/09/Randomness.html Section 9.5 of that link elaborates on probabilities \"Over the centuries, there has been considerable philosophical debate about what probabilities are. Some people think that probabilities are relative frequencies; others think they are long run relative frequencies; still others think that probabilities are a subjective measure of their own personal degree of uncertainty.\" As a practical matter, most probabilities are relative frequencies. If you are a Bayesian statistician, its just conditioned relative frequency. By convention, probabilities are numbers between 0 and 1, or, equivalently, 0% and 100%. Impossible events have probability 0. Events that are certain have probability 1. As a silly example, the probability that a Great White shark will swim up your sewer pipe and bite you on the bottom, is zero. Unless the sewer pipe is pretty big, the shark cannot physically get to you - hence impossible. Now if you are swimming in a freshwater river, lets say the Columbia river on the Oregon border, that probability of sharkbite increases a bit, perhaps 1 in 100 million, or 0.000001% chance of a Great White shark (a pelagic species adapted to salt water), swimming upriver in freshwater, past a couple of fish ladders, still hungry enough bite your bottom. It would be a rare bite indeed; but not physically impossible. At the other end of the scale, \"sure things\" have a probability close to 1. If you run and jump off Glacier point in Yosemite Valley, its almost guarenteed that you will have a 1000 foot plunge until you hit the apron of the cliff and make a big red smear - there could be a gust of wind pushing you away into the trees, but pretty unlikely. So without a squirrel suit and a parachute you are pretty much going to expire with probability 100% chance. Math is the main tool for finding probabilities exactly, though computers are useful for this purpose too. Simulation can provide excellent approximations. In this section, we will informally develop a few simple rules that govern the calculation of probabilities. In subsequent sections we will return to simulations to approximate probabilities of complex events. We will use the standard notation \ud835\udc43(event) to denote the probability that \"event\" happens, and we will use the words \"chance\" and \"probability\" interchangeably. Simple Exclusion If the chance that event happens is 40%, then the chance that it doesn't happen is 60%. This natural calculation can be described in general as follows: \ud835\udc43(an event doesn't happen) = 1\u2212\ud835\udc43(the event happens) The result is correct if the entireity of possibilities are enumerated, that is the entire population is described. Complete Enumeration If you are rolling an ordinary die, a natural assumption is that all six faces are equally likely. Then probabilities of how one roll comes out can be easily calculated as a ratio. For example, the chance that the die shows an even number is \\frac{number~of~even~faces}{number~of~all~faces} = \\frac{\\#{2,4,6}}{\\#{1,2,3,4,5,6}} = \\frac{3}{6} Similarly, \ud835\udc43(die~shows~a~multiple~of~3) = \\frac{\\#{3,6}}{\\#{1,2,3,4,5,6}} = \\frac{2}{6} In general, \ud835\udc43(an event happens) = \\frac{outcomes that make the event happen}{all outcomes} Provided all the outcomes are equally likely. As above, this presumes the entireity of possibilities are enumerated. In the case of a single die, there are six outcomes - these comprise the entire population of outcomes. If we roll two die there are 12 outcomes, three die 18 and so on. Not all random phenomena are as simple as one roll of a die. The two main rules of probability, developed below, allow mathematicians to find probabilities even in complex situations. Conditioning (Two events must happen) Suppose you have a box that contains three tickets: one red, one blue, and one green. Suppose you draw two tickets at random without replacement; that is, you shuffle the three tickets, draw one, shuffle the remaining two, and draw another from those two. What is the chance you get the green ticket first, followed by the red one? There are six possible pairs of colors: RB, BR, RG, GR, BG, GB (we've abbreviated the names of each color to just its first letter). All of these are equally likely by the sampling scheme, and only one of them (GR) makes the event happen. So \ud835\udc43(green~first,~then~red) = \\frac{GR}{RB, BR, RG, GR, BG, GB} = \\frac{1}{6} But there is another way of arriving at the answer, by thinking about the event in two stages. First, the green ticket has to be drawn. That has chance 1/3, which means that the green ticket is drawn first in about 1/3 of all repetitions of the experiment. But that doesn't complete the event. Among the 1/3 of repetitions when green is drawn first, the red ticket has to be drawn next. That happens in about 1/2 of those repetitions, and so: \ud835\udc43(green~first,~then~red) = \\frac{1}{2} of \\frac{1}{3} = \\frac{1}{6} This calculation is usually written \"in chronological order,\" as follows. \ud835\udc43(green~first,~then~red) = \\frac{1}{3} of \\frac{1}{2} = \\frac{1}{6} The factor of \\frac{1}{2} is called \" the conditional chance that the red ticket appears second, given that the green ticket appeared first.\" In general, we have the multiplication rule: \ud835\udc43(two~events~both~happen) = \ud835\udc43(one~event~happens)\\times \ud835\udc43(the~other~event~happens, given~that~the~first~one~happened) Thus, when there are two conditions \u2013 one event must happen, as well as another \u2013 the chance is a fraction of a fraction, which is smaller than either of the two component fractions. The more conditions that have to be satisfied, the less likely they are to all be satisfied. Partitioning (When sequence doesn't matter) - A kind of enumeration! Suppose instead we want the chance that one of the two tickets is green and the other red. This event doesn't specify the order in which the colors must appear. So they can appear in either order. A good way to tackle problems like this is to partition the event so that it can happen in exactly one of several different ways. The natural partition of \"one green and one red\" is: GR, RG. Each of GR and RG has chance 1/6 by the calculation above. So you can calculate the chance of \"one green and one red\" by adding them up. \ud835\udc43(one~green~and~one~red) = \ud835\udc43(GR)+\ud835\udc43(RG) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6} In general, we have the addition rule: \ud835\udc43(an~event~happens) = \ud835\udc43(first~way~it~can~happen)+\ud835\udc43(second~way~it~can~happen) provided the event happens in exactly one of the two ways. Thus, when an event can happen in one of two different ways, the chance that it happens is a sum of chances, and hence bigger than the chance of either of the individual ways. The multiplication rule has a natural extension to more than two events, as we will see below. So also the addition rule has a natural extension to events that can happen in one of several different ways. Learn more at: https://ocw.mit.edu/courses/mathematics/18-440-probability-and-random-variables-spring-2014/lecture-notes/MIT18_440S14_Lecture3.pdf At Least One Success (A kind of exclusion/partition) Data scientists work with random samples from populations. A question that sometimes arises is about the likelihood that a particular individual in the population is selected to be in the sample. To work out the chance, that individual is called a \"success,\" and the problem is to find the chance that the sample contains a success. To see how such chances might be calculated, we start with a simpler setting: tossing a coin two times. If you toss a coin twice, there are four equally likely outcomes: HH, HT, TH, and TT. We have abbreviated \"Heads\" to H and \"Tails\" to T. The chance of getting at least one head in two tosses is therefore 3/4. Another way of coming up with this answer is to work out what happens if you don't get at least one head: both the tosses have to land tails. So \ud835\udc43(at~least~one~head~in~two~tosses) = 1\u2212\ud835\udc43(both~tails) = 1\u2212\\frac{1}{4} = \\frac{3}{4} Notice also that \ud835\udc43(both~tails) = \\frac{1}{4} = \\frac{1}{2} \\times \\frac{1}{2} = (\\frac{1}{2})^2 by the multiplication rule. These two observations allow us to find the chance of at least one head in any given number of tosses. For example, \ud835\udc43(at~least~one~head~in~17~tosses) = 1\u2212\ud835\udc43(all~17~are~tails) = 1\u2212(\\frac{1}{2})^{17} And now we are in a position to find the chance that the face with six spots comes up at least once in rolls of a die. For example, \ud835\udc43(a~single~roll~is~not~6) = \ud835\udc43(1)+\ud835\udc43(2)+\ud835\udc43(3)+\ud835\udc43(4)+\ud835\udc43(5) = \\frac{5}{6} Therefore, \ud835\udc43(at~least~one~6~in~two~rolls) = 1\u2212\ud835\udc43(both~rolls~are~not~6) = 1\u2212(\\frac{5}{6})^2 and \ud835\udc43(at~least~one~6~in~17~rolls) = 1\u2212(\\frac{5}{6})^{17} The table below shows these probabilities as the number of rolls increases from 1 to 50. import pandas as pd HowManyRollsToTake = 50 numRolls = [] probabilities = [] for i in range(HowManyRollsToTake+1): numRolls.append(i) probabilities.append(1-(5/6)**i) rolls = { \"NumRolls\": numRolls, \"Prob at least one 6\": probabilities } df = pd.DataFrame(rolls) df.plot.scatter(x=\"NumRolls\", y=\"Prob at least one 6\") <AxesSubplot:xlabel='NumRolls', ylabel='Prob at least one 6'> df.describe() Why Should anyone buy Flood Protection? Lets apply these ideas to insurance. Suppose you have a house that is located in the 100-year ARI (Annual Recurrance Interval) regulatory flood plain; and you are in a community with a good engineer, who got the probability correct, that is the chance in any year of a total loss is 1 in 100 or 0.01. Thus the chance of no loss in any year is 99 in 100 or 0.99 (pretty good odds)! So what is the chance during a 30-year loan, of no loss? We can just apply the multiplication rule on the no loss probability P(No~Loss) = 0.99^{30} But lets simulate - literally adapting the prior script. import pandas as pd HowManyYears = 600 numYears = [] nolossprobabilities = [] lossprobabilities = [] for i in range(HowManyYears+1): numYears.append(i) # How many years in the sequence nolossprobabilities.append((1-(1/100))**i) #Probability of No Loss after i-years lossprobabilities.append(1 - (1-(1/100))**i) #Probability of Loss after i-years years = { \"Years from Start of Loan\": numYears, \"Probability of No Loss\": nolossprobabilities, \"Probability of Loss\": lossprobabilities } df = pd.DataFrame(years) df.plot.line(x=\"Years from Start of Loan\", y=\"Probability of Loss\") # df.plot.line(x=\"Years from Start of Loan\", y=\"Probability of No Loss\") <AxesSubplot:xlabel='Years from Start of Loan'> df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Years from Start of Loan Probability of No Loss Probability of Loss 0 0 1.000000 0.000000 1 1 0.990000 0.010000 2 2 0.980100 0.019900 3 3 0.970299 0.029701 4 4 0.960596 0.039404 df[\"Probability of Loss\"].loc[30] 0.2602996266117198","title":"Lesson10"},{"location":"1-Lessons/Lesson11/old_src/lesson10/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 25 February 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson11/old_src/lesson10/#lesson-10-data-modeling-randomness-and-probability","text":"This lesson introduces probability as the chance of occurance of some event. Concepts of sampling and empirical distributions are introduced.","title":"Lesson 10 Data Modeling: Randomness and Probability"},{"location":"1-Lessons/Lesson11/old_src/lesson10/#objectives","text":"To be able to find probabilities of enumerable (discrete) events. To be able to approximate probabilities of enumerable and/or continuous events. Explain the concepts of sample, population, and probabilities Computing probability: single events, both events, at least event.","title":"Objectives"},{"location":"1-Lessons/Lesson11/old_src/lesson10/#computational-thinking-concepts","text":"The CT concepts include: Decomposition => Reduce complex observations into concept of an event Abstraction => Outcome == an event, and its likelihood Pattern Matching => Fliping coins, rolling die == map to an event space; apply gambling principles System Integration => Iteration, Simulation","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson11/old_src/lesson10/#randomness-and-probabilities","text":"The textbook discusses randomness at: https://www.inferentialthinking.com/chapters/09/Randomness.html Section 9.5 of that link elaborates on probabilities \"Over the centuries, there has been considerable philosophical debate about what probabilities are. Some people think that probabilities are relative frequencies; others think they are long run relative frequencies; still others think that probabilities are a subjective measure of their own personal degree of uncertainty.\" As a practical matter, most probabilities are relative frequencies. If you are a Bayesian statistician, its just conditioned relative frequency. By convention, probabilities are numbers between 0 and 1, or, equivalently, 0% and 100%. Impossible events have probability 0. Events that are certain have probability 1. As a silly example, the probability that a Great White shark will swim up your sewer pipe and bite you on the bottom, is zero. Unless the sewer pipe is pretty big, the shark cannot physically get to you - hence impossible. Now if you are swimming in a freshwater river, lets say the Columbia river on the Oregon border, that probability of sharkbite increases a bit, perhaps 1 in 100 million, or 0.000001% chance of a Great White shark (a pelagic species adapted to salt water), swimming upriver in freshwater, past a couple of fish ladders, still hungry enough bite your bottom. It would be a rare bite indeed; but not physically impossible. At the other end of the scale, \"sure things\" have a probability close to 1. If you run and jump off Glacier point in Yosemite Valley, its almost guarenteed that you will have a 1000 foot plunge until you hit the apron of the cliff and make a big red smear - there could be a gust of wind pushing you away into the trees, but pretty unlikely. So without a squirrel suit and a parachute you are pretty much going to expire with probability 100% chance. Math is the main tool for finding probabilities exactly, though computers are useful for this purpose too. Simulation can provide excellent approximations. In this section, we will informally develop a few simple rules that govern the calculation of probabilities. In subsequent sections we will return to simulations to approximate probabilities of complex events. We will use the standard notation \ud835\udc43(event) to denote the probability that \"event\" happens, and we will use the words \"chance\" and \"probability\" interchangeably.","title":"Randomness and Probabilities"},{"location":"1-Lessons/Lesson11/old_src/lesson10/#simple-exclusion","text":"If the chance that event happens is 40%, then the chance that it doesn't happen is 60%. This natural calculation can be described in general as follows: \ud835\udc43(an event doesn't happen) = 1\u2212\ud835\udc43(the event happens) The result is correct if the entireity of possibilities are enumerated, that is the entire population is described.","title":"Simple Exclusion"},{"location":"1-Lessons/Lesson11/old_src/lesson10/#complete-enumeration","text":"If you are rolling an ordinary die, a natural assumption is that all six faces are equally likely. Then probabilities of how one roll comes out can be easily calculated as a ratio. For example, the chance that the die shows an even number is \\frac{number~of~even~faces}{number~of~all~faces} = \\frac{\\#{2,4,6}}{\\#{1,2,3,4,5,6}} = \\frac{3}{6} Similarly, \ud835\udc43(die~shows~a~multiple~of~3) = \\frac{\\#{3,6}}{\\#{1,2,3,4,5,6}} = \\frac{2}{6} In general, \ud835\udc43(an event happens) = \\frac{outcomes that make the event happen}{all outcomes} Provided all the outcomes are equally likely. As above, this presumes the entireity of possibilities are enumerated. In the case of a single die, there are six outcomes - these comprise the entire population of outcomes. If we roll two die there are 12 outcomes, three die 18 and so on. Not all random phenomena are as simple as one roll of a die. The two main rules of probability, developed below, allow mathematicians to find probabilities even in complex situations.","title":"Complete Enumeration"},{"location":"1-Lessons/Lesson11/old_src/lesson10/#conditioning-two-events-must-happen","text":"Suppose you have a box that contains three tickets: one red, one blue, and one green. Suppose you draw two tickets at random without replacement; that is, you shuffle the three tickets, draw one, shuffle the remaining two, and draw another from those two. What is the chance you get the green ticket first, followed by the red one? There are six possible pairs of colors: RB, BR, RG, GR, BG, GB (we've abbreviated the names of each color to just its first letter). All of these are equally likely by the sampling scheme, and only one of them (GR) makes the event happen. So \ud835\udc43(green~first,~then~red) = \\frac{GR}{RB, BR, RG, GR, BG, GB} = \\frac{1}{6} But there is another way of arriving at the answer, by thinking about the event in two stages. First, the green ticket has to be drawn. That has chance 1/3, which means that the green ticket is drawn first in about 1/3 of all repetitions of the experiment. But that doesn't complete the event. Among the 1/3 of repetitions when green is drawn first, the red ticket has to be drawn next. That happens in about 1/2 of those repetitions, and so: \ud835\udc43(green~first,~then~red) = \\frac{1}{2} of \\frac{1}{3} = \\frac{1}{6} This calculation is usually written \"in chronological order,\" as follows. \ud835\udc43(green~first,~then~red) = \\frac{1}{3} of \\frac{1}{2} = \\frac{1}{6} The factor of \\frac{1}{2} is called \" the conditional chance that the red ticket appears second, given that the green ticket appeared first.\" In general, we have the multiplication rule: \ud835\udc43(two~events~both~happen) = \ud835\udc43(one~event~happens)\\times \ud835\udc43(the~other~event~happens, given~that~the~first~one~happened) Thus, when there are two conditions \u2013 one event must happen, as well as another \u2013 the chance is a fraction of a fraction, which is smaller than either of the two component fractions. The more conditions that have to be satisfied, the less likely they are to all be satisfied.","title":"Conditioning (Two events must happen)"},{"location":"1-Lessons/Lesson11/old_src/lesson10/#partitioning-when-sequence-doesnt-matter-a-kind-of-enumeration","text":"Suppose instead we want the chance that one of the two tickets is green and the other red. This event doesn't specify the order in which the colors must appear. So they can appear in either order. A good way to tackle problems like this is to partition the event so that it can happen in exactly one of several different ways. The natural partition of \"one green and one red\" is: GR, RG. Each of GR and RG has chance 1/6 by the calculation above. So you can calculate the chance of \"one green and one red\" by adding them up. \ud835\udc43(one~green~and~one~red) = \ud835\udc43(GR)+\ud835\udc43(RG) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6} In general, we have the addition rule: \ud835\udc43(an~event~happens) = \ud835\udc43(first~way~it~can~happen)+\ud835\udc43(second~way~it~can~happen) provided the event happens in exactly one of the two ways. Thus, when an event can happen in one of two different ways, the chance that it happens is a sum of chances, and hence bigger than the chance of either of the individual ways. The multiplication rule has a natural extension to more than two events, as we will see below. So also the addition rule has a natural extension to events that can happen in one of several different ways. Learn more at: https://ocw.mit.edu/courses/mathematics/18-440-probability-and-random-variables-spring-2014/lecture-notes/MIT18_440S14_Lecture3.pdf","title":"Partitioning (When sequence doesn't matter) - A kind of enumeration!"},{"location":"1-Lessons/Lesson11/old_src/lesson10/#at-least-one-success-a-kind-of-exclusionpartition","text":"Data scientists work with random samples from populations. A question that sometimes arises is about the likelihood that a particular individual in the population is selected to be in the sample. To work out the chance, that individual is called a \"success,\" and the problem is to find the chance that the sample contains a success. To see how such chances might be calculated, we start with a simpler setting: tossing a coin two times. If you toss a coin twice, there are four equally likely outcomes: HH, HT, TH, and TT. We have abbreviated \"Heads\" to H and \"Tails\" to T. The chance of getting at least one head in two tosses is therefore 3/4. Another way of coming up with this answer is to work out what happens if you don't get at least one head: both the tosses have to land tails. So \ud835\udc43(at~least~one~head~in~two~tosses) = 1\u2212\ud835\udc43(both~tails) = 1\u2212\\frac{1}{4} = \\frac{3}{4} Notice also that \ud835\udc43(both~tails) = \\frac{1}{4} = \\frac{1}{2} \\times \\frac{1}{2} = (\\frac{1}{2})^2 by the multiplication rule. These two observations allow us to find the chance of at least one head in any given number of tosses. For example, \ud835\udc43(at~least~one~head~in~17~tosses) = 1\u2212\ud835\udc43(all~17~are~tails) = 1\u2212(\\frac{1}{2})^{17} And now we are in a position to find the chance that the face with six spots comes up at least once in rolls of a die. For example, \ud835\udc43(a~single~roll~is~not~6) = \ud835\udc43(1)+\ud835\udc43(2)+\ud835\udc43(3)+\ud835\udc43(4)+\ud835\udc43(5) = \\frac{5}{6} Therefore, \ud835\udc43(at~least~one~6~in~two~rolls) = 1\u2212\ud835\udc43(both~rolls~are~not~6) = 1\u2212(\\frac{5}{6})^2 and \ud835\udc43(at~least~one~6~in~17~rolls) = 1\u2212(\\frac{5}{6})^{17} The table below shows these probabilities as the number of rolls increases from 1 to 50. import pandas as pd HowManyRollsToTake = 50 numRolls = [] probabilities = [] for i in range(HowManyRollsToTake+1): numRolls.append(i) probabilities.append(1-(5/6)**i) rolls = { \"NumRolls\": numRolls, \"Prob at least one 6\": probabilities } df = pd.DataFrame(rolls) df.plot.scatter(x=\"NumRolls\", y=\"Prob at least one 6\") <AxesSubplot:xlabel='NumRolls', ylabel='Prob at least one 6'> df.describe()","title":"At Least One Success (A kind of exclusion/partition)"},{"location":"1-Lessons/Lesson11/old_src/lesson10/#why-should-anyone-buy-flood-protection","text":"Lets apply these ideas to insurance. Suppose you have a house that is located in the 100-year ARI (Annual Recurrance Interval) regulatory flood plain; and you are in a community with a good engineer, who got the probability correct, that is the chance in any year of a total loss is 1 in 100 or 0.01. Thus the chance of no loss in any year is 99 in 100 or 0.99 (pretty good odds)! So what is the chance during a 30-year loan, of no loss? We can just apply the multiplication rule on the no loss probability P(No~Loss) = 0.99^{30} But lets simulate - literally adapting the prior script. import pandas as pd HowManyYears = 600 numYears = [] nolossprobabilities = [] lossprobabilities = [] for i in range(HowManyYears+1): numYears.append(i) # How many years in the sequence nolossprobabilities.append((1-(1/100))**i) #Probability of No Loss after i-years lossprobabilities.append(1 - (1-(1/100))**i) #Probability of Loss after i-years years = { \"Years from Start of Loan\": numYears, \"Probability of No Loss\": nolossprobabilities, \"Probability of Loss\": lossprobabilities } df = pd.DataFrame(years) df.plot.line(x=\"Years from Start of Loan\", y=\"Probability of Loss\") # df.plot.line(x=\"Years from Start of Loan\", y=\"Probability of No Loss\") <AxesSubplot:xlabel='Years from Start of Loan'> df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Years from Start of Loan Probability of No Loss Probability of Loss 0 0 1.000000 0.000000 1 1 0.990000 0.010000 2 2 0.980100 0.019900 3 3 0.970299 0.029701 4 4 0.960596 0.039404 df[\"Probability of Loss\"].loc[30] 0.2602996266117198","title":"Why Should anyone buy Flood Protection?"},{"location":"1-Lessons/Lesson12/lesson12/","text":"Copyright \u00a9 2021 Author, all rights reserved ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: Lesson 12 Linear Systems of Equations This lesson will Objectives Construct multivariate linear equation systems Solve using a Gaussian Reduction method Demonstrate practical application with a Matrix Arithmetic Use Dannemiller's matrix notes or Cleveland's old notes. Typeset to use typical Matrix-Vector forms. Scalar Multiply a Vector Vector Inner Product (Dot Product) Matrix Vector Product Matrix Matrix Product Matrix Inversion In many practical computational and theoretical operations we employ the concept of the inverse of a matrix. The inverse is somewhat analogous to \"dividing\" by the matrix. Consider our linear system: <script type=\"math/tex; mode=display\">\\begin{gather} \\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{b} \\end{gather} If we wished to solve for \\mathbf{x} we would \"divide\" both sides of the equation by \\mathbf{A} . Instead of division (which is undefined for matrices) we instead multiply by the inverse of the matrix. The inverse of a matrix \\mathbf{A} is denoted by \\mathbf{A}^{-1} and is by definition a matrix such that when \\mathbf{A}^{-1} and \\mathbf{A} are multiplied together, the identity matrix \\mathbf{I} results. e.g. \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I} Lets consider the matrixes below \\begin{gather} \\mathbf{A}= \\begin{pmatrix} 2 & 3 \\\\ 4 & -3 \\\\ \\end{pmatrix} \\end{gather} \\begin{gather} \\mathbf{A}^{-1}= \\begin{pmatrix} \\frac{1}{6} & \\frac{1}{6} \\\\ ~\\\\ \\frac{2}{9} & -\\frac{1}{9} \\\\ \\end{pmatrix} \\end{gather} We can check that the matrices are indeed inverses of each other using our Python code. # multiplymatrix.py -- Code to read and manipulate matrix amatrix = [] # null list to store matrix reads rowNumA = 0 colNumA = 0 ###################################### # connect and read file for MATRIX A # ###################################### amatrix = [] # null list for reading file afile = open(\"Amat.txt\",\"r\") for line in afile: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 afile.close() # Disconnect the file ###################################### # end file read ,disconnect file # ###################################### colNumA = len(amatrix[0]) # print all columns each row for i in range(0,rowNumA,1): print (amatrix[i][0:colNumA]) print (\"-----------------------------\") bmatrix = [] # null list to store matrix reads rowNumB = 0 colNumB = 0 ###################################### # connect and read file for MATRIX B # ###################################### bmatrix = [] # null list for reading file afile = open(\"Bmat.txt\",\"r\") for line in afile: bmatrix.append([float(n) for n in line.strip().split()]) rowNumB += 1 afile.close() # Disconnect the file ###################################### # end file read ,disconnect file # ###################################### colNumB = len(bmatrix[0]) # print all columns each row for i in range(0,rowNumB,1): print (bmatrix[i][0:colNumB]) print (\"------------------------------\") ########################################################## ### multiply the matrices, store result in result_matrix # ########################################################## result_matrix = [[0 for j in range(colNumB)] for i in range(rowNumA)] for i in range(0,rowNumA): for j in range(0,colNumB): for k in range(0,colNumA): result_matrix[i][j]=result_matrix[i][j]+amatrix[i][k]*bmatrix[k][j] # observe the triple for-loop structure and the counting scheme # print all cooumns each row for i in range(0,rowNumA,1): print (result_matrix[i][0:colNumB]) [2.0, 3.0] [4.0, -3.0] ----------------------------- [0.1666667, 0.1666667] [0.2222222, -0.1111111] ------------------------------ [1.0, 1.0000000000287557e-07] [2.0000000000575113e-07, 1.0000001] The script above is our multiplication script modified to read the \\mathbf{A} and \\mathbf{A}^{-1} perform the multiplication and then report the result. Now that we have some background on what an inverse is, it would be nice to know how to find them --- that is a remarkably challenging problem. Here we examine a classical algorithm for finding an inverse if we really need to --- computationally we only invert if necessary, there are other ways to \"divide\" that are faster. Gauss-Jordan method of finding \\mathbf{A}^{-1} There are a number of methods that can be used to find the inverse of a matrix using elementary row operations. An elementary row operation is any one of the three operations listed below: 1. Multiply or divide an entire row by a constant 2. Add or subtract a multiple of one row to/from another 3. Exchange the position of any 2 rows The Gauss-Jordan method of inverting a matrix can be divided into 4 main steps. In order to find the inverse we will be working with the original matrix, augmented with the identity matrix -- this new matrix is called the augmented matrix (because no-one has tried to think of a cooler name yet). \\begin{gather} \\mathbf{A} | \\mathbf{I} = \\begin{pmatrix} 2 & 3 & | & 1 & 0 \\\\ 4 & -3 & | & 0 & 1 \\\\ \\end{pmatrix} \\end{gather} We will perform elementary row operations based on the left matrix to convert it to an identity matrix -- we perform the same operations on the right matrix and the result when we are done is the inverse of the original matrix. So here goes -- in the theory here, we also get to do infinite-precision arithmetic, no rounding/truncation errors. 1) Divide row one by the a_{1,1} value to force a 1 in the a_{1,1} position. This is elementary row operation 1 in our list above. \\begin{gather} \\mathbf{A} | \\mathbf{I} = \\begin{pmatrix} 2/2 & 3/2 & | & 1/2 & 0 \\\\ 4 & -3 & | & 0 & 1 \\\\ \\end{pmatrix} = \\begin{pmatrix} 1 & 3/2 & | & 1/2 & 0 \\\\ 4 & -3 & | & 0 & 1 \\\\ \\end{pmatrix} \\end{gather} 2) For all rows below the first row, replace row_j with row_j - a_{j,1}*row_1 . This happens to be elementary row operation 2 in our list above. \\begin{gather} \\mathbf{A} | \\mathbf{I} = \\begin{pmatrix} 1 & 3/2 & | & 1/2 & 0 \\\\ 4 - 4(1) & -3 - 4(3/2) & | & 0-4(1/2) & 1-4(0) \\\\ \\end{pmatrix} = \\begin{pmatrix} 1 & 3/2 & | & 1/2 & 0 \\\\ 0 & -9 & | & -2 & 1 \\\\ \\end{pmatrix} \\end{gather} 3) Now multiply row_2 by \\frac{1}{ a_{2,2}} . This is again elementary row operation 1 in our list above. \\begin{gather} \\mathbf{A} | \\mathbf{I} = \\begin{pmatrix} 1 & 3/2 & | & 1/2 & 0 \\\\ 0 & -9/-9 & | & -2/-9 & 1/-9 \\\\ \\end{pmatrix} = \\begin{pmatrix} 1 & 3/2 & | & 1/2 & 0 \\\\ 0 & 1 & | & 2/9 & -1/9 \\\\ \\end{pmatrix} \\end{gather} 4) For all rows above and below this current row, replace row_j with row_j - a_{2,2}*row_2 . This happens to again be elementary row operation 2 in our list above. What we are doing is systematically converting the left matrix into an identity matrix by multiplication of constants and addition to eliminate off-diagonal values and force 1 on the diagonal. \\begin{gather} \\mathbf{A} | \\mathbf{I} = \\\\ \\begin{pmatrix} 1 & 3/2 - (3/2)(1) & | & 1/2 - (3/2)(2/9) & 0-(3/2)(-1/9) \\\\ 0 & 1 & | & 2/9 & -1/9 \\\\ \\end{pmatrix} = \\\\ \\begin{pmatrix} 1 & 0 & | & 1/6 & 1/6 \\\\ 0 & 1 & | & 2/9 & -1/9 \\\\ \\end{pmatrix} \\end{gather} 5) As far as this example is concerned we are done and have found the inverse. With more than a 2X2 system there will be many operations moving up and down the matrix to eliminate the off-diagonal terms. # InvertASystem.py # Code to read A and b # Then solve Ax = b for x by Gaussian elimination with back substitution # print (\"invert a matrix\") print (\"wrapper loop -- OK\") print (\"run wrapper through two iterations, same inputs\") print (\"added xmatrix,bmatrix -- get same result \") print (\"suppress vector only calcs\") print (\"clean up output\") amatrix = [] # null list to store matrix reads bvector = [] rowNumA = 0 colNumA = 0 rowNumB = 0 ###################################### # connect and read file for MATRIX A # ###################################### afile = open(\"A.txt\",\"r\") for line in afile: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 afile.close() # Disconnect the file ###################################### # end file read ,disconnect file # ###################################### colNumA = len(amatrix[0]) afile = open(\"B.txt\",\"r\") for line in afile: bvector.append(float(line)) # vector read different -- just float the line rowNumB += 1 afile.close() # Disconnect the file print (bvector) # check the arrays if rowNumA != rowNumB: print (\"row ranks not same -- aborting now\") quit() else: print (\"row ranks same -- continuing operation\") # print all columns each row cmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] dmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] bmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] xmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] xvector = [0 for i in range(rowNumA)] for i in range(0,rowNumA,1): print (amatrix[i][0:colNumA], cmatrix[i][0:colNumA]) bmatrix[i][i] = 1.0 print (\"-----------------------------\") # copy amatrix into dmatrix -- this is a static copy dmatrix = [[amatrix[i][j] for j in range(colNumA)]for i in range(rowNumA)] # now attempt invert # # outer wrapper loop # for jcol in range(rowNumA): # print \"run \",jcol xvector = [0 for i in range(rowNumA)] # xmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] for i in range(rowNumA): bvector[i]=bmatrix[i][jcol] amatrix = [[dmatrix[i][j] for j in range(colNumA)]for i in range(rowNumA)] cmatrix = [[dmatrix[i][j] for j in range(colNumA)]for i in range(rowNumA)] print (\"reset A matrix, x vector, b vector\") for i in range(0,rowNumA,1): print (amatrix[i][0:colNumA],xvector[i],bvector[i]) print (\"-----------------------------\") # build the diagonal -- assume diagonally dominant for k in range(rowNumA-1): l = k+1 for i in range(l,rowNumA): for j in range(colNumA): cmatrix[i][j]=amatrix[i][j]-amatrix[k][j]*amatrix[i][k]/amatrix[k][k] bvector[i] = bvector[i]-bvector[k]*amatrix[i][k]/amatrix[k][k] bmatrix[i][jcol] = bmatrix[i][jcol]-bmatrix[k][jcol]*amatrix[i][k]/amatrix[k][k] for i in range(rowNumA): for j in range(colNumA): amatrix[i][j] = cmatrix[i][j] # gaussian reduction done # now for the back substitution for i in range(0,rowNumA,1): print (amatrix[i][0:colNumA], bvector[i],cmatrix[i][0:colNumA]) print (\"-----------------------------\") for k in range(rowNumA-1,-1,-1): sum = 0.0 sum1 = 0.0 for i in range(rowNumA): if i == k: continue else: sum = sum + amatrix[k][i]*xvector[i] sum1 = sum1 + amatrix[k][i]*xmatrix[i][jcol] xvector[k]=(bvector[k]-sum)/amatrix[k][k] xmatrix[k][jcol]=(bmatrix[k][jcol]-sum1)/amatrix[k][k] for i in range(0,rowNumA,1): print (dmatrix[i][0:colNumA],xvector[i],bvector[i]) print (\"-----------------------------\") for i in range(0,rowNumA,1): print (dmatrix[i][0:colNumA],xmatrix[i][jcol],bmatrix[i][jcol]) print (\"-----------------------------\") # end of wrapper print (\"[ A-Matrix ]|[ A-Inverse ]\") print (\"_____________________________________________________\") for i in range(0,rowNumA,1): print (dmatrix[i][0:colNumA],\"|\", xmatrix[i][0:colNumA]) print (\"_____________________________________________________\") ofile = open(\"A-Matrix.txt\",\"w\") # \"w\" clobbers content already there! for i in range(0,rowNumA,1): message = ' '.join(map(repr, dmatrix[i][0:colNumA])) + \"\\n\" ofile.write(message) ofile.close() ofile = open(\"A-Inverse.txt\",\"w\") # \"w\" clobbers content already there! for i in range(0,rowNumA,1): message = ' '.join(map(repr, xmatrix[i][0:colNumA])) + \"\\n\" ofile.write(message) ofile.close() invert a matrix wrapper loop -- OK run wrapper through two iterations, same inputs added xmatrix,bmatrix -- get same result suppress vector only calcs clean up output [5.0, 6.0, 7.0, 8.0, 9.0] row ranks same -- continuing operation [4.0, 1.5, 0.7, 1.2, 0.5] [0, 0, 0, 0, 0] [1.0, 6.0, 0.9, 1.4, 0.7] [0, 0, 0, 0, 0] [0.5, 1.0, 3.9, 3.2, 0.9] [0, 0, 0, 0, 0] [0.2, 2.0, 0.2, 7.5, 1.9] [0, 0, 0, 0, 0] [1.7, 0.9, 1.2, 2.3, 4.9] [0, 0, 0, 0, 0] ----------------------------- reset A matrix, x vector, b vector [4.0, 1.5, 0.7, 1.2, 0.5] 0 1.0 [1.0, 6.0, 0.9, 1.4, 0.7] 0 0 [0.5, 1.0, 3.9, 3.2, 0.9] 0 0 [0.2, 2.0, 0.2, 7.5, 1.9] 0 0 [1.7, 0.9, 1.2, 2.3, 4.9] 0 0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 1.0 [4.0, 1.5, 0.7, 1.2, 0.5] [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] -0.25 [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] -0.08888888888888889 [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] 0.03356308061132753 [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] [0.0, 0.0, 0.0, 0.0, 4.231527930403315] -0.3975053957273071 [0.0, 0.0, 0.0, 0.0, 4.231527930403315] ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0.27196423630168165 1.0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.036786468827077756 -0.25 [0.5, 1.0, 3.9, 3.2, 0.9] -0.025949127789423248 -0.08888888888888889 [0.2, 2.0, 0.2, 7.5, 1.9] 0.027047195749338872 0.03356308061132753 [1.7, 0.9, 1.2, 2.3, 4.9] -0.0939389748254409 -0.3975053957273071 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0.27196423630168165 1.0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.036786468827077756 -0.25 [0.5, 1.0, 3.9, 3.2, 0.9] -0.025949127789423248 -0.08888888888888889 [0.2, 2.0, 0.2, 7.5, 1.9] 0.027047195749338872 0.03356308061132753 [1.7, 0.9, 1.2, 2.3, 4.9] -0.0939389748254409 -0.3975053957273071 ----------------------------- reset A matrix, x vector, b vector [4.0, 1.5, 0.7, 1.2, 0.5] 0 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0 1.0 [0.5, 1.0, 3.9, 3.2, 0.9] 0 0 [0.2, 2.0, 0.2, 7.5, 1.9] 0 0 [1.7, 0.9, 1.2, 2.3, 4.9] 0 0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0 [4.0, 1.5, 0.7, 1.2, 0.5] [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] 1.0 [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] -0.14444444444444443 [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] -0.3454599940065927 [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] [0.0, 0.0, 0.0, 0.0, 4.231527930403315] 0.03860910887892463 [0.0, 0.0, 0.0, 0.0, 4.231527930403315] ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.05581183146290884 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0.18691841183385363 1.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.0013334022990376664 -0.14444444444444443 [0.2, 2.0, 0.2, 7.5, 1.9] -0.05063248905238324 -0.3454599940065927 [1.7, 0.9, 1.2, 2.3, 4.9] 0.009124153146082323 0.03860910887892463 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.05581183146290884 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0.18691841183385363 1.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.0013334022990376664 -0.14444444444444443 [0.2, 2.0, 0.2, 7.5, 1.9] -0.05063248905238324 -0.3454599940065927 [1.7, 0.9, 1.2, 2.3, 4.9] 0.009124153146082323 0.03860910887892463 ----------------------------- reset A matrix, x vector, b vector [4.0, 1.5, 0.7, 1.2, 0.5] 0 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0 0 [0.5, 1.0, 3.9, 3.2, 0.9] 0 1.0 [0.2, 2.0, 0.2, 7.5, 1.9] 0 0 [1.7, 0.9, 1.2, 2.3, 4.9] 0 0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0 [4.0, 1.5, 0.7, 1.2, 0.5] [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] 0.0 [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] 1.0 [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] 0.022415343122565184 [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] [0.0, 0.0, 0.0, 0.0, 4.231527930403315] -0.23761967500359435 [0.0, 0.0, 0.0, 0.0, 4.231527930403315] ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.032853102922602934 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.032062455842026744 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] 0.26826513178341493 1.0 [0.2, 2.0, 0.2, 7.5, 1.9] 0.01649816113355711 0.022415343122565184 [1.7, 0.9, 1.2, 2.3, 4.9] -0.05615458031041434 -0.23761967500359435 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.032853102922602934 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.032062455842026744 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] 0.26826513178341493 1.0 [0.2, 2.0, 0.2, 7.5, 1.9] 0.01649816113355711 0.022415343122565184 [1.7, 0.9, 1.2, 2.3, 4.9] -0.05615458031041434 -0.23761967500359435 ----------------------------- reset A matrix, x vector, b vector [4.0, 1.5, 0.7, 1.2, 0.5] 0 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0 0 [0.5, 1.0, 3.9, 3.2, 0.9] 0 0 [0.2, 2.0, 0.2, 7.5, 1.9] 0 1.0 [1.7, 0.9, 1.2, 2.3, 4.9] 0 0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0 [4.0, 1.5, 0.7, 1.2, 0.5] [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] 0.0 [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] 0.0 [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] 1.0 [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] [0.0, 0.0, 0.0, 0.0, 4.231527930403315] -0.1488884423394965 [0.0, 0.0, 0.0, 0.0, 4.231527930403315] ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.016869919448735553 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.011456196435011407 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.10875073215127727 0.0 [0.2, 2.0, 0.2, 7.5, 1.9] 0.1486518640705042 1.0 [1.7, 0.9, 1.2, 2.3, 4.9] -0.03518550386250331 -0.1488884423394965 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.016869919448735553 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.011456196435011407 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.10875073215127727 0.0 [0.2, 2.0, 0.2, 7.5, 1.9] 0.1486518640705042 1.0 [1.7, 0.9, 1.2, 2.3, 4.9] -0.03518550386250331 -0.1488884423394965 ----------------------------- reset A matrix, x vector, b vector [4.0, 1.5, 0.7, 1.2, 0.5] 0 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0 0 [0.5, 1.0, 3.9, 3.2, 0.9] 0 0 [0.2, 2.0, 0.2, 7.5, 1.9] 0 0 [1.7, 0.9, 1.2, 2.3, 4.9] 0 1.0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0 [4.0, 1.5, 0.7, 1.2, 0.5] [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] 0.0 [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] 0.0 [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] 0.0 [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] [0.0, 0.0, 0.0, 0.0, 4.231527930403315] 1.0 [0.0, 0.0, 0.0, 0.0, 4.231527930403315] ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.0072026931722172435 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.012617687833839365 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.004266180002777282 0.0 [0.2, 2.0, 0.2, 7.5, 1.9] -0.05619749842697155 0.0 [1.7, 0.9, 1.2, 2.3, 4.9] 0.23632125710787594 1.0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.0072026931722172435 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.012617687833839365 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.004266180002777282 0.0 [0.2, 2.0, 0.2, 7.5, 1.9] -0.05619749842697155 0.0 [1.7, 0.9, 1.2, 2.3, 4.9] 0.23632125710787594 1.0 ----------------------------- [ A-Matrix ]|[ A-Inverse ] _____________________________________________________ [4.0, 1.5, 0.7, 1.2, 0.5] | [0.27196423630168165, -0.05581183146290884, -0.032853102922602934, -0.016869919448735553, -0.0072026931722172435] [1.0, 6.0, 0.9, 1.4, 0.7] | [-0.036786468827077756, 0.18691841183385363, -0.032062455842026744, -0.011456196435011407, -0.012617687833839365] [0.5, 1.0, 3.9, 3.2, 0.9] | [-0.025949127789423248, -0.0013334022990376664, 0.26826513178341493, -0.10875073215127727, -0.004266180002777282] [0.2, 2.0, 0.2, 7.5, 1.9] | [0.027047195749338872, -0.05063248905238324, 0.01649816113355711, 0.1486518640705042, -0.05619749842697155] [1.7, 0.9, 1.2, 2.3, 4.9] | [-0.0939389748254409, 0.009124153146082323, -0.05615458031041434, -0.03518550386250331, 0.23632125710787594] _____________________________________________________ print (amatrix) [[4.0, 1.5, 0.7, 1.2, 0.5], [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575], [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445], [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575], [0.0, 0.0, 0.0, 0.0, 4.231527930403315]] Supply Code with Pivoting Supply numpy code Do a couple examples Practical Application of Linear Solvers A typical static truss analysis problem goes like \"The figure below is a simply supported, statically determinate truss with pin connections (zero moment transfer connections). Find the forces in each member for the loading shown.\" This notebook will illustrate how to leverage our linear systems solver(s) to analyze the truss. The approach uses concepts from statics and computational thinking. From statics 1) method of joints (for reactions and internal forcez) 2) direction cosines From computational thinking 1) read input file 2) construct linear system \\textbf{Ax=b}; solve for \\textbf{x} 3) report results Before even contemplating writing/using a program we need to build a mathematical model of the truss and assemble the system of linear equations that result from the model. So the first step is to sketch a free-body-diagram as below and build a node naming convention and force names. Next we will write the force balance for each of the six nodes ( N1 - N6 ), which will produce a total of 12 equations in the 12 unknowns (the 9 member forces, and 3 reactions). The figure below is the force balance for node N1 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & +F_1cos(45) & + F_2 & & & & & & & + A_x & & & & & \\\\ \\sum F_y = 0 = & +F_1sin(45) & & & & & & & & & & + A_y & & & \\\\ \\end{matrix} \\end{gather} The equation above is the force balance equation pair for the node. The x component equation will later be named N1_x to indicate it arises from Node 1, x component equation. A similar notation convention will also be adopted for the y component equation. There will be an equation pair for each node. Below is a sketch of the force balance for node N2 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & & -F_2 & & & & +F_6 & & & & & & & & \\\\ \\sum F_y = 0 = & & & +F_3 & & & & & & & & & & & \\\\ \\end{matrix} \\end{gather} Below is a sketch of the force balance for node N3 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & & & & & -F_5cos(30) & -F_6 & & +F_8 & & & & & & \\\\ \\sum F_y = 0 = & & & & & F_5sin(30) & & +F_7 & & & & & & & -P_3\\\\ \\end{matrix} \\end{gather} Above is the force balance equation pair for node N3 . Below is a sketch of the force balance for node N4 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & & & & & & & & -F_8 & -F_9cos(45) & & & & & \\\\ \\sum F_y = 0 = & & & & & & & & & F_9sin(45) & & & +B_y & & \\\\ \\end{matrix} \\end{gather} Above is the force balance equation pair for node N4 . Below is a sketch of the force balance for node N5 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & -F_1cos(45) & & & +F_4 & +F_5cos(30) & & & & & & & & & \\\\ \\sum F_y = 0 = & -F_1sin(45) & & -F_3 & & -F_5sin(30) & & & & & & & & & -P_1\\\\ \\end{matrix} \\end{gather} Above is the force balance equation pair for node N5 . Below is a sketch of the force balance for node N6 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & & & & -F_4 & & & & & F_9sin(45) & & & & & \\\\ \\sum F_y = 0 = & & & & & & & -F_7 & & -F_9cos(45) & & & & & P_2\\\\ \\end{matrix} \\end{gather} Above is the force balance equation pair for node N6 . The next step is to gather the equation pairs into a system of linear equations. We will move the known loads to the right hand side and essentially construct the matrix equation \\mathbf{A}\\mathbf{x} = \\mathbf{b} . The system below is a matrix representation of the equation pairs with the forces moved to the right hand side \\mathbf{b} = RHS . \\begin{gather} \\begin{pmatrix} ~ & F_1 & F_2 & F_3 & F_4 & F_5 & F_6 & F_7 & F_8 & F_9 & A_x & A_y & B_y & | & RHS\\\\ \\hline N1_x & 0.707 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & | & 0\\\\ N1_y & 0.707 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & | & 0\\\\ N2_x & 0 & -1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & | & 0\\\\ N2_y & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & | & 0\\\\ N3_x & 0 & 0 & 0 & 0 & -0.866 & -1 & 0 & 1 & 0 & 0 & 0 & 0 & | & 0\\\\ N3_y & 0 & 0 & 0 & 0 & 0.5 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & | & P_3\\\\ N4_x & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 & -0.707 & 0 & 0 & 0 & | & 0\\\\ N4_y & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.707 & 0 & 0 & 0 & | & 0\\\\ N5_x & -0.707 & 0 & 0 & 1 & 0.866 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & | & 0\\\\ N5_y & -0.707 & 0 & -1 & 0 & -0.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & | & P_1\\\\ N6_x & 0 & 0 & 0 & -1 & 0 & 0 & 0 & 0 & 0.707 & 0 & 0 & 0 & | & 0\\\\ N6_y & 0 & 0 & 0 & 0 & 0 & 0 & -1 & 0 & -0.707 & 0 & 0 & 0 & | & -P_2\\\\ \\end{pmatrix} \\end{gather} In the system, the rows are labeled on the left-most column with their node-related equation name. Thus each row of the matrix corresponds to an equation derived from a node. The columns are labeled with their respective unknown force (except the last column, which represents the right-hand-side of the system of linear equations). Thus the coefficient in each column corresponds to a force in each node equation. The sign of the coefficient refers to the assumed direction the force acts. In the analysis all the members were assumed to be in tension (except for the reaction forces). If a coefficient has a value of zero in a particular row, then that force does no act at the node to which the row corresponds. From this representation the transition to the formal vector-matrix representation is straightforward. \\begin{gather} \\mathbf{A} = \\begin{pmatrix} 0.707 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\ 0.707 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\ 0 & -1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & -0.866 & -1 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0.5 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 & -0.707 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.707 & 0 & 0 & 0 \\\\ -0.707 & 0 & 0 & 1 & 0.866 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ -0.707 & 0 & -1 & 0 & -0.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & -1 & 0 & 0 & 0 & 0 & 0.707 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & -1 & 0 & -0.707 & 0 & 0 & 0 \\\\ \\end{pmatrix} \\end{gather} \\begin{gather} \\mathbf{x} = \\begin{pmatrix} F_1\\\\ F_2\\\\ F_3\\\\ F_4\\\\ F_5\\\\ F_6\\\\ F_7\\\\ F_8\\\\ F_9\\\\ A_x\\\\ A_y\\\\ B_y\\\\ \\end{pmatrix} \\end{gather} \\begin{gather} \\mathbf{b} = \\begin{pmatrix} 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ P_3\\\\ 0\\\\ 0\\\\ 0\\\\ P_1\\\\ 0\\\\ -P_2\\\\ \\end{pmatrix} \\end{gather} The various matrices above are entere into text files named A.txt and B.txt, we can examine the file contents using the host OS as below # list contents of the A matrix, uses call to OS host !(cat A.txt) 4.0 1.5 0.7 1.2 0.5 1.0 6.0 0.9 1.4 0.7 0.5 1.0 3.9 3.2 0.9 0.2 2.0 0.2 7.5 1.9 1.7 0.9 1.2 2.3 4.9 # list contents of RHS or b vector !(cat B.txt) 5.0 6.0 7.0 8.0 9.0 Now we use our solver tools, here I have not doen much on tidy output, thats left for the reader. # Linear System Solver from LinearSolverPivot import linearsolver amatrix = [] # null list to store matrix reads bvector = [] rowNumA = 0 colNumA = 0 rowNumB = 0 afile = open(\"A.txt\",\"r\") # connect and read file for MATRIX A for line in afile: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 afile.close() # Disconnect the file colNumA = len(amatrix[0]) afile = open(\"B.txt\",\"r\") # connect and read file for MATRIX B for line in afile: bvector.append(float(line)) # vector read different -- just float the line rowNumB += 1 afile.close() # Disconnect the file #print (bvector) if rowNumA != rowNumB: # check the arrays print (\"row ranks not same -- aborting now\") quit() else: print (\"row ranks same -- continuing operation\") # print all columns each row cmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] dmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] xvector = [0 for i in range(rowNumA)] dvector = [0 for i in range(rowNumA)] for i in range(0,rowNumA,1): print (amatrix[i][0:colNumA], bvector[i]) print (\"-----------------------------\") # copy amatrix into cmatrix cmatrix = [[amatrix[i][j] for j in range(colNumA)]for i in range(rowNumA)] dmatrix = [[amatrix[i][j] for j in range(colNumA)]for i in range(rowNumA)] dvector = [bvector[i] for i in range(rowNumA)] dvector = linearsolver(amatrix,bvector) for i in range(0,rowNumA,1): print (dvector[i]) print (\"-----------------------------\") row ranks same -- continuing operation [4.0, 1.5, 0.7, 1.2, 0.5] 5.0 [1.0, 6.0, 0.9, 1.4, 0.7] 6.0 [0.5, 1.0, 3.9, 3.2, 0.9] 7.0 [0.2, 2.0, 0.2, 7.5, 1.9] 8.0 [1.7, 0.9, 1.2, 2.3, 4.9] 9.0 ----------------------------- 0.5951948781328953 0.5079321739889 0.831708392507349 0.6303655990885844 1.0373752656472461 ----------------------------- References Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. import numpy as np amatrix = [] # null list to store matrix reads bvector = [] rowNumA = 0 colNumA = 0 rowNumB = 0 afile = open(\"A.txt\",\"r\") # connect and read file for MATRIX A for line in afile: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 afile.close() # Disconnect the file afile = open(\"B.txt\",\"r\") # connect and read file for MATRIX B for line in afile: bvector.append(float(line)) # vector read different -- just float the line rowNumB += 1 afile.close() # Disconnect the file A = np.array(amatrix) b = np.array(bvector) x = np.linalg.solve(A, b) print(x) [0.59519488 0.50793217 0.83170839 0.6303656 1.03737527]","title":"Linear Equation Systems"},{"location":"1-Lessons/Lesson12/lesson12/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date:","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson12/lesson12/#lesson-12-linear-systems-of-equations","text":"This lesson will","title":"Lesson 12 Linear Systems of Equations"},{"location":"1-Lessons/Lesson12/lesson12/#objectives","text":"Construct multivariate linear equation systems Solve using a Gaussian Reduction method Demonstrate practical application with a","title":"Objectives"},{"location":"1-Lessons/Lesson12/lesson12/#matrix-arithmetic","text":"Use Dannemiller's matrix notes or Cleveland's old notes. Typeset to use typical Matrix-Vector forms.","title":"Matrix Arithmetic"},{"location":"1-Lessons/Lesson12/lesson12/#scalar-multiply-a-vector","text":"","title":"Scalar Multiply a Vector"},{"location":"1-Lessons/Lesson12/lesson12/#vector-inner-product-dot-product","text":"","title":"Vector Inner Product (Dot Product)"},{"location":"1-Lessons/Lesson12/lesson12/#matrix-vector-product","text":"","title":"Matrix Vector Product"},{"location":"1-Lessons/Lesson12/lesson12/#matrix-matrix-product","text":"","title":"Matrix Matrix Product"},{"location":"1-Lessons/Lesson12/lesson12/#matrix-inversion","text":"In many practical computational and theoretical operations we employ the concept of the inverse of a matrix. The inverse is somewhat analogous to \"dividing\" by the matrix. Consider our linear system: <script type=\"math/tex; mode=display\">\\begin{gather} \\mathbf{A} \\cdot \\mathbf{x} = \\mathbf{b} \\end{gather} If we wished to solve for \\mathbf{x} we would \"divide\" both sides of the equation by \\mathbf{A} . Instead of division (which is undefined for matrices) we instead multiply by the inverse of the matrix. The inverse of a matrix \\mathbf{A} is denoted by \\mathbf{A}^{-1} and is by definition a matrix such that when \\mathbf{A}^{-1} and \\mathbf{A} are multiplied together, the identity matrix \\mathbf{I} results. e.g. \\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I} Lets consider the matrixes below \\begin{gather} \\mathbf{A}= \\begin{pmatrix} 2 & 3 \\\\ 4 & -3 \\\\ \\end{pmatrix} \\end{gather} \\begin{gather} \\mathbf{A}^{-1}= \\begin{pmatrix} \\frac{1}{6} & \\frac{1}{6} \\\\ ~\\\\ \\frac{2}{9} & -\\frac{1}{9} \\\\ \\end{pmatrix} \\end{gather} We can check that the matrices are indeed inverses of each other using our Python code. # multiplymatrix.py -- Code to read and manipulate matrix amatrix = [] # null list to store matrix reads rowNumA = 0 colNumA = 0 ###################################### # connect and read file for MATRIX A # ###################################### amatrix = [] # null list for reading file afile = open(\"Amat.txt\",\"r\") for line in afile: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 afile.close() # Disconnect the file ###################################### # end file read ,disconnect file # ###################################### colNumA = len(amatrix[0]) # print all columns each row for i in range(0,rowNumA,1): print (amatrix[i][0:colNumA]) print (\"-----------------------------\") bmatrix = [] # null list to store matrix reads rowNumB = 0 colNumB = 0 ###################################### # connect and read file for MATRIX B # ###################################### bmatrix = [] # null list for reading file afile = open(\"Bmat.txt\",\"r\") for line in afile: bmatrix.append([float(n) for n in line.strip().split()]) rowNumB += 1 afile.close() # Disconnect the file ###################################### # end file read ,disconnect file # ###################################### colNumB = len(bmatrix[0]) # print all columns each row for i in range(0,rowNumB,1): print (bmatrix[i][0:colNumB]) print (\"------------------------------\") ########################################################## ### multiply the matrices, store result in result_matrix # ########################################################## result_matrix = [[0 for j in range(colNumB)] for i in range(rowNumA)] for i in range(0,rowNumA): for j in range(0,colNumB): for k in range(0,colNumA): result_matrix[i][j]=result_matrix[i][j]+amatrix[i][k]*bmatrix[k][j] # observe the triple for-loop structure and the counting scheme # print all cooumns each row for i in range(0,rowNumA,1): print (result_matrix[i][0:colNumB]) [2.0, 3.0] [4.0, -3.0] ----------------------------- [0.1666667, 0.1666667] [0.2222222, -0.1111111] ------------------------------ [1.0, 1.0000000000287557e-07] [2.0000000000575113e-07, 1.0000001] The script above is our multiplication script modified to read the \\mathbf{A} and \\mathbf{A}^{-1} perform the multiplication and then report the result. Now that we have some background on what an inverse is, it would be nice to know how to find them --- that is a remarkably challenging problem. Here we examine a classical algorithm for finding an inverse if we really need to --- computationally we only invert if necessary, there are other ways to \"divide\" that are faster.","title":"Matrix Inversion"},{"location":"1-Lessons/Lesson12/lesson12/#gauss-jordan-method-of-finding-mathbfa-1","text":"There are a number of methods that can be used to find the inverse of a matrix using elementary row operations. An elementary row operation is any one of the three operations listed below: 1. Multiply or divide an entire row by a constant 2. Add or subtract a multiple of one row to/from another 3. Exchange the position of any 2 rows The Gauss-Jordan method of inverting a matrix can be divided into 4 main steps. In order to find the inverse we will be working with the original matrix, augmented with the identity matrix -- this new matrix is called the augmented matrix (because no-one has tried to think of a cooler name yet). \\begin{gather} \\mathbf{A} | \\mathbf{I} = \\begin{pmatrix} 2 & 3 & | & 1 & 0 \\\\ 4 & -3 & | & 0 & 1 \\\\ \\end{pmatrix} \\end{gather} We will perform elementary row operations based on the left matrix to convert it to an identity matrix -- we perform the same operations on the right matrix and the result when we are done is the inverse of the original matrix. So here goes -- in the theory here, we also get to do infinite-precision arithmetic, no rounding/truncation errors. 1) Divide row one by the a_{1,1} value to force a 1 in the a_{1,1} position. This is elementary row operation 1 in our list above. \\begin{gather} \\mathbf{A} | \\mathbf{I} = \\begin{pmatrix} 2/2 & 3/2 & | & 1/2 & 0 \\\\ 4 & -3 & | & 0 & 1 \\\\ \\end{pmatrix} = \\begin{pmatrix} 1 & 3/2 & | & 1/2 & 0 \\\\ 4 & -3 & | & 0 & 1 \\\\ \\end{pmatrix} \\end{gather} 2) For all rows below the first row, replace row_j with row_j - a_{j,1}*row_1 . This happens to be elementary row operation 2 in our list above. \\begin{gather} \\mathbf{A} | \\mathbf{I} = \\begin{pmatrix} 1 & 3/2 & | & 1/2 & 0 \\\\ 4 - 4(1) & -3 - 4(3/2) & | & 0-4(1/2) & 1-4(0) \\\\ \\end{pmatrix} = \\begin{pmatrix} 1 & 3/2 & | & 1/2 & 0 \\\\ 0 & -9 & | & -2 & 1 \\\\ \\end{pmatrix} \\end{gather} 3) Now multiply row_2 by \\frac{1}{ a_{2,2}} . This is again elementary row operation 1 in our list above. \\begin{gather} \\mathbf{A} | \\mathbf{I} = \\begin{pmatrix} 1 & 3/2 & | & 1/2 & 0 \\\\ 0 & -9/-9 & | & -2/-9 & 1/-9 \\\\ \\end{pmatrix} = \\begin{pmatrix} 1 & 3/2 & | & 1/2 & 0 \\\\ 0 & 1 & | & 2/9 & -1/9 \\\\ \\end{pmatrix} \\end{gather} 4) For all rows above and below this current row, replace row_j with row_j - a_{2,2}*row_2 . This happens to again be elementary row operation 2 in our list above. What we are doing is systematically converting the left matrix into an identity matrix by multiplication of constants and addition to eliminate off-diagonal values and force 1 on the diagonal. \\begin{gather} \\mathbf{A} | \\mathbf{I} = \\\\ \\begin{pmatrix} 1 & 3/2 - (3/2)(1) & | & 1/2 - (3/2)(2/9) & 0-(3/2)(-1/9) \\\\ 0 & 1 & | & 2/9 & -1/9 \\\\ \\end{pmatrix} = \\\\ \\begin{pmatrix} 1 & 0 & | & 1/6 & 1/6 \\\\ 0 & 1 & | & 2/9 & -1/9 \\\\ \\end{pmatrix} \\end{gather} 5) As far as this example is concerned we are done and have found the inverse. With more than a 2X2 system there will be many operations moving up and down the matrix to eliminate the off-diagonal terms. # InvertASystem.py # Code to read A and b # Then solve Ax = b for x by Gaussian elimination with back substitution # print (\"invert a matrix\") print (\"wrapper loop -- OK\") print (\"run wrapper through two iterations, same inputs\") print (\"added xmatrix,bmatrix -- get same result \") print (\"suppress vector only calcs\") print (\"clean up output\") amatrix = [] # null list to store matrix reads bvector = [] rowNumA = 0 colNumA = 0 rowNumB = 0 ###################################### # connect and read file for MATRIX A # ###################################### afile = open(\"A.txt\",\"r\") for line in afile: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 afile.close() # Disconnect the file ###################################### # end file read ,disconnect file # ###################################### colNumA = len(amatrix[0]) afile = open(\"B.txt\",\"r\") for line in afile: bvector.append(float(line)) # vector read different -- just float the line rowNumB += 1 afile.close() # Disconnect the file print (bvector) # check the arrays if rowNumA != rowNumB: print (\"row ranks not same -- aborting now\") quit() else: print (\"row ranks same -- continuing operation\") # print all columns each row cmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] dmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] bmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] xmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] xvector = [0 for i in range(rowNumA)] for i in range(0,rowNumA,1): print (amatrix[i][0:colNumA], cmatrix[i][0:colNumA]) bmatrix[i][i] = 1.0 print (\"-----------------------------\") # copy amatrix into dmatrix -- this is a static copy dmatrix = [[amatrix[i][j] for j in range(colNumA)]for i in range(rowNumA)] # now attempt invert # # outer wrapper loop # for jcol in range(rowNumA): # print \"run \",jcol xvector = [0 for i in range(rowNumA)] # xmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] for i in range(rowNumA): bvector[i]=bmatrix[i][jcol] amatrix = [[dmatrix[i][j] for j in range(colNumA)]for i in range(rowNumA)] cmatrix = [[dmatrix[i][j] for j in range(colNumA)]for i in range(rowNumA)] print (\"reset A matrix, x vector, b vector\") for i in range(0,rowNumA,1): print (amatrix[i][0:colNumA],xvector[i],bvector[i]) print (\"-----------------------------\") # build the diagonal -- assume diagonally dominant for k in range(rowNumA-1): l = k+1 for i in range(l,rowNumA): for j in range(colNumA): cmatrix[i][j]=amatrix[i][j]-amatrix[k][j]*amatrix[i][k]/amatrix[k][k] bvector[i] = bvector[i]-bvector[k]*amatrix[i][k]/amatrix[k][k] bmatrix[i][jcol] = bmatrix[i][jcol]-bmatrix[k][jcol]*amatrix[i][k]/amatrix[k][k] for i in range(rowNumA): for j in range(colNumA): amatrix[i][j] = cmatrix[i][j] # gaussian reduction done # now for the back substitution for i in range(0,rowNumA,1): print (amatrix[i][0:colNumA], bvector[i],cmatrix[i][0:colNumA]) print (\"-----------------------------\") for k in range(rowNumA-1,-1,-1): sum = 0.0 sum1 = 0.0 for i in range(rowNumA): if i == k: continue else: sum = sum + amatrix[k][i]*xvector[i] sum1 = sum1 + amatrix[k][i]*xmatrix[i][jcol] xvector[k]=(bvector[k]-sum)/amatrix[k][k] xmatrix[k][jcol]=(bmatrix[k][jcol]-sum1)/amatrix[k][k] for i in range(0,rowNumA,1): print (dmatrix[i][0:colNumA],xvector[i],bvector[i]) print (\"-----------------------------\") for i in range(0,rowNumA,1): print (dmatrix[i][0:colNumA],xmatrix[i][jcol],bmatrix[i][jcol]) print (\"-----------------------------\") # end of wrapper print (\"[ A-Matrix ]|[ A-Inverse ]\") print (\"_____________________________________________________\") for i in range(0,rowNumA,1): print (dmatrix[i][0:colNumA],\"|\", xmatrix[i][0:colNumA]) print (\"_____________________________________________________\") ofile = open(\"A-Matrix.txt\",\"w\") # \"w\" clobbers content already there! for i in range(0,rowNumA,1): message = ' '.join(map(repr, dmatrix[i][0:colNumA])) + \"\\n\" ofile.write(message) ofile.close() ofile = open(\"A-Inverse.txt\",\"w\") # \"w\" clobbers content already there! for i in range(0,rowNumA,1): message = ' '.join(map(repr, xmatrix[i][0:colNumA])) + \"\\n\" ofile.write(message) ofile.close() invert a matrix wrapper loop -- OK run wrapper through two iterations, same inputs added xmatrix,bmatrix -- get same result suppress vector only calcs clean up output [5.0, 6.0, 7.0, 8.0, 9.0] row ranks same -- continuing operation [4.0, 1.5, 0.7, 1.2, 0.5] [0, 0, 0, 0, 0] [1.0, 6.0, 0.9, 1.4, 0.7] [0, 0, 0, 0, 0] [0.5, 1.0, 3.9, 3.2, 0.9] [0, 0, 0, 0, 0] [0.2, 2.0, 0.2, 7.5, 1.9] [0, 0, 0, 0, 0] [1.7, 0.9, 1.2, 2.3, 4.9] [0, 0, 0, 0, 0] ----------------------------- reset A matrix, x vector, b vector [4.0, 1.5, 0.7, 1.2, 0.5] 0 1.0 [1.0, 6.0, 0.9, 1.4, 0.7] 0 0 [0.5, 1.0, 3.9, 3.2, 0.9] 0 0 [0.2, 2.0, 0.2, 7.5, 1.9] 0 0 [1.7, 0.9, 1.2, 2.3, 4.9] 0 0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 1.0 [4.0, 1.5, 0.7, 1.2, 0.5] [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] -0.25 [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] -0.08888888888888889 [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] 0.03356308061132753 [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] [0.0, 0.0, 0.0, 0.0, 4.231527930403315] -0.3975053957273071 [0.0, 0.0, 0.0, 0.0, 4.231527930403315] ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0.27196423630168165 1.0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.036786468827077756 -0.25 [0.5, 1.0, 3.9, 3.2, 0.9] -0.025949127789423248 -0.08888888888888889 [0.2, 2.0, 0.2, 7.5, 1.9] 0.027047195749338872 0.03356308061132753 [1.7, 0.9, 1.2, 2.3, 4.9] -0.0939389748254409 -0.3975053957273071 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0.27196423630168165 1.0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.036786468827077756 -0.25 [0.5, 1.0, 3.9, 3.2, 0.9] -0.025949127789423248 -0.08888888888888889 [0.2, 2.0, 0.2, 7.5, 1.9] 0.027047195749338872 0.03356308061132753 [1.7, 0.9, 1.2, 2.3, 4.9] -0.0939389748254409 -0.3975053957273071 ----------------------------- reset A matrix, x vector, b vector [4.0, 1.5, 0.7, 1.2, 0.5] 0 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0 1.0 [0.5, 1.0, 3.9, 3.2, 0.9] 0 0 [0.2, 2.0, 0.2, 7.5, 1.9] 0 0 [1.7, 0.9, 1.2, 2.3, 4.9] 0 0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0 [4.0, 1.5, 0.7, 1.2, 0.5] [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] 1.0 [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] -0.14444444444444443 [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] -0.3454599940065927 [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] [0.0, 0.0, 0.0, 0.0, 4.231527930403315] 0.03860910887892463 [0.0, 0.0, 0.0, 0.0, 4.231527930403315] ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.05581183146290884 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0.18691841183385363 1.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.0013334022990376664 -0.14444444444444443 [0.2, 2.0, 0.2, 7.5, 1.9] -0.05063248905238324 -0.3454599940065927 [1.7, 0.9, 1.2, 2.3, 4.9] 0.009124153146082323 0.03860910887892463 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.05581183146290884 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0.18691841183385363 1.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.0013334022990376664 -0.14444444444444443 [0.2, 2.0, 0.2, 7.5, 1.9] -0.05063248905238324 -0.3454599940065927 [1.7, 0.9, 1.2, 2.3, 4.9] 0.009124153146082323 0.03860910887892463 ----------------------------- reset A matrix, x vector, b vector [4.0, 1.5, 0.7, 1.2, 0.5] 0 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0 0 [0.5, 1.0, 3.9, 3.2, 0.9] 0 1.0 [0.2, 2.0, 0.2, 7.5, 1.9] 0 0 [1.7, 0.9, 1.2, 2.3, 4.9] 0 0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0 [4.0, 1.5, 0.7, 1.2, 0.5] [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] 0.0 [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] 1.0 [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] 0.022415343122565184 [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] [0.0, 0.0, 0.0, 0.0, 4.231527930403315] -0.23761967500359435 [0.0, 0.0, 0.0, 0.0, 4.231527930403315] ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.032853102922602934 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.032062455842026744 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] 0.26826513178341493 1.0 [0.2, 2.0, 0.2, 7.5, 1.9] 0.01649816113355711 0.022415343122565184 [1.7, 0.9, 1.2, 2.3, 4.9] -0.05615458031041434 -0.23761967500359435 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.032853102922602934 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.032062455842026744 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] 0.26826513178341493 1.0 [0.2, 2.0, 0.2, 7.5, 1.9] 0.01649816113355711 0.022415343122565184 [1.7, 0.9, 1.2, 2.3, 4.9] -0.05615458031041434 -0.23761967500359435 ----------------------------- reset A matrix, x vector, b vector [4.0, 1.5, 0.7, 1.2, 0.5] 0 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0 0 [0.5, 1.0, 3.9, 3.2, 0.9] 0 0 [0.2, 2.0, 0.2, 7.5, 1.9] 0 1.0 [1.7, 0.9, 1.2, 2.3, 4.9] 0 0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0 [4.0, 1.5, 0.7, 1.2, 0.5] [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] 0.0 [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] 0.0 [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] 1.0 [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] [0.0, 0.0, 0.0, 0.0, 4.231527930403315] -0.1488884423394965 [0.0, 0.0, 0.0, 0.0, 4.231527930403315] ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.016869919448735553 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.011456196435011407 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.10875073215127727 0.0 [0.2, 2.0, 0.2, 7.5, 1.9] 0.1486518640705042 1.0 [1.7, 0.9, 1.2, 2.3, 4.9] -0.03518550386250331 -0.1488884423394965 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.016869919448735553 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.011456196435011407 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.10875073215127727 0.0 [0.2, 2.0, 0.2, 7.5, 1.9] 0.1486518640705042 1.0 [1.7, 0.9, 1.2, 2.3, 4.9] -0.03518550386250331 -0.1488884423394965 ----------------------------- reset A matrix, x vector, b vector [4.0, 1.5, 0.7, 1.2, 0.5] 0 0 [1.0, 6.0, 0.9, 1.4, 0.7] 0 0 [0.5, 1.0, 3.9, 3.2, 0.9] 0 0 [0.2, 2.0, 0.2, 7.5, 1.9] 0 0 [1.7, 0.9, 1.2, 2.3, 4.9] 0 1.0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] 0 [4.0, 1.5, 0.7, 1.2, 0.5] [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] 0.0 [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575] [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] 0.0 [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445] [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] 0.0 [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575] [0.0, 0.0, 0.0, 0.0, 4.231527930403315] 1.0 [0.0, 0.0, 0.0, 0.0, 4.231527930403315] ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.0072026931722172435 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.012617687833839365 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.004266180002777282 0.0 [0.2, 2.0, 0.2, 7.5, 1.9] -0.05619749842697155 0.0 [1.7, 0.9, 1.2, 2.3, 4.9] 0.23632125710787594 1.0 ----------------------------- [4.0, 1.5, 0.7, 1.2, 0.5] -0.0072026931722172435 0 [1.0, 6.0, 0.9, 1.4, 0.7] -0.012617687833839365 0.0 [0.5, 1.0, 3.9, 3.2, 0.9] -0.004266180002777282 0.0 [0.2, 2.0, 0.2, 7.5, 1.9] -0.05619749842697155 0.0 [1.7, 0.9, 1.2, 2.3, 4.9] 0.23632125710787594 1.0 ----------------------------- [ A-Matrix ]|[ A-Inverse ] _____________________________________________________ [4.0, 1.5, 0.7, 1.2, 0.5] | [0.27196423630168165, -0.05581183146290884, -0.032853102922602934, -0.016869919448735553, -0.0072026931722172435] [1.0, 6.0, 0.9, 1.4, 0.7] | [-0.036786468827077756, 0.18691841183385363, -0.032062455842026744, -0.011456196435011407, -0.012617687833839365] [0.5, 1.0, 3.9, 3.2, 0.9] | [-0.025949127789423248, -0.0013334022990376664, 0.26826513178341493, -0.10875073215127727, -0.004266180002777282] [0.2, 2.0, 0.2, 7.5, 1.9] | [0.027047195749338872, -0.05063248905238324, 0.01649816113355711, 0.1486518640705042, -0.05619749842697155] [1.7, 0.9, 1.2, 2.3, 4.9] | [-0.0939389748254409, 0.009124153146082323, -0.05615458031041434, -0.03518550386250331, 0.23632125710787594] _____________________________________________________ print (amatrix) [[4.0, 1.5, 0.7, 1.2, 0.5], [0.0, 5.625, 0.7250000000000001, 1.0999999999999999, 0.575], [0.0, 0.0, 3.707777777777778, 2.8911111111111114, 0.7544444444444445], [0.0, 0.0, 0.0, 7.128360803116572, 1.6951333533113575], [0.0, 0.0, 0.0, 0.0, 4.231527930403315]]","title":"Gauss-Jordan method of finding \\mathbf{A}^{-1}"},{"location":"1-Lessons/Lesson12/lesson12/#supply-code-with-pivoting","text":"","title":"Supply Code with Pivoting"},{"location":"1-Lessons/Lesson12/lesson12/#supply-numpy-code","text":"","title":"Supply numpy code"},{"location":"1-Lessons/Lesson12/lesson12/#do-a-couple-examples","text":"","title":"Do a couple examples"},{"location":"1-Lessons/Lesson12/lesson12/#practical-application-of-linear-solvers","text":"A typical static truss analysis problem goes like \"The figure below is a simply supported, statically determinate truss with pin connections (zero moment transfer connections). Find the forces in each member for the loading shown.\" This notebook will illustrate how to leverage our linear systems solver(s) to analyze the truss. The approach uses concepts from statics and computational thinking. From statics 1) method of joints (for reactions and internal forcez) 2) direction cosines From computational thinking 1) read input file 2) construct linear system \\textbf{Ax=b}; solve for \\textbf{x} 3) report results Before even contemplating writing/using a program we need to build a mathematical model of the truss and assemble the system of linear equations that result from the model. So the first step is to sketch a free-body-diagram as below and build a node naming convention and force names. Next we will write the force balance for each of the six nodes ( N1 - N6 ), which will produce a total of 12 equations in the 12 unknowns (the 9 member forces, and 3 reactions). The figure below is the force balance for node N1 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & +F_1cos(45) & + F_2 & & & & & & & + A_x & & & & & \\\\ \\sum F_y = 0 = & +F_1sin(45) & & & & & & & & & & + A_y & & & \\\\ \\end{matrix} \\end{gather} The equation above is the force balance equation pair for the node. The x component equation will later be named N1_x to indicate it arises from Node 1, x component equation. A similar notation convention will also be adopted for the y component equation. There will be an equation pair for each node. Below is a sketch of the force balance for node N2 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & & -F_2 & & & & +F_6 & & & & & & & & \\\\ \\sum F_y = 0 = & & & +F_3 & & & & & & & & & & & \\\\ \\end{matrix} \\end{gather} Below is a sketch of the force balance for node N3 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & & & & & -F_5cos(30) & -F_6 & & +F_8 & & & & & & \\\\ \\sum F_y = 0 = & & & & & F_5sin(30) & & +F_7 & & & & & & & -P_3\\\\ \\end{matrix} \\end{gather} Above is the force balance equation pair for node N3 . Below is a sketch of the force balance for node N4 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & & & & & & & & -F_8 & -F_9cos(45) & & & & & \\\\ \\sum F_y = 0 = & & & & & & & & & F_9sin(45) & & & +B_y & & \\\\ \\end{matrix} \\end{gather} Above is the force balance equation pair for node N4 . Below is a sketch of the force balance for node N5 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & -F_1cos(45) & & & +F_4 & +F_5cos(30) & & & & & & & & & \\\\ \\sum F_y = 0 = & -F_1sin(45) & & -F_3 & & -F_5sin(30) & & & & & & & & & -P_1\\\\ \\end{matrix} \\end{gather} Above is the force balance equation pair for node N5 . Below is a sketch of the force balance for node N6 , the two force equations (for the horizontal, x , direction and the vertical, y , direction) are listed below the figure. \\begin{gather} \\begin{matrix} \\sum F_x = 0 = & & & & -F_4 & & & & & F_9sin(45) & & & & & \\\\ \\sum F_y = 0 = & & & & & & & -F_7 & & -F_9cos(45) & & & & & P_2\\\\ \\end{matrix} \\end{gather} Above is the force balance equation pair for node N6 . The next step is to gather the equation pairs into a system of linear equations. We will move the known loads to the right hand side and essentially construct the matrix equation \\mathbf{A}\\mathbf{x} = \\mathbf{b} . The system below is a matrix representation of the equation pairs with the forces moved to the right hand side \\mathbf{b} = RHS . \\begin{gather} \\begin{pmatrix} ~ & F_1 & F_2 & F_3 & F_4 & F_5 & F_6 & F_7 & F_8 & F_9 & A_x & A_y & B_y & | & RHS\\\\ \\hline N1_x & 0.707 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & | & 0\\\\ N1_y & 0.707 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & | & 0\\\\ N2_x & 0 & -1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & | & 0\\\\ N2_y & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & | & 0\\\\ N3_x & 0 & 0 & 0 & 0 & -0.866 & -1 & 0 & 1 & 0 & 0 & 0 & 0 & | & 0\\\\ N3_y & 0 & 0 & 0 & 0 & 0.5 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & | & P_3\\\\ N4_x & 0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 & -0.707 & 0 & 0 & 0 & | & 0\\\\ N4_y & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.707 & 0 & 0 & 0 & | & 0\\\\ N5_x & -0.707 & 0 & 0 & 1 & 0.866 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & | & 0\\\\ N5_y & -0.707 & 0 & -1 & 0 & -0.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & | & P_1\\\\ N6_x & 0 & 0 & 0 & -1 & 0 & 0 & 0 & 0 & 0.707 & 0 & 0 & 0 & | & 0\\\\ N6_y & 0 & 0 & 0 & 0 & 0 & 0 & -1 & 0 & -0.707 & 0 & 0 & 0 & | & -P_2\\\\ \\end{pmatrix} \\end{gather} In the system, the rows are labeled on the left-most column with their node-related equation name. Thus each row of the matrix corresponds to an equation derived from a node. The columns are labeled with their respective unknown force (except the last column, which represents the right-hand-side of the system of linear equations). Thus the coefficient in each column corresponds to a force in each node equation. The sign of the coefficient refers to the assumed direction the force acts. In the analysis all the members were assumed to be in tension (except for the reaction forces). If a coefficient has a value of zero in a particular row, then that force does no act at the node to which the row corresponds. From this representation the transition to the formal vector-matrix representation is straightforward. \\begin{gather} \\mathbf{A} = \\begin{pmatrix} 0.707 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\\\ 0.707 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\\\ 0 & -1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & -0.866 & -1 & 0 & 1 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0.5 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 & -0.707 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0.707 & 0 & 0 & 0 \\\\ -0.707 & 0 & 0 & 1 & 0.866 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ -0.707 & 0 & -1 & 0 & -0.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & -1 & 0 & 0 & 0 & 0 & 0.707 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & -1 & 0 & -0.707 & 0 & 0 & 0 \\\\ \\end{pmatrix} \\end{gather} \\begin{gather} \\mathbf{x} = \\begin{pmatrix} F_1\\\\ F_2\\\\ F_3\\\\ F_4\\\\ F_5\\\\ F_6\\\\ F_7\\\\ F_8\\\\ F_9\\\\ A_x\\\\ A_y\\\\ B_y\\\\ \\end{pmatrix} \\end{gather} \\begin{gather} \\mathbf{b} = \\begin{pmatrix} 0\\\\ 0\\\\ 0\\\\ 0\\\\ 0\\\\ P_3\\\\ 0\\\\ 0\\\\ 0\\\\ P_1\\\\ 0\\\\ -P_2\\\\ \\end{pmatrix} \\end{gather} The various matrices above are entere into text files named A.txt and B.txt, we can examine the file contents using the host OS as below # list contents of the A matrix, uses call to OS host !(cat A.txt) 4.0 1.5 0.7 1.2 0.5 1.0 6.0 0.9 1.4 0.7 0.5 1.0 3.9 3.2 0.9 0.2 2.0 0.2 7.5 1.9 1.7 0.9 1.2 2.3 4.9 # list contents of RHS or b vector !(cat B.txt) 5.0 6.0 7.0 8.0 9.0 Now we use our solver tools, here I have not doen much on tidy output, thats left for the reader. # Linear System Solver from LinearSolverPivot import linearsolver amatrix = [] # null list to store matrix reads bvector = [] rowNumA = 0 colNumA = 0 rowNumB = 0 afile = open(\"A.txt\",\"r\") # connect and read file for MATRIX A for line in afile: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 afile.close() # Disconnect the file colNumA = len(amatrix[0]) afile = open(\"B.txt\",\"r\") # connect and read file for MATRIX B for line in afile: bvector.append(float(line)) # vector read different -- just float the line rowNumB += 1 afile.close() # Disconnect the file #print (bvector) if rowNumA != rowNumB: # check the arrays print (\"row ranks not same -- aborting now\") quit() else: print (\"row ranks same -- continuing operation\") # print all columns each row cmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] dmatrix = [[0 for j in range(colNumA)]for i in range(rowNumA)] xvector = [0 for i in range(rowNumA)] dvector = [0 for i in range(rowNumA)] for i in range(0,rowNumA,1): print (amatrix[i][0:colNumA], bvector[i]) print (\"-----------------------------\") # copy amatrix into cmatrix cmatrix = [[amatrix[i][j] for j in range(colNumA)]for i in range(rowNumA)] dmatrix = [[amatrix[i][j] for j in range(colNumA)]for i in range(rowNumA)] dvector = [bvector[i] for i in range(rowNumA)] dvector = linearsolver(amatrix,bvector) for i in range(0,rowNumA,1): print (dvector[i]) print (\"-----------------------------\") row ranks same -- continuing operation [4.0, 1.5, 0.7, 1.2, 0.5] 5.0 [1.0, 6.0, 0.9, 1.4, 0.7] 6.0 [0.5, 1.0, 3.9, 3.2, 0.9] 7.0 [0.2, 2.0, 0.2, 7.5, 1.9] 8.0 [1.7, 0.9, 1.2, 2.3, 4.9] 9.0 ----------------------------- 0.5951948781328953 0.5079321739889 0.831708392507349 0.6303655990885844 1.0373752656472461 -----------------------------","title":"Practical Application of Linear Solvers"},{"location":"1-Lessons/Lesson12/lesson12/#references","text":"Overland, B. (2018). Python Without Fear. Addison-Wesley ISBN 978-0-13-468747-6. Grus, Joel (2015). Data Science from Scratch: First Principles with Python O\u2019Reilly Media. Kindle Edition. Precord, C. (2010) wxPython 2.8 Application Development Cookbook Packt Publishing Ltd. Birmingham , B27 6PA, UK ISBN 978-1-849511-78-0. import numpy as np amatrix = [] # null list to store matrix reads bvector = [] rowNumA = 0 colNumA = 0 rowNumB = 0 afile = open(\"A.txt\",\"r\") # connect and read file for MATRIX A for line in afile: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 afile.close() # Disconnect the file afile = open(\"B.txt\",\"r\") # connect and read file for MATRIX B for line in afile: bvector.append(float(line)) # vector read different -- just float the line rowNumB += 1 afile.close() # Disconnect the file A = np.array(amatrix) b = np.array(bvector) x = np.linalg.solve(A, b) print(x) [0.59519488 0.50793217 0.83170839 0.6303656 1.03737527]","title":"References"},{"location":"1-Lessons/Lesson13/lesson13/","text":"Copyright \u00a9 DATE Author, all rights reserved ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: Lesson 13 Nonlinear Systems of Equations This lesson will Objectives Construct multivariate non-linear equation systems Construct the Jacobian using analytical partial derivatives Solve using a Newton-Raphson method Construct the Jacobian using finite-difference approximations of the partial derivatives Demonstrate practical application with a pipeline network Nonlinear Systems of Equations Non-linear systems are extensions of the linear systems cases except the systems involve products and powers of the unknown variables. Non-linear problems are often quite difficult to manage, especially when the systems are large (many rows and many variables). The solution to non-linear systems, if non-trivial or even possible, are usually iterative. Within the iterative steps is a linearization component \u2013 these linear systems which are intermediate computations within the overall solution process are treated by an appropriate linear system method (direct or iterative). Consider the system below: \\begin{gather} \\begin{matrix} x^2 & +~y^2 \\\\ e^x & +~y \\\\ \\end{matrix} \\begin{matrix} = 4\\\\ = 1\\\\ \\end{matrix} \\end{gather} Suppose we have a solution guess x_{k},y_{k} , which of course could be wrong, but we could linearize about that guess as \\begin{gather} \\mathbf{A} = \\begin{pmatrix} x_k & + ~y_k \\\\ 0 & + ~1 \\\\ \\end{pmatrix} ~\\mathbf{x} = \\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\\\ \\end{pmatrix} ~ \\mathbf{b} = \\begin{pmatrix} 4\\\\ 1 - e^{x_k}\\\\ \\end{pmatrix} \\end{gather} Now if we assemble the system in the usual fashion, \\mathbf{A} \\cdot \\mathbf{x_{k+1}} = ~ \\mathbf{b}~ we have a system of linear equations\\footnote{Linear in \\mathbf{x_{k+1}} }, which expanded look like: \\begin{gather} \\begin{pmatrix} x_k & + ~y_k \\\\ 0 & + ~1 \\\\ \\end{pmatrix} \\cdot \\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\\\ \\end{pmatrix} ~ = \\begin{pmatrix} 4\\\\ 1 - e^{x_k}\\\\ \\end{pmatrix} \\end{gather} Now that the system is linear, and we can solve for \\mathbf{x_{k+1}} using our linear system solver for the new guess. If the system is convergent (not all are) then if we update, and repeat we will eventually find a result. What one really needs is a way to construct the linear system that has a systematic update method, that is discussed below Multiple-variable extension of Newton\u2019s Method This section presents the Newton-Raphson method as a way to sometimes solve systems of non-linear equations. Consider an example where the function \\textbf{f} is a vector-valued function of a vector argument. \\begin{gather} \\mathbf{f(x)} = \\begin{matrix} f_1 = & x^2 & +~y^2 & - 4\\\\ f_2 = & e^x & +~y & - 1\\\\ \\end{matrix} \\end{gather} Let's also recall Newtons method for scalar valued function of a single variable. \\begin{equation} x_{k+1}=x_{k} - \\frac{ f(x_{k}) }{ \\frac{df}{dx}\\rvert_{x_k} } \\label{eqn:NewtonFormula} \\end{equation} When extending to higher dimensions, the analog for x is the vector \\textbf{x} and the analog for the function f() is the vector function \\textbf{f()}. What remains is an analog for the first derivative in the denominator (and the concept of division of a matrix). The analog to the first derivative is a matrix called the Jacobian which is comprised of the first derivatives of the function \\textbf{f} with respect to the arguments \\textbf{x}. For example for a 2-value function of 2 arguments (as our example above) \\begin{equation} \\frac{df}{dx}\\rvert_{x_k} => \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\ ~ & ~ \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} \\\\ \\end{pmatrix} \\label{eqn:Jacobian} \\end{equation} Next recall that division is replaced by matrix multiplication with the multiplicative inverse, so the analogy continues as \\begin{equation} \\frac{1}{\\frac{df}{dx}\\rvert_{x_k}} => {\\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\ ~ & ~ \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} \\\\ \\end{pmatrix}}^{-1} \\label{eqn:JacobianInverse} \\end{equation} Let's name the Jacobian \\textbf{J(x)}. So the multi-variate Newton's method can be written as \\begin{equation} \\mathbf{x}_{k+1}=\\mathbf{x}_{k} - \\mathbf{J(x)}^{-1}\\rvert_{x_k} \\cdot \\mathbf{f(x)}\\rvert_{x_k} \\label{eqn:VectorNewtonFormula} \\end{equation} In the linear systems lessons we did find a way to solve for an inverse, but it's not necessary, and is computationally expensive to invert in these examples -- a series of rearrangement of the system above yields a nice scheme that does not require inversion of a matrix. First, move the \\mathbf{x}_{k} to the left-hand side. \\begin{equation} \\mathbf{x}_{k+1}-\\mathbf{x}_{k} = - \\mathbf{J(x)}^{-1}\\rvert_{x_k} \\cdot \\mathbf{f(x)}\\rvert_{x_k} \\end{equation} Next multiply both sides by the Jacobian (The Jacobian must be non-singular otherwise we are dividing by zero) \\begin{equation} \\mathbf{J(x)}\\rvert_{x_k} \\cdot (\\mathbf{x}_{k+1}-\\mathbf{x}_{k}) = - \\mathbf{J(x)}\\rvert_{x_k} \\cdot \\mathbf{J(x)}^{-1}\\rvert_{x_k} \\cdot \\mathbf{f(x)}\\rvert_{x_k} \\end{equation} Recall a matrix multiplied by its inverse returns the identity matrix (the matrix equivalent of unity) \\begin{equation} -\\mathbf{J(x)}\\rvert_{x_k} \\cdot (\\mathbf{x}_{k+1}-\\mathbf{x}_{k}) = \\mathbf{f(x)}\\rvert_{x_k} \\end{equation} So we now have an algorithm: 1) Start with an initial guess \\mathbf{x}_{k} , compute \\mathbf{f(x)}\\rvert_{x_k} , and \\mathbf{J(x)}\\rvert_{x_k} . 2) Test for stopping. Is \\mathbf{f(x)}\\rvert_{x_k} close to zero? If yes, exit and report results, otherwise continue. 3) Solve the linear system \\mathbf{J(x)}\\rvert_{x_k} \\cdot (\\mathbf{x}_{k+1}-\\mathbf{x}_{k}) = \\mathbf{f(x)}\\rvert_{x_k} . 4) Test for stopping. Is (\\mathbf{x}_{k+1}-\\mathbf{x}_{k}) close to zero? If yes, exit and report results, otherwise continue. 5) Compute the update \\mathbf{x}_{k+1} = \\mathbf{x}_{k} - (\\mathbf{x}_{k+1}-\\mathbf{x}_{k}) , then 6) Move the update into the guess vector \\mathbf{x}_{k} <=\\mathbf{x}_{k+1} =and repeat step 1. Stop after too many steps. Example using Analytical Derivatives Now to complete the example we will employ this algorithm. The function (repeated) \\begin{gather} \\mathbf{f(x)} = \\begin{matrix} f_1 = & x^2 & +~y^2 & - 4\\\\ f_2 = & e^x & +~y & - 1\\\\ \\end{matrix} \\end{gather} Then the Jacobian, here we will compute it analytically because we can \\begin{equation} \\mathbf{J(x)}=> {\\begin{pmatrix} 2x & 2y \\\\ ~ & ~ \\\\ e^x & 1 \\\\ \\end{pmatrix}} \\end{equation} Now for the scripts. We will start by defining the two equations, and their derivatives, as well a a vector valued function func and its Jacobian jacob as below. Here the two modules LinearSolverPivot and vector_matrix_lib are just python source code files containing prototype functions. ################################################################# # Newton Solver Example -- Analytical Derivatives # ################################################################# import math # This will import math module from python distribution from LinearSolverPivot import linearsolver # This will import our solver module from vector_matrix_lib import writeM,writeV,vdotv,vvsub # This will import our vector functions def eq1(x,y): eq1 = x**2 + y**2 - 4.0 return(eq1) def eq2(x,y): eq2 = math.exp(x) + y - 1.0 return(eq2) def ddxeq1(x,y): ddxeq1 = 2.0*x return(ddxeq1) def ddyeq1(x,y): ddyeq1 = 2.0*y return(ddyeq1) def ddxeq2(x,y): ddxeq2 = math.exp(x) return(ddxeq2) def ddyeq2(x,y): ddyeq2 = 1.0 return(ddyeq2) def func(x,y): func = [0.0 for i in range(2)] # null list # build the function func[0] = eq1(x,y) func[1] = eq2(x,y) return(func) def jacob(x,y): jacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list #build the jacobian jacob[0][0]=ddxeq1(x,y) jacob[0][1]=ddyeq1(x,y) jacob[1][0]=ddxeq2(x,y) jacob[1][1]=ddyeq2(x,y) return(jacob) Next we create vectors to store values, and supply initial guesses to the system, and echo the inputs. deltax = [0.0 for i in range(2)] # null list xguess = [0.0 for i in range(2)] # null list myfunc = [0.0 for i in range(2)] # null list myjacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list # supply initial guess xguess[0] = float(input(\"Value for x : \")) xguess[1] = float(input(\"Value for y : \")) # build the initial function myfunc = func(xguess[0],xguess[1]) #build the initial jacobian myjacob=jacob(xguess[0],xguess[1]) #write initial results writeV(xguess,2,\"Initial X vector \") writeV(myfunc,2,\"Initial FUNC vector \") writeM(myjacob,2,2,\"Initial Jacobian \") # solver parameters tolerancef = 1.0e-9 tolerancex = 1.0e-9 Value for x : 1 Value for y : 2 ------ Initial X vector ------ 1.0 2.0 ----------------------------- ------ Initial FUNC vector ------ 1.0 3.7182818284590446 ----------------------------- ------ Initial Jacobian ------ [2.0, 4.0] [2.718281828459045, 1.0] ----------------------------- Now we apply the algorithm a few times, here the count is set to 10. So eneter the loop, test for stopping, then update. # Newton-Raphson for iteration in range(10): myfunc = func(xguess[0],xguess[1]) testf = vdotv(myfunc,myfunc,2) if testf <= tolerancef : print(\"f(x) close to zero\\n test value : \", testf) break myjacob=jacob(xguess[0],xguess[1]) deltax=linearsolver(myjacob,myfunc) testx = vdotv(deltax,deltax,2) if testx <= tolerancex : print(\"solution change small\\n test value : \", testx) break xguess=vvsub(xguess,deltax,2) ## print(\"iteration : \",iteration) ## writeV(xguess,2,\"Current X vector \") ## writeV(myfunc,2,\"Current FUNC vector \") print(\"Exiting Iteration : \",iteration) writeV(xguess,2,\"Exiting X vector \") writeV(myfunc,2,\"Exiting FUNC vector \") solution change small test value : 2.1803484657072266e-10 Exiting Iteration : 5 ------ Exiting X vector ------ -1.8162775103511606 0.8373739123240562 ----------------------------- ------ Exiting FUNC vector ------ 5.90636483064344e-05 3.926422552424924e-06 ----------------------------- Quasi-Newton Method using Finite Difference Approximations for the Derivative The next variant is to approximate the derivatives -- usually a Finite-Difference approximation is used, either forward, backward, or centered differences -- generally determined based on the actual behavior of the functions themselves or by trial and error. For really huge systems, we usually make the program itself make the adaptions as it proceeds. The coding for a finite-difference representation of a Jacobian is shown in Listing that follows In constructing the Jacobian, we observe that each column of the Jacobian is simply the directional derivative of the function with respect to the variable associated with the column. For instance, the first column of the Jacobian in the example is first derivative of the first function (all rows) with respect to the first variable, in this case x . The second column is the first derivative of the second function with respect to the second variable, y . This structure is useful to generalize the Jacobian construction method because we could write (yet another) prototype function that can take the directional derivatives for us, and just insert the returns as columns; in the example we simply modified the ddx and ddy functions from analytical to simple finite differences. The example listing is specific to the 2X2 function in the example, but the extension to more general cases is evident. ################################################################# # Newton Solver Example -- Numerical Derivatives # ################################################################# import math # This will import math module from python distribution from LinearSolverPivot import linearsolver # This will import our solver module from vector_matrix_lib import writeM,writeV,vdotv,vvsub # This will import our vector functions def eq1(x,y): eq1 = x**2 + y**2 - 4.0 return(eq1) def eq2(x,y): eq2 = math.exp(x) + y - 1.0 return(eq2) ############################################################## # This portion is changed for finite-difference method to evaluate derivatives # ############################################################## def ddxeq1(x,y): delta = 1.0e-6 ddxeq1 = (eq1(x+delta,y)-eq1(x,y))/delta return(ddxeq1) def ddyeq1(x,y): delta = 1.0e-6 ddyeq1 = (eq1(x,y+delta)-eq1(x,y))/delta return(ddyeq1) def ddxeq2(x,y): delta = 1.0e-6 ddxeq2 = (eq2(x+delta,y)-eq2(x,y))/delta return(ddxeq2) def ddyeq2(x,y): delta = 1.0e-6 ddyeq2 = (eq2(x,y+delta)-eq2(x,y))/delta return(ddyeq2) ############################################################## def func(x,y): func = [0.0 for i in range(2)] # null list # build the function func[0] = eq1(x,y) func[1] = eq2(x,y) return(func) def jacob(x,y): jacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list #build the jacobian jacob[0][0]=ddxeq1(x,y) jacob[0][1]=ddyeq1(x,y) jacob[1][0]=ddxeq2(x,y) jacob[1][1]=ddyeq2(x,y) return(jacob) deltax = [0.0 for i in range(2)] # null list xguess = [0.0 for i in range(2)] # null list myfunc = [0.0 for i in range(2)] # null list myjacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list # supply initial guess xguess[0] = float(input(\"Value for x : \")) xguess[1] = float(input(\"Value for y : \")) # build the initial function myfunc = func(xguess[0],xguess[1]) #build the initial jacobian myjacob=jacob(xguess[0],xguess[1]) #write initial results writeV(xguess,2,\"Initial X vector \") writeV(myfunc,2,\"Initial FUNC vector \") writeM(myjacob,2,2,\"Initial Jacobian \") # solver parameters tolerancef = 1.0e-9 tolerancex = 1.0e-9 # Newton-Raphson for iteration in range(10): myfunc = func(xguess[0],xguess[1]) testf = vdotv(myfunc,myfunc,2) if testf <= tolerancef : print(\"f(x) close to zero\\n test value : \", testf) break myjacob=jacob(xguess[0],xguess[1]) deltax=linearsolver(myjacob,myfunc) testx = vdotv(deltax,deltax,2) if testx <= tolerancex : print(\"solution change small\\n test value : \", testx) break xguess=vvsub(xguess,deltax,2) ## print(\"iteration : \",iteration) ## writeV(xguess,2,\"Current X vector \") ## writeV(myfunc,2,\"Current FUNC vector \") print(\"Exiting Iteration : \",iteration) writeV(xguess,2,\"Exiting X vector \") writeV(myfunc,2,\"Exiting FUNC vector using Finite-Differences\") Value for x : 1 Value for y : 2 ------ Initial X vector ------ 1.0 2.0 ----------------------------- ------ Initial FUNC vector ------ 1.0 3.7182818284590446 ----------------------------- ------ Initial Jacobian ------ [2.0000010003684565, 4.0000010006480125] [2.718283187874704, 1.0000000010279564] ----------------------------- solution change small test value : 2.1800662368653786e-10 Exiting Iteration : 5 ------ Exiting X vector ------ -1.8162775096127992 0.8373739116345714 ----------------------------- ------ Exiting FUNC vector using Finite-Differences ------ 5.905981145470918e-05 3.925853147235259e-06 ----------------------------- Exercises Write a script that forward defines the multi-variate functions and implements the Newton-Raphson technique. Implement the method, using analytical derivatives, and find a solution to: \\begin{gather} \\begin{matrix} x^3 & +~3y^2 & = 21\\\\ x^2& +~2y & = -2 \\\\ \\end{matrix} \\end{gather} Repeat the exercise, except use finite-differences to approximate the derivatives. Write a script that forward defines the multi-variate functions and implements the Newton-Raphson technique. Implement the method, using analytical derivatives, and find a solution to: \\begin{gather} \\begin{matrix} x^2 & +~ y^2 & +~z^2 & =~ 9\\\\ ~ & ~ & xyz & =~ 1\\\\ x & +~ y & -z^2 & =~ 0\\\\ \\end{matrix} \\end{gather} Repeat the exercise, except use finite-differences to approximate the derivatives. Write a script that forward defines the multi-variate functions and implements the Newton-Raphson technique. Implement the method, using analytical derivatives, and find a solution to: \\begin{gather} \\begin{matrix} xyz & -~ x^2 & +~y^2 & =~ 1.34\\\\ ~ & xy &-~z^2 & =~ 0.09\\\\ e^x & -~ e^y & +z & =~ 0.41\\\\ \\end{matrix} \\end{gather} Repeat the exercise, except use finite-differences to approximate the derivatives. Branched System 3-Reservoir Example Consider the branched system shown in below. In this example the friction factor is assumed constant for simplicity, but in practice would vary during the solution computations. The hydraulics question is what is the discharge in each pipe and what is the total head at the junction (notice we don't know the junction elevation in this example --- if the elevation were specified, we could also find the pressure head). Solution Approach First populate the four equations with the appropriate numerical values. \\begin{equation} 70 = h_D + (0.015)\\frac{5000}{0.6}\\frac{V_{AD}|V_{AD}|}{2(9.8)} \\end{equation} \\begin{equation} 100 = h_D + (0.015)\\frac{3000}{0.8}\\frac{V_{BD}|V_{BD}|}{2(9.8)} \\end{equation} \\begin{equation} -80 = - h_D + (0.015)\\frac{4000}{1.2}\\frac{V_{DC}|V_{DC}|}{2(9.8)} \\end{equation} \\begin{equation} \\frac{\\pi(0.6)^2}{4} V_{AD}+\\frac{\\pi(0.8)^2}{4} V_{BD} = \\frac{\\pi(1.2)^2}{4} V_{DC} \\end{equation} Next, compute all the constants, and organize the 4 equations into a system of simultaneous equations \\begin{equation} \\begin{matrix} h_D & 6.377 V_{AD}|V_{AD}| & 0 & 0 & = 70\\\\ h_D & 0 & 2.869 V_{BD}|V_{BD}| & 0 & = 100\\\\ h_D & 0 & 0 & - 2.551 V_{DC}|V_{DC}| & = 80\\\\ 0 & 0.2827 V_{AD} & 0.5026 V_{BD}& -1.1309 V_{DC} & = 0\\\\ \\end{matrix} \\end{equation} Because the system is non-linear we need to employ some appropriate method, herein we will use a Quasi-Newton method with numerical approximations to derivatives. First a more conventional vector-matrix structure \\begin{gather} \\begin{pmatrix} 1 & 6.377 |V_{AD}| & 0 & 0 \\\\ 1 & 0 & 2.869 |V_{BD}| & 0 \\\\ 1 & 0 & 0 & - 2.551 |V_{DC}| \\\\ 0 & 0.2827 & 0.5026 & -1.1309 \\\\ \\end{pmatrix} \\bullet \\begin{pmatrix} h_D \\\\ V_{AD} \\\\ V_{BD} \\\\ V_{DC} \\\\ \\end{pmatrix} - \\begin{pmatrix} 70 \\\\ 100 \\\\ 80 \\\\ 0 \\\\ \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{pmatrix} \\end{gather} Apply the multiple-variable Extension of Newton\u2019s Method The solution can use the methods directly from ENGR-1330 and similar courses, here we just implement the methods. A very brief explaination is in the readings, but multi-variable Newton's method is usually taught in calculus. Here we will define some support modules linearsolver() and vector_matrix_lib which are just python source code containing prototype functions which are embedded into the notebook, but ordinarily would save them in separate file and use import . # SolveLinearSystem.py # Code to read A and b # Then solve Ax = b for x by Gaussian elimination with back substitution ########## def linearsolver(A,b): n = len(A) # M = A #this is object to object equivalence # copy A into M element by element M=[[0.0 for jcol in range(n)]for irow in range(n)] for irow in range(n): for jcol in range(n): M[irow][jcol]=A[irow][jcol] i = 0 for x in M: x.append(b[i]) i += 1 for k in range(n): for i in range(k,n): if abs(M[i][k]) > abs(M[k][k]): M[k], M[i] = M[i],M[k] else: pass for j in range(k+1,n): q = float(M[j][k]) / M[k][k] for m in range(k, n+1): M[j][m] -= q * M[k][m] x = [0 for i in range(n)] x[n-1] =float(M[n-1][n])/M[n-1][n-1] for i in range (n-1,-1,-1): z = 0 for j in range(i+1,n): z = z + float(M[i][j])*x[j] x[i] = float(M[i][n] - z)/M[i][i] # print (x) return(x) # vector_matrix_lib.py # useful linear algebra tools import math # This will import math module def writeM(M,ir,jc,label): print (\"------\",label,\"------\") for i in range(0,ir,1): print (M[i][0:jc]) print (\"-----------------------------\") #return def writeV(V,ir,label): print (\"------\",label,\"------\") for i in range(0,ir,1): print (V[i]) print (\"-----------------------------\") #return def mmmult(amatrix,bmatrix,rowNumA,colNumA,rowNumB,colNumB): AB =[[0.0 for j in range(colNumB)] for i in range(rowNumA)] for i in range(0,rowNumA): for j in range(0,colNumB): for k in range(0,colNumA): AB[i][j]=AB[i][j]+amatrix[i][k]*bmatrix[k][j] return(AB) def mvmult(amatrix,xvector,rowNumA,colNumA): bvector=[0.0 for i in range(rowNumA)] for i in range(0,rowNumA): for j in range(0,1): for k in range(0,colNumA): bvector[i]=bvector[i]+amatrix[i][k]*xvector[k] return(bvector) def vvadd(avector,bvector,length): aplusb=[0.0 for i in range(length)] for i in range(length): aplusb[i] = avector[i] + bvector[i] return(aplusb) def vvsub(avector,bvector,length): aminusb=[0.0 for i in range(length)] for i in range(length): aminusb[i] = avector[i] - bvector[i] return(aminusb) def vdotv(avector,bvector,length): adotb=0.0 for i in range(length): adotb=adotb+avector[i]*bvector[i] return(adotb) Next we define the four equations, as well a a vector valued function func and its Jacobian jacob as below. ################################################################# # Newton Solver Example -- Numerical Derivatives # ################################################################# import math # This will import math module from python distribution #from LinearSolverPivot import linearsolver # This will import our solver module #from vector_matrix_lib import writeM,writeV,vdotv,vvsub # This will import our vector functions def eq1(x1,x2,x3,x4): eq1 = 1*x1 + 6.377*x2 +0*x3 + 0*x4 - 70 return(eq1) def eq2(x1,x2,x3,x4): eq2 = 1*x1 + 0*x2 +2.869*x3 + 0*x4 - 100 return(eq2) def eq3(x1,x2,x3,x4): eq3 = 1*x1 + 0*x2 +0*x3 + -2.551*x4 -80 return(eq3) def eq4(x1,x2,x3,x4): eq4 = 0*x1 + 0.2827*x2 +0.5026*x3 + -1.1309*x4 - 0 return(eq4) ############################################################## def func(x1,x2,x3,x4): func = [0.0 for i in range(4)] # null list # build the function func[0] = eq1(x1,x2,x3,x4) func[1] = eq2(x1,x2,x3,x4) func[2] = eq3(x1,x2,x3,x4) func[3] = eq4(x1,x2,x3,x4) return(func) def jacob(x1,x2,x3,x4): jacob = [[0.0 for j in range(4)] for i in range(4)] # constructed list #build the jacobian jacob[0][0]=1 jacob[0][1]=6.377*x2 jacob[0][2]=0 jacob[0][3]=0 jacob[1][0]=1 jacob[1][1]=0 jacob[1][2]=2.869*x3 jacob[1][3]=0 jacob[2][0]=1 jacob[2][1]=0 jacob[2][2]=0 jacob[2][3]=-2.551*x4 jacob[3][0]=0 jacob[3][1]=0.2827 jacob[3][2]=0.5026 jacob[3][3]=-1.1309 return(jacob) Next we create vectors to store values, and supply initial guesses to the system, and echo the inputs. deltax = [0.0 for i in range(4)] # null list xguess = [0.0 for i in range(4)] # null list myfunc = [0.0 for i in range(4)] # null list myjacob = [[0.0 for j in range(4)] for i in range(4)] # constructed list # supply initial guess xguess[0] = float(input(\"Value for x1 : \")) xguess[1] = float(input(\"Value for x2 : \")) xguess[2] = float(input(\"Value for x3 : \")) xguess[3] = float(input(\"Value for x4 : \")) # build the initial function myfunc = func(xguess[0],xguess[1],xguess[2],xguess[3]) #build the initial jacobian myjacob=jacob(xguess[0],xguess[1],xguess[2],xguess[3]) #write initial results writeV(xguess,4,\"Initial X vector \") writeV(myfunc,4,\"Initial FUNC vector \") writeM(myjacob,4,4,\"Initial Jacobian \") # solver parameters tolerancef = 1.0e-9 tolerancex = 1.0e-9 # Newton-Raphson Value for x1 : 1 Value for x2 : 1 Value for x3 : 1 Value for x4 : 1 ------ Initial X vector ------ 1.0 1.0 1.0 1.0 ----------------------------- ------ Initial FUNC vector ------ -62.623 -96.131 -81.551 -0.3455999999999999 ----------------------------- ------ Initial Jacobian ------ [1, 6.377, 0, 0] [1, 0, 2.869, 0] [1, 0, 0, -2.551] [0, 0.2827, 0.5026, -1.1309] ----------------------------- Now we apply the algorithm a few times, here the count is set to 24. So eneter the loop, test for stopping, then update. for iteration in range(24): myfunc = func(xguess[0],xguess[1],xguess[2],xguess[3]) testf = vdotv(myfunc,myfunc,4) if testf <= tolerancef : print(\"f(x) close to zero\\n test value : \", testf) break myjacob=jacob(xguess[0],xguess[1],xguess[2],xguess[3]) deltax=linearsolver(myjacob,myfunc) testx = vdotv(deltax,deltax,4) if testx <= tolerancex : print(\"solution change small\\n test value : \", testx) break xguess=vvsub(xguess,deltax,4) ## print(\"iteration : \",iteration) ## writeV(xguess,2,\"Current X vector \") ## writeV(myfunc,2,\"Current FUNC vector \") print(\"Exiting Iteration : \",iteration) writeV(xguess,4,\"Exiting X vector \") writeV(myfunc,4,\"Exiting FUNC vector using Finite-Differences\") f(x) close to zero test value : 2.0273725264180003e-28 Exiting Iteration : 1 ------ Exiting X vector ------ 84.61708956275575 -2.292157685864162 5.361767318663032 1.8099135879089594 ----------------------------- ------ Exiting FUNC vector using Finite-Differences ------ -1.4210854715202004e-14 0.0 0.0 -8.881784197001252e-16 ----------------------------- Now tidy up the output, convert back to discharges (from velocities) and report results. print(\"Head at Junction : %.3f\" % xguess[0], \" feet \") print(\"Discharge in Pipe AD: %.3f\" % (0.2827 * xguess[1]), \" cubic feet per second\") print(\"Discharge in Pipe BD: %.3f\" % (0.5026 * xguess[2]), \" cubic feet per second \") print(\"Discharge in Pipe DC: %.3f\" % (1.1309 * xguess[3]), \" cubic feet per second \") Head at Junction : 84.617 feet Discharge in Pipe AD: -0.648 cubic feet per second Discharge in Pipe BD: 2.695 cubic feet per second Discharge in Pipe DC: 2.047 cubic feet per second","title":"Non-Linear Equation Systems"},{"location":"1-Lessons/Lesson13/lesson13/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date:","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson13/lesson13/#lesson-13-nonlinear-systems-of-equations","text":"This lesson will","title":"Lesson 13 Nonlinear Systems of Equations"},{"location":"1-Lessons/Lesson13/lesson13/#objectives","text":"Construct multivariate non-linear equation systems Construct the Jacobian using analytical partial derivatives Solve using a Newton-Raphson method Construct the Jacobian using finite-difference approximations of the partial derivatives Demonstrate practical application with a pipeline network","title":"Objectives"},{"location":"1-Lessons/Lesson13/lesson13/#nonlinear-systems-of-equations","text":"Non-linear systems are extensions of the linear systems cases except the systems involve products and powers of the unknown variables. Non-linear problems are often quite difficult to manage, especially when the systems are large (many rows and many variables). The solution to non-linear systems, if non-trivial or even possible, are usually iterative. Within the iterative steps is a linearization component \u2013 these linear systems which are intermediate computations within the overall solution process are treated by an appropriate linear system method (direct or iterative). Consider the system below: \\begin{gather} \\begin{matrix} x^2 & +~y^2 \\\\ e^x & +~y \\\\ \\end{matrix} \\begin{matrix} = 4\\\\ = 1\\\\ \\end{matrix} \\end{gather} Suppose we have a solution guess x_{k},y_{k} , which of course could be wrong, but we could linearize about that guess as \\begin{gather} \\mathbf{A} = \\begin{pmatrix} x_k & + ~y_k \\\\ 0 & + ~1 \\\\ \\end{pmatrix} ~\\mathbf{x} = \\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\\\ \\end{pmatrix} ~ \\mathbf{b} = \\begin{pmatrix} 4\\\\ 1 - e^{x_k}\\\\ \\end{pmatrix} \\end{gather} Now if we assemble the system in the usual fashion, \\mathbf{A} \\cdot \\mathbf{x_{k+1}} = ~ \\mathbf{b}~ we have a system of linear equations\\footnote{Linear in \\mathbf{x_{k+1}} }, which expanded look like: \\begin{gather} \\begin{pmatrix} x_k & + ~y_k \\\\ 0 & + ~1 \\\\ \\end{pmatrix} \\cdot \\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\\\ \\end{pmatrix} ~ = \\begin{pmatrix} 4\\\\ 1 - e^{x_k}\\\\ \\end{pmatrix} \\end{gather} Now that the system is linear, and we can solve for \\mathbf{x_{k+1}} using our linear system solver for the new guess. If the system is convergent (not all are) then if we update, and repeat we will eventually find a result. What one really needs is a way to construct the linear system that has a systematic update method, that is discussed below","title":"Nonlinear Systems of Equations"},{"location":"1-Lessons/Lesson13/lesson13/#multiple-variable-extension-of-newtons-method","text":"This section presents the Newton-Raphson method as a way to sometimes solve systems of non-linear equations. Consider an example where the function \\textbf{f} is a vector-valued function of a vector argument. \\begin{gather} \\mathbf{f(x)} = \\begin{matrix} f_1 = & x^2 & +~y^2 & - 4\\\\ f_2 = & e^x & +~y & - 1\\\\ \\end{matrix} \\end{gather} Let's also recall Newtons method for scalar valued function of a single variable. \\begin{equation} x_{k+1}=x_{k} - \\frac{ f(x_{k}) }{ \\frac{df}{dx}\\rvert_{x_k} } \\label{eqn:NewtonFormula} \\end{equation} When extending to higher dimensions, the analog for x is the vector \\textbf{x} and the analog for the function f() is the vector function \\textbf{f()}. What remains is an analog for the first derivative in the denominator (and the concept of division of a matrix). The analog to the first derivative is a matrix called the Jacobian which is comprised of the first derivatives of the function \\textbf{f} with respect to the arguments \\textbf{x}. For example for a 2-value function of 2 arguments (as our example above) \\begin{equation} \\frac{df}{dx}\\rvert_{x_k} => \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\ ~ & ~ \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} \\\\ \\end{pmatrix} \\label{eqn:Jacobian} \\end{equation} Next recall that division is replaced by matrix multiplication with the multiplicative inverse, so the analogy continues as \\begin{equation} \\frac{1}{\\frac{df}{dx}\\rvert_{x_k}} => {\\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\ ~ & ~ \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2} \\\\ \\end{pmatrix}}^{-1} \\label{eqn:JacobianInverse} \\end{equation} Let's name the Jacobian \\textbf{J(x)}. So the multi-variate Newton's method can be written as \\begin{equation} \\mathbf{x}_{k+1}=\\mathbf{x}_{k} - \\mathbf{J(x)}^{-1}\\rvert_{x_k} \\cdot \\mathbf{f(x)}\\rvert_{x_k} \\label{eqn:VectorNewtonFormula} \\end{equation} In the linear systems lessons we did find a way to solve for an inverse, but it's not necessary, and is computationally expensive to invert in these examples -- a series of rearrangement of the system above yields a nice scheme that does not require inversion of a matrix. First, move the \\mathbf{x}_{k} to the left-hand side. \\begin{equation} \\mathbf{x}_{k+1}-\\mathbf{x}_{k} = - \\mathbf{J(x)}^{-1}\\rvert_{x_k} \\cdot \\mathbf{f(x)}\\rvert_{x_k} \\end{equation} Next multiply both sides by the Jacobian (The Jacobian must be non-singular otherwise we are dividing by zero) \\begin{equation} \\mathbf{J(x)}\\rvert_{x_k} \\cdot (\\mathbf{x}_{k+1}-\\mathbf{x}_{k}) = - \\mathbf{J(x)}\\rvert_{x_k} \\cdot \\mathbf{J(x)}^{-1}\\rvert_{x_k} \\cdot \\mathbf{f(x)}\\rvert_{x_k} \\end{equation} Recall a matrix multiplied by its inverse returns the identity matrix (the matrix equivalent of unity) \\begin{equation} -\\mathbf{J(x)}\\rvert_{x_k} \\cdot (\\mathbf{x}_{k+1}-\\mathbf{x}_{k}) = \\mathbf{f(x)}\\rvert_{x_k} \\end{equation} So we now have an algorithm: 1) Start with an initial guess \\mathbf{x}_{k} , compute \\mathbf{f(x)}\\rvert_{x_k} , and \\mathbf{J(x)}\\rvert_{x_k} . 2) Test for stopping. Is \\mathbf{f(x)}\\rvert_{x_k} close to zero? If yes, exit and report results, otherwise continue. 3) Solve the linear system \\mathbf{J(x)}\\rvert_{x_k} \\cdot (\\mathbf{x}_{k+1}-\\mathbf{x}_{k}) = \\mathbf{f(x)}\\rvert_{x_k} . 4) Test for stopping. Is (\\mathbf{x}_{k+1}-\\mathbf{x}_{k}) close to zero? If yes, exit and report results, otherwise continue. 5) Compute the update \\mathbf{x}_{k+1} = \\mathbf{x}_{k} - (\\mathbf{x}_{k+1}-\\mathbf{x}_{k}) , then 6) Move the update into the guess vector \\mathbf{x}_{k} <=\\mathbf{x}_{k+1} =and repeat step 1. Stop after too many steps.","title":"Multiple-variable extension of Newton\u2019s Method"},{"location":"1-Lessons/Lesson13/lesson13/#example-using-analytical-derivatives","text":"Now to complete the example we will employ this algorithm. The function (repeated) \\begin{gather} \\mathbf{f(x)} = \\begin{matrix} f_1 = & x^2 & +~y^2 & - 4\\\\ f_2 = & e^x & +~y & - 1\\\\ \\end{matrix} \\end{gather} Then the Jacobian, here we will compute it analytically because we can \\begin{equation} \\mathbf{J(x)}=> {\\begin{pmatrix} 2x & 2y \\\\ ~ & ~ \\\\ e^x & 1 \\\\ \\end{pmatrix}} \\end{equation} Now for the scripts. We will start by defining the two equations, and their derivatives, as well a a vector valued function func and its Jacobian jacob as below. Here the two modules LinearSolverPivot and vector_matrix_lib are just python source code files containing prototype functions. ################################################################# # Newton Solver Example -- Analytical Derivatives # ################################################################# import math # This will import math module from python distribution from LinearSolverPivot import linearsolver # This will import our solver module from vector_matrix_lib import writeM,writeV,vdotv,vvsub # This will import our vector functions def eq1(x,y): eq1 = x**2 + y**2 - 4.0 return(eq1) def eq2(x,y): eq2 = math.exp(x) + y - 1.0 return(eq2) def ddxeq1(x,y): ddxeq1 = 2.0*x return(ddxeq1) def ddyeq1(x,y): ddyeq1 = 2.0*y return(ddyeq1) def ddxeq2(x,y): ddxeq2 = math.exp(x) return(ddxeq2) def ddyeq2(x,y): ddyeq2 = 1.0 return(ddyeq2) def func(x,y): func = [0.0 for i in range(2)] # null list # build the function func[0] = eq1(x,y) func[1] = eq2(x,y) return(func) def jacob(x,y): jacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list #build the jacobian jacob[0][0]=ddxeq1(x,y) jacob[0][1]=ddyeq1(x,y) jacob[1][0]=ddxeq2(x,y) jacob[1][1]=ddyeq2(x,y) return(jacob) Next we create vectors to store values, and supply initial guesses to the system, and echo the inputs. deltax = [0.0 for i in range(2)] # null list xguess = [0.0 for i in range(2)] # null list myfunc = [0.0 for i in range(2)] # null list myjacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list # supply initial guess xguess[0] = float(input(\"Value for x : \")) xguess[1] = float(input(\"Value for y : \")) # build the initial function myfunc = func(xguess[0],xguess[1]) #build the initial jacobian myjacob=jacob(xguess[0],xguess[1]) #write initial results writeV(xguess,2,\"Initial X vector \") writeV(myfunc,2,\"Initial FUNC vector \") writeM(myjacob,2,2,\"Initial Jacobian \") # solver parameters tolerancef = 1.0e-9 tolerancex = 1.0e-9 Value for x : 1 Value for y : 2 ------ Initial X vector ------ 1.0 2.0 ----------------------------- ------ Initial FUNC vector ------ 1.0 3.7182818284590446 ----------------------------- ------ Initial Jacobian ------ [2.0, 4.0] [2.718281828459045, 1.0] ----------------------------- Now we apply the algorithm a few times, here the count is set to 10. So eneter the loop, test for stopping, then update. # Newton-Raphson for iteration in range(10): myfunc = func(xguess[0],xguess[1]) testf = vdotv(myfunc,myfunc,2) if testf <= tolerancef : print(\"f(x) close to zero\\n test value : \", testf) break myjacob=jacob(xguess[0],xguess[1]) deltax=linearsolver(myjacob,myfunc) testx = vdotv(deltax,deltax,2) if testx <= tolerancex : print(\"solution change small\\n test value : \", testx) break xguess=vvsub(xguess,deltax,2) ## print(\"iteration : \",iteration) ## writeV(xguess,2,\"Current X vector \") ## writeV(myfunc,2,\"Current FUNC vector \") print(\"Exiting Iteration : \",iteration) writeV(xguess,2,\"Exiting X vector \") writeV(myfunc,2,\"Exiting FUNC vector \") solution change small test value : 2.1803484657072266e-10 Exiting Iteration : 5 ------ Exiting X vector ------ -1.8162775103511606 0.8373739123240562 ----------------------------- ------ Exiting FUNC vector ------ 5.90636483064344e-05 3.926422552424924e-06 -----------------------------","title":"Example using Analytical Derivatives"},{"location":"1-Lessons/Lesson13/lesson13/#quasi-newton-method-using-finite-difference-approximations-for-the-derivative","text":"The next variant is to approximate the derivatives -- usually a Finite-Difference approximation is used, either forward, backward, or centered differences -- generally determined based on the actual behavior of the functions themselves or by trial and error. For really huge systems, we usually make the program itself make the adaptions as it proceeds. The coding for a finite-difference representation of a Jacobian is shown in Listing that follows In constructing the Jacobian, we observe that each column of the Jacobian is simply the directional derivative of the function with respect to the variable associated with the column. For instance, the first column of the Jacobian in the example is first derivative of the first function (all rows) with respect to the first variable, in this case x . The second column is the first derivative of the second function with respect to the second variable, y . This structure is useful to generalize the Jacobian construction method because we could write (yet another) prototype function that can take the directional derivatives for us, and just insert the returns as columns; in the example we simply modified the ddx and ddy functions from analytical to simple finite differences. The example listing is specific to the 2X2 function in the example, but the extension to more general cases is evident. ################################################################# # Newton Solver Example -- Numerical Derivatives # ################################################################# import math # This will import math module from python distribution from LinearSolverPivot import linearsolver # This will import our solver module from vector_matrix_lib import writeM,writeV,vdotv,vvsub # This will import our vector functions def eq1(x,y): eq1 = x**2 + y**2 - 4.0 return(eq1) def eq2(x,y): eq2 = math.exp(x) + y - 1.0 return(eq2) ############################################################## # This portion is changed for finite-difference method to evaluate derivatives # ############################################################## def ddxeq1(x,y): delta = 1.0e-6 ddxeq1 = (eq1(x+delta,y)-eq1(x,y))/delta return(ddxeq1) def ddyeq1(x,y): delta = 1.0e-6 ddyeq1 = (eq1(x,y+delta)-eq1(x,y))/delta return(ddyeq1) def ddxeq2(x,y): delta = 1.0e-6 ddxeq2 = (eq2(x+delta,y)-eq2(x,y))/delta return(ddxeq2) def ddyeq2(x,y): delta = 1.0e-6 ddyeq2 = (eq2(x,y+delta)-eq2(x,y))/delta return(ddyeq2) ############################################################## def func(x,y): func = [0.0 for i in range(2)] # null list # build the function func[0] = eq1(x,y) func[1] = eq2(x,y) return(func) def jacob(x,y): jacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list #build the jacobian jacob[0][0]=ddxeq1(x,y) jacob[0][1]=ddyeq1(x,y) jacob[1][0]=ddxeq2(x,y) jacob[1][1]=ddyeq2(x,y) return(jacob) deltax = [0.0 for i in range(2)] # null list xguess = [0.0 for i in range(2)] # null list myfunc = [0.0 for i in range(2)] # null list myjacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list # supply initial guess xguess[0] = float(input(\"Value for x : \")) xguess[1] = float(input(\"Value for y : \")) # build the initial function myfunc = func(xguess[0],xguess[1]) #build the initial jacobian myjacob=jacob(xguess[0],xguess[1]) #write initial results writeV(xguess,2,\"Initial X vector \") writeV(myfunc,2,\"Initial FUNC vector \") writeM(myjacob,2,2,\"Initial Jacobian \") # solver parameters tolerancef = 1.0e-9 tolerancex = 1.0e-9 # Newton-Raphson for iteration in range(10): myfunc = func(xguess[0],xguess[1]) testf = vdotv(myfunc,myfunc,2) if testf <= tolerancef : print(\"f(x) close to zero\\n test value : \", testf) break myjacob=jacob(xguess[0],xguess[1]) deltax=linearsolver(myjacob,myfunc) testx = vdotv(deltax,deltax,2) if testx <= tolerancex : print(\"solution change small\\n test value : \", testx) break xguess=vvsub(xguess,deltax,2) ## print(\"iteration : \",iteration) ## writeV(xguess,2,\"Current X vector \") ## writeV(myfunc,2,\"Current FUNC vector \") print(\"Exiting Iteration : \",iteration) writeV(xguess,2,\"Exiting X vector \") writeV(myfunc,2,\"Exiting FUNC vector using Finite-Differences\") Value for x : 1 Value for y : 2 ------ Initial X vector ------ 1.0 2.0 ----------------------------- ------ Initial FUNC vector ------ 1.0 3.7182818284590446 ----------------------------- ------ Initial Jacobian ------ [2.0000010003684565, 4.0000010006480125] [2.718283187874704, 1.0000000010279564] ----------------------------- solution change small test value : 2.1800662368653786e-10 Exiting Iteration : 5 ------ Exiting X vector ------ -1.8162775096127992 0.8373739116345714 ----------------------------- ------ Exiting FUNC vector using Finite-Differences ------ 5.905981145470918e-05 3.925853147235259e-06 -----------------------------","title":"Quasi-Newton Method using Finite Difference Approximations for the Derivative"},{"location":"1-Lessons/Lesson13/lesson13/#exercises","text":"Write a script that forward defines the multi-variate functions and implements the Newton-Raphson technique. Implement the method, using analytical derivatives, and find a solution to: \\begin{gather} \\begin{matrix} x^3 & +~3y^2 & = 21\\\\ x^2& +~2y & = -2 \\\\ \\end{matrix} \\end{gather} Repeat the exercise, except use finite-differences to approximate the derivatives. Write a script that forward defines the multi-variate functions and implements the Newton-Raphson technique. Implement the method, using analytical derivatives, and find a solution to: \\begin{gather} \\begin{matrix} x^2 & +~ y^2 & +~z^2 & =~ 9\\\\ ~ & ~ & xyz & =~ 1\\\\ x & +~ y & -z^2 & =~ 0\\\\ \\end{matrix} \\end{gather} Repeat the exercise, except use finite-differences to approximate the derivatives. Write a script that forward defines the multi-variate functions and implements the Newton-Raphson technique. Implement the method, using analytical derivatives, and find a solution to: \\begin{gather} \\begin{matrix} xyz & -~ x^2 & +~y^2 & =~ 1.34\\\\ ~ & xy &-~z^2 & =~ 0.09\\\\ e^x & -~ e^y & +z & =~ 0.41\\\\ \\end{matrix} \\end{gather} Repeat the exercise, except use finite-differences to approximate the derivatives.","title":"Exercises"},{"location":"1-Lessons/Lesson13/lesson13/#branched-system-3-reservoir-example","text":"Consider the branched system shown in below. In this example the friction factor is assumed constant for simplicity, but in practice would vary during the solution computations. The hydraulics question is what is the discharge in each pipe and what is the total head at the junction (notice we don't know the junction elevation in this example --- if the elevation were specified, we could also find the pressure head).","title":"Branched System 3-Reservoir Example"},{"location":"1-Lessons/Lesson13/lesson13/#solution-approach","text":"First populate the four equations with the appropriate numerical values. \\begin{equation} 70 = h_D + (0.015)\\frac{5000}{0.6}\\frac{V_{AD}|V_{AD}|}{2(9.8)} \\end{equation} \\begin{equation} 100 = h_D + (0.015)\\frac{3000}{0.8}\\frac{V_{BD}|V_{BD}|}{2(9.8)} \\end{equation} \\begin{equation} -80 = - h_D + (0.015)\\frac{4000}{1.2}\\frac{V_{DC}|V_{DC}|}{2(9.8)} \\end{equation} \\begin{equation} \\frac{\\pi(0.6)^2}{4} V_{AD}+\\frac{\\pi(0.8)^2}{4} V_{BD} = \\frac{\\pi(1.2)^2}{4} V_{DC} \\end{equation} Next, compute all the constants, and organize the 4 equations into a system of simultaneous equations \\begin{equation} \\begin{matrix} h_D & 6.377 V_{AD}|V_{AD}| & 0 & 0 & = 70\\\\ h_D & 0 & 2.869 V_{BD}|V_{BD}| & 0 & = 100\\\\ h_D & 0 & 0 & - 2.551 V_{DC}|V_{DC}| & = 80\\\\ 0 & 0.2827 V_{AD} & 0.5026 V_{BD}& -1.1309 V_{DC} & = 0\\\\ \\end{matrix} \\end{equation} Because the system is non-linear we need to employ some appropriate method, herein we will use a Quasi-Newton method with numerical approximations to derivatives. First a more conventional vector-matrix structure \\begin{gather} \\begin{pmatrix} 1 & 6.377 |V_{AD}| & 0 & 0 \\\\ 1 & 0 & 2.869 |V_{BD}| & 0 \\\\ 1 & 0 & 0 & - 2.551 |V_{DC}| \\\\ 0 & 0.2827 & 0.5026 & -1.1309 \\\\ \\end{pmatrix} \\bullet \\begin{pmatrix} h_D \\\\ V_{AD} \\\\ V_{BD} \\\\ V_{DC} \\\\ \\end{pmatrix} - \\begin{pmatrix} 70 \\\\ 100 \\\\ 80 \\\\ 0 \\\\ \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ \\end{pmatrix} \\end{gather}","title":"Solution Approach"},{"location":"1-Lessons/Lesson13/lesson13/#apply-the-multiple-variable-extension-of-newtons-method","text":"The solution can use the methods directly from ENGR-1330 and similar courses, here we just implement the methods. A very brief explaination is in the readings, but multi-variable Newton's method is usually taught in calculus. Here we will define some support modules linearsolver() and vector_matrix_lib which are just python source code containing prototype functions which are embedded into the notebook, but ordinarily would save them in separate file and use import . # SolveLinearSystem.py # Code to read A and b # Then solve Ax = b for x by Gaussian elimination with back substitution ########## def linearsolver(A,b): n = len(A) # M = A #this is object to object equivalence # copy A into M element by element M=[[0.0 for jcol in range(n)]for irow in range(n)] for irow in range(n): for jcol in range(n): M[irow][jcol]=A[irow][jcol] i = 0 for x in M: x.append(b[i]) i += 1 for k in range(n): for i in range(k,n): if abs(M[i][k]) > abs(M[k][k]): M[k], M[i] = M[i],M[k] else: pass for j in range(k+1,n): q = float(M[j][k]) / M[k][k] for m in range(k, n+1): M[j][m] -= q * M[k][m] x = [0 for i in range(n)] x[n-1] =float(M[n-1][n])/M[n-1][n-1] for i in range (n-1,-1,-1): z = 0 for j in range(i+1,n): z = z + float(M[i][j])*x[j] x[i] = float(M[i][n] - z)/M[i][i] # print (x) return(x) # vector_matrix_lib.py # useful linear algebra tools import math # This will import math module def writeM(M,ir,jc,label): print (\"------\",label,\"------\") for i in range(0,ir,1): print (M[i][0:jc]) print (\"-----------------------------\") #return def writeV(V,ir,label): print (\"------\",label,\"------\") for i in range(0,ir,1): print (V[i]) print (\"-----------------------------\") #return def mmmult(amatrix,bmatrix,rowNumA,colNumA,rowNumB,colNumB): AB =[[0.0 for j in range(colNumB)] for i in range(rowNumA)] for i in range(0,rowNumA): for j in range(0,colNumB): for k in range(0,colNumA): AB[i][j]=AB[i][j]+amatrix[i][k]*bmatrix[k][j] return(AB) def mvmult(amatrix,xvector,rowNumA,colNumA): bvector=[0.0 for i in range(rowNumA)] for i in range(0,rowNumA): for j in range(0,1): for k in range(0,colNumA): bvector[i]=bvector[i]+amatrix[i][k]*xvector[k] return(bvector) def vvadd(avector,bvector,length): aplusb=[0.0 for i in range(length)] for i in range(length): aplusb[i] = avector[i] + bvector[i] return(aplusb) def vvsub(avector,bvector,length): aminusb=[0.0 for i in range(length)] for i in range(length): aminusb[i] = avector[i] - bvector[i] return(aminusb) def vdotv(avector,bvector,length): adotb=0.0 for i in range(length): adotb=adotb+avector[i]*bvector[i] return(adotb) Next we define the four equations, as well a a vector valued function func and its Jacobian jacob as below. ################################################################# # Newton Solver Example -- Numerical Derivatives # ################################################################# import math # This will import math module from python distribution #from LinearSolverPivot import linearsolver # This will import our solver module #from vector_matrix_lib import writeM,writeV,vdotv,vvsub # This will import our vector functions def eq1(x1,x2,x3,x4): eq1 = 1*x1 + 6.377*x2 +0*x3 + 0*x4 - 70 return(eq1) def eq2(x1,x2,x3,x4): eq2 = 1*x1 + 0*x2 +2.869*x3 + 0*x4 - 100 return(eq2) def eq3(x1,x2,x3,x4): eq3 = 1*x1 + 0*x2 +0*x3 + -2.551*x4 -80 return(eq3) def eq4(x1,x2,x3,x4): eq4 = 0*x1 + 0.2827*x2 +0.5026*x3 + -1.1309*x4 - 0 return(eq4) ############################################################## def func(x1,x2,x3,x4): func = [0.0 for i in range(4)] # null list # build the function func[0] = eq1(x1,x2,x3,x4) func[1] = eq2(x1,x2,x3,x4) func[2] = eq3(x1,x2,x3,x4) func[3] = eq4(x1,x2,x3,x4) return(func) def jacob(x1,x2,x3,x4): jacob = [[0.0 for j in range(4)] for i in range(4)] # constructed list #build the jacobian jacob[0][0]=1 jacob[0][1]=6.377*x2 jacob[0][2]=0 jacob[0][3]=0 jacob[1][0]=1 jacob[1][1]=0 jacob[1][2]=2.869*x3 jacob[1][3]=0 jacob[2][0]=1 jacob[2][1]=0 jacob[2][2]=0 jacob[2][3]=-2.551*x4 jacob[3][0]=0 jacob[3][1]=0.2827 jacob[3][2]=0.5026 jacob[3][3]=-1.1309 return(jacob) Next we create vectors to store values, and supply initial guesses to the system, and echo the inputs. deltax = [0.0 for i in range(4)] # null list xguess = [0.0 for i in range(4)] # null list myfunc = [0.0 for i in range(4)] # null list myjacob = [[0.0 for j in range(4)] for i in range(4)] # constructed list # supply initial guess xguess[0] = float(input(\"Value for x1 : \")) xguess[1] = float(input(\"Value for x2 : \")) xguess[2] = float(input(\"Value for x3 : \")) xguess[3] = float(input(\"Value for x4 : \")) # build the initial function myfunc = func(xguess[0],xguess[1],xguess[2],xguess[3]) #build the initial jacobian myjacob=jacob(xguess[0],xguess[1],xguess[2],xguess[3]) #write initial results writeV(xguess,4,\"Initial X vector \") writeV(myfunc,4,\"Initial FUNC vector \") writeM(myjacob,4,4,\"Initial Jacobian \") # solver parameters tolerancef = 1.0e-9 tolerancex = 1.0e-9 # Newton-Raphson Value for x1 : 1 Value for x2 : 1 Value for x3 : 1 Value for x4 : 1 ------ Initial X vector ------ 1.0 1.0 1.0 1.0 ----------------------------- ------ Initial FUNC vector ------ -62.623 -96.131 -81.551 -0.3455999999999999 ----------------------------- ------ Initial Jacobian ------ [1, 6.377, 0, 0] [1, 0, 2.869, 0] [1, 0, 0, -2.551] [0, 0.2827, 0.5026, -1.1309] ----------------------------- Now we apply the algorithm a few times, here the count is set to 24. So eneter the loop, test for stopping, then update. for iteration in range(24): myfunc = func(xguess[0],xguess[1],xguess[2],xguess[3]) testf = vdotv(myfunc,myfunc,4) if testf <= tolerancef : print(\"f(x) close to zero\\n test value : \", testf) break myjacob=jacob(xguess[0],xguess[1],xguess[2],xguess[3]) deltax=linearsolver(myjacob,myfunc) testx = vdotv(deltax,deltax,4) if testx <= tolerancex : print(\"solution change small\\n test value : \", testx) break xguess=vvsub(xguess,deltax,4) ## print(\"iteration : \",iteration) ## writeV(xguess,2,\"Current X vector \") ## writeV(myfunc,2,\"Current FUNC vector \") print(\"Exiting Iteration : \",iteration) writeV(xguess,4,\"Exiting X vector \") writeV(myfunc,4,\"Exiting FUNC vector using Finite-Differences\") f(x) close to zero test value : 2.0273725264180003e-28 Exiting Iteration : 1 ------ Exiting X vector ------ 84.61708956275575 -2.292157685864162 5.361767318663032 1.8099135879089594 ----------------------------- ------ Exiting FUNC vector using Finite-Differences ------ -1.4210854715202004e-14 0.0 0.0 -8.881784197001252e-16 ----------------------------- Now tidy up the output, convert back to discharges (from velocities) and report results. print(\"Head at Junction : %.3f\" % xguess[0], \" feet \") print(\"Discharge in Pipe AD: %.3f\" % (0.2827 * xguess[1]), \" cubic feet per second\") print(\"Discharge in Pipe BD: %.3f\" % (0.5026 * xguess[2]), \" cubic feet per second \") print(\"Discharge in Pipe DC: %.3f\" % (1.1309 * xguess[3]), \" cubic feet per second \") Head at Junction : 84.617 feet Discharge in Pipe AD: -0.648 cubic feet per second Discharge in Pipe BD: 2.695 cubic feet per second Discharge in Pipe DC: 2.047 cubic feet per second","title":"Apply the multiple-variable Extension of Newton\u2019s Method"},{"location":"1-Lessons/Lesson15/lesson15/","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 4 Jun 2021\u00df Lesson 15 Descriptive Statistics A fundamental part of working with data is describing it. Descriptive statistics help simplify and summarize large amounts of data in a sensible manner. The ultimate goal is to be able to explain observed behavior with a model (like \\textbf{F} = m\\textbf{a} ) so that the model can be used to predict behavior. Objectives To understand the fundamental concepts involved in measurements of a data collection; Central tendency Dispersion Assymmetry Comparison of two sets of data, are they the same? Computational Thinking Concepts The CT concepts include: Abstraction => Represent indvidual behavior with a generalization (mean, median, deviation, \\dots ) Algorithm Design => Simulation Descriptive Statistics with Python In this lecture, we will discuss descriptive statistics and cover a variety of methods for summarizing, describing, and representing datasets in Python. The contents of this notebook are inspired by various online resources including the following links: - \"Descriptive statistics with Python-NumPy\" by Rashmi Jain , available @ https://www.hackerearth.com/blog/developers/descriptive-statistics-python-numpy/. \"Python Statistics Fundamentals: How to Describe Your Data\" by Mirko Stojiljkovi\u0107 , available @ https://realpython.com/python-statistics/. \"A Quick Guide on Descriptive Statistics using Pandas and Seaborn\" by Bee Guan Teo , available @ https://towardsdatascience.com/a-quick-guide-on-descriptive-statistics-using-pandas-and-seaborn-2aadc7395f32. \"Tutorial: Basic Statistics in Python \u2014 Descriptive Statistics\" , available @ https://www.dataquest.io/blog/basic-statistics-with-python-descriptive-statistics/. First lets start with fabricated data. Suppose we made 10,000 observations of some real variable which we named series1 . Then later we made another 10,000 observations on the same variable named series2 . Are the two series similar? How can we quickly quantify? Below is a script, that simulates this situation - parts are left unexplained for now. The script produces the two series, and makes a histogram of the two series. # Example import math import matplotlib.pyplot # the python plotting library import numpy as np import pandas as pd series1 = np.random.normal(0,1.9,10000)# syntax is func(mean,variance,how_many_things) series2 = np.random.normal(6,1,10000)# syntax is func(mean,variance,how_many_things) mydata={'s1':series1,'s2':series2} mydata=pd.DataFrame.from_dict(mydata) mydata.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } s1 s2 count 10000.000000 10000.000000 mean 0.017907 5.984835 std 1.902299 0.994961 min -9.011004 2.463720 25% -1.274411 5.314034 50% 0.012283 6.001274 75% 1.293006 6.649160 max 8.410946 9.768979 Now lets plot the two collections: myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.hist(series1, color ='blue', bins = 100) matplotlib.pyplot.hist(series2, color ='red', bins = 100) matplotlib.pyplot.xlabel(\"Value of RV\") matplotlib.pyplot.ylabel(\"Relative Frequency or Count\") matplotlib.pyplot.title(\"Example Data\") matplotlib.pyplot.show() We will use the \"HighestGrossingMovies.csv\" dataset as an illustrative example. Let's have a look at it first. #Import the necessary external packages import numpy as np import pandas as pd Movies = pd.read_csv(\"HighestGrossingMovies.csv\") #Dataset of the Top10 highest-grossing films as of 2019 (adjusted for inflation) #5 columns (Movie, Director, Year, Budget, Gross) and 10 rows Movies .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Movie Director Year Budget_million$ Gross_million$ 0 Gone with the Wind Victor Fleming 1939 3.9 3706 1 Avatar James Cameron 2009 237.0 3257 2 Titanic James Cameron 1997 200.0 3081 3 Star Wars George Lucas 1977 11.0 3043 4 Avengers: Endgame Joe & Anthony Russo 2019 356.0 2798 5 The Sound of Music Robert Wise 1965 8.2 2549 6 E.T. the Extra-Terrestrial Steven Spielberg 1982 10.5 2489 7 The Ten Commandments Cecil B. DeMille 1956 13.0 2356 8 Doctor Zhivago David Lean 1965 11.0 2233 9 Star Wars: The Force Awakens J.J. Abrams 2015 306.0 2202 Here is an overall look at some but not all of measures we will be discussing today: Measures of Central Tendency Centrality measures give us an estimate of the center of a distribution and a sense of a typical value we would expect to see. The three major measures of center include the mean, median, and mode . Mean Mean aka arithmetic mean aka average is the sum of all the values, divided by the number of values. Mean represents the typical value that acts as a yardstick for all observations. Let's calculate the average budget of the Top10 highest-grossing films. Budget = Movies['Budget_million$'] Budget 0 3.9 1 237.0 2 200.0 3 11.0 4 356.0 5 8.2 6 10.5 7 13.0 8 11.0 9 306.0 Name: Budget_million$, dtype: float64 We can use primitive python to calculate the mean of set of numbers: # Create a list of all the numbers: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] mean1 = sum(budget) / len(budget) print(\"The average budget of the Top10 highest-grossing films is \",mean1,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD We can also utilize a variety of external libraries. (You may find some of them familiar!) # The usual suspects! import numpy as np import pandas as pd # Also, these two libraries offer useful functions for descriptive statistics import statistics import scipy.stats # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the Pandas library mean2 = Budget.mean() print(\"The average budget of the Top10 highest-grossing films is \",mean2,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the Numpy library mean3 = np.mean(Budget) print(\"The average budget of the Top10 highest-grossing films is \",mean3,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the statistics library mean4 = statistics.mean(Budget) print(\"The average budget of the Top10 highest-grossing films is \",mean4,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD Harmonic Mean The harmonic mean is the reciprocal of the mean of the reciprocals of all items in the dataset. Let's calculate the harmonic mean for the same set of numbers: # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] hmean1 = len(budget) / sum(1 / item for item in budget) hmean1 = round(hmean1,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] hmean2 = statistics.harmonic_mean(Budget) hmean2 = round(hmean2,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean2,\"million USD\") # via the scipy.stats library: Budget = Movies['Budget_million$'] hmean3 = scipy.stats.hmean(Budget) hmean3 = round(hmean3,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean3,\"million USD\") The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD Geometric Mean The geometric mean is the \ud835\udc5b-th root of the product of all \ud835\udc5b elements \ud835\udc65\u1d62 in a dataset. Let's calculate the geometric mean for the same set of numbers: # Primitive Python: -it is getting more lengthy and labour-intensive budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] gmean1 = 1 for item in budget: gmean1 *= item gmean1 **= 1 / len(budget) gmean1 = round(gmean1,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] gmean2 = statistics.geometric_mean(Budget) gmean2 = round(gmean2,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean2,\"million USD\") # via the scipy.stats library: Budget = Movies['Budget_million$'] gmean3 = scipy.stats.gmean(Budget) gmean3 = round(gmean3,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean3,\"million USD\") The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD Arithmetic or Geometric or Harmonic?- How to be Mean! If values have the same units: Use the arithmetic mean. If values have differing units: Use the geometric mean. | Also, commonly used for growth rates, like population growth or interest rates. If values are rates: Use the harmonic mean. If you are interested in knowing more about these 3 and their differences, you may find these interesting: - \"Arithmetic, Geometric, and Harmonic Means for Machine Learning Arithmetic, Geometric, and Harmonic Means for Machine Learning\" by Jason Brownlee , available @ https://machinelearningmastery.com/arithmetic-geometric-and-harmonic-means-for-machine-learning/#:~:text=The%20arithmetic%20mean%20is%20appropriate,with%20different%20measures%2C%20called%20rates. \"On Average, You\u2019re Using the Wrong Average: Geometric & Harmonic Means in Data Analysis\" by Daniel McNichol , available @ https://towardsdatascience.com/on-average-youre-using-the-wrong-average-geometric-harmonic-means-in-data-analysis-2a703e21ea0 Median Median is the middle element of a sorted dataset. The value where the upper half of the data lies above it and lower half lies below it. In other words, it is the middle value of a data set. To calculate the median, arrange the data points in the increasing (or decreasing) order and the middle value is the median. If the number of elements \ud835\udc5b of the dataset is odd, then the median is the value at the middle position: 0.5(\ud835\udc5b + 1). If \ud835\udc5b is even, then the median is the arithmetic mean of the two values in the middle, that is, the items at the positions 0.5\ud835\udc5b and 0.5\ud835\udc5b + 1. Let's find the median of the gross of the Top10 highest-grossing films: Gross = Movies['Gross_million$'] Gross 0 3706 1 3257 2 3081 3 3043 4 2798 5 2549 6 2489 7 2356 8 2233 9 2202 Name: Gross_million$, dtype: int64 We can use primitive python to calculate the median of a set of numbers: # Create a list of all the numbers: gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202] n = len(gross) if n % 2: median1 = sorted(gross)[round(0.5*(n-1))] else: gross_ord, index = sorted(gross), round(0.5 * n) median1 = 0.5 * (gross_ord[index-1] + gross_ord[index]) print(\"The median of gross of the Top10 highest-grossing films is \",median1,\"million USD\") The median of gross of the Top10 highest-grossing films is 2673.5 million USD We can use also use external libraries: #via the Pandas library: Gross = Movies['Gross_million$'] median2 = Gross.median() print(\"The median of gross of the Top10 highest-grossing films is \",median2,\"million USD\") #via the Numpy library: Gross = Movies['Gross_million$'] median3 = np.median(Gross) print(\"The median of gross of the Top10 highest-grossing films is \",median3,\"million USD\") #via the Statistics library: Gross = Movies['Gross_million$'] median4 = statistics.median(Gross) print(\"The median of gross of the Top10 highest-grossing films is \",median4,\"million USD\") #2 more functions from the same library- For even number of cases: print(\"low median :\",statistics.median_low(Gross)) print(\"high median :\",statistics.median_high(Gross)) The median of gross of the Top10 highest-grossing films is 2673.5 million USD The median of gross of the Top10 highest-grossing films is 2673.5 million USD The median of gross of the Top10 highest-grossing films is 2673.5 million USD low median : 2549 high median : 2798 The main difference between the behavior of the mean and median is related to dataset outliers or extremes. The mean is heavily affected by outliers, but the median only depends on outliers either slightly or not at all. You can compare the mean and median as one way to detect outliers and asymmetry in your data. Whether the mean value or the median value is more useful to you depends on the context of your particular problem. The mean is a better choice when there are no extreme values that can affect it. It is a better summary because the information from every observation is included rather than median, which is just the middle value. However, in the presence of outliers, median is considered a better alternative. Check this out: newgross = [99999,3257,3081,3043,2798,2549,2489,2356,2233,2202] #We have replaced 3706 with 99999- an extremely high number (an outlier) newmean = np.mean(newgross) newmedian = np.median(newgross) print(newmean) #A huge change from the previous value (115.66) - Mean is very sensitive to outliers and extreme values print(newmedian) #No Change- the median only depends on outliers either slightly or not at all. 12400.7 2673.5 To read more about the differences of mean and median, check these out: - \"Stuck in the middle \u2013 mean vs. median\" , available @ https://www.clinfo.eu/mean-median/ \"Mean vs Median: When to Use Which Measure?\" , available @ https://www.datascienceblog.net/post/basic-statistics/mean_vs_median/ \"Mean vs. Median\" by AnswerMiner , available @ https://www.answerminer.com/blog/mean-vs-median Mode The value that occurs the most number of times in our data set. Closely tied to the concept of frequency, mode provides information on the most recurrent elements in a dataset. When the mode is not unique, we say that the data set is bimodal, while a data set with more than two modes is multimodal. Let's find the mode in the gross of the Top10 highest-grossing films: # In primitive Python: # Create a list of all the numbers: gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202] mode1 = max((gross.count(item), item) for item in gross)[1] print(mode1) #Since each item is repeated only once, only the first element is printed- This is a multimodal set. #via the Pandas library: Gross = Movies['Gross_million$'] mode2 = Gross.mode() print(mode2) #Returns all modal values- This is a multimodal set. #via the Statistics library: Gross = Movies['Gross_million$'] mode3 = statistics.mode(Gross) print(mode3) #Return a single value mode4 = statistics.multimode(Gross) print(mode4) #Returns a list of all modes #via the scipy.stats library: Gross = Movies['Gross_million$'] mode5 = scipy.stats.mode(Gross) print(mode5) #Returns the object with the modal value and the number of times it occurs- If multimodal: only the smallest value 3706 0 2202 1 2233 2 2356 3 2489 4 2549 5 2798 6 3043 7 3081 8 3257 9 3706 dtype: int64 3706 [3706, 3257, 3081, 3043, 2798, 2549, 2489, 2356, 2233, 2202] ModeResult(mode=array([2202]), count=array([1])) Mode is not useful when our distribution is flat; i.e., the frequencies of all groups are similar. Mode makes sense when we do not have a numeric-valued data set which is required in case of the mean and the median. For instance: Director = Movies['Director'] # via statistics: mode6 = statistics.mode(Director) print(mode6) #\"James Cameron\" with two films (x2 repeats) is the mode # via pandas: mode7 = Director.mode() print(mode7) #\"James Cameron\" with two films (x2 repeats) is the mode James Cameron 0 James Cameron dtype: object To read more about mode, check these out: - \"Mode: A statistical measure of central tendency\" , available @ https://corporatefinanceinstitute.com/resources/knowledge/other/mode/ \"When to use each measure of Central Tendency\" , available @ https://courses.lumenlearning.com/introstats1/chapter/when-to-use-each-measure-of-central-tendency/ \"Mean, Median, Mode: What They Are, How to Find Them\" , available @ https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/ Measures of Dispersion Measures of dispersion are values that describe how the data varies. It gives us a sense of how much the data tends to diverge from the typical value. Aka measures of variability, they quantify the spread of data points.The major measures of dispersion include range, percentiles, inter-quentile range, variance, standard deviation, skeness and kurtosis . Range The range gives a quick sense of the spread of the distribution to those who require only a rough indication of the data. There are some disadvantages of using the range as a measure of spread. One being it does not give any information of the data in between maximum and minimum. Also, the range is very sensitive to extreme values. Let's calculate the range for the budget of the Top10 highest-grossing films: # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] range1 = max(budget)-min(budget) print(\"The range of the budget of the Top10 highest-grossing films is \",range1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] range2 = np.ptp(Budget) #ptp stands for Peak To Peak print(\"The range of the budget of the Top10 highest-grossing films is \",range2,\"million USD\") The range of the budget of the Top10 highest-grossing films is 352.1 million USD The range of the budget of the Top10 highest-grossing films is 352.1 million USD Percentiles and Quartiles A measure which indicates the value below which a given percentage of points in a dataset fall. The sample \ud835\udc5d percentile is the element in the dataset such that \ud835\udc5d% of the elements in the dataset are less than or equal to that value. Also, (100 \u2212 \ud835\udc5d)% of the elements are greater than or equal to that value. For example, median represents the 50th percentile. Similarly, we can have 0th percentile representing the minimum and 100th percentile representing the maximum of all data points. Percentile gives the relative position of a particular value within the dataset. It also helps in comparing the data sets which have different means and deviations. Each dataset has three quartiles, which are the percentiles that divide the dataset into four parts: The first quartile (Q1) is the sample 25th percentile. It divides roughly 25% of the smallest items from the rest of the dataset. The second quartile Q2) is the sample 50th percentile or the median. Approximately 25% of the items lie between the first and second quartiles and another 25% between the second and third quartiles. The third quartile (Q3) is the sample 75th percentile. It divides roughly 25% of the largest items from the rest of the dataset. Budget = Movies['Budget_million$'] #via Numpy: p10 = np.percentile(Budget, 10) #returns the 10th percentile print(\"The 10th percentile of the budget of the Top10 highest-grossing films is \",p10) p4070 = np.percentile(Budget, [40,70]) #returns the 40th and 70th percentile print(\"The 40th and 70th percentile of the budget of the Top10 highest-grossing films are \",p4070) #via Pandas: p10n = Budget.quantile(0.10) #returns the 10th percentile - notice the difference from Numpy print(\"The 10th percentile of the budget of the Top10 highest-grossing films is \",p10n) #via Statistics: Qs = statistics.quantiles(Budget, n=4, method='inclusive') #The parameter n defines the number of resulting equal-probability percentiles: #n=4 returns the quartiles | n=2 returns the median print(\"The quartiles of the budget of the Top10 highest-grossing films is \",Qs) The 10th percentile of the budget of the Top10 highest-grossing films is 7.77 The 40th and 70th percentile of the budget of the Top10 highest-grossing films are [ 11. 211.1] The 10th percentile of the budget of the Top10 highest-grossing films is 7.77 The quartiles of the budget of the Top10 highest-grossing films is [10.625, 12.0, 227.75] InterQuartile Range (IQR) IQR is the difference between the third quartile and the first quartile (Q3-Q1). The interquartile range is a better option than range because it is not affected by outliers. It removes the outliers by just focusing on the distance within the middle 50% of the data. Budget = Movies['Budget_million$'] #via Numpy: IQR1 = np.percentile(Budget, 75) -np.percentile(Budget, 25) #returns the IQR = Q3-Q1 = P75-P25 print(\"The IQR of the budget of the Top10 highest-grossing films is \",IQR1) #via scipy.stats: IQR2 = scipy.stats.iqr(Budget) #returns the IQR- Can be used for other percentile differences as well >> iqr(object, rng=(p1, p2)) print(\"The IQR of the budget of the Top10 highest-grossing films is \",IQR2) The IQR of the budget of the Top10 highest-grossing films is 217.125 The IQR of the budget of the Top10 highest-grossing films is 217.125 The Five-number Summary A five-number summary is especially useful in descriptive analyses or during the preliminary investigation of a large data set. A summary consists of five values: the most extreme values in the data set (the maximum and minimum values), the lower and upper quartiles, and the median. Five-number summary can be used to describe any data distribution. Boxplots are extremely useful graphical representation of the 5-number summary that we will discuss later. Budget = Movies['Budget_million$'] Budget.describe() #Remember this jewel from Pandas? -It directly return the 5-number summary AND MORE! count 10.000000 mean 115.660000 std 142.739991 min 3.900000 25% 10.625000 50% 12.000000 75% 227.750000 max 356.000000 Name: Budget_million$, dtype: float64 Boxplots are extremely useful graphical representation of the 5-number summary. It can show the range, interquartile range, median, mode, outliers, and all quartiles. import matplotlib.pyplot as plt #Required for the plot gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202,5000] #same data + an outlier: 5000 fig = plt.figure(figsize =(7, 5)) plt.boxplot(gross,medianprops={'linewidth': 1, 'color': 'purple'}) plt.show() To read more about the 5-number summary, check these out: - \"Find a Five-Number Summary in Statistics: Easy Steps\" , available @ https://www.statisticshowto.com/how-to-find-a-five-number-summary-in-statistics/ \"The Five-Number Summary\" , available @ https://www.purplemath.com/modules/boxwhisk2.htm \"What Is the 5 Number Summary?\" by Courtney Taylor , available @ https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/ Variance The sample variance quantifies the spread of the data. It shows numerically how far the data points are from the mean. The observations may or may not be meaningful if observations in data sets are highly spread. Let's calculate the variance for budget of the Top10 highest-grossing films. Note that if we are working with the entire population (and not the sample), the denominator should be \"n\" instead of \"n-1\". Note that if we are working with the entire population (and not the sample), the denominator should be \"n\" instead of \"n-1\". # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var1 = sum((item - mean)**2 for item in budget) / (n - 1) print(\"The variance of the budget of the Top10 highest-grossing films is \",var1) # via the Statistics library: Budget = Movies['Budget_million$'] var2 = statistics.variance(Budget) print(\"The variance of the budget of the Top10 highest-grossing films is \",var2) The variance of the budget of the Top10 highest-grossing films is 20374.70488888889 The variance of the budget of the Top10 highest-grossing films is 20374.70488888889 Standard Deviation The sample standard deviation is another measure of data spread. It\u2019s connected to the sample variance, as standard deviation, \ud835\udc60, is the positive square root of the sample variance. The standard deviation is often more convenient than the variance because it has the same unit as the data points. # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var = sum((item - mean)**2 for item in budget) / (n - 1) sd1 = var**0.5 print(\"The standard deviation of the budget of the Top10 highest-grossing films is \",sd1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] sd2 = statistics.stdev(Budget) print(\"The standard deviation of the budget of the Top10 highest-grossing films is \",sd2,\"million USD\") The standard deviation of the budget of the Top10 highest-grossing films is 142.73999050332353 million USD The standard deviation of the budget of the Top10 highest-grossing films is 142.73999050332353 million USD Skewness The sample skewness measures the asymmetry of a data sample. There are several mathematical definitions of skewness. The Fisher-Pearson standardized moment coefficient is calculated by using mean, median and standard deviation of the data. Usually, negative skewness values indicate that there\u2019s a dominant tail on the left side. Positive skewness values correspond to a longer or fatter tail on the right side. If the skewness is close to 0 (for example, between \u22120.5 and 0.5), then the dataset is considered quite symmetrical. # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var = sum((item - mean)**2 for item in budget) / (n - 1) std = var**0.5 skew1 = (sum((item - mean)**3 for item in budget) * n / ((n - 1) * (n - 2) * std**3)) print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew1) # via the scipy.stats library: Budget = Movies['Budget_million$'] skew2 = scipy.stats.skew(Budget, bias=False) print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew2) # via the Pandas library: Budget = Movies['Budget_million$'] skew3 = Budget.skew() print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew3) The skewness of the budget of the Top10 highest-grossing films is 0.7636547490528159 The skewness of the budget of the Top10 highest-grossing films is 0.763654749052816 The skewness of the budget of the Top10 highest-grossing films is 0.763654749052816 Kurtosis Kurtosis describes the peakedness of the distribution. In other words, Kurtosis identifies whether the tails of a given distribution contain extreme values. While Skewness essentially measures the symmetry of the distribution, kurtosis determines the heaviness of the distribution tails. If the distribution is tall and thin it is called a leptokurtic distribution. Values in a leptokurtic distribution are near the mean or at the extremes. A flat distribution where the values are moderately spread out (i.e., unlike leptokurtic) is called platykurtic distribution. A distribution whose shape is in between a leptokurtic distribution and a platykurtic distribution is called a mesokurtic distribution. # via the scipy.stats library: Budget = Movies['Budget_million$'] Kurt = scipy.stats.kurtosis(Budget) print(\"The kurtosis of the budget of the Top10 highest-grossing films is \",Kurt) #a platykurtic distribution | the tails are heavy The kurtosis of the budget of the Top10 highest-grossing films is -1.3110307923262225 To read more about skewness and kurtosis, check these out: - \"Measures of Skewness and Kurtosis\" , available @ https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm#:~:text=Skewness%20is%20a%20measure%20of,relative%20to%20a%20normal%20distribution. \"Are the Skewness and Kurtosis Useful Statistics?\" , available @ https://www.spcforexcel.com/knowledge/basic-statistics/are-skewness-and-kurtosis-useful-statistics \"Skew and Kurtosis: 2 Important Statistics terms you need to know in Data Science\" by Diva Dugar , available @ https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa \"Measures of Shape: Skewness and Kurtosis\" by Stan Brown , available @ https://brownmath.com/stat/shape.htm","title":"Descriptive Statistics"},{"location":"1-Lessons/Lesson15/lesson15/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 4 Jun 2021\u00df","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson15/lesson15/#lesson-15-descriptive-statistics","text":"A fundamental part of working with data is describing it. Descriptive statistics help simplify and summarize large amounts of data in a sensible manner. The ultimate goal is to be able to explain observed behavior with a model (like \\textbf{F} = m\\textbf{a} ) so that the model can be used to predict behavior.","title":"Lesson 15 Descriptive Statistics"},{"location":"1-Lessons/Lesson15/lesson15/#objectives","text":"To understand the fundamental concepts involved in measurements of a data collection; Central tendency Dispersion Assymmetry Comparison of two sets of data, are they the same?","title":"Objectives"},{"location":"1-Lessons/Lesson15/lesson15/#computational-thinking-concepts","text":"The CT concepts include: Abstraction => Represent indvidual behavior with a generalization (mean, median, deviation, \\dots ) Algorithm Design => Simulation","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson15/lesson15/#descriptive-statistics-with-python","text":"In this lecture, we will discuss descriptive statistics and cover a variety of methods for summarizing, describing, and representing datasets in Python. The contents of this notebook are inspired by various online resources including the following links: - \"Descriptive statistics with Python-NumPy\" by Rashmi Jain , available @ https://www.hackerearth.com/blog/developers/descriptive-statistics-python-numpy/. \"Python Statistics Fundamentals: How to Describe Your Data\" by Mirko Stojiljkovi\u0107 , available @ https://realpython.com/python-statistics/. \"A Quick Guide on Descriptive Statistics using Pandas and Seaborn\" by Bee Guan Teo , available @ https://towardsdatascience.com/a-quick-guide-on-descriptive-statistics-using-pandas-and-seaborn-2aadc7395f32. \"Tutorial: Basic Statistics in Python \u2014 Descriptive Statistics\" , available @ https://www.dataquest.io/blog/basic-statistics-with-python-descriptive-statistics/. First lets start with fabricated data. Suppose we made 10,000 observations of some real variable which we named series1 . Then later we made another 10,000 observations on the same variable named series2 . Are the two series similar? How can we quickly quantify? Below is a script, that simulates this situation - parts are left unexplained for now. The script produces the two series, and makes a histogram of the two series. # Example import math import matplotlib.pyplot # the python plotting library import numpy as np import pandas as pd series1 = np.random.normal(0,1.9,10000)# syntax is func(mean,variance,how_many_things) series2 = np.random.normal(6,1,10000)# syntax is func(mean,variance,how_many_things) mydata={'s1':series1,'s2':series2} mydata=pd.DataFrame.from_dict(mydata) mydata.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } s1 s2 count 10000.000000 10000.000000 mean 0.017907 5.984835 std 1.902299 0.994961 min -9.011004 2.463720 25% -1.274411 5.314034 50% 0.012283 6.001274 75% 1.293006 6.649160 max 8.410946 9.768979 Now lets plot the two collections: myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.hist(series1, color ='blue', bins = 100) matplotlib.pyplot.hist(series2, color ='red', bins = 100) matplotlib.pyplot.xlabel(\"Value of RV\") matplotlib.pyplot.ylabel(\"Relative Frequency or Count\") matplotlib.pyplot.title(\"Example Data\") matplotlib.pyplot.show() We will use the \"HighestGrossingMovies.csv\" dataset as an illustrative example. Let's have a look at it first. #Import the necessary external packages import numpy as np import pandas as pd Movies = pd.read_csv(\"HighestGrossingMovies.csv\") #Dataset of the Top10 highest-grossing films as of 2019 (adjusted for inflation) #5 columns (Movie, Director, Year, Budget, Gross) and 10 rows Movies .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Movie Director Year Budget_million$ Gross_million$ 0 Gone with the Wind Victor Fleming 1939 3.9 3706 1 Avatar James Cameron 2009 237.0 3257 2 Titanic James Cameron 1997 200.0 3081 3 Star Wars George Lucas 1977 11.0 3043 4 Avengers: Endgame Joe & Anthony Russo 2019 356.0 2798 5 The Sound of Music Robert Wise 1965 8.2 2549 6 E.T. the Extra-Terrestrial Steven Spielberg 1982 10.5 2489 7 The Ten Commandments Cecil B. DeMille 1956 13.0 2356 8 Doctor Zhivago David Lean 1965 11.0 2233 9 Star Wars: The Force Awakens J.J. Abrams 2015 306.0 2202 Here is an overall look at some but not all of measures we will be discussing today:","title":"Descriptive Statistics with Python"},{"location":"1-Lessons/Lesson15/lesson15/#measures-of-central-tendency","text":"Centrality measures give us an estimate of the center of a distribution and a sense of a typical value we would expect to see. The three major measures of center include the mean, median, and mode .","title":"Measures of Central Tendency"},{"location":"1-Lessons/Lesson15/lesson15/#mean","text":"Mean aka arithmetic mean aka average is the sum of all the values, divided by the number of values. Mean represents the typical value that acts as a yardstick for all observations. Let's calculate the average budget of the Top10 highest-grossing films. Budget = Movies['Budget_million$'] Budget 0 3.9 1 237.0 2 200.0 3 11.0 4 356.0 5 8.2 6 10.5 7 13.0 8 11.0 9 306.0 Name: Budget_million$, dtype: float64 We can use primitive python to calculate the mean of set of numbers: # Create a list of all the numbers: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] mean1 = sum(budget) / len(budget) print(\"The average budget of the Top10 highest-grossing films is \",mean1,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD We can also utilize a variety of external libraries. (You may find some of them familiar!) # The usual suspects! import numpy as np import pandas as pd # Also, these two libraries offer useful functions for descriptive statistics import statistics import scipy.stats # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the Pandas library mean2 = Budget.mean() print(\"The average budget of the Top10 highest-grossing films is \",mean2,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the Numpy library mean3 = np.mean(Budget) print(\"The average budget of the Top10 highest-grossing films is \",mean3,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD # Read the column of interest from the Movies dataframe Budget = Movies['Budget_million$'] # Use the mean function from the statistics library mean4 = statistics.mean(Budget) print(\"The average budget of the Top10 highest-grossing films is \",mean4,\"million USD\") The average budget of the Top10 highest-grossing films is 115.66 million USD","title":"Mean"},{"location":"1-Lessons/Lesson15/lesson15/#harmonic-mean","text":"The harmonic mean is the reciprocal of the mean of the reciprocals of all items in the dataset. Let's calculate the harmonic mean for the same set of numbers: # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] hmean1 = len(budget) / sum(1 / item for item in budget) hmean1 = round(hmean1,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] hmean2 = statistics.harmonic_mean(Budget) hmean2 = round(hmean2,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean2,\"million USD\") # via the scipy.stats library: Budget = Movies['Budget_million$'] hmean3 = scipy.stats.hmean(Budget) hmean3 = round(hmean3,2) print(\"The harmonic mean of the budget of the Top10 highest-grossing films is \",hmean3,\"million USD\") The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD The harmonic mean of the budget of the Top10 highest-grossing films is 13.38 million USD","title":"Harmonic Mean"},{"location":"1-Lessons/Lesson15/lesson15/#geometric-mean","text":"The geometric mean is the \ud835\udc5b-th root of the product of all \ud835\udc5b elements \ud835\udc65\u1d62 in a dataset. Let's calculate the geometric mean for the same set of numbers: # Primitive Python: -it is getting more lengthy and labour-intensive budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] gmean1 = 1 for item in budget: gmean1 *= item gmean1 **= 1 / len(budget) gmean1 = round(gmean1,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] gmean2 = statistics.geometric_mean(Budget) gmean2 = round(gmean2,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean2,\"million USD\") # via the scipy.stats library: Budget = Movies['Budget_million$'] gmean3 = scipy.stats.gmean(Budget) gmean3 = round(gmean3,2) print(\"The geometric mean of the budget of the Top10 highest-grossing films is \",gmean3,\"million USD\") The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD The geometric mean of the budget of the Top10 highest-grossing films is 34.96 million USD","title":"Geometric Mean"},{"location":"1-Lessons/Lesson15/lesson15/#arithmetic-or-geometric-or-harmonic-how-to-be-mean","text":"If values have the same units: Use the arithmetic mean. If values have differing units: Use the geometric mean. | Also, commonly used for growth rates, like population growth or interest rates. If values are rates: Use the harmonic mean. If you are interested in knowing more about these 3 and their differences, you may find these interesting: - \"Arithmetic, Geometric, and Harmonic Means for Machine Learning Arithmetic, Geometric, and Harmonic Means for Machine Learning\" by Jason Brownlee , available @ https://machinelearningmastery.com/arithmetic-geometric-and-harmonic-means-for-machine-learning/#:~:text=The%20arithmetic%20mean%20is%20appropriate,with%20different%20measures%2C%20called%20rates. \"On Average, You\u2019re Using the Wrong Average: Geometric & Harmonic Means in Data Analysis\" by Daniel McNichol , available @ https://towardsdatascience.com/on-average-youre-using-the-wrong-average-geometric-harmonic-means-in-data-analysis-2a703e21ea0","title":"Arithmetic or Geometric or Harmonic?- How to be Mean!"},{"location":"1-Lessons/Lesson15/lesson15/#median","text":"Median is the middle element of a sorted dataset. The value where the upper half of the data lies above it and lower half lies below it. In other words, it is the middle value of a data set. To calculate the median, arrange the data points in the increasing (or decreasing) order and the middle value is the median. If the number of elements \ud835\udc5b of the dataset is odd, then the median is the value at the middle position: 0.5(\ud835\udc5b + 1). If \ud835\udc5b is even, then the median is the arithmetic mean of the two values in the middle, that is, the items at the positions 0.5\ud835\udc5b and 0.5\ud835\udc5b + 1. Let's find the median of the gross of the Top10 highest-grossing films: Gross = Movies['Gross_million$'] Gross 0 3706 1 3257 2 3081 3 3043 4 2798 5 2549 6 2489 7 2356 8 2233 9 2202 Name: Gross_million$, dtype: int64 We can use primitive python to calculate the median of a set of numbers: # Create a list of all the numbers: gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202] n = len(gross) if n % 2: median1 = sorted(gross)[round(0.5*(n-1))] else: gross_ord, index = sorted(gross), round(0.5 * n) median1 = 0.5 * (gross_ord[index-1] + gross_ord[index]) print(\"The median of gross of the Top10 highest-grossing films is \",median1,\"million USD\") The median of gross of the Top10 highest-grossing films is 2673.5 million USD We can use also use external libraries: #via the Pandas library: Gross = Movies['Gross_million$'] median2 = Gross.median() print(\"The median of gross of the Top10 highest-grossing films is \",median2,\"million USD\") #via the Numpy library: Gross = Movies['Gross_million$'] median3 = np.median(Gross) print(\"The median of gross of the Top10 highest-grossing films is \",median3,\"million USD\") #via the Statistics library: Gross = Movies['Gross_million$'] median4 = statistics.median(Gross) print(\"The median of gross of the Top10 highest-grossing films is \",median4,\"million USD\") #2 more functions from the same library- For even number of cases: print(\"low median :\",statistics.median_low(Gross)) print(\"high median :\",statistics.median_high(Gross)) The median of gross of the Top10 highest-grossing films is 2673.5 million USD The median of gross of the Top10 highest-grossing films is 2673.5 million USD The median of gross of the Top10 highest-grossing films is 2673.5 million USD low median : 2549 high median : 2798 The main difference between the behavior of the mean and median is related to dataset outliers or extremes. The mean is heavily affected by outliers, but the median only depends on outliers either slightly or not at all. You can compare the mean and median as one way to detect outliers and asymmetry in your data. Whether the mean value or the median value is more useful to you depends on the context of your particular problem. The mean is a better choice when there are no extreme values that can affect it. It is a better summary because the information from every observation is included rather than median, which is just the middle value. However, in the presence of outliers, median is considered a better alternative. Check this out: newgross = [99999,3257,3081,3043,2798,2549,2489,2356,2233,2202] #We have replaced 3706 with 99999- an extremely high number (an outlier) newmean = np.mean(newgross) newmedian = np.median(newgross) print(newmean) #A huge change from the previous value (115.66) - Mean is very sensitive to outliers and extreme values print(newmedian) #No Change- the median only depends on outliers either slightly or not at all. 12400.7 2673.5 To read more about the differences of mean and median, check these out: - \"Stuck in the middle \u2013 mean vs. median\" , available @ https://www.clinfo.eu/mean-median/ \"Mean vs Median: When to Use Which Measure?\" , available @ https://www.datascienceblog.net/post/basic-statistics/mean_vs_median/ \"Mean vs. Median\" by AnswerMiner , available @ https://www.answerminer.com/blog/mean-vs-median","title":"Median"},{"location":"1-Lessons/Lesson15/lesson15/#mode","text":"The value that occurs the most number of times in our data set. Closely tied to the concept of frequency, mode provides information on the most recurrent elements in a dataset. When the mode is not unique, we say that the data set is bimodal, while a data set with more than two modes is multimodal. Let's find the mode in the gross of the Top10 highest-grossing films: # In primitive Python: # Create a list of all the numbers: gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202] mode1 = max((gross.count(item), item) for item in gross)[1] print(mode1) #Since each item is repeated only once, only the first element is printed- This is a multimodal set. #via the Pandas library: Gross = Movies['Gross_million$'] mode2 = Gross.mode() print(mode2) #Returns all modal values- This is a multimodal set. #via the Statistics library: Gross = Movies['Gross_million$'] mode3 = statistics.mode(Gross) print(mode3) #Return a single value mode4 = statistics.multimode(Gross) print(mode4) #Returns a list of all modes #via the scipy.stats library: Gross = Movies['Gross_million$'] mode5 = scipy.stats.mode(Gross) print(mode5) #Returns the object with the modal value and the number of times it occurs- If multimodal: only the smallest value 3706 0 2202 1 2233 2 2356 3 2489 4 2549 5 2798 6 3043 7 3081 8 3257 9 3706 dtype: int64 3706 [3706, 3257, 3081, 3043, 2798, 2549, 2489, 2356, 2233, 2202] ModeResult(mode=array([2202]), count=array([1])) Mode is not useful when our distribution is flat; i.e., the frequencies of all groups are similar. Mode makes sense when we do not have a numeric-valued data set which is required in case of the mean and the median. For instance: Director = Movies['Director'] # via statistics: mode6 = statistics.mode(Director) print(mode6) #\"James Cameron\" with two films (x2 repeats) is the mode # via pandas: mode7 = Director.mode() print(mode7) #\"James Cameron\" with two films (x2 repeats) is the mode James Cameron 0 James Cameron dtype: object To read more about mode, check these out: - \"Mode: A statistical measure of central tendency\" , available @ https://corporatefinanceinstitute.com/resources/knowledge/other/mode/ \"When to use each measure of Central Tendency\" , available @ https://courses.lumenlearning.com/introstats1/chapter/when-to-use-each-measure-of-central-tendency/ \"Mean, Median, Mode: What They Are, How to Find Them\" , available @ https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/","title":"Mode"},{"location":"1-Lessons/Lesson15/lesson15/#measures-of-dispersion","text":"Measures of dispersion are values that describe how the data varies. It gives us a sense of how much the data tends to diverge from the typical value. Aka measures of variability, they quantify the spread of data points.The major measures of dispersion include range, percentiles, inter-quentile range, variance, standard deviation, skeness and kurtosis .","title":"Measures of Dispersion"},{"location":"1-Lessons/Lesson15/lesson15/#range","text":"The range gives a quick sense of the spread of the distribution to those who require only a rough indication of the data. There are some disadvantages of using the range as a measure of spread. One being it does not give any information of the data in between maximum and minimum. Also, the range is very sensitive to extreme values. Let's calculate the range for the budget of the Top10 highest-grossing films: # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] range1 = max(budget)-min(budget) print(\"The range of the budget of the Top10 highest-grossing films is \",range1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] range2 = np.ptp(Budget) #ptp stands for Peak To Peak print(\"The range of the budget of the Top10 highest-grossing films is \",range2,\"million USD\") The range of the budget of the Top10 highest-grossing films is 352.1 million USD The range of the budget of the Top10 highest-grossing films is 352.1 million USD","title":"Range"},{"location":"1-Lessons/Lesson15/lesson15/#percentiles-and-quartiles","text":"A measure which indicates the value below which a given percentage of points in a dataset fall. The sample \ud835\udc5d percentile is the element in the dataset such that \ud835\udc5d% of the elements in the dataset are less than or equal to that value. Also, (100 \u2212 \ud835\udc5d)% of the elements are greater than or equal to that value. For example, median represents the 50th percentile. Similarly, we can have 0th percentile representing the minimum and 100th percentile representing the maximum of all data points. Percentile gives the relative position of a particular value within the dataset. It also helps in comparing the data sets which have different means and deviations. Each dataset has three quartiles, which are the percentiles that divide the dataset into four parts: The first quartile (Q1) is the sample 25th percentile. It divides roughly 25% of the smallest items from the rest of the dataset. The second quartile Q2) is the sample 50th percentile or the median. Approximately 25% of the items lie between the first and second quartiles and another 25% between the second and third quartiles. The third quartile (Q3) is the sample 75th percentile. It divides roughly 25% of the largest items from the rest of the dataset. Budget = Movies['Budget_million$'] #via Numpy: p10 = np.percentile(Budget, 10) #returns the 10th percentile print(\"The 10th percentile of the budget of the Top10 highest-grossing films is \",p10) p4070 = np.percentile(Budget, [40,70]) #returns the 40th and 70th percentile print(\"The 40th and 70th percentile of the budget of the Top10 highest-grossing films are \",p4070) #via Pandas: p10n = Budget.quantile(0.10) #returns the 10th percentile - notice the difference from Numpy print(\"The 10th percentile of the budget of the Top10 highest-grossing films is \",p10n) #via Statistics: Qs = statistics.quantiles(Budget, n=4, method='inclusive') #The parameter n defines the number of resulting equal-probability percentiles: #n=4 returns the quartiles | n=2 returns the median print(\"The quartiles of the budget of the Top10 highest-grossing films is \",Qs) The 10th percentile of the budget of the Top10 highest-grossing films is 7.77 The 40th and 70th percentile of the budget of the Top10 highest-grossing films are [ 11. 211.1] The 10th percentile of the budget of the Top10 highest-grossing films is 7.77 The quartiles of the budget of the Top10 highest-grossing films is [10.625, 12.0, 227.75]","title":"Percentiles and Quartiles"},{"location":"1-Lessons/Lesson15/lesson15/#interquartile-range-iqr","text":"IQR is the difference between the third quartile and the first quartile (Q3-Q1). The interquartile range is a better option than range because it is not affected by outliers. It removes the outliers by just focusing on the distance within the middle 50% of the data. Budget = Movies['Budget_million$'] #via Numpy: IQR1 = np.percentile(Budget, 75) -np.percentile(Budget, 25) #returns the IQR = Q3-Q1 = P75-P25 print(\"The IQR of the budget of the Top10 highest-grossing films is \",IQR1) #via scipy.stats: IQR2 = scipy.stats.iqr(Budget) #returns the IQR- Can be used for other percentile differences as well >> iqr(object, rng=(p1, p2)) print(\"The IQR of the budget of the Top10 highest-grossing films is \",IQR2) The IQR of the budget of the Top10 highest-grossing films is 217.125 The IQR of the budget of the Top10 highest-grossing films is 217.125","title":"InterQuartile Range (IQR)"},{"location":"1-Lessons/Lesson15/lesson15/#the-five-number-summary","text":"A five-number summary is especially useful in descriptive analyses or during the preliminary investigation of a large data set. A summary consists of five values: the most extreme values in the data set (the maximum and minimum values), the lower and upper quartiles, and the median. Five-number summary can be used to describe any data distribution. Boxplots are extremely useful graphical representation of the 5-number summary that we will discuss later. Budget = Movies['Budget_million$'] Budget.describe() #Remember this jewel from Pandas? -It directly return the 5-number summary AND MORE! count 10.000000 mean 115.660000 std 142.739991 min 3.900000 25% 10.625000 50% 12.000000 75% 227.750000 max 356.000000 Name: Budget_million$, dtype: float64 Boxplots are extremely useful graphical representation of the 5-number summary. It can show the range, interquartile range, median, mode, outliers, and all quartiles. import matplotlib.pyplot as plt #Required for the plot gross = [3706,3257,3081,3043,2798,2549,2489,2356,2233,2202,5000] #same data + an outlier: 5000 fig = plt.figure(figsize =(7, 5)) plt.boxplot(gross,medianprops={'linewidth': 1, 'color': 'purple'}) plt.show() To read more about the 5-number summary, check these out: - \"Find a Five-Number Summary in Statistics: Easy Steps\" , available @ https://www.statisticshowto.com/how-to-find-a-five-number-summary-in-statistics/ \"The Five-Number Summary\" , available @ https://www.purplemath.com/modules/boxwhisk2.htm \"What Is the 5 Number Summary?\" by Courtney Taylor , available @ https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/","title":"The Five-number Summary"},{"location":"1-Lessons/Lesson15/lesson15/#variance","text":"The sample variance quantifies the spread of the data. It shows numerically how far the data points are from the mean. The observations may or may not be meaningful if observations in data sets are highly spread. Let's calculate the variance for budget of the Top10 highest-grossing films. Note that if we are working with the entire population (and not the sample), the denominator should be \"n\" instead of \"n-1\". Note that if we are working with the entire population (and not the sample), the denominator should be \"n\" instead of \"n-1\". # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var1 = sum((item - mean)**2 for item in budget) / (n - 1) print(\"The variance of the budget of the Top10 highest-grossing films is \",var1) # via the Statistics library: Budget = Movies['Budget_million$'] var2 = statistics.variance(Budget) print(\"The variance of the budget of the Top10 highest-grossing films is \",var2) The variance of the budget of the Top10 highest-grossing films is 20374.70488888889 The variance of the budget of the Top10 highest-grossing films is 20374.70488888889","title":"Variance"},{"location":"1-Lessons/Lesson15/lesson15/#standard-deviation","text":"The sample standard deviation is another measure of data spread. It\u2019s connected to the sample variance, as standard deviation, \ud835\udc60, is the positive square root of the sample variance. The standard deviation is often more convenient than the variance because it has the same unit as the data points. # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var = sum((item - mean)**2 for item in budget) / (n - 1) sd1 = var**0.5 print(\"The standard deviation of the budget of the Top10 highest-grossing films is \",sd1,\"million USD\") # via the Statistics library: Budget = Movies['Budget_million$'] sd2 = statistics.stdev(Budget) print(\"The standard deviation of the budget of the Top10 highest-grossing films is \",sd2,\"million USD\") The standard deviation of the budget of the Top10 highest-grossing films is 142.73999050332353 million USD The standard deviation of the budget of the Top10 highest-grossing films is 142.73999050332353 million USD","title":"Standard Deviation"},{"location":"1-Lessons/Lesson15/lesson15/#skewness","text":"The sample skewness measures the asymmetry of a data sample. There are several mathematical definitions of skewness. The Fisher-Pearson standardized moment coefficient is calculated by using mean, median and standard deviation of the data. Usually, negative skewness values indicate that there\u2019s a dominant tail on the left side. Positive skewness values correspond to a longer or fatter tail on the right side. If the skewness is close to 0 (for example, between \u22120.5 and 0.5), then the dataset is considered quite symmetrical. # Primitive Python: budget = [3.9,237,200,11,356,8.2,10.5,13,11,306] n = len(budget) mean = sum(budget) / n var = sum((item - mean)**2 for item in budget) / (n - 1) std = var**0.5 skew1 = (sum((item - mean)**3 for item in budget) * n / ((n - 1) * (n - 2) * std**3)) print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew1) # via the scipy.stats library: Budget = Movies['Budget_million$'] skew2 = scipy.stats.skew(Budget, bias=False) print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew2) # via the Pandas library: Budget = Movies['Budget_million$'] skew3 = Budget.skew() print(\"The skewness of the budget of the Top10 highest-grossing films is \",skew3) The skewness of the budget of the Top10 highest-grossing films is 0.7636547490528159 The skewness of the budget of the Top10 highest-grossing films is 0.763654749052816 The skewness of the budget of the Top10 highest-grossing films is 0.763654749052816","title":"Skewness"},{"location":"1-Lessons/Lesson15/lesson15/#kurtosis","text":"Kurtosis describes the peakedness of the distribution. In other words, Kurtosis identifies whether the tails of a given distribution contain extreme values. While Skewness essentially measures the symmetry of the distribution, kurtosis determines the heaviness of the distribution tails. If the distribution is tall and thin it is called a leptokurtic distribution. Values in a leptokurtic distribution are near the mean or at the extremes. A flat distribution where the values are moderately spread out (i.e., unlike leptokurtic) is called platykurtic distribution. A distribution whose shape is in between a leptokurtic distribution and a platykurtic distribution is called a mesokurtic distribution. # via the scipy.stats library: Budget = Movies['Budget_million$'] Kurt = scipy.stats.kurtosis(Budget) print(\"The kurtosis of the budget of the Top10 highest-grossing films is \",Kurt) #a platykurtic distribution | the tails are heavy The kurtosis of the budget of the Top10 highest-grossing films is -1.3110307923262225 To read more about skewness and kurtosis, check these out: - \"Measures of Skewness and Kurtosis\" , available @ https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm#:~:text=Skewness%20is%20a%20measure%20of,relative%20to%20a%20normal%20distribution. \"Are the Skewness and Kurtosis Useful Statistics?\" , available @ https://www.spcforexcel.com/knowledge/basic-statistics/are-skewness-and-kurtosis-useful-statistics \"Skew and Kurtosis: 2 Important Statistics terms you need to know in Data Science\" by Diva Dugar , available @ https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa \"Measures of Shape: Skewness and Kurtosis\" by Stan Brown , available @ https://brownmath.com/stat/shape.htm","title":"Kurtosis"},{"location":"1-Lessons/Lesson17/lesson17/","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} Copyright \u00a9 2021 Theodore G. Cleveland and Farhang Forghanparast, all rights reserved ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 2 Mar 2021 Lesson 17 : Data Modeling with Special Functions (Probability Distributions) Objectives To understand the fundamental concepts involved in representing a data collection; Interpolation Extrapolation Concept of a fitting function Introduce select special functions Normal distribution function Gamma distribution function Extreme value distribution function Pearson Type 3 distribution function Computational Thinking Concepts The CT concepts include: Decomposition => Assert data are drawn from some process that is functionally explainable Abstraction => Represent data behavior with a function Algorithm Design => Use the function to predict \"new\" values of observations Explaining Data Recall our speed and time example, repeated below. # Our data time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Our model def poly1(b0,b1,x): # return y = b0 + b1*x poly1=b0+b1*x return(poly1) # Our plotting function import matplotlib.pyplot as plt def make2plot(listx1,listy1,listx2,listy2,strlablx,strlably,strtitle): mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(listx1,listy1, c='red', marker='v',linewidth=0) # basic data plot plt.plot(listx2,listy2, c='blue',linewidth=1) # basic model plot plt.xlabel(strlablx) plt.ylabel(strlably) plt.legend(['Data','Model'])# modify for argument insertion plt.title(strtitle) plt.show() # Our \"fitting\" process intercept = 5.0 slope = 3.0 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly1(intercept,slope,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') We are limited in the ability to fit the data because our representation function is limited to a straight line, now lets make a quadratic a possible model option. # Our new model def poly2(b0,b1,b2,x): # return y = b0 + b1*x poly2=b0+(b1+b2*x)*x # faster than b0 + b1*x + b2*x**2 return(poly2) Now try fitting and plotting using our new model and should get indetical result, then we can explore using the new parameter b2 # Our \"fitting\" process intercept = 5.0 # set to 0.0 slope = 3.0 # adjust to 2.0 curvature = 0.0 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') # Our \"fitting\" process intercept = 0.0 # set to 0.0 slope = 2.0 # adjust to 2.0 curvature = 0.9 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') Now which \"model\" is more useful for these data? Explain your reasoning. Lets take a look over the process we just implemented Prepare our data series Select a function type as the data model (in this case polynomials of order 1 and 2) Use a plotting tool to plot observed data (red) and our model (blue) Adjust model parameters (b0,b1,b2, ...) to get the blue model to pass through the red dots as best we can. That's it, later we will explore ways to quantify the fit, which will help us choose a data model when multiple models appear good. ## Now lets apply our tools to different data, first we will read data from a file amatrix = [] xvalue = [] yvalue = [] rowNumA = 0 file1 = open(\"MyFile.txt\", \"r\") # get the data for line in file1: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 file1.close() # Disconnect the file for i in range(len(amatrix)): # deconstruct the list, rename each column xvalue.append(amatrix[i][0]) yvalue.append(amatrix[i][1]) make2plot(xvalue,yvalue,[],[],'x-value','y-value','EDA Plot of model and observations') # Our \"fitting\" process intercept = 0.0 # 0.0 slope = 0.01 # 0.018 curvature = 1e-09 # -0.0001 modelY = [] # empty list for i in range(len(xvalue)): modelY.append(poly2(intercept,slope,curvature,xvalue[i])) # Plotting results make2plot(xvalue,yvalue,xvalue,modelY,'x-value','y-value','EDA Plot of model and observations') Lets build a different type of data model, here we will use a special function called the normal distribution function. A useful notation using the Normal density function as an example is: \\text{pdf(x)} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\times exp (-\\frac{(x-\\mu)^2}{2 \\sigma^2}) In the function, x is the random variable, \\mu is the population mean and \\sigma^2 is the population variance. These parameters ( \\mu , and \\sigma^2 ) play the same role that b_0,b_1,b_2, \\dots play in our polynomial model - they simply adjust shape of the model. Often we don't actually know the population values so we estimate them from the collection of observations, in this context these are called the sample mean and variance. Computation of the sample values is done using methods described in the lesson on descriptive statistics. The integral of the \\text{pdf(x)} from -\\infty~to ~ X , produces a result called the cumulative distribution function. The value X is not a random variable, but the integral value of the probability of the random variable x being less than or equal to X . A useful notation using the Normal distribution as an example is: F(X) = \\int_{-\\infty}^X{\\frac{1}{\\sigma \\sqrt{2\\pi}} \\times exp (-\\frac{(x-\\mu)^2}{2 \\sigma^2}) dx} For the Normal distribution the integral is a special function called the Error function and can be written as: F(X) = \\frac{1}{2} \\cdot (1+erf(\\frac{(X-\\mu)}{\\sqrt{2} \\sigma})) We will use these concepts to build an alternative to poly1 and poly2 as data models. Normal Distribution Model (Using Math Package) Here we will build a normal distribution model, essentially the functions for the above equations, and then will plot them. Then we will sample from a list of numbers from 1 to 100 and see if the data model is representative of the sample. import math def normdensity(x,mu,sigma): weight = 1.0 /(sigma * math.sqrt(2.0*math.pi)) argument = ((x - mu)**2)/(2.0*sigma**2) normdensity = weight*math.exp(-1.0*argument) return normdensity def normdist(x,mu,sigma): argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist # Our \"fitting\" process mu = 50.0 # 50.0 sigma = 10 # 850.01**0.5 modelY = [] # empty list for i in range(len(xvalue)): modelY.append(normdist(mu,sigma,xvalue[i])) # Plotting results make2plot(xvalue,yvalue,xvalue,modelY,'x-value','y-value','EDA Plot of model and observations') Interpolation Lets return to our time/speed model and estimate the speed at 4.5 seconds # Our \"fitting\" process intercept = 0.0 # set to 0.0 slope = 2.0 # adjust to 2.0 curvature = 0.9 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') print('Speed estimate at time ',4.5,'is ',poly2(intercept,slope,curvature,4.5)) Speed estimate at time 4.5 is 27.224999999999998 Extrapolation print('Speed estimate at time ',9.5,'is ',poly2(intercept,slope,curvature,9.5)) Speed estimate at time 9.5 is 100.22500000000001 Probability Estimation Modeling Probability estimation modeling is the use of probability distributions (population data models) to model or explain behavior in observed (sample data) values. Once a particular distribution is selected, then the concept of risk (probability) can be explored for events of varying magnitudes. Two important \u201cextremes\u201d in engineering: Uncommon (rare) events (floods, nuclear plant explosions, etc.) Common, almost predictable events (routine discharges, traffic accidents at a dangerous intersection, network failure on a due date, etc.) The probability distribution is just a model of the data, like a trend line for deterministic behavior; different distributions have different shapes, and domains and can explain certain types of observations better than others. Some Useful Distributions (data models) include: Normal LogNormal Gamma Weibull Extreme Value (Gumbell) Beta There are many more; they all have the common property that they integrate to unity on the domain -\\infty~to ~ \\infty . The probability distributions (models) are often expressed as a density function or a cumulative distribution function. # Standard Normal mu = 0 sigma = 1 x = [] ypdf = [] ycdf = [] xlow = -10 xhigh = 10 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdensity(xlow + i*xstep,mu,sigma) ypdf.append(yvalue) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) #x #ypdf #ycdf Make the plot below, nothing too special just yet. Plots of the density (in blue) and cumulative density (probability) in red. import matplotlib.pyplot # the python plotting library myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.plot(x, ypdf, color ='blue') matplotlib.pyplot.plot(x, ycdf, color ='red') matplotlib.pyplot.xlabel(\"Value of RV\") matplotlib.pyplot.ylabel(\"Density or Quantile Value\") matplotlib.pyplot.title(\"Normal Distribution Data Model\") matplotlib.pyplot.show() Exceedence Probability The purpose of distributions is to model data and allow us to estimate an answer to the question, what is the probability that we will observe a value of the random variable less than or equal to some sentinel value. A common way to plot the quantile function is with accumulated probability on the horizontal axis, and random variable value on the vertical axis. Consider the figure below; The RV Value is about 50,000 indicated by the horizontal magenta line. The blue curve is some data model, for instance one of our distributions below. The accumulated probability value at 50,000 is 0.1 or roughly 10% chance, but we also have to stipulate whether we are interested in less than or greater than. In the figure shown, P(x <= 50,000)~ =~1.00~-~0.1~= 0.9~or~90\\% and is a non-exceedence probability. In words we would state \"The probability of observing a value less than or equal to 50,000 is 90%\" the other side of the vertical line is the exceedence probability; in the figure P(x > 50,000)~=~0.1~or~10\\% . In words we would state \"The probability of observing a value equal to or greater than 50,000 is 10%.\" In risk analysis the sense of the probability is easily confusing, so when you can - make a plot. Another way to look at the situation is to simply realize that the blue curve is the quantile function F(X) with X plotted on the vertical axis, and F(X) plotted on the horizontal axis. Now lets put these ideas to use. We will sample from the population of integers from 0 to 100, with replacement. Any single pull from the population is equally likely. Lets take 25 samples (about 1/4 of the total population - usually we dont know the size of the population). import numpy population = [] for i in range(0,101,1): population.append(i) sample = numpy.random.choice(population,25) # lets get some statistics sample_mean = sample.mean() sample_variance = sample.std()**2 # sort the sample in place! sample.sort() # built a relative frequency approximation to probability, assume each pick is equally likely weibull_pp = [] for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Density or Quantile Value\") matplotlib.pyplot.title(\"Normal Distribution Data Model\") matplotlib.pyplot.show() What a horrible plot, but lets now use the sample statistics to \"fit\" the data model (red) to the observations (blue). Notice we have already rotated the axes so this plot and ones that follow are structured like the \"Exceedence\" plot above. # Fitted Model mu = sample_mean sigma = math.sqrt(sample_variance) x = [] ycdf = [] xlow = 0 xhigh = 100 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Quantile Value\") mytitle = \"Normal Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() popmean = numpy.array(population).mean() popvar = numpy.array(population).std()**2 # Fitted Model mu = popmean sigma = math.sqrt(popvar) x = [] ycdf = [] xlow = 0 xhigh = 100 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Quantile Value\") mytitle = \"Normal Distribution Data Model Population mean = : \" + str(popmean)+ \" Population variance =:\" + str(popvar) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Some observations are in order: The population is a uniformly distributed collection. By random sampling, and keeping the sample size small, the sample distribution appears approximately normal. Real things of engineering interest are not always bounded as shown here, the choice of the Weibull plotting position is not arbitrary. The blue dot scatterplot in practice is called the empirical distribution function, or empirical quantile function. Now we will apply these ideas to some realistic data. Beargrass Creek The file beargrass.txt contains annual peak flows for Beargrass Creek. The year is a water year, so the peaks occur on different days in each year; thus it is not a time series. Let's examine the data and see how well a Normal distribution data model fits, then estimate from the distribution the peak magnitude with exceedence probability 0.01 (1%-chance that will observe a value equal to or greater). import pandas beargrass = pandas.read_csv('beargrass.txt') #Reading a .csv file beargrass.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Peak 0 1945 1810 1 1946 791 2 1947 839 3 1948 1750 4 1949 898 # beargrass.plot() Now we will just copy code (the miracle of cut-n-paste!) sample = beargrass['Peak'].tolist() # put the peaks into a list sample_mean = numpy.array(sample).mean() sample_variance = numpy.array(sample).std()**2 sample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(sample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Normal Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() beargrass['Peak'].describe() count 31.000000 mean 1599.258065 std 1006.239500 min 707.000000 25% 908.000000 50% 1250.000000 75% 1945.000000 max 5200.000000 Name: Peak, dtype: float64 A 1% chance exceedence is on the right side of the chart, it is the compliment of 99% non-exceedence, in terms of our quantile function we want to find the value X that returns a quantile of 0.99. myguess = 6000 print(mu,sigma) print(normdist(myguess,mu,sigma)) 1599.258064516129 989.8767915427474 0.9999956206542673 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): mu = 1599.258064516129 sigma = 989.8767915427474 quantile = 0.99999 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) 5820.974479887303 So a peak discharge of 4000 or so is expected to be observed with 1% chance, notice we took the value from the fitted distribution, not the empirical set. As an observation, the Normal model is not a very good data model for these observations. Log-Normal Another data model we can try is log-normal, where we stipulate that the logarithms of the observations are normal. The scripts are practically the same, but there is an inverse transformation required to recover original value scale. Again we will use Beargrass creek. def loggit(x): # A prototype function to log transform x return(math.log(x)) logsample = beargrass['Peak'].apply(loggit).tolist() # put the peaks into a list sample_mean = numpy.array(logsample).mean() sample_variance = numpy.array(logsample).std()**2 logsample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model in Log Space sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(logsample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, logsample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Normal Data Model log sample mean = : \" + str(sample_mean)+ \" log sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() The plot doesn't look too bad, but we are in log-space, which is hard to interpret, so we will transform back to arithmetic space def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) sample = beargrass['Peak'].tolist() # pull original list sample.sort() # sort in place ################ mu = sample_mean # Fitted Model in Log Space sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(logsample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(antiloggit(xlow + i*xstep)) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Normal Data Model sample log mean = : \" + str((sample_mean))+ \" sample log variance =:\" + str((sample_variance)) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Visually a better data model, now lets determine the 1% chance value. myguess = 4440 print(mu,sigma) print(normdist(loggit(myguess),mu,sigma)) # mu, sigma already in log space - convert myguess 7.23730905616488 0.4984855728993489 0.9900772507418302 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): mu = 7.23730905616488 sigma = 0.4984855728993489 quantile = 0.99 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) 4433.567789173262 Now we have a decent method, we should put stuff into functions to keep code concise, lets examine a couple more data models Gumbell (Double Exponential) Distribution The Gumbell is also called the Extreme-Value Type I distribution, the density and quantile function are: \\text{pdf(x)} = \\frac{1}{\\beta} \\cdot exp [-\\frac{(x-\\alpha)}{\\beta} - exp (-\\frac{(x-\\alpha)}{\\beta}) ] F(X) = \\int_{-\\infty}^X{\\frac{1}{\\beta} \\cdot exp [-\\frac{(x-\\alpha)}{\\beta} - exp (-\\frac{(x-\\alpha)}{\\beta}) ] dx} = exp [- exp (-\\frac{(X-\\alpha)}{\\beta})] The distribution has two parameters, \\alpha and \\beta , which in some sense play the same role as mean and variance. Lets modify our scripts further to see how this data model performs on the Bearcreek data. Of course we need a way to estimate the parameters, a good approximation can be obtained using: \\alpha = \\mu \\cdot \\frac{\\sqrt{6}}{\\pi} and \\beta = 0.45 \\cdot \\sigma where \\mu and \\sigma^2 are the sample mean and variance. def ev1dist(x,alpha,beta): argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist Now literally substitute into our prior code! sample = beargrass['Peak'].tolist() # put the peaks into a list sample_mean = numpy.array(sample).mean() sample_variance = numpy.array(sample).std()**2 alpha_mom = sample_mean*math.sqrt(6)/math.pi beta_mom = math.sqrt(sample_variance)*0.45 sample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(sample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = ev1dist(xlow + i*xstep,alpha_mom,beta_mom) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Extreme Value Type 1 Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Again a so-so visual fit. To find the 1% chance value myguess = 3300 print(alpha_mom,beta_mom) print(ev1dist(myguess,alpha_mom,beta_mom)) # 1246.9363972503857 445.4445561942363 0.990087892543188 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): alpha = 1246.9363972503857 beta = 445.4445561942363 quantile = 0.99 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) 3296.0478279991366 Gamma Distribution (as Pearson Type 3) One last data model to consider is one that is specifically stipulated for use by federal agencies for probability estimation of extreme hydrologic events. The data model ia called the Log-Pearson Type III distribution, its actually a specific case of a Gamma distrubution. This example we will dispense with tyring to build it in python primative, and just use a package - the density function is not all that hard, but the quantile function is elaborate. Learn more at http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/3-Readings/NumericalRecipesinF77.pdf (in particular around Page 276) As usual, lets let Google do some work for us, using the search term \"gamma quantile function; scipy\" we get to this nice blog entry https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html which is a good start. A Pearson Type III data model has the following density function: f(x|\\tau,\\alpha,\\beta) = \\frac{(\\frac{x-\\tau}{\\beta})^{\\alpha -1}\\cdot exp( - \\frac{x-\\tau}{\\beta})}{|\\beta| \\Gamma(\\alpha)} If we make some substitutions: \\lambda = \\frac{1}{\\beta} ; \\hat{x} = x -\\tau then the density function is f(\\hat{x}) = \\frac{ 1}{\\Gamma(\\alpha)} (\\lambda \\hat{x})^{\\alpha -1}\\cdot exp( - \\lambda \\hat{x} ) which is now a one parameter Gamma density function just like the example in the link. Reading a little from http://atomickitty.ddns.net/documents/university-courses/ce-5361-swhydrology/1-Lessons.src/Lesson22/AdditionalReading/Bulletin17C-tm4b5-draft-ACWI-17Jan2018.pdf we can relate the transformations to descriptive statistics (shown below without explaination) as: \\mu = \\text{sample mean} , \\sigma = \\text{sample standard deviation} , \\gamma = \\text{sample skew coefficient} = (\\frac{n}{\\sigma^3(n-1)(n-2)})\\sum_{i=1}^n(x_i - \\mu)^3 \\alpha = \\frac{4}{\\gamma^2} \\beta = sign(\\gamma)\\sqrt{\\frac{\\sigma^2}{\\alpha}} \\tau = \\mu - \\alpha \\cdot \\beta So we have a bit of work to do. The name of the functions in scipy we are interested in are gamma.pdf(x,a) and gamma.cdf(x,a) So lets build a tool to generate a Log-Pearson Type III data model, then apply it to Beargrass Creek. We will use a lot of glue here. First load in dependencies, and define support functions we will need import scipy.stats # import scipy stats package import math # import math package import numpy # import numpy package # log and antilog def loggit(x): # A prototype function to log transform x return(math.log(x)) def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) def weibull_pp(sample): # plotting position function # returns a list of plotting positions; sample must be a numeric list weibull_pp = [] # null list to return after fill sample.sort() # sort the sample list in place for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) return weibull_pp Then the gamma distribution from scipy, modified for our type of inputs. def gammacdf(x,tau,alpha,beta): # Gamma Cumulative Density function - with three parameter to one parameter convert xhat = x-tau lamda = 1.0/beta gammacdf = scipy.stats.gamma.cdf(lamda*xhat, alpha) return gammacdf Then load in the data from the data frame, log transform and generate descriptive statistics. #sample = beargrass['Peak'].tolist() # put the peaks into a list sample = beargrass['Peak'].apply(loggit).tolist() # put the log peaks into a list sample_mean = numpy.array(sample).mean() sample_stdev = numpy.array(sample).std() sample_skew = 3.0 # scipy.stats.skew(sample) sample_alpha = 4.0/(sample_skew**2) sample_beta = numpy.sign(sample_skew)*math.sqrt(sample_stdev**2/sample_alpha) sample_tau = sample_mean - sample_alpha*sample_beta Now generate plotting positions for the sample observations plotting = weibull_pp(sample) Now generate values for the data model (for plotting our red line \"fit\"), set limits to be a little beyond the sample range. x = []; ycdf = [] xlow = (0.9*min(sample)); xhigh = (1.1*max(sample)) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = gammacdf(xlow + i*xstep,sample_tau,sample_alpha,sample_beta) ycdf.append(yvalue) Now reverse transform back to native scale, and plot the sample values vs plotting position in blue, and the data model in red # reverse transform the peaks, and the data model peaks for i in range(len(sample)): sample[i] = antiloggit(sample[i]) for i in range(len(x)): x[i] = antiloggit(x[i]) myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(plotting, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Pearson Type III Distribution Data Model\\n \" mytitle += \"Mean = \" + str(antiloggit(sample_mean)) + \"\\n\" mytitle += \"SD = \" + str(antiloggit(sample_stdev)) + \"\\n\" mytitle += \"Skew = \" + str(antiloggit(sample_skew)) + \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() And as before lets find the value that retruns the 99% quantile - we will just use the newton method above. First recover the required model parameters. Then we will paste these into the f(x) function for the Newton's method. print(sample_tau) print(sample_alpha) print(sample_beta) 6.904985340898647 0.4444444444444444 0.7477283593490234 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): sample_tau = 5.976005311346212 sample_alpha = 6.402272915026134 sample_beta = 0.1970087438569494 quantile = 0.9900 argument = loggit(x) gammavalue = gammacdf(argument,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile myguess = 5000 print(newton(f, myguess)) 5856.10913158364 Trust, but verify! round(gammacdf(loggit(5856.109),sample_tau,sample_alpha,sample_beta),4) 0.9753 Now lets summarize our efforts regarding Beargrass Creek annual peaks and probabilities anticipated. Data Model 99% Peak Flow Remarks Normal 3902 so-so visual fit Log-Normal 4433 better visual fit Gumbell 3296 better visual fit Log-Pearson III 5856 best (of the set) visual fit At this point, now we have to choose our model and then can investigate different questions. So using LP3 as our favorite, lets now determine anticipated flow values for different probabilities (from the data model) - easy enought to just change the quantile value and rerun the newtons optimizer, for example: Exceedence Probability Flow Value Remarks 25% 968 First Quartile Divider 50% 1302 Median, and Second Quartile Divider 75% 1860 3rd Quartile Divider 90% 2706 10% chance of greater value 99% 5856 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% 9420 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% 11455 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): sample_tau = 5.976005311346212 sample_alpha = 6.402272915026134 sample_beta = 0.1970087438569494 quantile = 0.50 argument = loggit(x) gammavalue = gammacdf(argument,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile myguess = 1000 print(newton(f, myguess)) 1302.814639184079 References: Jamie Chan (2014) Learn Python in One Day and Learn It Well. LCF Publishing. Kindle Edition. http://www.learncodingfast.com/python Grus, Joel. Data Science from Scratch: First Principles with Python. O'Reilly Media. Kindle Edition. (http://safaribooksonline.com) Christian, B, and Griffiths Tom (2016) Algorithms to live by: The computer science of human decisions. Henry Holt and Company, ISBN 9781627790369 (hardcover)|ISBN 9781627790376 (electronic book) https://www.amazon.com/Distributional-Statistics-Environment-Statistical-Computing/dp/1463508417 England, J.F. Jr., Cohn, T.A., Faber, B.A., Stedinger, J.R., Thomas Jr., W.O., Veilleux, A.G., Kiang, J.E., and Mason, R.R.Jr., 2018, Guidelines for Determining Flood Flow Frequency\u2014Bulletin 17C: U.S. Geological Survey Techniques andMethods, book 4, chap. B5, 146 p., https://doi.org/10.3133/tm4B5 https://www.astroml.org/book_figures/chapter3/fig_gamma_distribution.html https://www.inferentialthinking.com/chapters/10/Sampling_and_Empirical_Distributions.html https://www.inferentialthinking.com/chapters/15/Prediction.html","title":"Probability Estimation Modeling"},{"location":"1-Lessons/Lesson17/lesson17/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 2 Mar 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"1-Lessons/Lesson17/lesson17/#lesson-17-data-modeling-with-special-functions-probability-distributions","text":"","title":"Lesson 17 : Data Modeling with Special Functions (Probability Distributions)"},{"location":"1-Lessons/Lesson17/lesson17/#objectives","text":"To understand the fundamental concepts involved in representing a data collection; Interpolation Extrapolation Concept of a fitting function Introduce select special functions Normal distribution function Gamma distribution function Extreme value distribution function Pearson Type 3 distribution function","title":"Objectives"},{"location":"1-Lessons/Lesson17/lesson17/#computational-thinking-concepts","text":"The CT concepts include: Decomposition => Assert data are drawn from some process that is functionally explainable Abstraction => Represent data behavior with a function Algorithm Design => Use the function to predict \"new\" values of observations","title":"Computational Thinking Concepts"},{"location":"1-Lessons/Lesson17/lesson17/#explaining-data","text":"Recall our speed and time example, repeated below. # Our data time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Our model def poly1(b0,b1,x): # return y = b0 + b1*x poly1=b0+b1*x return(poly1) # Our plotting function import matplotlib.pyplot as plt def make2plot(listx1,listy1,listx2,listy2,strlablx,strlably,strtitle): mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(listx1,listy1, c='red', marker='v',linewidth=0) # basic data plot plt.plot(listx2,listy2, c='blue',linewidth=1) # basic model plot plt.xlabel(strlablx) plt.ylabel(strlably) plt.legend(['Data','Model'])# modify for argument insertion plt.title(strtitle) plt.show() # Our \"fitting\" process intercept = 5.0 slope = 3.0 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly1(intercept,slope,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') We are limited in the ability to fit the data because our representation function is limited to a straight line, now lets make a quadratic a possible model option. # Our new model def poly2(b0,b1,b2,x): # return y = b0 + b1*x poly2=b0+(b1+b2*x)*x # faster than b0 + b1*x + b2*x**2 return(poly2) Now try fitting and plotting using our new model and should get indetical result, then we can explore using the new parameter b2 # Our \"fitting\" process intercept = 5.0 # set to 0.0 slope = 3.0 # adjust to 2.0 curvature = 0.0 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') # Our \"fitting\" process intercept = 0.0 # set to 0.0 slope = 2.0 # adjust to 2.0 curvature = 0.9 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') Now which \"model\" is more useful for these data? Explain your reasoning. Lets take a look over the process we just implemented Prepare our data series Select a function type as the data model (in this case polynomials of order 1 and 2) Use a plotting tool to plot observed data (red) and our model (blue) Adjust model parameters (b0,b1,b2, ...) to get the blue model to pass through the red dots as best we can. That's it, later we will explore ways to quantify the fit, which will help us choose a data model when multiple models appear good. ## Now lets apply our tools to different data, first we will read data from a file amatrix = [] xvalue = [] yvalue = [] rowNumA = 0 file1 = open(\"MyFile.txt\", \"r\") # get the data for line in file1: amatrix.append([float(n) for n in line.strip().split()]) rowNumA += 1 file1.close() # Disconnect the file for i in range(len(amatrix)): # deconstruct the list, rename each column xvalue.append(amatrix[i][0]) yvalue.append(amatrix[i][1]) make2plot(xvalue,yvalue,[],[],'x-value','y-value','EDA Plot of model and observations') # Our \"fitting\" process intercept = 0.0 # 0.0 slope = 0.01 # 0.018 curvature = 1e-09 # -0.0001 modelY = [] # empty list for i in range(len(xvalue)): modelY.append(poly2(intercept,slope,curvature,xvalue[i])) # Plotting results make2plot(xvalue,yvalue,xvalue,modelY,'x-value','y-value','EDA Plot of model and observations') Lets build a different type of data model, here we will use a special function called the normal distribution function. A useful notation using the Normal density function as an example is: \\text{pdf(x)} = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\times exp (-\\frac{(x-\\mu)^2}{2 \\sigma^2}) In the function, x is the random variable, \\mu is the population mean and \\sigma^2 is the population variance. These parameters ( \\mu , and \\sigma^2 ) play the same role that b_0,b_1,b_2, \\dots play in our polynomial model - they simply adjust shape of the model. Often we don't actually know the population values so we estimate them from the collection of observations, in this context these are called the sample mean and variance. Computation of the sample values is done using methods described in the lesson on descriptive statistics. The integral of the \\text{pdf(x)} from -\\infty~to ~ X , produces a result called the cumulative distribution function. The value X is not a random variable, but the integral value of the probability of the random variable x being less than or equal to X . A useful notation using the Normal distribution as an example is: F(X) = \\int_{-\\infty}^X{\\frac{1}{\\sigma \\sqrt{2\\pi}} \\times exp (-\\frac{(x-\\mu)^2}{2 \\sigma^2}) dx} For the Normal distribution the integral is a special function called the Error function and can be written as: F(X) = \\frac{1}{2} \\cdot (1+erf(\\frac{(X-\\mu)}{\\sqrt{2} \\sigma})) We will use these concepts to build an alternative to poly1 and poly2 as data models.","title":"Explaining Data"},{"location":"1-Lessons/Lesson17/lesson17/#normal-distribution-model-using-math-package","text":"Here we will build a normal distribution model, essentially the functions for the above equations, and then will plot them. Then we will sample from a list of numbers from 1 to 100 and see if the data model is representative of the sample. import math def normdensity(x,mu,sigma): weight = 1.0 /(sigma * math.sqrt(2.0*math.pi)) argument = ((x - mu)**2)/(2.0*sigma**2) normdensity = weight*math.exp(-1.0*argument) return normdensity def normdist(x,mu,sigma): argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist # Our \"fitting\" process mu = 50.0 # 50.0 sigma = 10 # 850.01**0.5 modelY = [] # empty list for i in range(len(xvalue)): modelY.append(normdist(mu,sigma,xvalue[i])) # Plotting results make2plot(xvalue,yvalue,xvalue,modelY,'x-value','y-value','EDA Plot of model and observations')","title":"Normal Distribution Model (Using Math Package)"},{"location":"1-Lessons/Lesson17/lesson17/#interpolation","text":"Lets return to our time/speed model and estimate the speed at 4.5 seconds # Our \"fitting\" process intercept = 0.0 # set to 0.0 slope = 2.0 # adjust to 2.0 curvature = 0.9 # adjust to 0.9 modelSpeed = [] # empty list for i in range(len(time)): modelSpeed.append(poly2(intercept,slope,curvature,time[i])) # Plotting results make2plot(time,speed,time,modelSpeed,'time (sec.)','speed (m/s)','Plot of model and observations') print('Speed estimate at time ',4.5,'is ',poly2(intercept,slope,curvature,4.5)) Speed estimate at time 4.5 is 27.224999999999998","title":"Interpolation"},{"location":"1-Lessons/Lesson17/lesson17/#extrapolation","text":"print('Speed estimate at time ',9.5,'is ',poly2(intercept,slope,curvature,9.5)) Speed estimate at time 9.5 is 100.22500000000001","title":"Extrapolation"},{"location":"1-Lessons/Lesson17/lesson17/#probability-estimation-modeling","text":"Probability estimation modeling is the use of probability distributions (population data models) to model or explain behavior in observed (sample data) values. Once a particular distribution is selected, then the concept of risk (probability) can be explored for events of varying magnitudes. Two important \u201cextremes\u201d in engineering: Uncommon (rare) events (floods, nuclear plant explosions, etc.) Common, almost predictable events (routine discharges, traffic accidents at a dangerous intersection, network failure on a due date, etc.) The probability distribution is just a model of the data, like a trend line for deterministic behavior; different distributions have different shapes, and domains and can explain certain types of observations better than others. Some Useful Distributions (data models) include: Normal LogNormal Gamma Weibull Extreme Value (Gumbell) Beta There are many more; they all have the common property that they integrate to unity on the domain -\\infty~to ~ \\infty . The probability distributions (models) are often expressed as a density function or a cumulative distribution function. # Standard Normal mu = 0 sigma = 1 x = [] ypdf = [] ycdf = [] xlow = -10 xhigh = 10 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdensity(xlow + i*xstep,mu,sigma) ypdf.append(yvalue) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) #x #ypdf #ycdf Make the plot below, nothing too special just yet. Plots of the density (in blue) and cumulative density (probability) in red. import matplotlib.pyplot # the python plotting library myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.plot(x, ypdf, color ='blue') matplotlib.pyplot.plot(x, ycdf, color ='red') matplotlib.pyplot.xlabel(\"Value of RV\") matplotlib.pyplot.ylabel(\"Density or Quantile Value\") matplotlib.pyplot.title(\"Normal Distribution Data Model\") matplotlib.pyplot.show()","title":"Probability Estimation Modeling"},{"location":"1-Lessons/Lesson17/lesson17/#exceedence-probability","text":"The purpose of distributions is to model data and allow us to estimate an answer to the question, what is the probability that we will observe a value of the random variable less than or equal to some sentinel value. A common way to plot the quantile function is with accumulated probability on the horizontal axis, and random variable value on the vertical axis. Consider the figure below; The RV Value is about 50,000 indicated by the horizontal magenta line. The blue curve is some data model, for instance one of our distributions below. The accumulated probability value at 50,000 is 0.1 or roughly 10% chance, but we also have to stipulate whether we are interested in less than or greater than. In the figure shown, P(x <= 50,000)~ =~1.00~-~0.1~= 0.9~or~90\\% and is a non-exceedence probability. In words we would state \"The probability of observing a value less than or equal to 50,000 is 90%\" the other side of the vertical line is the exceedence probability; in the figure P(x > 50,000)~=~0.1~or~10\\% . In words we would state \"The probability of observing a value equal to or greater than 50,000 is 10%.\" In risk analysis the sense of the probability is easily confusing, so when you can - make a plot. Another way to look at the situation is to simply realize that the blue curve is the quantile function F(X) with X plotted on the vertical axis, and F(X) plotted on the horizontal axis. Now lets put these ideas to use. We will sample from the population of integers from 0 to 100, with replacement. Any single pull from the population is equally likely. Lets take 25 samples (about 1/4 of the total population - usually we dont know the size of the population). import numpy population = [] for i in range(0,101,1): population.append(i) sample = numpy.random.choice(population,25) # lets get some statistics sample_mean = sample.mean() sample_variance = sample.std()**2 # sort the sample in place! sample.sort() # built a relative frequency approximation to probability, assume each pick is equally likely weibull_pp = [] for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Density or Quantile Value\") matplotlib.pyplot.title(\"Normal Distribution Data Model\") matplotlib.pyplot.show() What a horrible plot, but lets now use the sample statistics to \"fit\" the data model (red) to the observations (blue). Notice we have already rotated the axes so this plot and ones that follow are structured like the \"Exceedence\" plot above. # Fitted Model mu = sample_mean sigma = math.sqrt(sample_variance) x = [] ycdf = [] xlow = 0 xhigh = 100 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Quantile Value\") mytitle = \"Normal Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() popmean = numpy.array(population).mean() popvar = numpy.array(population).std()**2 # Fitted Model mu = popmean sigma = math.sqrt(popvar) x = [] ycdf = [] xlow = 0 xhigh = 100 howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.scatter(weibull_pp, sample, color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.ylabel(\"Value of RV\") matplotlib.pyplot.xlabel(\"Quantile Value\") mytitle = \"Normal Distribution Data Model Population mean = : \" + str(popmean)+ \" Population variance =:\" + str(popvar) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Some observations are in order: The population is a uniformly distributed collection. By random sampling, and keeping the sample size small, the sample distribution appears approximately normal. Real things of engineering interest are not always bounded as shown here, the choice of the Weibull plotting position is not arbitrary. The blue dot scatterplot in practice is called the empirical distribution function, or empirical quantile function. Now we will apply these ideas to some realistic data.","title":"Exceedence Probability"},{"location":"1-Lessons/Lesson17/lesson17/#beargrass-creek","text":"The file beargrass.txt contains annual peak flows for Beargrass Creek. The year is a water year, so the peaks occur on different days in each year; thus it is not a time series. Let's examine the data and see how well a Normal distribution data model fits, then estimate from the distribution the peak magnitude with exceedence probability 0.01 (1%-chance that will observe a value equal to or greater). import pandas beargrass = pandas.read_csv('beargrass.txt') #Reading a .csv file beargrass.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Year Peak 0 1945 1810 1 1946 791 2 1947 839 3 1948 1750 4 1949 898 # beargrass.plot() Now we will just copy code (the miracle of cut-n-paste!) sample = beargrass['Peak'].tolist() # put the peaks into a list sample_mean = numpy.array(sample).mean() sample_variance = numpy.array(sample).std()**2 sample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(sample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Normal Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() beargrass['Peak'].describe() count 31.000000 mean 1599.258065 std 1006.239500 min 707.000000 25% 908.000000 50% 1250.000000 75% 1945.000000 max 5200.000000 Name: Peak, dtype: float64 A 1% chance exceedence is on the right side of the chart, it is the compliment of 99% non-exceedence, in terms of our quantile function we want to find the value X that returns a quantile of 0.99. myguess = 6000 print(mu,sigma) print(normdist(myguess,mu,sigma)) 1599.258064516129 989.8767915427474 0.9999956206542673 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): mu = 1599.258064516129 sigma = 989.8767915427474 quantile = 0.99999 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) 5820.974479887303 So a peak discharge of 4000 or so is expected to be observed with 1% chance, notice we took the value from the fitted distribution, not the empirical set. As an observation, the Normal model is not a very good data model for these observations.","title":"Beargrass Creek"},{"location":"1-Lessons/Lesson17/lesson17/#log-normal","text":"Another data model we can try is log-normal, where we stipulate that the logarithms of the observations are normal. The scripts are practically the same, but there is an inverse transformation required to recover original value scale. Again we will use Beargrass creek. def loggit(x): # A prototype function to log transform x return(math.log(x)) logsample = beargrass['Peak'].apply(loggit).tolist() # put the peaks into a list sample_mean = numpy.array(logsample).mean() sample_variance = numpy.array(logsample).std()**2 logsample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model in Log Space sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(logsample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, logsample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Normal Data Model log sample mean = : \" + str(sample_mean)+ \" log sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() The plot doesn't look too bad, but we are in log-space, which is hard to interpret, so we will transform back to arithmetic space def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) sample = beargrass['Peak'].tolist() # pull original list sample.sort() # sort in place ################ mu = sample_mean # Fitted Model in Log Space sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(logsample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(antiloggit(xlow + i*xstep)) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Normal Data Model sample log mean = : \" + str((sample_mean))+ \" sample log variance =:\" + str((sample_variance)) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Visually a better data model, now lets determine the 1% chance value. myguess = 4440 print(mu,sigma) print(normdist(loggit(myguess),mu,sigma)) # mu, sigma already in log space - convert myguess 7.23730905616488 0.4984855728993489 0.9900772507418302 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): mu = 7.23730905616488 sigma = 0.4984855728993489 quantile = 0.99 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) 4433.567789173262 Now we have a decent method, we should put stuff into functions to keep code concise, lets examine a couple more data models","title":"Log-Normal"},{"location":"1-Lessons/Lesson17/lesson17/#gumbell-double-exponential-distribution","text":"The Gumbell is also called the Extreme-Value Type I distribution, the density and quantile function are: \\text{pdf(x)} = \\frac{1}{\\beta} \\cdot exp [-\\frac{(x-\\alpha)}{\\beta} - exp (-\\frac{(x-\\alpha)}{\\beta}) ] F(X) = \\int_{-\\infty}^X{\\frac{1}{\\beta} \\cdot exp [-\\frac{(x-\\alpha)}{\\beta} - exp (-\\frac{(x-\\alpha)}{\\beta}) ] dx} = exp [- exp (-\\frac{(X-\\alpha)}{\\beta})] The distribution has two parameters, \\alpha and \\beta , which in some sense play the same role as mean and variance. Lets modify our scripts further to see how this data model performs on the Bearcreek data. Of course we need a way to estimate the parameters, a good approximation can be obtained using: \\alpha = \\mu \\cdot \\frac{\\sqrt{6}}{\\pi} and \\beta = 0.45 \\cdot \\sigma where \\mu and \\sigma^2 are the sample mean and variance. def ev1dist(x,alpha,beta): argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist Now literally substitute into our prior code! sample = beargrass['Peak'].tolist() # put the peaks into a list sample_mean = numpy.array(sample).mean() sample_variance = numpy.array(sample).std()**2 alpha_mom = sample_mean*math.sqrt(6)/math.pi beta_mom = math.sqrt(sample_variance)*0.45 sample.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) ################ mu = sample_mean # Fitted Model sigma = math.sqrt(sample_variance) x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(sample) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = ev1dist(xlow + i*xstep,alpha_mom,beta_mom) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Extreme Value Type 1 Distribution Data Model sample mean = : \" + str(sample_mean)+ \" sample variance =:\" + str(sample_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Again a so-so visual fit. To find the 1% chance value myguess = 3300 print(alpha_mom,beta_mom) print(ev1dist(myguess,alpha_mom,beta_mom)) # 1246.9363972503857 445.4445561942363 0.990087892543188 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): alpha = 1246.9363972503857 beta = 445.4445561942363 quantile = 0.99 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) 3296.0478279991366","title":"Gumbell (Double Exponential) Distribution"},{"location":"1-Lessons/Lesson17/lesson17/#gamma-distribution-as-pearson-type-3","text":"One last data model to consider is one that is specifically stipulated for use by federal agencies for probability estimation of extreme hydrologic events. The data model ia called the Log-Pearson Type III distribution, its actually a specific case of a Gamma distrubution. This example we will dispense with tyring to build it in python primative, and just use a package - the density function is not all that hard, but the quantile function is elaborate. Learn more at http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/3-Readings/NumericalRecipesinF77.pdf (in particular around Page 276) As usual, lets let Google do some work for us, using the search term \"gamma quantile function; scipy\" we get to this nice blog entry https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html which is a good start. A Pearson Type III data model has the following density function: f(x|\\tau,\\alpha,\\beta) = \\frac{(\\frac{x-\\tau}{\\beta})^{\\alpha -1}\\cdot exp( - \\frac{x-\\tau}{\\beta})}{|\\beta| \\Gamma(\\alpha)} If we make some substitutions: \\lambda = \\frac{1}{\\beta} ; \\hat{x} = x -\\tau then the density function is f(\\hat{x}) = \\frac{ 1}{\\Gamma(\\alpha)} (\\lambda \\hat{x})^{\\alpha -1}\\cdot exp( - \\lambda \\hat{x} ) which is now a one parameter Gamma density function just like the example in the link. Reading a little from http://atomickitty.ddns.net/documents/university-courses/ce-5361-swhydrology/1-Lessons.src/Lesson22/AdditionalReading/Bulletin17C-tm4b5-draft-ACWI-17Jan2018.pdf we can relate the transformations to descriptive statistics (shown below without explaination) as: \\mu = \\text{sample mean} , \\sigma = \\text{sample standard deviation} , \\gamma = \\text{sample skew coefficient} = (\\frac{n}{\\sigma^3(n-1)(n-2)})\\sum_{i=1}^n(x_i - \\mu)^3 \\alpha = \\frac{4}{\\gamma^2} \\beta = sign(\\gamma)\\sqrt{\\frac{\\sigma^2}{\\alpha}} \\tau = \\mu - \\alpha \\cdot \\beta So we have a bit of work to do. The name of the functions in scipy we are interested in are gamma.pdf(x,a) and gamma.cdf(x,a) So lets build a tool to generate a Log-Pearson Type III data model, then apply it to Beargrass Creek. We will use a lot of glue here. First load in dependencies, and define support functions we will need import scipy.stats # import scipy stats package import math # import math package import numpy # import numpy package # log and antilog def loggit(x): # A prototype function to log transform x return(math.log(x)) def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) def weibull_pp(sample): # plotting position function # returns a list of plotting positions; sample must be a numeric list weibull_pp = [] # null list to return after fill sample.sort() # sort the sample list in place for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) return weibull_pp Then the gamma distribution from scipy, modified for our type of inputs. def gammacdf(x,tau,alpha,beta): # Gamma Cumulative Density function - with three parameter to one parameter convert xhat = x-tau lamda = 1.0/beta gammacdf = scipy.stats.gamma.cdf(lamda*xhat, alpha) return gammacdf Then load in the data from the data frame, log transform and generate descriptive statistics. #sample = beargrass['Peak'].tolist() # put the peaks into a list sample = beargrass['Peak'].apply(loggit).tolist() # put the log peaks into a list sample_mean = numpy.array(sample).mean() sample_stdev = numpy.array(sample).std() sample_skew = 3.0 # scipy.stats.skew(sample) sample_alpha = 4.0/(sample_skew**2) sample_beta = numpy.sign(sample_skew)*math.sqrt(sample_stdev**2/sample_alpha) sample_tau = sample_mean - sample_alpha*sample_beta Now generate plotting positions for the sample observations plotting = weibull_pp(sample) Now generate values for the data model (for plotting our red line \"fit\"), set limits to be a little beyond the sample range. x = []; ycdf = [] xlow = (0.9*min(sample)); xhigh = (1.1*max(sample)) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = gammacdf(xlow + i*xstep,sample_tau,sample_alpha,sample_beta) ycdf.append(yvalue) Now reverse transform back to native scale, and plot the sample values vs plotting position in blue, and the data model in red # reverse transform the peaks, and the data model peaks for i in range(len(sample)): sample[i] = antiloggit(sample[i]) for i in range(len(x)): x[i] = antiloggit(x[i]) myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(plotting, sample ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Pearson Type III Distribution Data Model\\n \" mytitle += \"Mean = \" + str(antiloggit(sample_mean)) + \"\\n\" mytitle += \"SD = \" + str(antiloggit(sample_stdev)) + \"\\n\" mytitle += \"Skew = \" + str(antiloggit(sample_skew)) + \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() And as before lets find the value that retruns the 99% quantile - we will just use the newton method above. First recover the required model parameters. Then we will paste these into the f(x) function for the Newton's method. print(sample_tau) print(sample_alpha) print(sample_beta) 6.904985340898647 0.4444444444444444 0.7477283593490234 # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): sample_tau = 5.976005311346212 sample_alpha = 6.402272915026134 sample_beta = 0.1970087438569494 quantile = 0.9900 argument = loggit(x) gammavalue = gammacdf(argument,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile myguess = 5000 print(newton(f, myguess)) 5856.10913158364 Trust, but verify! round(gammacdf(loggit(5856.109),sample_tau,sample_alpha,sample_beta),4) 0.9753 Now lets summarize our efforts regarding Beargrass Creek annual peaks and probabilities anticipated. Data Model 99% Peak Flow Remarks Normal 3902 so-so visual fit Log-Normal 4433 better visual fit Gumbell 3296 better visual fit Log-Pearson III 5856 best (of the set) visual fit At this point, now we have to choose our model and then can investigate different questions. So using LP3 as our favorite, lets now determine anticipated flow values for different probabilities (from the data model) - easy enought to just change the quantile value and rerun the newtons optimizer, for example: Exceedence Probability Flow Value Remarks 25% 968 First Quartile Divider 50% 1302 Median, and Second Quartile Divider 75% 1860 3rd Quartile Divider 90% 2706 10% chance of greater value 99% 5856 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% 9420 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% 11455 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # If we want to get fancy we can use Newton's method to get really close to the root from scipy.optimize import newton def f(x): sample_tau = 5.976005311346212 sample_alpha = 6.402272915026134 sample_beta = 0.1970087438569494 quantile = 0.50 argument = loggit(x) gammavalue = gammacdf(argument,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile myguess = 1000 print(newton(f, myguess)) 1302.814639184079","title":"Gamma Distribution (as Pearson Type 3)"},{"location":"1-Lessons/Lesson17/lesson17/#references","text":"Jamie Chan (2014) Learn Python in One Day and Learn It Well. LCF Publishing. Kindle Edition. http://www.learncodingfast.com/python Grus, Joel. Data Science from Scratch: First Principles with Python. O'Reilly Media. Kindle Edition. (http://safaribooksonline.com) Christian, B, and Griffiths Tom (2016) Algorithms to live by: The computer science of human decisions. Henry Holt and Company, ISBN 9781627790369 (hardcover)|ISBN 9781627790376 (electronic book) https://www.amazon.com/Distributional-Statistics-Environment-Statistical-Computing/dp/1463508417 England, J.F. Jr., Cohn, T.A., Faber, B.A., Stedinger, J.R., Thomas Jr., W.O., Veilleux, A.G., Kiang, J.E., and Mason, R.R.Jr., 2018, Guidelines for Determining Flood Flow Frequency\u2014Bulletin 17C: U.S. Geological Survey Techniques andMethods, book 4, chap. B5, 146 p., https://doi.org/10.3133/tm4B5 https://www.astroml.org/book_figures/chapter3/fig_gamma_distribution.html https://www.inferentialthinking.com/chapters/10/Sampling_and_Empirical_Distributions.html https://www.inferentialthinking.com/chapters/15/Prediction.html","title":"References:"},{"location":"1-Lessons/integration_differentiation/numerical_integration/","text":"Integration of Functions At this point we have enough Python to consider doing some useful computations. We will start with numerical integration because it is useful and only requires count-controlled repetition and single subscript lists. Background Numerical integration is the numerical approximation of \\begin{equation} I = \\int_a^b f(x)dx \\end{equation} Consider the problem of determining the shaded area under the curve y = f(x) from x = a to x = b , as depicted in the figure below, and suppose that analytical integration is not feasible. The function may be known in tabular form from experimental measurements or it may be known in an analytical form. The function is taken to be continuous within the interval a < x < b . We may divide the area into n vertical panels, each of width \\Delta x = (b - a)/n , and then add the areas of all strips to obtain A~\\approx \\int ydx . A representative panel of area A_i is shown with darker shading in the figure. Three useful numerical approximations are listed in the following sections. The approximations differ in how the function is represented by the panels --- in all cases the function is approximated by known polynomial models between the panel end points. In each case the greater the number of strips, and correspondingly smaller value of \\Delta x , the more accurate the approximation. Typically, one can begin with a relatively small number of panels and increase the number until the resulting area approximation stops changing. Rectangular Panels The figure below is a schematic of a rectangular panels. The figure is assuming the function structure is known and can be evaluated at an arbitrary location in the \\Delta x dimension. Each panel is treated as a rectangle, as shown by the representative panel whose height y_m is chosen visually so that the small cross-hatched areas are as nearly equal as possible. Thus, we form the sum \\sum y_m of the effective heights and multiply by \\Delta x . For a function known in analytical form, a value for y_m equal to that of the function at the midpoint x_i + \\Delta x /2 may be calculated and used in the summation. For tabulated functions, we have to choose to either take y_m as the value at the left endpoint or right endpoint. This limitation is often quite handy when we are trying to integrate a function that is integrable, but undefined on one endpoint. Lets try some examples in Python. Find the area under the curve y= x\\sqrt{1+x^2} from x = 0 to x = 2 . First lets read in the value for the lowerlimit, we will do some limited error checks to be sure user enters a number, but won't check that the number is non-negative. # RectangularPanels.py # Numerical Integration print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Program finds area under curve y = x * sqrt(1+x) Verify that value is indeed what we entered print(x_low) Now do the same for the upper limit, notice how we are using the yes variable. We set a \"fail\" value, and demand input until we get \"success\". The structure used here is called a try -- exception structure and is very common in programming. Error checking is really important so that garbled input does not hang things up. yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Again verify! print(x_high) Now use the try - exception structure to input how many panels we wish to use. Notice you can enter a negative value which will ultimately break things. Also observe this value is an integer. yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Again verify! print(how_many) Now we can actually perform the integration by evaluating the function at the panel half-widths. In this example we are using primitive arithmetic, so the \\sqrt{} is accomplished by exponentation, the syntax is c = a ** b is the operation c = a^b . The integration uses an accumulator, which is a memory location where subsquent results are added (accumulated) back into the accumulator. This structure is so common that there are alternate, compact syntax to perform this task, here it is all out in the open. The counting loop where we evaluate the function at different x values, starts at 1 and ends at how_many+1 because python for loops use an increment skip if equal structure. When the value in range equals how_many the for loop exits ( break is implied.) A loop control structure starting from 0 is shown in the code as a comment line. Simply uncomment this line, and comment the line just below to have the structure typical in python scripts. In the start from 1 case, we want to evaluate at the last value of how_many . # OK we should have the three things we need for evaluating the integral delta_x = (x_high - x_low)/float(how_many) # compute panel width xx = x_low + delta_x/2 # initial value for x ### OK THIS IS THE ACTUAL INTEGRATOR PART ### accumulated_area = 0.0 # initial value in an accumulator #for i in range(0,how_many,1): #note we are counting from 0 for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * ( (1+xx**2)**(0.5) ) ) * delta_x xx = xx + delta_x ### AND WE ARE DONE INTEGRATING ############# Finally, we want to report our result print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) # the backslash \\ # \" to x = ..... lets us use multiple lines # the \\n is a \"newline\" character The code implements rudimentary error checking -- it forces us to enter numeric values for the lower and upper values of x as well as the number of panels to use. It does not check for undefined ranges and such, but you should get the idea -- notice that a large fraction of the entire program is error trapping; this devotion to error trapping is typical for professional programs where you are going to distribute executable modules and not expect the end user to be a programmer. Using the math package The actual computations are done rather crudely -- there is a math package that would give us the ability to compute the square root as a function call rather than exponentiation to a real values exponent. That is illustrated below # RectangularPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator xx = x_low + delta_x/2 # initial value for x for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * sqrt(1+xx**2) ) * delta_x xx = xx + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) Trapezoidal Panels The trapezoidal panels are approximated as shown in the figure below. The area A_i is the average height (y_i + y_{i+1} )/2 times \\Delta x . Adding the areas gives the area approximation as tabulated. For the example with the curvature shown, the approximation will be on the low side. For the reverse curvature, the approximation will be on the high side. The trapezoidal approximation is commonly used with tabulated values. The script below illustrates the trapezoidal method for approximating an integral. In the example, the left and right panel endpoints in x are set as separate variables x_{left} and x_{right} and incremented by \\Delta x as we step through the count-controlled repetition to accumulate the area. The corresponding y values are computed within the loop and averaged, then multiplied by \\Delta x and added to the accumulator. Finally the x values are incremented --- for grins, we used the += operator on the accumulator # TrapezoidalPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_right = x_left + delta_x # initial value for x_right edge panel for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left* sqrt(1+x_left**2) ) y_right = ( x_right* sqrt(1+x_right**2) ) accumulated_area += + (1./2.) * ( y_left + y_right ) * delta_x x_left += delta_x x_right += delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) Parabolic Panels Parabolic panels approximate the shape of the panel with a parabola. The area between the chord and the curve (neglected in the trapezoidal solution) may be accounted for by approximating the function with a parabola passing through the points defined by three successive values of y . This area may be calculated from the geometry of the parabola and added to the trapezoidal area of the pair of strips to give the area \\Delta A of the pair as illustrated. Adding all of the \\Delta A s produces the tabulation shown, which is known as Simpson's rule. To use Simpson's rule, the number n of strips must be even. The same example as presented for rectangular panels is repeated, except using parabolic panels. The code is changed yet again because we will evaluate at each end of the panel as well as at an intermediate value. # ParabolicPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_middle = x_left + delta_x # initial value for x_middle edge panel x_right = x_middle + delta_x # initial value for x_right edge panel how_many = int(how_many/2) # using 2 panels every step, so 1/2 many steps -- force integer result for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left * sqrt(1+ x_left**2) ) y_middle = ( x_middle * sqrt(1+ x_middle**2) ) y_right = ( x_right * sqrt(1+ x_right**2) ) accumulated_area = accumulated_area + \\ (1./3.) * ( y_left + 4.* y_middle + y_right ) * delta_x x_left = x_left + 2*delta_x x_middle = x_left + delta_x x_right = x_middle + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) If we study all the forms of the numerical method we observe that the numerical integration method is really the sum of function values at specific locations in the interval of interest, with each value multiplied by a specific weight. In this development the weights were based on polynomials, but other method use different weighting functions. An extremely important method is called gaussian quadrature. This method is valuable because one can approximate convolution integrals quite effectively using quadrature routines, while the number of function evaluations for a polynomial based approximation could be hopeless. When the function values are tabular, we are going to have to accept the rectangular (with adaptations) and trapezoidal as our best tools to approximate an integral because we don't have any really effective way to evaluate the function between the tabulated values. Integration of Tabular Data This section is going to work with tabular data -- different from function evaluation, but similar. To be really useful, we need to learn how to read data from a file; manually entering tabular data is really time consuming, error prone, and just plain idiotic. So in this chapter we will learn how to read data from a file into a list, then we can process the list as if it were a function and integrate its contents. Reading from a file --- open, read, close files First, lets consider a file named MyFile.txt . The extension is important so that the Shell does not think it is a Python script. The contents of MyFile.txt are: 1 1 2 4 3 9 4 16 5 25 The code fragment below, will let us look at the file (already existing in our local directory) import subprocess # lets us run \"shell\" commands and recover stdio stream usefull_cat_call = subprocess.run([\"cat\",\"MyFile.txt\"], stdout=subprocess.PIPE, text=True) # this is the call to run the bash command \"cat MyFile.txt\" which will display the contents of the file if it exists. print(usefull_cat_call.stdout) 1 1 2 4 3 9 4 16 5 25 Now that we know that the file exists,to read the contents into a Python script we have to do the following: Open a connection to the file --- this is a concept common to all languages, it might be called something different, but the program needs to somehow know the location and name of the file. Read the contents into an object --- we have a lot of control on how this gets done, for the time being we won't exercise much control yet. When you do substantial programs, you will depend on the control of the reads (and writes). Disconnect the file --- this too is common to all languages. Its a really easy step to forget. Not a big deal if the program ends as planned but terrible if there is a error in the program and the connection is still open. Usually nothing bad happens, but with an open connection it is possible for the file to get damaged. If that file represents millions of customers credit card numbers, that's kind of a problem, and time to go work on your resume, or get your passport collection out and choose a country without extradition. The code fragment below performs these three tasks and prints the things we read Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read the five lines line1 = Afile.readline() line2 = Afile.readline() line3 = Afile.readline() line4 = Afile.readline() line5 = Afile.readline() Afile.close() # disconnect from the file # echo the input print(line1,end=\"\") print(line2,end=\"\") print(line3,end=\"\") print(line4,end=\"\") print(line5,end=\"\") 1 1 2 4 3 9 4 16 5 25 Read into a list A far more useful and elegant way to read from a file is to use a for loop. The attribute line within a file is an iterable, hence construction the loop is pretty straightforward. A script fragment below does the same thing as the example above, but uses a for loop to accomplish stepping through the file. Additionally, I have added a counter to keep track of how many lines were read --- in a lot of engineering programs, the number of things read becomes important later in a program, hence it is usually a good idea to capture the count when the data are first read. First lets work out if we can automatically detect the end of the file. So this script just reads and prints the attribute line from object Afile . Notice how the print statement is changed, to suppress the extra line feed. Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) Now we will add a list to receive the input, here it reads the file above as a string into a list xy , then splits that list and places the contents into two other lists, x and y . The script has several parts to discuss. First, the destination variables (lists) must be created -- I used the null list concept here because I don't know how big the list is until I read the list. Next I used the .append() method which operates on the xy list. The arguments of the method [str(n) for n in line.strip().split()] tells the program that the elements are to be interpreted as a string, and to split (split) the line into sub-strings based on a null delimiter (whitespace), and to remove all the whitespace (strip) characters. Once the line is split, the strings are appended into the xy list. The xy list is printed to show that it is a list of 5 things, each thing being a string comprised of two sets of characters separated by a comma. xy is a list of strings. The next section of the code then uses the pair function within another .append() method to break the character sets in each element of xy into two parts x and y . Lastly during the pair operation, the code also converts the data into real values (float) and then prints the data in two columns. This seems like a lot of work, but we could easily get this code to be super reliable, then save it as a function and never have to write it again. That too comes later -- suffice to say for now we can read a file, parse its contents into two lists x and y . Thus we are now able to integrate tabular data. xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) The list is: [['1', '1'], ['2', '4'], ['3', '9'], ['4', '16'], ['5', '25']] x = 1.0 y = 1.0 x = 2.0 y = 4.0 x = 3.0 y = 9.0 x = 4.0 y = 16.0 x = 5.0 y = 25.0 Integrating the Tabular Data Suppose instead of a function we only have tabulations and wist to estimate the area under the curve represented by the tabular values. Then our integration rules from the prior chapter still work more or less, except the rectangular panels will have to be shifted to either the left edge or right edge of a panel (where the tabulation exists). Lets just examine an example. Suppose some measurement technology produced a table of related values. The excitation variable is x and f(x) is the response. x f(x) 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 To integrate this table using the trapezoidal method is straightforward. We will modify our earlier code to read the table (which we put into a file), and compute the integral. # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 File has 9 records (lines) The list is: [['1.0', '1.543'], ['1.1', '1.668'], ['1.2', '1.811'], ['1.3', '1.971'], ['1.4', '2.151'], ['1.5', '2.352'], ['1.6', '2.577'], ['1.7', '2.828'], ['1.8', '3.107']] x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Cool, it seems to work -- now tidy the code a bit by suppressing extra outputs # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: ##print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nRecords read =: \",how_many_lines) ##print(\"The list is: \",end=\"\") ##print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result Records read =: 9 x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Realistically the only other simple integration method for tabular data is the rectangular rule, either using the left edge of a panel or the right edge of a panel (and I suppose you could do both and average the result which would be the trapezoidal method). Exercises 1) Approximate \\int_0^2 f(x) dx from the tabulation in the Table below: x f(x) 0.00 1.0000 0.12 0.8869 0.53 0.5886 0.87 0.4190 1.08 0.3396 1.43 0.2393 2.00 0.1353 # 2) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{9.0} cosh(x) dx from the tabulation above. 3) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{4.2} cosh(x) dx from the tabulation above. Briefly explain how you handle starting and stopping the integration from values that are intermediate and are tabulated. # 4) (Advanced) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{4.0} cosh(x) dx from the tabulation above. Explain how handled working with values that fall between tabulated values.","title":"Numerical integration"},{"location":"1-Lessons/integration_differentiation/numerical_integration/#integration-of-functions","text":"At this point we have enough Python to consider doing some useful computations. We will start with numerical integration because it is useful and only requires count-controlled repetition and single subscript lists.","title":"Integration of Functions"},{"location":"1-Lessons/integration_differentiation/numerical_integration/#background","text":"Numerical integration is the numerical approximation of \\begin{equation} I = \\int_a^b f(x)dx \\end{equation} Consider the problem of determining the shaded area under the curve y = f(x) from x = a to x = b , as depicted in the figure below, and suppose that analytical integration is not feasible. The function may be known in tabular form from experimental measurements or it may be known in an analytical form. The function is taken to be continuous within the interval a < x < b . We may divide the area into n vertical panels, each of width \\Delta x = (b - a)/n , and then add the areas of all strips to obtain A~\\approx \\int ydx . A representative panel of area A_i is shown with darker shading in the figure. Three useful numerical approximations are listed in the following sections. The approximations differ in how the function is represented by the panels --- in all cases the function is approximated by known polynomial models between the panel end points. In each case the greater the number of strips, and correspondingly smaller value of \\Delta x , the more accurate the approximation. Typically, one can begin with a relatively small number of panels and increase the number until the resulting area approximation stops changing.","title":"Background"},{"location":"1-Lessons/integration_differentiation/numerical_integration/#rectangular-panels","text":"The figure below is a schematic of a rectangular panels. The figure is assuming the function structure is known and can be evaluated at an arbitrary location in the \\Delta x dimension. Each panel is treated as a rectangle, as shown by the representative panel whose height y_m is chosen visually so that the small cross-hatched areas are as nearly equal as possible. Thus, we form the sum \\sum y_m of the effective heights and multiply by \\Delta x . For a function known in analytical form, a value for y_m equal to that of the function at the midpoint x_i + \\Delta x /2 may be calculated and used in the summation. For tabulated functions, we have to choose to either take y_m as the value at the left endpoint or right endpoint. This limitation is often quite handy when we are trying to integrate a function that is integrable, but undefined on one endpoint. Lets try some examples in Python. Find the area under the curve y= x\\sqrt{1+x^2} from x = 0 to x = 2 . First lets read in the value for the lowerlimit, we will do some limited error checks to be sure user enters a number, but won't check that the number is non-negative. # RectangularPanels.py # Numerical Integration print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Program finds area under curve y = x * sqrt(1+x) Verify that value is indeed what we entered print(x_low) Now do the same for the upper limit, notice how we are using the yes variable. We set a \"fail\" value, and demand input until we get \"success\". The structure used here is called a try -- exception structure and is very common in programming. Error checking is really important so that garbled input does not hang things up. yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Again verify! print(x_high) Now use the try - exception structure to input how many panels we wish to use. Notice you can enter a negative value which will ultimately break things. Also observe this value is an integer. yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") # exit the while loop when finally have a valid number Again verify! print(how_many) Now we can actually perform the integration by evaluating the function at the panel half-widths. In this example we are using primitive arithmetic, so the \\sqrt{} is accomplished by exponentation, the syntax is c = a ** b is the operation c = a^b . The integration uses an accumulator, which is a memory location where subsquent results are added (accumulated) back into the accumulator. This structure is so common that there are alternate, compact syntax to perform this task, here it is all out in the open. The counting loop where we evaluate the function at different x values, starts at 1 and ends at how_many+1 because python for loops use an increment skip if equal structure. When the value in range equals how_many the for loop exits ( break is implied.) A loop control structure starting from 0 is shown in the code as a comment line. Simply uncomment this line, and comment the line just below to have the structure typical in python scripts. In the start from 1 case, we want to evaluate at the last value of how_many . # OK we should have the three things we need for evaluating the integral delta_x = (x_high - x_low)/float(how_many) # compute panel width xx = x_low + delta_x/2 # initial value for x ### OK THIS IS THE ACTUAL INTEGRATOR PART ### accumulated_area = 0.0 # initial value in an accumulator #for i in range(0,how_many,1): #note we are counting from 0 for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * ( (1+xx**2)**(0.5) ) ) * delta_x xx = xx + delta_x ### AND WE ARE DONE INTEGRATING ############# Finally, we want to report our result print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) # the backslash \\ # \" to x = ..... lets us use multiple lines # the \\n is a \"newline\" character The code implements rudimentary error checking -- it forces us to enter numeric values for the lower and upper values of x as well as the number of panels to use. It does not check for undefined ranges and such, but you should get the idea -- notice that a large fraction of the entire program is error trapping; this devotion to error trapping is typical for professional programs where you are going to distribute executable modules and not expect the end user to be a programmer.","title":"Rectangular Panels"},{"location":"1-Lessons/integration_differentiation/numerical_integration/#using-the-math-package","text":"The actual computations are done rather crudely -- there is a math package that would give us the ability to compute the square root as a function call rather than exponentiation to a real values exponent. That is illustrated below # RectangularPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator xx = x_low + delta_x/2 # initial value for x for i in range(1,how_many+1,1): #note we are counting from 1 accumulated_area = accumulated_area + ( xx * sqrt(1+xx**2) ) * delta_x xx = xx + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area)","title":"Using the math package"},{"location":"1-Lessons/integration_differentiation/numerical_integration/#trapezoidal-panels","text":"The trapezoidal panels are approximated as shown in the figure below. The area A_i is the average height (y_i + y_{i+1} )/2 times \\Delta x . Adding the areas gives the area approximation as tabulated. For the example with the curvature shown, the approximation will be on the low side. For the reverse curvature, the approximation will be on the high side. The trapezoidal approximation is commonly used with tabulated values. The script below illustrates the trapezoidal method for approximating an integral. In the example, the left and right panel endpoints in x are set as separate variables x_{left} and x_{right} and incremented by \\Delta x as we step through the count-controlled repetition to accumulate the area. The corresponding y values are computed within the loop and averaged, then multiplied by \\Delta x and added to the accumulator. Finally the x values are incremented --- for grins, we used the += operator on the accumulator # TrapezoidalPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_right = x_left + delta_x # initial value for x_right edge panel for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left* sqrt(1+x_left**2) ) y_right = ( x_right* sqrt(1+x_right**2) ) accumulated_area += + (1./2.) * ( y_left + y_right ) * delta_x x_left += delta_x x_right += delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area)","title":"Trapezoidal Panels"},{"location":"1-Lessons/integration_differentiation/numerical_integration/#parabolic-panels","text":"Parabolic panels approximate the shape of the panel with a parabola. The area between the chord and the curve (neglected in the trapezoidal solution) may be accounted for by approximating the function with a parabola passing through the points defined by three successive values of y . This area may be calculated from the geometry of the parabola and added to the trapezoidal area of the pair of strips to give the area \\Delta A of the pair as illustrated. Adding all of the \\Delta A s produces the tabulation shown, which is known as Simpson's rule. To use Simpson's rule, the number n of strips must be even. The same example as presented for rectangular panels is repeated, except using parabolic panels. The code is changed yet again because we will evaluate at each end of the panel as well as at an intermediate value. # ParabolicPanels.py # Numerical Integration # Use built-in math functions import math # a package of math functions # we are naming an object \"sqrt\" that will compute the square root def sqrt (x): return math.sqrt(x) # saves us having to type math.NAME every time we wish to use a function # in this program not all that meaningful, but in complex programs handy! print (\"Program finds area under curve y = x * sqrt(1+x)\") # Get input data -- use error checking yes = 0 while yes == 0: x_low = input(\"Enter a lower bound x_low \\n\") try: x_low = float(x_low) yes = 1 except: print (\"x_low really needs to be a number, try again \\n\") yes = 0 while yes == 0: x_high = input(\"Enter an upper bound x_high \\n\") try: x_high = float(x_high) yes = 1 except: print (\"x_high really needs to be a number, try again \\n\") yes = 0 while yes == 0: how_many = input(\"Enter how many panels \\n\") try: how_many = int(how_many) yes = 1 except: print (\"Panels really needs to be a number, try again \\n\") delta_x = (x_high - x_low)/float(how_many) # compute panel width accumulated_area = 0.0 # initial value in an accumulator x_left = x_low # initial value for x_left edge panel x_middle = x_left + delta_x # initial value for x_middle edge panel x_right = x_middle + delta_x # initial value for x_right edge panel how_many = int(how_many/2) # using 2 panels every step, so 1/2 many steps -- force integer result for i in range(1,how_many+1,1): #note we are counting from 1 y_left = ( x_left * sqrt(1+ x_left**2) ) y_middle = ( x_middle * sqrt(1+ x_middle**2) ) y_right = ( x_right * sqrt(1+ x_right**2) ) accumulated_area = accumulated_area + \\ (1./3.) * ( y_left + 4.* y_middle + y_right ) * delta_x x_left = x_left + 2*delta_x x_middle = x_left + delta_x x_right = x_middle + delta_x print (\"Area under curve y = x * sqrt(1+x) from x = \",x_low,\\ \" to x = \",x_high,\"\\n is approximately: \",accumulated_area) If we study all the forms of the numerical method we observe that the numerical integration method is really the sum of function values at specific locations in the interval of interest, with each value multiplied by a specific weight. In this development the weights were based on polynomials, but other method use different weighting functions. An extremely important method is called gaussian quadrature. This method is valuable because one can approximate convolution integrals quite effectively using quadrature routines, while the number of function evaluations for a polynomial based approximation could be hopeless. When the function values are tabular, we are going to have to accept the rectangular (with adaptations) and trapezoidal as our best tools to approximate an integral because we don't have any really effective way to evaluate the function between the tabulated values.","title":"Parabolic Panels"},{"location":"1-Lessons/integration_differentiation/numerical_integration/#integration-of-tabular-data","text":"This section is going to work with tabular data -- different from function evaluation, but similar. To be really useful, we need to learn how to read data from a file; manually entering tabular data is really time consuming, error prone, and just plain idiotic. So in this chapter we will learn how to read data from a file into a list, then we can process the list as if it were a function and integrate its contents.","title":"Integration of Tabular Data"},{"location":"1-Lessons/integration_differentiation/numerical_integration/#reading-from-a-file-open-read-close-files","text":"First, lets consider a file named MyFile.txt . The extension is important so that the Shell does not think it is a Python script. The contents of MyFile.txt are: 1 1 2 4 3 9 4 16 5 25 The code fragment below, will let us look at the file (already existing in our local directory) import subprocess # lets us run \"shell\" commands and recover stdio stream usefull_cat_call = subprocess.run([\"cat\",\"MyFile.txt\"], stdout=subprocess.PIPE, text=True) # this is the call to run the bash command \"cat MyFile.txt\" which will display the contents of the file if it exists. print(usefull_cat_call.stdout) 1 1 2 4 3 9 4 16 5 25 Now that we know that the file exists,to read the contents into a Python script we have to do the following: Open a connection to the file --- this is a concept common to all languages, it might be called something different, but the program needs to somehow know the location and name of the file. Read the contents into an object --- we have a lot of control on how this gets done, for the time being we won't exercise much control yet. When you do substantial programs, you will depend on the control of the reads (and writes). Disconnect the file --- this too is common to all languages. Its a really easy step to forget. Not a big deal if the program ends as planned but terrible if there is a error in the program and the connection is still open. Usually nothing bad happens, but with an open connection it is possible for the file to get damaged. If that file represents millions of customers credit card numbers, that's kind of a problem, and time to go work on your resume, or get your passport collection out and choose a country without extradition. The code fragment below performs these three tasks and prints the things we read Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read the five lines line1 = Afile.readline() line2 = Afile.readline() line3 = Afile.readline() line4 = Afile.readline() line5 = Afile.readline() Afile.close() # disconnect from the file # echo the input print(line1,end=\"\") print(line2,end=\"\") print(line3,end=\"\") print(line4,end=\"\") print(line5,end=\"\") 1 1 2 4 3 9 4 16 5 25","title":"Reading from a file --- open, read, close files"},{"location":"1-Lessons/integration_differentiation/numerical_integration/#read-into-a-list","text":"A far more useful and elegant way to read from a file is to use a for loop. The attribute line within a file is an iterable, hence construction the loop is pretty straightforward. A script fragment below does the same thing as the example above, but uses a for loop to accomplish stepping through the file. Additionally, I have added a counter to keep track of how many lines were read --- in a lot of engineering programs, the number of things read becomes important later in a program, hence it is usually a good idea to capture the count when the data are first read. First lets work out if we can automatically detect the end of the file. So this script just reads and prints the attribute line from object Afile . Notice how the print statement is changed, to suppress the extra line feed. Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) Now we will add a list to receive the input, here it reads the file above as a string into a list xy , then splits that list and places the contents into two other lists, x and y . The script has several parts to discuss. First, the destination variables (lists) must be created -- I used the null list concept here because I don't know how big the list is until I read the list. Next I used the .append() method which operates on the xy list. The arguments of the method [str(n) for n in line.strip().split()] tells the program that the elements are to be interpreted as a string, and to split (split) the line into sub-strings based on a null delimiter (whitespace), and to remove all the whitespace (strip) characters. Once the line is split, the strings are appended into the xy list. The xy list is printed to show that it is a list of 5 things, each thing being a string comprised of two sets of characters separated by a comma. xy is a list of strings. The next section of the code then uses the pair function within another .append() method to break the character sets in each element of xy into two parts x and y . Lastly during the pair operation, the code also converts the data into real values (float) and then prints the data in two columns. This seems like a lot of work, but we could easily get this code to be super reliable, then save it as a function and never have to write it again. That too comes later -- suffice to say for now we can read a file, parse its contents into two lists x and y . Thus we are now able to integrate tabular data. xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyFile.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) 1 1 2 4 3 9 4 16 5 25 File has 5 records (lines) The list is: [['1', '1'], ['2', '4'], ['3', '9'], ['4', '16'], ['5', '25']] x = 1.0 y = 1.0 x = 2.0 y = 4.0 x = 3.0 y = 9.0 x = 4.0 y = 16.0 x = 5.0 y = 25.0","title":"Read into a list"},{"location":"1-Lessons/integration_differentiation/numerical_integration/#integrating-the-tabular-data","text":"Suppose instead of a function we only have tabulations and wist to estimate the area under the curve represented by the tabular values. Then our integration rules from the prior chapter still work more or less, except the rectangular panels will have to be shifted to either the left edge or right edge of a panel (where the tabulation exists). Lets just examine an example. Suppose some measurement technology produced a table of related values. The excitation variable is x and f(x) is the response. x f(x) 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 To integrate this table using the trapezoidal method is straightforward. We will modify our earlier code to read the table (which we put into a file), and compute the integral. # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nFile has \",how_many_lines,\" records (lines)\") print(\"The list is: \",end=\"\") print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result 1.0 1.543 1.1 1.668 1.2 1.811 1.3 1.971 1.4 2.151 1.5 2.352 1.6 2.577 1.7 2.828 1.8 3.107 File has 9 records (lines) The list is: [['1.0', '1.543'], ['1.1', '1.668'], ['1.2', '1.811'], ['1.3', '1.971'], ['1.4', '2.151'], ['1.5', '2.352'], ['1.6', '2.577'], ['1.7', '2.828'], ['1.8', '3.107']] x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Cool, it seems to work -- now tidy the code a bit by suppressing extra outputs # My Tabular Integration # Integrate a table of values using Trapezoidal Panels xy = [] # null list to store the lines x = [] # a null list for the first column y = [] # a null list for the second column Afile = open(\"MyTableOfData.txt\",\"r\") # open a connection to the file; set to \"read\" # read using a for loop, exit when at end of file and report line count how_many_lines = 0 # start our counter! for line in Afile: ##print(line,end=\"\") xy.append([str(n) for n in line.strip().split()]) # append line to xy, split the line on whitespace, strip out whitespace how_many_lines += 1 Afile.close() # disconnect from the file print(\"\\nRecords read =: \",how_many_lines) ##print(\"The list is: \",end=\"\") ##print(xy) # the list for pair in xy: # parse into x and y x.append(float(pair[0])) y.append(float(pair[1])) # verify parsed for i in range (0,how_many_lines,1): print(\"x = \",x[i],\" y = \",y[i]) # now the actual integration accumulated_area = 0 # an accumulator for i in range(0,how_many_lines-1,1): #index stops at n-1 things because each panel evaluated at both ends delta_x = x[i+1]-x[i] height =(y[i+1]+y[i])/2.0 accumulated_area += height*delta_x print(\"Area = \",accumulated_area) # report the result Records read =: 9 x = 1.0 y = 1.543 x = 1.1 y = 1.668 x = 1.2 y = 1.811 x = 1.3 y = 1.971 x = 1.4 y = 2.151 x = 1.5 y = 2.352 x = 1.6 y = 2.577 x = 1.7 y = 2.828 x = 1.8 y = 3.107 Area = 1.7683000000000002 Realistically the only other simple integration method for tabular data is the rectangular rule, either using the left edge of a panel or the right edge of a panel (and I suppose you could do both and average the result which would be the trapezoidal method).","title":"Integrating the Tabular Data"},{"location":"1-Lessons/integration_differentiation/numerical_integration/#exercises","text":"1) Approximate \\int_0^2 f(x) dx from the tabulation in the Table below: x f(x) 0.00 1.0000 0.12 0.8869 0.53 0.5886 0.87 0.4190 1.08 0.3396 1.43 0.2393 2.00 0.1353 # 2) The table below is a tabulation of various values of the hyperbolic cosine function. x cosh(x) 1.0 1.54308063481524 1.1 1.66851855382226 1.2 1.81065556732437 1.3 1.97091423032663 1.4 2.15089846539314 1.5 2.35240961524325 1.6 2.57746447119489 1.7 2.82831545788997 1.8 3.10747317631727 2.0 3.76219569108363 2.2 4.56790832889823 2.4 5.55694716696551 2.6 6.76900580660801 2.8 8.25272841686113 3.0 10.0676619957778 3.3 13.5747610440296 3.6 18.3127790830626 3.9 24.711345508488 4.2 33.3506633088728 4.6 49.7471837388392 5.0 74.2099485247878 5.5 122.348009517829 6.0 201.715636122456 7.0 548.317035155212 8.0 1490.47916125218 9.0 4051.54202549259 Approximate \\int_1^{9.0} cosh(x) dx from the tabulation above.","title":"Exercises"},{"location":"6-Projects/P-ConcreteStrength/ConcreteStrength-Project/","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 \u2013 Computational Thinking and Data Science Fall 2020 Concrete Strength Predictor Final Project - Background The Compressive Strength of Concrete determines the quality of Concrete. The strength is determined by a standard crushing test on a concrete cylinder, that requires engineers to build small concrete cylinders with different combinations of raw materials and test these cylinders for strength variations with a change in each raw material. The recommended wait time for testing the cylinder is 28 days to ensure correct results, although there are formulas for making estimates from shorter cure times. The formal 28-day approach consumes a lot of time and labor to prepare different prototypes and test them; the method itself is error prone and mistakes can cause the wait time to drastically increase. One way of reducing the wait time and reducing the number of combinations to try is to make use of digital simulations, where we can provide information to the computer about what we know and the computer tries different combinations to predict the compressive strength. This approach can reduce the number of combinations we can try physically and reduce the total amount of time for experimentation. But, to design such software we have to know the relations between all the raw materials and how one material affects the strength. It is possible to derive mathematical equations and run simulations based on these equations, but we cannot expect the relations to be same in real-world. Also, these tests have been performed for many numbers of times now and we have enough real-world data that can be used for predictive modelling. Objective(s): Literature scan on concrete design, and utility of a predictive approach Analyse an existing concrete compressive strength database and build a data model to predict the compressive strength of a concrete mixture. Build an interface to allow users to enter concrete mixtures and return an estimated strength and an assessment of the uncertainty in the estimate Build an interface to allow users to add observations to the underlying database, and automatically update the Data Model to incorporate the new observations Tasks: Literature Research: - Describe the challenge of concrete mixture design and the importance of compressive strength. - Summarize the value of a data model in the context of the conventional approach to strength prediction Some places to start are: - I-Cheng Yeh, \u201c Modeling of strength of high performance concrete using artificial neural networks,\u201d Cement and Concrete Research, Vol. 28, \u211612, pp. 1797\u20131808 (1998). https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength Laskar, Aminul Islam. (2011). Mix design of high-performance concrete. Materials Research, 14(4), 429-433. Epub November 21, 2011.https://doi.org/10.1590/S1516-14392011005000088 or http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-ConcreteStrength/Mix_Design_of_High-performance_Concrete.pdf Ahsanul Kabir, Md Monjurul Hasan, Khasro Miah, \u201c Strength Prediction Model for Concrete\u201d, ACEE Int. J. on Civil and Environmental Engineering, Vol. 2, \u21161, Aug 2013. https://towardsdatascience.com/concrete-compressive-strength-prediction-using-machine-learning-4a531b3c43f3 Castro, A.L. & Liborio, J.B.L. & Valenzuela, Federico & Pandolfelli, Victor. (2008). The application of rheological concepts on the evaluation of high-performance concrete workability. American Concrete Institute, ACI Special Publication. 119-131. copy at: http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-ConcreteStrength/ArtigoHPC026_CASTROetalfinal.pdf de Larrard, Fran\u00e7ois & Sedran, Thierry. (2002). Mixture-proportioning of high-performance concrete. Cement and Concrete Research. 32. 1699-1704. 10.1016/S0008-8846(02)00861-X. copy at: http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-ConcreteStrength/Mixture-ProportioningCCR-deLarrardSedran-full.pdf Database Acquisition - Get the database from the repository: https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls - Supply links to any additional databases discovered during the literature research - A copy of the database is located at: http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-ConcreteStrength/concreteData.xls If you cannot access the original database you can use this copy Exploratory Data Analysis - Describe (in words) the database. - Reformat as needed (column headings perhaps) the database for subsequent analysis. - Select possible data model structures (multi-feature linear model, power-law, ...) - Select possible data model \"fitting\" tools (ordinary least squares,lasso regression, decision trees, random forests, ...) Model Building - Build data models - Assess data model quality (decide which model is best) - Build the input data interface for using the \"best\" model - Using your best model determine projected concrete strength for 5 possible mixtures in the table below: Cement BlastFurnaceSlag FlyAsh CoarseAggregate FineAggregate Water Superplasticizer Age 175.0 13.0 172.0 1000.0 856.0 156.0 4.0 3.0 320.0 0.0 0.0 970.0 850.0 192.0 0.0 7.0 320.0 0.0 126.0 860.0 856.0 209.0 5.70 28.0 320.0 73.0 54.0 972.0 773.0 181.0 6.0 45.0 530.0 359.0 200.0 1145.0 992.0 247.0 32.0 365.0 Documentation - Training video on how to use your tool, and demonstrate the tool(s) as they are run - Project management video - Interim report (see deliverables below); this document must be rendered as a .pdf, but you are free to use your favorite writing software (Word,LibreOffice, ...). - Final Report (see deliverables below) Deliverables: Part 1 (due November 24): A report that briefly describes the concrete strength database and how you plan to solve the tasks of creating a suitable data model. - Break down each task into manageable subtasks and describe how you intend to solve the subtasks and how you will test each task. (Perhaps make a simple Gantt Chart) - Address the responsibilities of each team member for tasks completed and tasks to be completed until the end of the semester. (Perhaps make explicit subtask assignments) Your report should be limited to 4 pages, 12 pt font size, double linespacing (exclusive of references which are NOT included in the page count). You need to cite/reference all sources you used. Part 2 (due on Final Exam day): A well-documented JupyterLab (using a python kernel) analysis and implementation for the data model. A well-documented JupyterLab (using a python kernel) implementation for the data model user interface. A well-documented JupyterLab (using a python kernel) implementation for the database update interface. Above items can reside in a single notebook; but clearly identify sections that perform different tasks. A how-to video demonstrating performance and description of problems that you were not able to solve. A project management video (up to 5 minutes) in which you explain how you completed the project and how you worked as a team. Above items can reside in a single video; but structure the video into the two parts; use an obvious transition when moving from \"how to ...\" into the project management portion. Keep the total video length to less than 10 minutes; submit as an unlisted YouTube video, and just supply the link (someone on each team is likely to have a YouTube creator account). Keep in mind a 10 minute video can approach 100MB file size before compression, so it won't upload to Blackboard and cannot be emailed.","title":"ConcreteStrength Project"},{"location":"6-Projects/P-ConcreteStrength/ConcreteStrength-Project/#engr-1330-computational-thinking-and-data-science-fall-2020","text":"","title":"ENGR 1330 \u2013 Computational Thinking and Data Science Fall 2020"},{"location":"6-Projects/P-ConcreteStrength/ConcreteStrength-Project/#concrete-strength-predictor-final-project-background","text":"The Compressive Strength of Concrete determines the quality of Concrete. The strength is determined by a standard crushing test on a concrete cylinder, that requires engineers to build small concrete cylinders with different combinations of raw materials and test these cylinders for strength variations with a change in each raw material. The recommended wait time for testing the cylinder is 28 days to ensure correct results, although there are formulas for making estimates from shorter cure times. The formal 28-day approach consumes a lot of time and labor to prepare different prototypes and test them; the method itself is error prone and mistakes can cause the wait time to drastically increase. One way of reducing the wait time and reducing the number of combinations to try is to make use of digital simulations, where we can provide information to the computer about what we know and the computer tries different combinations to predict the compressive strength. This approach can reduce the number of combinations we can try physically and reduce the total amount of time for experimentation. But, to design such software we have to know the relations between all the raw materials and how one material affects the strength. It is possible to derive mathematical equations and run simulations based on these equations, but we cannot expect the relations to be same in real-world. Also, these tests have been performed for many numbers of times now and we have enough real-world data that can be used for predictive modelling.","title":"Concrete Strength Predictor Final Project - Background"},{"location":"6-Projects/P-ConcreteStrength/ConcreteStrength-Project/#objectives","text":"Literature scan on concrete design, and utility of a predictive approach Analyse an existing concrete compressive strength database and build a data model to predict the compressive strength of a concrete mixture. Build an interface to allow users to enter concrete mixtures and return an estimated strength and an assessment of the uncertainty in the estimate Build an interface to allow users to add observations to the underlying database, and automatically update the Data Model to incorporate the new observations","title":"Objective(s):"},{"location":"6-Projects/P-ConcreteStrength/ConcreteStrength-Project/#tasks","text":"Literature Research: - Describe the challenge of concrete mixture design and the importance of compressive strength. - Summarize the value of a data model in the context of the conventional approach to strength prediction Some places to start are: - I-Cheng Yeh, \u201c Modeling of strength of high performance concrete using artificial neural networks,\u201d Cement and Concrete Research, Vol. 28, \u211612, pp. 1797\u20131808 (1998). https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength Laskar, Aminul Islam. (2011). Mix design of high-performance concrete. Materials Research, 14(4), 429-433. Epub November 21, 2011.https://doi.org/10.1590/S1516-14392011005000088 or http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-ConcreteStrength/Mix_Design_of_High-performance_Concrete.pdf Ahsanul Kabir, Md Monjurul Hasan, Khasro Miah, \u201c Strength Prediction Model for Concrete\u201d, ACEE Int. J. on Civil and Environmental Engineering, Vol. 2, \u21161, Aug 2013. https://towardsdatascience.com/concrete-compressive-strength-prediction-using-machine-learning-4a531b3c43f3 Castro, A.L. & Liborio, J.B.L. & Valenzuela, Federico & Pandolfelli, Victor. (2008). The application of rheological concepts on the evaluation of high-performance concrete workability. American Concrete Institute, ACI Special Publication. 119-131. copy at: http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-ConcreteStrength/ArtigoHPC026_CASTROetalfinal.pdf de Larrard, Fran\u00e7ois & Sedran, Thierry. (2002). Mixture-proportioning of high-performance concrete. Cement and Concrete Research. 32. 1699-1704. 10.1016/S0008-8846(02)00861-X. copy at: http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-ConcreteStrength/Mixture-ProportioningCCR-deLarrardSedran-full.pdf Database Acquisition - Get the database from the repository: https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls - Supply links to any additional databases discovered during the literature research - A copy of the database is located at: http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-ConcreteStrength/concreteData.xls If you cannot access the original database you can use this copy Exploratory Data Analysis - Describe (in words) the database. - Reformat as needed (column headings perhaps) the database for subsequent analysis. - Select possible data model structures (multi-feature linear model, power-law, ...) - Select possible data model \"fitting\" tools (ordinary least squares,lasso regression, decision trees, random forests, ...) Model Building - Build data models - Assess data model quality (decide which model is best) - Build the input data interface for using the \"best\" model - Using your best model determine projected concrete strength for 5 possible mixtures in the table below: Cement BlastFurnaceSlag FlyAsh CoarseAggregate FineAggregate Water Superplasticizer Age 175.0 13.0 172.0 1000.0 856.0 156.0 4.0 3.0 320.0 0.0 0.0 970.0 850.0 192.0 0.0 7.0 320.0 0.0 126.0 860.0 856.0 209.0 5.70 28.0 320.0 73.0 54.0 972.0 773.0 181.0 6.0 45.0 530.0 359.0 200.0 1145.0 992.0 247.0 32.0 365.0 Documentation - Training video on how to use your tool, and demonstrate the tool(s) as they are run - Project management video - Interim report (see deliverables below); this document must be rendered as a .pdf, but you are free to use your favorite writing software (Word,LibreOffice, ...). - Final Report (see deliverables below)","title":"Tasks:"},{"location":"6-Projects/P-ConcreteStrength/ConcreteStrength-Project/#deliverables","text":"","title":"Deliverables:"},{"location":"6-Projects/P-ConcreteStrength/ConcreteStrength-Project/#part-1-due-november-24","text":"A report that briefly describes the concrete strength database and how you plan to solve the tasks of creating a suitable data model. - Break down each task into manageable subtasks and describe how you intend to solve the subtasks and how you will test each task. (Perhaps make a simple Gantt Chart) - Address the responsibilities of each team member for tasks completed and tasks to be completed until the end of the semester. (Perhaps make explicit subtask assignments) Your report should be limited to 4 pages, 12 pt font size, double linespacing (exclusive of references which are NOT included in the page count). You need to cite/reference all sources you used.","title":"Part 1 (due November 24):"},{"location":"6-Projects/P-ConcreteStrength/ConcreteStrength-Project/#part-2-due-on-final-exam-day","text":"A well-documented JupyterLab (using a python kernel) analysis and implementation for the data model. A well-documented JupyterLab (using a python kernel) implementation for the data model user interface. A well-documented JupyterLab (using a python kernel) implementation for the database update interface. Above items can reside in a single notebook; but clearly identify sections that perform different tasks. A how-to video demonstrating performance and description of problems that you were not able to solve. A project management video (up to 5 minutes) in which you explain how you completed the project and how you worked as a team. Above items can reside in a single video; but structure the video into the two parts; use an obvious transition when moving from \"how to ...\" into the project management portion. Keep the total video length to less than 10 minutes; submit as an unlisted YouTube video, and just supply the link (someone on each team is likely to have a YouTube creator account). Keep in mind a 10 minute video can approach 100MB file size before compression, so it won't upload to Blackboard and cannot be emailed.","title":"Part 2 (due on Final Exam day):"},{"location":"6-Projects/P-DTMF/DTMF-Part1/","text":"DTMF Project Part 1: Working with Audio Files # Import libraries here import numpy as np import matplotlib.pyplot as plt Objective In this first part of the project you are learning to create audio files in Python that you can store on your computer in form of a wav-file and listen to using a standard audio player. Terminology Pure tones: A pure tone is an audio signal that only consists of a single frequency and has sinusoidal shape. Naming our pure tone y(t) , it can be expressed as: y(t)= A\\cdot \\cos(2\\pi f_0 t+\\phi) where t is the independent variable and refers to time. The amplitude A determines the volume of the signal, the frequency f_0 the pitch, and \\phi (phi) is a phase shift that is not audible and determines the value of y(t) at t=0 . The best known example for a pure signal is the sound created by a tuning fork. The standard orchestra pitch tuning fork creates the note A above the middle C, which which has a frequency of f_0=440 Hz. Here Hz stands for the unit hertz, which is identical to 1/seconds and measures the number of oscillations per second. Below the signal y(t) is graphed for a duration of 10 ms. Given that the signal has a frequency of 440 Hz, there are 440 full oscillations (or periods) in a second, or - as can be seen below - 4.4 oscillations in 0.01 s. A = 0.8 # amplitude f0 = 440 # frequency of tone A phi = np.pi/2 # phase shift t=np.arange(0,1e-2,1e-5) # time y=A*np.cos(2*np.pi*f0*t+phi) # signal y(t) # Create graph figure1=plt.figure(figsize = (15,5)) plt.plot(t, y) plt.xlabel(\"time [s]\") plt.ylabel(\"amplitude\") plt.title(\"Pure tone A of frequency 440 Hz\") plt.grid() plt.xlim([0,0.01]) plt.show() Audible frequencies : As human beings, we can hear frequencies in the range from roughly 20 Hz to 20,000 Hz, where the exact limits depend on the person and factors such as age and exposure to noise and can be determined in a hearing test. Speech transmission in telephony : The telephone was designed to transmit frequencies most prominent in speech ranging from approximately 300 Hz to 3400 Hz. Note that music contains higher frequencies and that is why music played across a telephone line is of low quality. Storing Audio Files on a Hard Drive / Computer To store sound in a file on a computer, the sound wave captured by the microphone needs to be converted into a sequence of numbers. This process is called analog-to-digital conversion and consists of two steps: - sampling refers to the discretization of time: every T_s seconds a value of the sound wave is stored; the values in-between are omitted. - quantization refers to the discretization of the amplitude: the amplitude is converted to a format that can be stored with B bits (binary digits, i.e. 1's and 0's). Sampling : Another way of specifying the time discretization is by specifying the so called sampling rate f_s denoting the number of values stored per second. Note that f_s=1/T_s . Industry standards for sampling frequencies are f_s=8000 Hz for speech, f_s = 44,100 Hz for audio compact disks, and f_s = 48,000 Hz for DVD's. Quantization : This step concern the discretization of amplitude (y-axis). In a fist step, the dynamic range is limited to amplitudes between -1 and 1, i.e. all values outside that range are cut off and set to 1 (or -1 if negative). Next, the range from -1 to 1 is split into 2^B levels and each amplitude value is approximated by the closest level and can then be stored using B bits. For speech signals, B=8 is chosen, and for high fidelity audio signal B = 16 or B=24 . In a simplified way, quantization can be seen as a type cast from floating point to integer (in8, int16) format. Hardware Considerations : Generally speaking, the cost of an analog-to-digital converter (ADC) increases with both, f_s and B . The file size also increases with those two variables, since for every second of a mono audio signal B\\cdot f_s bits need to be stored. The graph below shows the sequence of numbers for our 440 Hz pure tone that are stored in an audio file if the sampling frequency is chosen as fs=8000 Hz and the number of bits per value as B=8 . A = 0.8 # amplitude f0 = 440 # frequency of tone A phi = np.pi/2 # phase shift fs=8000; B=16; t=np.arange(0,1,1/fs) # discretized time (1 second duration) y=A*np.cos(2*np.pi*f0*t+phi) # discretized signal y(t) yq = (2**(B-1))*y yq = yq.astype(np.int16) # convert to integer 16-bit format # Create graph figure1=plt.figure(figsize = (15,5)) plt.plot(t, yq, marker = 'x', linestyle = '') plt.xlabel(\"time [s]\") plt.ylabel(\"amplitude\") plt.title(\"Pure tone A of frequency 440 Hz\") plt.grid() plt.xlim([0,0.01]) plt.show() Storing the audio signal as a wav-file : one standardized format to store audio signal is the wav-file. It stores the signal in a binary format and has header information that specifies the sampling rate f_s and the bits per value B . If you open a .wav-file in a text editor (e.g. notepad) you will recognize this header information following by a sequence of \"junk\" characters. To store our signal as a .wav file, we first need to import this capability from the scipy library. After running the cell below, you will find the file \"A440.wav\" in your current directory and can play it using a standard audio player. from scipy.io import wavfile wavfile.write(\"A440.wav\", fs, y) Reading wav-files : The following cell shows how you can read a .wav-file and store it in a numpy array. Remember, all you are reading in is a sequence of numbers. To recover the correct time information, you also need to read the sampling rate. sampling_rate, data = wavfile.read(\"A440.wav\") data=np.array(data) # convert to numpy array time = np.arange(0,len(data),1) #create vector of same length as data time = time/sampling_rate # normalize to represent correct time information # Create graph figure1=plt.figure(figsize = (15,5)) plt.plot(time, data, marker = 'x', linestyle = '') plt.xlabel(\"time [s]\") plt.ylabel(\"amplitude\") plt.title(\"Pure tone A of frequency 440 Hz read from .wav-file\") plt.grid() plt.xlim([0,0.01]) # only show 10 ms remove if you want to see the entire signal plt.show() Tasks (15 points) By performing the following tasks you prepare yourself for working on Dual-Tone Multi-Frequency (DTMF) signals. Task-1 (3 points) A piano keyboard covers several octaves. Research the frequencies that correspond to the different octaves of the note \"C\", specifically the middle C (C4), the tenor C (C3, one octave below the middle C), and the treble C (C5, one octave above the middle C). Reference your source. # frequencies of C freq_C3 = freq_C4 = freq_C5 = Reference: YOUR ANSWER GOES HERE Task-2 (5 points): Assuming a sampling rate of fs=8000 Hz, and amplitude of A=0.5 and an amplitude quanitzation with B=16 bits, create the a graph that shows 10 ms of all three signals. How do they relate to each other? Note-1: this task includes several steps, define a time vector, create the pure tone signals, quantize the amplitudes, then graph the signals. Note-2: the phase shift \\phi is not specified, so you may choose them as you please. The following relationship between the signals can be observed from the graph above: YOUR ANSWER GOES HERE Task-3 (3 points): Create a wav-file for each tone with a duration of 1 second. Listen to your .wav-files to and compare the sound you created to note C sounds you find on the Internet to ensure that you performed this step correctly. Task-4 (4 points): Read the file my_tone.wav and graph the first 10 ms of it. From your graph determine the frequency of the tone (the sound you hear, not the sampling rate). By listening to the sound and comparing it to your note C tones for which you know the frequencies, you can determine the range of frequencies in which it lies. This is how we verified that the frequency we determined makes sense: YOUR ANSWER GOES HERE","title":"DTMF Part1"},{"location":"6-Projects/P-DTMF/DTMF-Part1/#dtmf-project-part-1-working-with-audio-files","text":"# Import libraries here import numpy as np import matplotlib.pyplot as plt","title":"DTMF Project Part 1: Working with Audio Files"},{"location":"6-Projects/P-DTMF/DTMF-Part1/#objective","text":"In this first part of the project you are learning to create audio files in Python that you can store on your computer in form of a wav-file and listen to using a standard audio player.","title":"Objective"},{"location":"6-Projects/P-DTMF/DTMF-Part1/#terminology","text":"Pure tones: A pure tone is an audio signal that only consists of a single frequency and has sinusoidal shape. Naming our pure tone y(t) , it can be expressed as: y(t)= A\\cdot \\cos(2\\pi f_0 t+\\phi) where t is the independent variable and refers to time. The amplitude A determines the volume of the signal, the frequency f_0 the pitch, and \\phi (phi) is a phase shift that is not audible and determines the value of y(t) at t=0 . The best known example for a pure signal is the sound created by a tuning fork. The standard orchestra pitch tuning fork creates the note A above the middle C, which which has a frequency of f_0=440 Hz. Here Hz stands for the unit hertz, which is identical to 1/seconds and measures the number of oscillations per second. Below the signal y(t) is graphed for a duration of 10 ms. Given that the signal has a frequency of 440 Hz, there are 440 full oscillations (or periods) in a second, or - as can be seen below - 4.4 oscillations in 0.01 s. A = 0.8 # amplitude f0 = 440 # frequency of tone A phi = np.pi/2 # phase shift t=np.arange(0,1e-2,1e-5) # time y=A*np.cos(2*np.pi*f0*t+phi) # signal y(t) # Create graph figure1=plt.figure(figsize = (15,5)) plt.plot(t, y) plt.xlabel(\"time [s]\") plt.ylabel(\"amplitude\") plt.title(\"Pure tone A of frequency 440 Hz\") plt.grid() plt.xlim([0,0.01]) plt.show() Audible frequencies : As human beings, we can hear frequencies in the range from roughly 20 Hz to 20,000 Hz, where the exact limits depend on the person and factors such as age and exposure to noise and can be determined in a hearing test. Speech transmission in telephony : The telephone was designed to transmit frequencies most prominent in speech ranging from approximately 300 Hz to 3400 Hz. Note that music contains higher frequencies and that is why music played across a telephone line is of low quality.","title":"Terminology"},{"location":"6-Projects/P-DTMF/DTMF-Part1/#storing-audio-files-on-a-hard-drive-computer","text":"To store sound in a file on a computer, the sound wave captured by the microphone needs to be converted into a sequence of numbers. This process is called analog-to-digital conversion and consists of two steps: - sampling refers to the discretization of time: every T_s seconds a value of the sound wave is stored; the values in-between are omitted. - quantization refers to the discretization of the amplitude: the amplitude is converted to a format that can be stored with B bits (binary digits, i.e. 1's and 0's). Sampling : Another way of specifying the time discretization is by specifying the so called sampling rate f_s denoting the number of values stored per second. Note that f_s=1/T_s . Industry standards for sampling frequencies are f_s=8000 Hz for speech, f_s = 44,100 Hz for audio compact disks, and f_s = 48,000 Hz for DVD's. Quantization : This step concern the discretization of amplitude (y-axis). In a fist step, the dynamic range is limited to amplitudes between -1 and 1, i.e. all values outside that range are cut off and set to 1 (or -1 if negative). Next, the range from -1 to 1 is split into 2^B levels and each amplitude value is approximated by the closest level and can then be stored using B bits. For speech signals, B=8 is chosen, and for high fidelity audio signal B = 16 or B=24 . In a simplified way, quantization can be seen as a type cast from floating point to integer (in8, int16) format. Hardware Considerations : Generally speaking, the cost of an analog-to-digital converter (ADC) increases with both, f_s and B . The file size also increases with those two variables, since for every second of a mono audio signal B\\cdot f_s bits need to be stored. The graph below shows the sequence of numbers for our 440 Hz pure tone that are stored in an audio file if the sampling frequency is chosen as fs=8000 Hz and the number of bits per value as B=8 . A = 0.8 # amplitude f0 = 440 # frequency of tone A phi = np.pi/2 # phase shift fs=8000; B=16; t=np.arange(0,1,1/fs) # discretized time (1 second duration) y=A*np.cos(2*np.pi*f0*t+phi) # discretized signal y(t) yq = (2**(B-1))*y yq = yq.astype(np.int16) # convert to integer 16-bit format # Create graph figure1=plt.figure(figsize = (15,5)) plt.plot(t, yq, marker = 'x', linestyle = '') plt.xlabel(\"time [s]\") plt.ylabel(\"amplitude\") plt.title(\"Pure tone A of frequency 440 Hz\") plt.grid() plt.xlim([0,0.01]) plt.show() Storing the audio signal as a wav-file : one standardized format to store audio signal is the wav-file. It stores the signal in a binary format and has header information that specifies the sampling rate f_s and the bits per value B . If you open a .wav-file in a text editor (e.g. notepad) you will recognize this header information following by a sequence of \"junk\" characters. To store our signal as a .wav file, we first need to import this capability from the scipy library. After running the cell below, you will find the file \"A440.wav\" in your current directory and can play it using a standard audio player. from scipy.io import wavfile wavfile.write(\"A440.wav\", fs, y) Reading wav-files : The following cell shows how you can read a .wav-file and store it in a numpy array. Remember, all you are reading in is a sequence of numbers. To recover the correct time information, you also need to read the sampling rate. sampling_rate, data = wavfile.read(\"A440.wav\") data=np.array(data) # convert to numpy array time = np.arange(0,len(data),1) #create vector of same length as data time = time/sampling_rate # normalize to represent correct time information # Create graph figure1=plt.figure(figsize = (15,5)) plt.plot(time, data, marker = 'x', linestyle = '') plt.xlabel(\"time [s]\") plt.ylabel(\"amplitude\") plt.title(\"Pure tone A of frequency 440 Hz read from .wav-file\") plt.grid() plt.xlim([0,0.01]) # only show 10 ms remove if you want to see the entire signal plt.show()","title":"Storing Audio Files on a Hard Drive / Computer"},{"location":"6-Projects/P-DTMF/DTMF-Part1/#tasks-15-points","text":"By performing the following tasks you prepare yourself for working on Dual-Tone Multi-Frequency (DTMF) signals.","title":"Tasks (15 points)"},{"location":"6-Projects/P-DTMF/DTMF-Part1/#task-1-3-points","text":"A piano keyboard covers several octaves. Research the frequencies that correspond to the different octaves of the note \"C\", specifically the middle C (C4), the tenor C (C3, one octave below the middle C), and the treble C (C5, one octave above the middle C). Reference your source. # frequencies of C freq_C3 = freq_C4 = freq_C5 = Reference: YOUR ANSWER GOES HERE","title":"Task-1 (3 points)"},{"location":"6-Projects/P-DTMF/DTMF-Part1/#task-2-5-points","text":"Assuming a sampling rate of fs=8000 Hz, and amplitude of A=0.5 and an amplitude quanitzation with B=16 bits, create the a graph that shows 10 ms of all three signals. How do they relate to each other? Note-1: this task includes several steps, define a time vector, create the pure tone signals, quantize the amplitudes, then graph the signals. Note-2: the phase shift \\phi is not specified, so you may choose them as you please. The following relationship between the signals can be observed from the graph above: YOUR ANSWER GOES HERE","title":"Task-2  (5 points):"},{"location":"6-Projects/P-DTMF/DTMF-Part1/#task-3-3-points","text":"Create a wav-file for each tone with a duration of 1 second. Listen to your .wav-files to and compare the sound you created to note C sounds you find on the Internet to ensure that you performed this step correctly.","title":"Task-3 (3 points):"},{"location":"6-Projects/P-DTMF/DTMF-Part1/#task-4-4-points","text":"Read the file my_tone.wav and graph the first 10 ms of it. From your graph determine the frequency of the tone (the sound you hear, not the sampling rate). By listening to the sound and comparing it to your note C tones for which you know the frequencies, you can determine the range of frequencies in which it lies. This is how we verified that the frequency we determined makes sense: YOUR ANSWER GOES HERE","title":"Task-4 (4 points):"},{"location":"6-Projects/P-ImageClassification/MyANN/","text":"import sys print(\"Echo system status -- reset environment if kernel NOT 3.8...\") print(sys.executable) print(sys.version) print(sys.version_info) # testing on aws lightsail instance 21 July 2020 Echo system status -- reset environment if kernel NOT 3.8... /opt/jupyterhub/bin/python3 3.6.9 (default, Oct 8 2020, 12:12:24) [GCC 8.4.0] sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0) Warning!! This notebook takes a long time to run (3+ minutes on 1GB/1CPU/2GHz) Artifical Neural Network From Wikipedia: An artificial neural network is an interconnected group of nodes, inspired by a simplification of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another. Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process. An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. In ANN implementations, the \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times. The original goal of the ANN approach was to solve problems in the same way that a human brain would. But over time, attention moved to performing specific tasks, leading to deviations from biology. ANNs have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games, medical diagnosis, and even in activities that have traditionally been considered as reserved to humans, like painting. First some set-up, for the next part of the background: import numpy # useful numerical routines import scipy.special # special functions library import scipy.misc # image processing code import imageio import matplotlib.pyplot # import plotting routines Classification Computers are nothing more than calculators at heart. They are very very fast at doing arithmetic. This is great for doing tasks that match what a calculator does: summing numbers to work out sales, applying percentages to work out tax, plotting graphs of existing data. Even watching catchup TV or streaming music through your computer doesn\u2019t involve much more than the computer executing simple arithmetic instructions again and again. It may surprise you but reconstructing a video frame from the ones and zeros that are piped across the internet to your computer is done using arithmetic not much more complex than the sums we did at school. Adding up numbers really quickly thousands, or even millions of times a second may be impressive but it isn\u2019t intelligence. A human may find it hard to do large sums very quickly but the process of doing it doesn\u2019t require much intelligence at all. It simply requires an ability to follow very basic instructions, and this is what the electronics inside a computer does. Now let\u2019s flips things and turn the tables on computers! Look at the following images and see if you can recognise what they contain: You can immediately recognize people, a cat, and a tree -- you are ably to classify the pictures very fast. We can process the quite large amount of information that the images contain, and very successfully process it to recognise what\u2019s in the image. This kind of task isn\u2019t easy for computers in fact it\u2019s incredibly difficult. Consider what happens when we reduce the information into a 27X27 pixel map to see one reason why classification is hard for a machine -- a resolution issue, also we will see how at reduce resolution the pictures look alike. First render the people image in reduced resolution about 1/2 of the original -- still barely recognizable for us humans img_array = imageio.imread(\"image_src/people784.png\", as_gray = True) img_data0 = 255.0 - img_array.reshape(784) img_data0 = ((img_data0/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data0).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') print(\"people784 statistics : \",img_data0.mean(),img_data0.var()) people784 statistics : 0.48325375 0.06275265 Now render the cat image in reduced resolution about 1/2 of the original -- still recognizable for us humans img_array = imageio.imread(\"image_src/cat784.png\", as_gray = True) img_data1 = 255.0 - img_array.reshape(784) img_data1 = ((img_data1/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data1).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') print(\"cat784 statistics : \",img_data1.mean(),img_data1.var()) cat784 statistics : 0.60355407 0.023282547 Now render the tree image in reduced resolution about 1/3 of the original -- still recognizable for us humans img_array = imageio.imread(\"image_src/tree784.png\", as_gray = True) img_data2 = 255.0 - img_array.reshape(784) img_data2 = ((img_data2/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data2).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') print(\"tree784 statistics : \",img_data2.mean(),img_data2.var()) tree784 statistics : 0.484061 0.049499817 Using the image statistics, which is just the gray-scale value of each pixel (0-254), we see that the images are different with this simple metric but not by much Image Mean Variance People 0.48325375 0.06275265 Cat 0.60355407 0.023282547 Tree 0.484061 0.049499817 If we used just a statistical description, in the mean people and tree are the same, whereas a cat is different. But not all cats will have the same mean (or variance). So simplistic numerical descriptors are useless, we need more that a couple of metrics for the image perhaps higher moments, or a way to consider all pixels at once -- sort of like a regression model. We humans naturally fill in missing information and can classify very fast -- cognative scientists think (now thats a pun!) that our mind performs \"regressions\" on the whole image and reduces it to a set of classifiers then these are compared in our brain to historical results and the classification that throw off the most dopamine (our brain's drug of choice) is selected. It happens fast because the chemical reactions involved can be processed in parallel, the message is sent evreywhere at once and the molecules themselves don't even have to arrive for the classification to occur. Things to do link videos - explain prediction vs clasification - explain neuron functions - ANN write ups/future fixes modify so my photos can be added to a training file modify so can ask for filenames modify so can render images and send output to PDF/PNG files modify to do something engineering useful -- like partition watersheds into developed, undeveloped and a few other classes based on GoogleEarth image captures (28054)**(1/2) #not sure why I put this here, looks like figuring pixel counts 167.49328344742662 Define the ANN class Need write up on OOP and why using classes. class neuralNetwork: # Class Definitions # initialize the neural network def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate): # set number of nodes in input, hidden, and output layer self.inodes = inputnodes self.hnodes = hiddennodes self.onodes = outputnodes # learning rate self.lr = learningrate # initalize weight matrices # # link weight matrices, wih (input to hidden) and # who (hidden to output) # weights inside the arrays are w_i_j where link is from node i # to node j in next layer # # w11 w21 # w12 w22 etc. self.wih = (numpy.random.rand(self.hnodes, self.inodes) - 0.5) self.who = (numpy.random.rand(self.onodes, self.hnodes) - 0.5) # activation function self.activation_function = lambda x:scipy.special.expit(x) pass # train the neural network def train(self, inputs_list, targets_list): # convert input list into 2D array inputs = numpy.array(inputs_list, ndmin=2).T # convert target list into 2D array targets = numpy.array(targets_list, ndmin=2).T # calculate signals into hidden layer hidden_inputs = numpy.dot(self.wih, inputs) # calculate signals from hidden layer hidden_outputs = self.activation_function(hidden_inputs) # calculate signals into output layer final_inputs = numpy.dot(self.who, hidden_outputs) # calculate signals from output layer final_outputs = self.activation_function(final_inputs) # calculate output errors (target - model) output_errors = targets - final_outputs # calculate hidden layer errors (split by weigths recombined in hidden layer) hidden_errors = numpy.dot(self.who.T, output_errors) # update the weights for the links from hidden to output layer self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs)) # update the weights for the links from input to hidden layer self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs)) pass # query the neural network def query(self, inputs_list): # convert input list into 2D array inputs = numpy.array(inputs_list, ndmin=2).T # calculate signals into hidden layer hidden_inputs = numpy.dot(self.wih, inputs) # calculate signals from hidden layer hidden_outputs = self.activation_function(hidden_inputs) # calculate signals into output layer final_inputs = numpy.dot(self.who, hidden_outputs) # calculate signals from output layer final_outputs = self.activation_function(final_inputs) return final_outputs pass print(\"neuralNetwork Class Loads OK\") neuralNetwork Class Loads OK Explain why a test case # Test case 1 p130 MYONN # number of input, hidden, and output nodes input_nodes = 784 # 28X28 Pixel Image hidden_nodes = 110 # Should be smaller than input count (or kind of useless) output_nodes = 10 # Classifications learning_rate = 0.1 # set learning rate n = neuralNetwork(input_nodes,hidden_nodes,output_nodes,learning_rate) # create an instance print(\"Instance n Created\") Instance n Created Explain Training Explain Concept of Learning Rate and Training Episodes (kind of a bootstrap here!) # load a training file # replace code here with a URL get ## training_data_file = open(\"mnist_train_100.csv\",'r') #connect the file# training_data_file = open(\"mnist_train.csv\",'r') #connect the file# training_data_list = training_data_file.readlines() #read entire contents of file into object: data_list# training_data_file.close() #disconnect the file# # print(len(training_data_list)) ## activate for debugging otherwise leave disabled # train the neural network howManyTrainingTimes = 0 for times in range(0,1): # added outer loop for repeat training same data set howManyTrainingRecords = 0 for record in training_data_list: # split the values on the commas all_values = record.split(',') # split datalist on commas - all records. Is thing going to work? # inputs = (numpy.asfarray(all_values[1:])/255.0 * 0.99) + 0.01 # inputs = (numpy.asfarray(all_values[1:])/255.0 * 0.99) + 0.01 # print(inputs) ## activate for debugging otherwise leave disabled # create target output values -- all 0.01 except for the label of 0.99 targets = numpy.zeros(output_nodes) + 0.01 # all_values[0] is the target for this record targets[int(all_values[0])] = 0.99 n.train(inputs, targets) howManyTrainingRecords += 1 pass howManyTrainingTimes += 1 learning_rate *= 0.9 pass print (\"training records processed = \",howManyTrainingRecords) print (\"training episodes = \",howManyTrainingTimes) # load a production file ## test_data_file = open(\"mnist_test_10.csv\",'r') #connect the file# test_data_file = open(\"mnist_test.csv\",'r') #connect the file# test_data_list = test_data_file.readlines() #read entire contents of file into object: data_list# test_data_file.close() #disconnect the file# training records processed = 60000 training episodes = 1 Runtime above cell ~ 3 minutes on AWS 1GB/1CPU/2GHz virtual machine Explain Testing # test the neural network scorecard = [] # empty array for keeping score # run through the records in test_data_list howManyTestRecords = 0 for record in test_data_list: # split the values on the commas all_values = record.split(',') # split datalist on commas - all records # correct_label = int(all_values[0]) # correct answer is first element of all_values # scale and shift the inputs inputs = (numpy.asfarray(all_values[1:])/255.0 * 0.99) + 0.01 # query the neural network outputs = n.query(inputs) predict_label = numpy.argmax(outputs) ## print \"predict =\",predict_label,correct_label,\"= correct\" # activate for small test sets only! if (predict_label == correct_label): scorecard.append(1) else: scorecard.append(0) pass howManyTestRecords += 1 pass print (\"production records processed =\", howManyTestRecords) ## print scorecard # activate for small test sets only! # calculate performance score, fraction of correct answers scorecard_array = numpy.asfarray(scorecard) print (\"performance = \",scorecard_array.sum()/scorecard_array.size) production records processed = 10000 performance = 0.9472 Explain using own images How were images obtained? How are they pre-processed to get 28x28 size? (GraphicConverter and linear aggregation) -- This can probably be done in python subsystem (function) this worksheet. Why are the colors and alpha-channel squashed into greyscale? Play with the training episodes - see if can get 90% recognition (9/10) correct. Examine failed recognition, try to explain why. # lets try one of my own pictures # first read and render #img_array = scipy.misc.imread(\"MyZero.png\", flatten = True) Fuckers deprecated this utility! img_array = imageio.imread(\"image_src/MyZero.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 0 \",\"my network thinks its = \",mylabel) my number = 0 my network thinks its = 0 m0=img_data.mean() v0=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyOne.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 1 \",\"my network thinks its = \",mylabel) my number = 1 my network thinks its = 1 m1=img_data.mean() v1=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyTwo.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 2 \",\"my network thinks its = \",mylabel) my number = 2 my network thinks its = 2 m2=img_data.mean() v2=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyThree.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 3 \",\"my network thinks its = \",mylabel) my number = 3 my network thinks its = 3 m3=img_data.mean() v3=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyFour.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 4 \",\"my network thinks its = \",mylabel) my number = 4 my network thinks its = 4 m4=img_data.mean() v4=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyFive.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 5 \",\"my network thinks its = \",mylabel) my number = 5 my network thinks its = 5 m5=img_data.mean() v5=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MySix.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 6 \",\"my network thinks its = \",mylabel) my number = 6 my network thinks its = 5 m6=img_data.mean() v6=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MySeven.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 7 \",\"my network thinks its = \",mylabel) my number = 7 my network thinks its = 7 m7=img_data.mean() v7=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyEight.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 8 \",\"my network thinks its = \",mylabel) my number = 8 my network thinks its = 8 m8=img_data.mean() v8=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyNine.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 9 \",\"my network thinks its = \",mylabel) pass my number = 9 my network thinks its = 9 m9=img_data.mean() v9=img_data.var() print(\"my 0 statistics : \",m0,v0) print(\"my 1 statistics : \",m1,v1) print(\"my 2 statistics : \",m2,v2) print(\"my 3 statistics : \",m3,v3) print(\"my 4 statistics : \",m4,v4) print(\"my 5 statistics : \",m5,v5) print(\"my 6 statistics : \",m6,v6) print(\"my 7 statistics : \",m7,v7) print(\"my 8 statistics : \",m8,v8) print(\"my 9 statistics : \",m9,v9) my 0 statistics : 0.10840577 0.07128458 my 1 statistics : 0.0691415 0.045747854 my 2 statistics : 0.10085894 0.06906258 my 3 statistics : 0.15961523 0.071290515 my 4 statistics : 0.1313567 0.06362193 my 5 statistics : 0.11012904 0.07498277 my 6 statistics : 0.18414135 0.09940878 my 7 statistics : 0.11907936 0.056603365 my 8 statistics : 0.12214256 0.08051322 my 9 statistics : 0.0691415 0.045747854 References http://myselph.de/neuralNet.html LazyProgrammer. Unsupervised Machine Learning in Python: Master Data Science and Machine Learning with Cluster Analysis, Gaussian Mixture Models, and Principal Components Analysis . Kindle Edition. Rashid, Tariq. Make Your Own Neural Network. Kindle Edition. https://www.digitalocean.com/community/tutorials/an-introduction-to-machine-learning ######## Useful Stuff Below ###################### ##an_input_list = ([1.0,0.5,-1.5]) ##print(\"Trying a Query - input list first\") ##print(an_input_list) ##print(\"Now the response\") ##print(n.query(an_input_list)) ##print(\"Instance works up to pg 138\") ### get a single test record ##all_values = test_data_list[0].split(',') ##print('classification for this test record = ', all_values[0]) ####image_array = numpy.asfarray(all_values[1:]).reshape((28,28)) ####matplotlib.pyplot.imshow(image_array) # construct a graphic object # ####matplotlib.pyplot.show() # show the graphic object to a window # ##print( n.query((numpy.asfarray(all_values[1:])/255.0 * 0.99) + 0.01) ) ##image_array = numpy.asfarray(scaled_input).reshape((28,28)) ##matplotlib.pyplot.imshow(image_array) # construct a graphic object # ##matplotlib.pyplot.show() # show the graphic object to a window # ##matplotlib.pyplot.imshow(image_array, cmap = 'Greys', interpolation = 'nearest' ) # construct a graphic object # ##matplotlib.pyplot.show() # show the graphic object to a window # #","title":"MyANN"},{"location":"6-Projects/P-ImageClassification/MyANN/#warning","text":"This notebook takes a long time to run (3+ minutes on 1GB/1CPU/2GHz)","title":"Warning!!"},{"location":"6-Projects/P-ImageClassification/MyANN/#artifical-neural-network","text":"From Wikipedia: An artificial neural network is an interconnected group of nodes, inspired by a simplification of neurons in a brain. Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another. Artificial neural networks (ANN) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process. An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron that receives a signal then processes it and can signal neurons connected to it. In ANN implementations, the \"signal\" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times. The original goal of the ANN approach was to solve problems in the same way that a human brain would. But over time, attention moved to performing specific tasks, leading to deviations from biology. ANNs have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games, medical diagnosis, and even in activities that have traditionally been considered as reserved to humans, like painting. First some set-up, for the next part of the background: import numpy # useful numerical routines import scipy.special # special functions library import scipy.misc # image processing code import imageio import matplotlib.pyplot # import plotting routines","title":"Artifical Neural Network"},{"location":"6-Projects/P-ImageClassification/MyANN/#classification","text":"Computers are nothing more than calculators at heart. They are very very fast at doing arithmetic. This is great for doing tasks that match what a calculator does: summing numbers to work out sales, applying percentages to work out tax, plotting graphs of existing data. Even watching catchup TV or streaming music through your computer doesn\u2019t involve much more than the computer executing simple arithmetic instructions again and again. It may surprise you but reconstructing a video frame from the ones and zeros that are piped across the internet to your computer is done using arithmetic not much more complex than the sums we did at school. Adding up numbers really quickly thousands, or even millions of times a second may be impressive but it isn\u2019t intelligence. A human may find it hard to do large sums very quickly but the process of doing it doesn\u2019t require much intelligence at all. It simply requires an ability to follow very basic instructions, and this is what the electronics inside a computer does. Now let\u2019s flips things and turn the tables on computers! Look at the following images and see if you can recognise what they contain: You can immediately recognize people, a cat, and a tree -- you are ably to classify the pictures very fast. We can process the quite large amount of information that the images contain, and very successfully process it to recognise what\u2019s in the image. This kind of task isn\u2019t easy for computers in fact it\u2019s incredibly difficult. Consider what happens when we reduce the information into a 27X27 pixel map to see one reason why classification is hard for a machine -- a resolution issue, also we will see how at reduce resolution the pictures look alike. First render the people image in reduced resolution about 1/2 of the original -- still barely recognizable for us humans img_array = imageio.imread(\"image_src/people784.png\", as_gray = True) img_data0 = 255.0 - img_array.reshape(784) img_data0 = ((img_data0/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data0).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') print(\"people784 statistics : \",img_data0.mean(),img_data0.var()) people784 statistics : 0.48325375 0.06275265 Now render the cat image in reduced resolution about 1/2 of the original -- still recognizable for us humans img_array = imageio.imread(\"image_src/cat784.png\", as_gray = True) img_data1 = 255.0 - img_array.reshape(784) img_data1 = ((img_data1/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data1).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') print(\"cat784 statistics : \",img_data1.mean(),img_data1.var()) cat784 statistics : 0.60355407 0.023282547 Now render the tree image in reduced resolution about 1/3 of the original -- still recognizable for us humans img_array = imageio.imread(\"image_src/tree784.png\", as_gray = True) img_data2 = 255.0 - img_array.reshape(784) img_data2 = ((img_data2/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data2).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') print(\"tree784 statistics : \",img_data2.mean(),img_data2.var()) tree784 statistics : 0.484061 0.049499817 Using the image statistics, which is just the gray-scale value of each pixel (0-254), we see that the images are different with this simple metric but not by much Image Mean Variance People 0.48325375 0.06275265 Cat 0.60355407 0.023282547 Tree 0.484061 0.049499817 If we used just a statistical description, in the mean people and tree are the same, whereas a cat is different. But not all cats will have the same mean (or variance). So simplistic numerical descriptors are useless, we need more that a couple of metrics for the image perhaps higher moments, or a way to consider all pixels at once -- sort of like a regression model. We humans naturally fill in missing information and can classify very fast -- cognative scientists think (now thats a pun!) that our mind performs \"regressions\" on the whole image and reduces it to a set of classifiers then these are compared in our brain to historical results and the classification that throw off the most dopamine (our brain's drug of choice) is selected. It happens fast because the chemical reactions involved can be processed in parallel, the message is sent evreywhere at once and the molecules themselves don't even have to arrive for the classification to occur.","title":"Classification"},{"location":"6-Projects/P-ImageClassification/MyANN/#things-to-do","text":"link videos - explain prediction vs clasification - explain neuron functions - ANN write ups/future fixes modify so my photos can be added to a training file modify so can ask for filenames modify so can render images and send output to PDF/PNG files modify to do something engineering useful -- like partition watersheds into developed, undeveloped and a few other classes based on GoogleEarth image captures (28054)**(1/2) #not sure why I put this here, looks like figuring pixel counts 167.49328344742662","title":"Things to do"},{"location":"6-Projects/P-ImageClassification/MyANN/#define-the-ann-class","text":"Need write up on OOP and why using classes. class neuralNetwork: # Class Definitions # initialize the neural network def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate): # set number of nodes in input, hidden, and output layer self.inodes = inputnodes self.hnodes = hiddennodes self.onodes = outputnodes # learning rate self.lr = learningrate # initalize weight matrices # # link weight matrices, wih (input to hidden) and # who (hidden to output) # weights inside the arrays are w_i_j where link is from node i # to node j in next layer # # w11 w21 # w12 w22 etc. self.wih = (numpy.random.rand(self.hnodes, self.inodes) - 0.5) self.who = (numpy.random.rand(self.onodes, self.hnodes) - 0.5) # activation function self.activation_function = lambda x:scipy.special.expit(x) pass # train the neural network def train(self, inputs_list, targets_list): # convert input list into 2D array inputs = numpy.array(inputs_list, ndmin=2).T # convert target list into 2D array targets = numpy.array(targets_list, ndmin=2).T # calculate signals into hidden layer hidden_inputs = numpy.dot(self.wih, inputs) # calculate signals from hidden layer hidden_outputs = self.activation_function(hidden_inputs) # calculate signals into output layer final_inputs = numpy.dot(self.who, hidden_outputs) # calculate signals from output layer final_outputs = self.activation_function(final_inputs) # calculate output errors (target - model) output_errors = targets - final_outputs # calculate hidden layer errors (split by weigths recombined in hidden layer) hidden_errors = numpy.dot(self.who.T, output_errors) # update the weights for the links from hidden to output layer self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs)) # update the weights for the links from input to hidden layer self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs)) pass # query the neural network def query(self, inputs_list): # convert input list into 2D array inputs = numpy.array(inputs_list, ndmin=2).T # calculate signals into hidden layer hidden_inputs = numpy.dot(self.wih, inputs) # calculate signals from hidden layer hidden_outputs = self.activation_function(hidden_inputs) # calculate signals into output layer final_inputs = numpy.dot(self.who, hidden_outputs) # calculate signals from output layer final_outputs = self.activation_function(final_inputs) return final_outputs pass print(\"neuralNetwork Class Loads OK\") neuralNetwork Class Loads OK","title":"Define the ANN class"},{"location":"6-Projects/P-ImageClassification/MyANN/#explain-why-a-test-case","text":"# Test case 1 p130 MYONN # number of input, hidden, and output nodes input_nodes = 784 # 28X28 Pixel Image hidden_nodes = 110 # Should be smaller than input count (or kind of useless) output_nodes = 10 # Classifications learning_rate = 0.1 # set learning rate n = neuralNetwork(input_nodes,hidden_nodes,output_nodes,learning_rate) # create an instance print(\"Instance n Created\") Instance n Created","title":"Explain why a test case"},{"location":"6-Projects/P-ImageClassification/MyANN/#explain-training","text":"","title":"Explain Training"},{"location":"6-Projects/P-ImageClassification/MyANN/#explain-concept-of-learning-rate-and-training-episodes-kind-of-a-bootstrap-here","text":"# load a training file # replace code here with a URL get ## training_data_file = open(\"mnist_train_100.csv\",'r') #connect the file# training_data_file = open(\"mnist_train.csv\",'r') #connect the file# training_data_list = training_data_file.readlines() #read entire contents of file into object: data_list# training_data_file.close() #disconnect the file# # print(len(training_data_list)) ## activate for debugging otherwise leave disabled # train the neural network howManyTrainingTimes = 0 for times in range(0,1): # added outer loop for repeat training same data set howManyTrainingRecords = 0 for record in training_data_list: # split the values on the commas all_values = record.split(',') # split datalist on commas - all records. Is thing going to work? # inputs = (numpy.asfarray(all_values[1:])/255.0 * 0.99) + 0.01 # inputs = (numpy.asfarray(all_values[1:])/255.0 * 0.99) + 0.01 # print(inputs) ## activate for debugging otherwise leave disabled # create target output values -- all 0.01 except for the label of 0.99 targets = numpy.zeros(output_nodes) + 0.01 # all_values[0] is the target for this record targets[int(all_values[0])] = 0.99 n.train(inputs, targets) howManyTrainingRecords += 1 pass howManyTrainingTimes += 1 learning_rate *= 0.9 pass print (\"training records processed = \",howManyTrainingRecords) print (\"training episodes = \",howManyTrainingTimes) # load a production file ## test_data_file = open(\"mnist_test_10.csv\",'r') #connect the file# test_data_file = open(\"mnist_test.csv\",'r') #connect the file# test_data_list = test_data_file.readlines() #read entire contents of file into object: data_list# test_data_file.close() #disconnect the file# training records processed = 60000 training episodes = 1 Runtime above cell ~ 3 minutes on AWS 1GB/1CPU/2GHz virtual machine","title":"Explain Concept of Learning Rate and Training Episodes (kind of a bootstrap here!)"},{"location":"6-Projects/P-ImageClassification/MyANN/#explain-testing","text":"# test the neural network scorecard = [] # empty array for keeping score # run through the records in test_data_list howManyTestRecords = 0 for record in test_data_list: # split the values on the commas all_values = record.split(',') # split datalist on commas - all records # correct_label = int(all_values[0]) # correct answer is first element of all_values # scale and shift the inputs inputs = (numpy.asfarray(all_values[1:])/255.0 * 0.99) + 0.01 # query the neural network outputs = n.query(inputs) predict_label = numpy.argmax(outputs) ## print \"predict =\",predict_label,correct_label,\"= correct\" # activate for small test sets only! if (predict_label == correct_label): scorecard.append(1) else: scorecard.append(0) pass howManyTestRecords += 1 pass print (\"production records processed =\", howManyTestRecords) ## print scorecard # activate for small test sets only! # calculate performance score, fraction of correct answers scorecard_array = numpy.asfarray(scorecard) print (\"performance = \",scorecard_array.sum()/scorecard_array.size) production records processed = 10000 performance = 0.9472","title":"Explain Testing"},{"location":"6-Projects/P-ImageClassification/MyANN/#explain-using-own-images","text":"How were images obtained? How are they pre-processed to get 28x28 size? (GraphicConverter and linear aggregation) -- This can probably be done in python subsystem (function) this worksheet. Why are the colors and alpha-channel squashed into greyscale? Play with the training episodes - see if can get 90% recognition (9/10) correct. Examine failed recognition, try to explain why. # lets try one of my own pictures # first read and render #img_array = scipy.misc.imread(\"MyZero.png\", flatten = True) Fuckers deprecated this utility! img_array = imageio.imread(\"image_src/MyZero.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 0 \",\"my network thinks its = \",mylabel) my number = 0 my network thinks its = 0 m0=img_data.mean() v0=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyOne.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 1 \",\"my network thinks its = \",mylabel) my number = 1 my network thinks its = 1 m1=img_data.mean() v1=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyTwo.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 2 \",\"my network thinks its = \",mylabel) my number = 2 my network thinks its = 2 m2=img_data.mean() v2=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyThree.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 3 \",\"my network thinks its = \",mylabel) my number = 3 my network thinks its = 3 m3=img_data.mean() v3=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyFour.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 4 \",\"my network thinks its = \",mylabel) my number = 4 my network thinks its = 4 m4=img_data.mean() v4=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyFive.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 5 \",\"my network thinks its = \",mylabel) my number = 5 my network thinks its = 5 m5=img_data.mean() v5=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MySix.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 6 \",\"my network thinks its = \",mylabel) my number = 6 my network thinks its = 5 m6=img_data.mean() v6=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MySeven.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 7 \",\"my network thinks its = \",mylabel) my number = 7 my network thinks its = 7 m7=img_data.mean() v7=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyEight.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 8 \",\"my network thinks its = \",mylabel) my number = 8 my network thinks its = 8 m8=img_data.mean() v8=img_data.var() #first read and render img_array = imageio.imread(\"image_src/MyNine.png\", as_gray = True) img_data = 255.0 - img_array.reshape(784) img_data = ((img_data/255.0)*0.99) + 0.01 matplotlib.pyplot.imshow(numpy.asfarray(img_data).reshape((28,28)),cmap = 'Greys') # construct a graphic object # matplotlib.pyplot.show() # show the graphic object to a window # matplotlib.pyplot.close('all') mynumber = n.query(img_data) mylabel = numpy.argmax(mynumber) print (\"my number = 9 \",\"my network thinks its = \",mylabel) pass my number = 9 my network thinks its = 9 m9=img_data.mean() v9=img_data.var() print(\"my 0 statistics : \",m0,v0) print(\"my 1 statistics : \",m1,v1) print(\"my 2 statistics : \",m2,v2) print(\"my 3 statistics : \",m3,v3) print(\"my 4 statistics : \",m4,v4) print(\"my 5 statistics : \",m5,v5) print(\"my 6 statistics : \",m6,v6) print(\"my 7 statistics : \",m7,v7) print(\"my 8 statistics : \",m8,v8) print(\"my 9 statistics : \",m9,v9) my 0 statistics : 0.10840577 0.07128458 my 1 statistics : 0.0691415 0.045747854 my 2 statistics : 0.10085894 0.06906258 my 3 statistics : 0.15961523 0.071290515 my 4 statistics : 0.1313567 0.06362193 my 5 statistics : 0.11012904 0.07498277 my 6 statistics : 0.18414135 0.09940878 my 7 statistics : 0.11907936 0.056603365 my 8 statistics : 0.12214256 0.08051322 my 9 statistics : 0.0691415 0.045747854","title":"Explain using own images"},{"location":"6-Projects/P-ImageClassification/MyANN/#references","text":"http://myselph.de/neuralNet.html LazyProgrammer. Unsupervised Machine Learning in Python: Master Data Science and Machine Learning with Cluster Analysis, Gaussian Mixture Models, and Principal Components Analysis . Kindle Edition. Rashid, Tariq. Make Your Own Neural Network. Kindle Edition. https://www.digitalocean.com/community/tutorials/an-introduction-to-machine-learning ######## Useful Stuff Below ###################### ##an_input_list = ([1.0,0.5,-1.5]) ##print(\"Trying a Query - input list first\") ##print(an_input_list) ##print(\"Now the response\") ##print(n.query(an_input_list)) ##print(\"Instance works up to pg 138\") ### get a single test record ##all_values = test_data_list[0].split(',') ##print('classification for this test record = ', all_values[0]) ####image_array = numpy.asfarray(all_values[1:]).reshape((28,28)) ####matplotlib.pyplot.imshow(image_array) # construct a graphic object # ####matplotlib.pyplot.show() # show the graphic object to a window # ##print( n.query((numpy.asfarray(all_values[1:])/255.0 * 0.99) + 0.01) ) ##image_array = numpy.asfarray(scaled_input).reshape((28,28)) ##matplotlib.pyplot.imshow(image_array) # construct a graphic object # ##matplotlib.pyplot.show() # show the graphic object to a window # ##matplotlib.pyplot.imshow(image_array, cmap = 'Greys', interpolation = 'nearest' ) # construct a graphic object # ##matplotlib.pyplot.show() # show the graphic object to a window # #","title":"References"},{"location":"6-Projects/P-InstrumentCalibration/InstrumentCalibration-Project/","text":"%%html <!--Script block to left align Markdown Tables--> <style> table {margin-left: 0 !important;} </style> table {margin-left: 0 !important;} ENGR 1330 \u2013 Computational Thinking and Data Science Fall 2020 Pressure Transducer Calibration Final Project Measurement is a process of comparison: the measured quantity is compared to a known standard. Before we measure, we must establish the relationship between the readout val-ues of our instrument and known input values of the measurand. This process is called as calibration. The purpose of calibrating a system is to: - Relate the actual sensor output (i.e. a voltage) to the quantity we are attempting to measure ( \\frac{F}{A} a normal stress, we usually call pressure) - Identify and correct bias error - Quantify random error associated with using each system. This project will develop scripts to calibrate two independent pressure measurement systems: an analog gauge and an electronic pressure transducer Background The pressures produced by the tester as shown by the values stamped on the weights as the exact true pressures. The true pressure, the pressure indicated on the analog gauge, and the voltage output by the pressure transducer are given to you, respectively. The figure below shows a picture of a typical deadweight tester instrument. An operational schematic is shown in the following figure Learn More at http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/Calibrationho.pdf Objective(s): Literature scan on instrument calibration and pressure measurement techniques Analyse an existing measurement database and build a data model to calibrate a pressure transducer, and an analog gage. The measurements database is comprised of 4 columns: - A test ID (in practice replicate tests are conducted. Why?) - The true applied pressures from 5 psig to 105 psig. - The analog pressures read from the gauge. - The voltages read from the transducer. Build an interface to allow users to enter voltages and return an estimated pressure and an assessment of the uncertainty in the estimate, and return the anticipated analog reading. Build an interface to allow users to add observations to the underlying database, and automatically update the calibration model to incorporate the new observations Tasks: Literature Research: - Describe the proces of pressure transducer calibration using a deadweight testbed (i.e. provide a description for the schematic figure above) - Summarize the value of a calibration (data) model in the context of converting voltage readings to applied pressure. Some places to start Beckwith, Marangoni, and Lienhard, Mechanical Measurements, Fifth Edition, Addison-Wesley PublishingCo., Reading, Massachusetts, 1993. Bevington and Robinson, Data Reduction and Error Analysis for the Physical Sciences, Second Edition,WCB/McGraw Hill, Boston, Massachusetts, 1992. Meyer, Paul L., Introductory Probability and Statistical Applications, 2nd Edition, Addison-WesleyPublishing Co., Reading, Massachusetts, 1970 Kline, S. J. and F. A. McClintock (1953). Describing uncertainties in single-sample experiments. Mechanical Engineering (No. 75), 3-9. http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/Kline_McClintock1953.pdf Holman, J. P. (1989). Experimental Methods for Engineers, 5th Edition. New York, NY: McGraw-Hill. http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/holman1989.pdf Doebelin, E. O. (1990). Measurement Systems (4th ed.). New York, NY: McGraw-Hill. http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/ce_5333_1.1_error_analysis.pdf http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/ce_5333_1.2_error_analysis.pdf Database Acquisition - Get the database from the repository: http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/PressureSensorData.csv Exploratory Data Analysis - Describe the database. Include visualization of the database. - Reformat as needed (column headings perhaps) the database for subsequent analysis. - For each measurement system (analog readout, sensor voltage): Plot the data (as symbols) e.g. true pressure versus analog pressure; analog pressure versus volts; true pressure versus volts. Describe the plots. - Select possible data model structures (linear model, power-law, ...) - Select possible data model \"fitting\" tools (ordinary least squares,lasso regression, decision trees, random forests, ...) Model Building - Build data models - Assess data model quality (decide which model is best) including visualization tools (red line through blue dots ...) - Build the input data interface for using the \"best\" model(s) - Assume that you are now using your calibrated analog pressure gauge to make a reading in the field. You read a value of 64 psi on the gauge. Estimate the true pressure at 95 percent confidence. Be sure to report your result with a value with an appropriate number of digits, an uncertainty bracket, units, and a confidence level. - Assume that you are now using your calibrated pressure transducer to make a reading in the field. You read a value of 1.4320 Volts . Estimate the true pressure at 95 percent confidence. Be sure to report your result with a value with an appropriate number of digits, an uncertainty bracket, units, and a confidence level. - Using your best model determine projected pressure readings and an assessment of uncertainty for 5 sensor voltages in the table below. |Test|Sensor Voltage| |:---|:---| |U1|3.0| |U2|0.2| |U3|0.03| |U4|5.0| |U5|0.07| Documentation - Training video on how to use your tool, and demonstrate the tool(s) as they are run - Project management video - Interim report (see deliverables below); this document must be rendered as a .pdf, but you are free to use your favorite writing software (Word,LibreOffice, ...). - Final Report (see deliverables below) Deliverables: Part 1 (due November 24): A report that briefly describes the concrete strength database and how you plan to solve the tasks of creating a suitable data model. - Break down each task into manageable subtasks and describe how you intend to solve the subtasks and how you will test each task. (Perhaps make a simple Gantt Chart) - Address the responsibilities of each team member for tasks completed and tasks to be completed until the end of the semester. (Perhaps make explicit subtask assignments) Your report should be limited to 4 pages, 12 pt font size, double linespacing (exclusive of references which are NOT included in the page count). You need to cite/reference all sources you used. Part 2 (due on Final Exam day): A well-documented JupyterLab (using a python kernel) analysis and implementation for the data model. A well-documented JupyterLab (using a python kernel) implementation for the data model user interface. A well-documented JupyterLab (using a python kernel) implementation for the database update interface. Above items can reside in a single notebook; but clearly identify sections that perform different tasks. A how-to video demonstrating performance and description of problems that you were not able to solve. A project management video (up to 5 minutes) in which you explain how you completed the project and how you worked as a team. Above items can reside in a single video; but structure the video into the two parts; use an obvious transition when moving from \"how to ...\" into the project management portion. Keep the total video length to less than 10 minutes; submit as an unlisted YouTube video, and just supply the link (someone on each team is likely to have a YouTube creator account). Keep in mind a 10 minute video can approach 100MB file size before compression, so it won't upload to Blackboard and cannot be emailed.","title":"InstrumentCalibration Project"},{"location":"6-Projects/P-InstrumentCalibration/InstrumentCalibration-Project/#engr-1330-computational-thinking-and-data-science-fall-2020","text":"","title":"ENGR 1330 \u2013 Computational Thinking and Data Science Fall 2020"},{"location":"6-Projects/P-InstrumentCalibration/InstrumentCalibration-Project/#pressure-transducer-calibration-final-project","text":"Measurement is a process of comparison: the measured quantity is compared to a known standard. Before we measure, we must establish the relationship between the readout val-ues of our instrument and known input values of the measurand. This process is called as calibration. The purpose of calibrating a system is to: - Relate the actual sensor output (i.e. a voltage) to the quantity we are attempting to measure ( \\frac{F}{A} a normal stress, we usually call pressure) - Identify and correct bias error - Quantify random error associated with using each system. This project will develop scripts to calibrate two independent pressure measurement systems: an analog gauge and an electronic pressure transducer","title":"Pressure Transducer Calibration Final Project"},{"location":"6-Projects/P-InstrumentCalibration/InstrumentCalibration-Project/#background","text":"The pressures produced by the tester as shown by the values stamped on the weights as the exact true pressures. The true pressure, the pressure indicated on the analog gauge, and the voltage output by the pressure transducer are given to you, respectively. The figure below shows a picture of a typical deadweight tester instrument. An operational schematic is shown in the following figure Learn More at http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/Calibrationho.pdf","title":"Background"},{"location":"6-Projects/P-InstrumentCalibration/InstrumentCalibration-Project/#objectives","text":"Literature scan on instrument calibration and pressure measurement techniques Analyse an existing measurement database and build a data model to calibrate a pressure transducer, and an analog gage. The measurements database is comprised of 4 columns: - A test ID (in practice replicate tests are conducted. Why?) - The true applied pressures from 5 psig to 105 psig. - The analog pressures read from the gauge. - The voltages read from the transducer. Build an interface to allow users to enter voltages and return an estimated pressure and an assessment of the uncertainty in the estimate, and return the anticipated analog reading. Build an interface to allow users to add observations to the underlying database, and automatically update the calibration model to incorporate the new observations","title":"Objective(s):"},{"location":"6-Projects/P-InstrumentCalibration/InstrumentCalibration-Project/#tasks","text":"Literature Research: - Describe the proces of pressure transducer calibration using a deadweight testbed (i.e. provide a description for the schematic figure above) - Summarize the value of a calibration (data) model in the context of converting voltage readings to applied pressure. Some places to start Beckwith, Marangoni, and Lienhard, Mechanical Measurements, Fifth Edition, Addison-Wesley PublishingCo., Reading, Massachusetts, 1993. Bevington and Robinson, Data Reduction and Error Analysis for the Physical Sciences, Second Edition,WCB/McGraw Hill, Boston, Massachusetts, 1992. Meyer, Paul L., Introductory Probability and Statistical Applications, 2nd Edition, Addison-WesleyPublishing Co., Reading, Massachusetts, 1970 Kline, S. J. and F. A. McClintock (1953). Describing uncertainties in single-sample experiments. Mechanical Engineering (No. 75), 3-9. http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/Kline_McClintock1953.pdf Holman, J. P. (1989). Experimental Methods for Engineers, 5th Edition. New York, NY: McGraw-Hill. http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/holman1989.pdf Doebelin, E. O. (1990). Measurement Systems (4th ed.). New York, NY: McGraw-Hill. http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/ce_5333_1.1_error_analysis.pdf http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/ce_5333_1.2_error_analysis.pdf Database Acquisition - Get the database from the repository: http://54.243.252.9/engr-1330-psuedo-course/CECE-1330-PsuedoCourse/6-Projects/P-InstrumentCalibration/PressureSensorData.csv Exploratory Data Analysis - Describe the database. Include visualization of the database. - Reformat as needed (column headings perhaps) the database for subsequent analysis. - For each measurement system (analog readout, sensor voltage): Plot the data (as symbols) e.g. true pressure versus analog pressure; analog pressure versus volts; true pressure versus volts. Describe the plots. - Select possible data model structures (linear model, power-law, ...) - Select possible data model \"fitting\" tools (ordinary least squares,lasso regression, decision trees, random forests, ...) Model Building - Build data models - Assess data model quality (decide which model is best) including visualization tools (red line through blue dots ...) - Build the input data interface for using the \"best\" model(s) - Assume that you are now using your calibrated analog pressure gauge to make a reading in the field. You read a value of 64 psi on the gauge. Estimate the true pressure at 95 percent confidence. Be sure to report your result with a value with an appropriate number of digits, an uncertainty bracket, units, and a confidence level. - Assume that you are now using your calibrated pressure transducer to make a reading in the field. You read a value of 1.4320 Volts . Estimate the true pressure at 95 percent confidence. Be sure to report your result with a value with an appropriate number of digits, an uncertainty bracket, units, and a confidence level. - Using your best model determine projected pressure readings and an assessment of uncertainty for 5 sensor voltages in the table below. |Test|Sensor Voltage| |:---|:---| |U1|3.0| |U2|0.2| |U3|0.03| |U4|5.0| |U5|0.07| Documentation - Training video on how to use your tool, and demonstrate the tool(s) as they are run - Project management video - Interim report (see deliverables below); this document must be rendered as a .pdf, but you are free to use your favorite writing software (Word,LibreOffice, ...). - Final Report (see deliverables below)","title":"Tasks:"},{"location":"6-Projects/P-InstrumentCalibration/InstrumentCalibration-Project/#deliverables","text":"","title":"Deliverables:"},{"location":"6-Projects/P-InstrumentCalibration/InstrumentCalibration-Project/#part-1-due-november-24","text":"A report that briefly describes the concrete strength database and how you plan to solve the tasks of creating a suitable data model. - Break down each task into manageable subtasks and describe how you intend to solve the subtasks and how you will test each task. (Perhaps make a simple Gantt Chart) - Address the responsibilities of each team member for tasks completed and tasks to be completed until the end of the semester. (Perhaps make explicit subtask assignments) Your report should be limited to 4 pages, 12 pt font size, double linespacing (exclusive of references which are NOT included in the page count). You need to cite/reference all sources you used.","title":"Part 1 (due November 24):"},{"location":"6-Projects/P-InstrumentCalibration/InstrumentCalibration-Project/#part-2-due-on-final-exam-day","text":"A well-documented JupyterLab (using a python kernel) analysis and implementation for the data model. A well-documented JupyterLab (using a python kernel) implementation for the data model user interface. A well-documented JupyterLab (using a python kernel) implementation for the database update interface. Above items can reside in a single notebook; but clearly identify sections that perform different tasks. A how-to video demonstrating performance and description of problems that you were not able to solve. A project management video (up to 5 minutes) in which you explain how you completed the project and how you worked as a team. Above items can reside in a single video; but structure the video into the two parts; use an obvious transition when moving from \"how to ...\" into the project management portion. Keep the total video length to less than 10 minutes; submit as an unlisted YouTube video, and just supply the link (someone on each team is likely to have a YouTube creator account). Keep in mind a 10 minute video can approach 100MB file size before compression, so it won't upload to Blackboard and cannot be emailed.","title":"Part 2 (due on Final Exam day):"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/","text":"1. The Dataset (20 points) Our dataset is a table of songs, each with a name, an artist, and a genre. For each song, we also know how frequently certain words occur in that song. More precisely, we have a list of approximately 5000 words. For each of these words, for each song, each item in the table describes the proportion of the song's lyrics that are the particular word. For example, the lyrics of \"In Your Eyes\" is 168 words long. The word \"like\" appears twice: \\frac{2}{168} \\approx 0.0119 of the words in the song. Similarly, the word \"love\" appears 10 times: \\frac{10}{168} \\approx 0.0595 of the words. Our dataset doesn't contain all information about a song. For example, it doesn't include the total number of words in each song, or information about the order of words in the song, let alone the melody, instruments, or rhythm. Nonetheless, you may find that word counts alone are sufficient to build an accurate genre classifier. Run the cell below to read the lyrics table. It may take up to a minute to load. import pandas as pd df = pd.read_csv(\"lyrics_clean.csv\") df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Artist Genre i the you to and a me ... writer motivo bake insist wel santo pe gee colleg kad 0 Slicker Than Your Average Craig David Hip-hop 0.049536 0.017028 0.035604 0.020124 0.007740 0.006192 0.058824 ... 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0 1 Right There MF Grimm Hip-hop 0.037825 0.054374 0.023641 0.049645 0.009456 0.016548 0.018913 ... 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0 2 Talkin' All That Cashis Hip-hop 0.056738 0.049645 0.051418 0.010638 0.026596 0.033688 0.007092 ... 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0 3 It Only Hurts Me When I Cry Raul Malo Country 0.096491 0.074561 0.030702 0.017544 0.026316 0.017544 0.021930 ... 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0 4 Is It Too Late Now Lester Flatt & Earl Scruggs Country 0.043902 0.000000 0.073171 0.019512 0.000000 0.014634 0.034146 ... 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0 5 rows \u00d7 4979 columns Question 1.1 : Print the number of rows and columns in the dataset Question 1.2 : Find the proportion of the word like in the song In Your Eyes Question 1.3: Set expected_row_sum to the number that you expect will result from summing all proportions in each row, excluding the first three columns. # Set row_sum to a number that's the (approximate) sum of each row of word proportions. expected_row_sum = ... Verify your answer by doing sum along the columns for each row Word Stemming The columns other than Title, Artist, and Genre in the lyrics table are all words that appear in some of the songs in our dataset. Some of those names have been stemmed , or abbreviated heuristically, in an attempt to make different inflected forms of the same base word into the same string. For example, the column \"manag\" is the sum of proportions of the words \"manage\", \"manager\", \"managed\", and \"managerial\" (and perhaps others) in each song. Stemming makes it a little tricky to search for the words you want to use, so we have provided another dataframe that will let you see examples of unstemmed versions of each stemmed word. Run the code below to load it. Question 1.4 : Read the vocabulary from the given file mxm_reverse_mapping_safe.csv and store it into a variale vocab_mapping vocab_mapping = ... vocab_mapping Question 1.5 : Compare if the number of stemmed words in the vocabulary is the same with one in the song lyrics dataset. Question 1.6: Assign unchanged to the percentage of words in vocab_table that are the same as their stemmed form. Question 1.7: Assign stemmed_message to the stemmed version of the word \"message\". # Set stemmed_message to the stemmed version of \"message\" (which # should be a string). Question 1.8: Assign unstemmed_singl to the word in vocab_table that has \"singl\" as its stemmed form. ( Note that multiple English words may stem to \"singl\", but only one example appears in vocab_table . ) # Set unstemmed_singl to the unstemmed version of \"single\" (which # should be a string). Question 1.9: What word in vocab_table was shortened the most by this stemming process? Assign most_shortened to the word. hint: function len(str) will return the length of the input string str . You will do a loop over rows of the vocabulary to compute the length of each word. Splitting the dataset We're going to use our lyrics dataset for three purposes. First, we want to train various song genre classifiers. Second, we want to validate which classifier is most effective. Finally, we want to test the performance of our final classifier. Hence, we need three different datasets: training , validation , and test . The purpose of a classifier is to generalize to unseen data that is similar to the training data. Therefore, we must ensure that there are no songs that appear in two different sets. We do so by splitting the dataset randomly. The dataset has already been permuted randomly, so it's easy to split. We just take the top for training, the next part for validation, and the last for test. Question 1.10 : Split the data with the ratio 80% for training and 20% for testing. Question 1.11 : Draw a horizontal bar chart with three bars that shows the proportion of Country songs in each of the training and testing datasets. 2. K-Nearest Neighbors (20 points) K-Nearest Neighbors (k-NN) is a classification algorithm. Given some features of an unseen example, it decides whether that example belongs to one or the other of two categories based on its similarity to previously seen examples. A feature we have about each song is the proportion of times a particular word appears in the lyrics , and the categories are two music genres: hip-hop and country. The algorithm requires many previously seen examples for which both the features and categories are known: that's the train_lyrics table. We're going to visualize the algorithm, instead of just describing it. To get started, let's pick colors for the genres. # Just run this cell to define genre_color. def genre_color(genre): \"\"\"Assign a color to each genre.\"\"\" if genre == 'Country': return 'gold' elif genre == 'Hip-hop': return 'blue' else: return 'green' genre_color('Country') 'gold' genre_color('Hip-hop') 'blue' Classifying a song In k-NN, we classify a song by finding the k songs in the training set that are most similar according to the features we choose. We call those songs with similar features the \"neighbors\". The k-NN algorithm assigns the song to the most common category among its k neighbors. Let's limit ourselves to just 2 features for now, so we can plot each song. The features we will use are the proportions of the words \"like\" and \"love\" in the lyrics. Taking the song \"In Your Eyes\" (in the test set), 0.0119 of its words are \"like\" and 0.0595 are \"love\". This song appears in the test set, so let's imagine that we don't yet know its genre. First, we need to make our notion of similarity more precise. We will say that the dissimilarity , or distance between two songs is the straight-line distance between them when we plot their features in a scatter diagram. This distance is called the Euclidean (\"yoo-KLID-ee-un\") distance. For example, in the song Insane in the Brain (in the training set), 0.0203 of all the words in the song are \"like\" and 0 are \"love\". Its distance from In Your Eyes on this 2-word feature set is \\sqrt{(0.0119 - 0.0203)^2 + (0.0595 - 0)^2} \\approx 0.06 . (If we included more or different features, the distance could be different.) A third song, Sangria Wine (in the training set), is 0.0044 \"like\" and 0.0925 \"love\". Question 2.1 : Define a function that creates a plot to display a test song and some training songs in a two-dimensional space defined by two features. Utilize the function to visualize the songs In Your Eyes , Sangria Wine , and Insane in the Brain . hint: the function has four arguments and it does not return anything but it plots the songs in 2D space: test_song: has string datatype, is the name of a song training_songs: has list datatype, is a list of songs x_feature: has string datatype, is the name of a feature. y_feature: has string datatype, is the name of another feature. import matplotlib.pyplot as plt def plot_with_two_features(test_song, training_songs, x_feature, y_feature): \"\"\"Plot a test song and training songs using two features.\"\"\" # visualize the distances of the songs In Your Eyes, Sangria Wine, and Insane in the Brain. training = [\"Sangria Wine\", \"Insane In The Brain\"] test_song = \"In Your Eyes\" plot_with_two_features(test_song, training, \"like\", \"love\") Question 2.2 : Utilize the plot_with_two_features function and plot the positions of the three songs Sangria Wine , Lookin' for Love , Insane In The Brain together with the song In Your Eyes . Which one is closer to In Your Eyes and what is its genre? Question 2.3. Complete the function distance_two_features that computes the Euclidean distance between any two songs, using two features. Utilize the function distance_two_features to show that Lookin' for Love is closer to In Your Eyes than Insane In The Brain . import math as m def distance_two_features(title0, title1, x_feature, y_feature): \"\"\"Compute the distance between two songs, represented as rows.\"\"\" return ... The nearest neighbor to a song is the example in the training set that has the smallest distance from that song. Question 2.4. What are the names and genres of the 7 closest songs to \"In Your Eyes\" in train_lyrics , by Euclidean distance for the 2 features \"like\" and \"love\"? To answer this question, make a dataframe named close_songs containing those 7 songs with columns \"Title\", \"Artist\", \"Genre\", \"like\", and \"love\" from the lyrics dataframe, as well as a column called distance that contains the distance from \"In Your Eyes\" sorted in ascending order . Question 2.5 . Find the most common value in the column Genre of the dataframe close_songs . In case of a tie, it can return any of the most common values. Congratulations are in order -- you've classified your first song! 3. Features (20 points) Now, we're going to extend our classifier to consider more than two features at a time. Euclidean distance still makes sense with more than two features. For n different features, we compute the difference between corresponding feature values for two songs, square each of the n differences, sum up the resulting numbers, and take the square root of the sum. Question 3.1 Write a function to compute the Euclidean distance between two arrays of features of arbitrary (but equal) length. Use it to compute the distance between the first song in the training set and the first song in the test set, using all of the features . (Remember that the title, artist, and genre of the songs are not features.) Hint: The function has two arguments which are two arrays representing the two lists of features: import numpy as np def distance(features1, features2): \"\"\"The Euclidean distance between two arrays of feature values.\"\"\" return ... Creating your own feature set Unfortunately, using all of the features has some downsides. One clear downside is computational -- computing Euclidean distances just takes a long time when we have lots of features. You might have noticed that in the last question! So we're going to select just 20. We'd like to choose features that are very discriminative . That is, features which lead us to correctly classify as much of the test set as possible. This process of choosing features that will make a classifier work well is sometimes called feature selection , or more broadly feature engineering . Question 3.2 Look through the list of features (the labels of the lyrics table after the first three). Choose 20 common words that you think might let you distinguish between country and hip-hop songs. Make sure to choose words that are frequent enough that every song contains at least one of them. Don't just choose the 20 most frequent, though... you can do much better. The first time you answer this question, spend some time looking through the features, but not more than 15 minutes. Question 3.3 In two sentences or less, describe how you selected your features. Question 3.4 Use the distance function developed above to compute the distance from the first song in the test set to all the songs in the training set, using your set of 20 features . Make a new dataframe called genre_and_distances with one row for each song in the training set and two columns: * The \"Genre\" of the training song * The \"Distance\" from the first song in the test set Ensure that genre_and_distances is sorted in increasing order by distance to the first test song . Question 3.5 Now compute the 5-nearest neighbors classification of the first song in the test set. That is, decide on its genre by finding the most common genre among its 5 nearest neighbors, according to the distances you've calculated. Then check whether your classifier chose the right genre. (Depending on the features you chose, your classifier might not get this song right, and that's okay.) A classifier function Now it's time to write a single function that encapsulates this whole process of classification. Question 3.6. Write a function called classify . It should take the following arguments: * An array of features for a song to classify , * A dataframe has similar structure of the original dataset, * k , the number of neighbors to use in classification. It should return the class your classifier picks for the given row of features (e.g., 'Country' or 'Hip-hop' ). Test if the function works by classifying the first song in the test set using k=5. def classify(test_features, train_dataframe, k): \"\"\"Return the most common class among k nearest neigbors to test_row.\"\"\" return ... Question 3.7. Assign grandpa_genre to the genre predicted by your classifier for the song \"Grandpa Got Runned Over By A John Deere\", using 9 neigbors. Evaluating your classifier Now that it's easy to use the classifier, let's see how accurate it is on the whole test set. But we will reduce the test set to 20 songs only to save computing power. Question 3.8. Generate a new test set of 20 songs from your current test set Question 3.9. Classify every song in the newly generated test set, then compute the proportion of correct classifications. (It may take some minutes to complete the classification of these 20 songs) At this point, you've gone through one cycle of classifier design. Let's summarize the steps: 1. From available data, select test and training sets. 2. Choose an algorithm you're going to use for classification. 3. Identify some features. 4. Define a classifier function using your features and the training set. 5. Evaluate its performance (the proportion of correct classifications) on the test set. 4. Feature design (15 points) One way to interpret the accuracy of a classifier is to compare it to another classifier. Question 4.1. Below we've provided 10 features selected by the staff [\"come\", \"do\", \"have\", \"heart\", \"make\", \"never\", \"now\", \"wanna\", \"with\", \"yo\"] . Build a 5-nearest-neighbor classifier using these features and compute its accuracy on the test set. Question 4.2. Are the features you chose better or worse than the staff features at classifying the test set? Why do you think this is so? Question 4.3. Is there anything random about a classifier's accuracy measured in this way? Is it possible that the difference in classifier performance is due to chance? If so, describe (in 2-3 sentences) how you would investigate that. 5. Computational thinking (15 points) The following questions are answered via a video of no more than 5 minutes. Everybody must speak. You will provide the link to that video in the answer box. Question 5.1 : Specifically refer to some lines of code, or the thought processes that you made in all the above solutions to elaborate computational concepts which are used in solving the project. Question 5.2 : How did you work as a team to complete the project? Question 5.3 (Optional - no credit): Draw a picture (or better yet, a data visualization) of life before, during, and/or after taking Computational Thinking with Data Science course.","title":"1. The Dataset (20 points)"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#1-the-dataset-20-points","text":"Our dataset is a table of songs, each with a name, an artist, and a genre. For each song, we also know how frequently certain words occur in that song. More precisely, we have a list of approximately 5000 words. For each of these words, for each song, each item in the table describes the proportion of the song's lyrics that are the particular word. For example, the lyrics of \"In Your Eyes\" is 168 words long. The word \"like\" appears twice: \\frac{2}{168} \\approx 0.0119 of the words in the song. Similarly, the word \"love\" appears 10 times: \\frac{10}{168} \\approx 0.0595 of the words. Our dataset doesn't contain all information about a song. For example, it doesn't include the total number of words in each song, or information about the order of words in the song, let alone the melody, instruments, or rhythm. Nonetheless, you may find that word counts alone are sufficient to build an accurate genre classifier. Run the cell below to read the lyrics table. It may take up to a minute to load. import pandas as pd df = pd.read_csv(\"lyrics_clean.csv\") df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Artist Genre i the you to and a me ... writer motivo bake insist wel santo pe gee colleg kad 0 Slicker Than Your Average Craig David Hip-hop 0.049536 0.017028 0.035604 0.020124 0.007740 0.006192 0.058824 ... 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0 1 Right There MF Grimm Hip-hop 0.037825 0.054374 0.023641 0.049645 0.009456 0.016548 0.018913 ... 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0 2 Talkin' All That Cashis Hip-hop 0.056738 0.049645 0.051418 0.010638 0.026596 0.033688 0.007092 ... 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0 3 It Only Hurts Me When I Cry Raul Malo Country 0.096491 0.074561 0.030702 0.017544 0.026316 0.017544 0.021930 ... 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0 4 Is It Too Late Now Lester Flatt & Earl Scruggs Country 0.043902 0.000000 0.073171 0.019512 0.000000 0.014634 0.034146 ... 0.0 0.0 0.0 0.0 0 0.0 0.0 0.0 0.0 0 5 rows \u00d7 4979 columns Question 1.1 : Print the number of rows and columns in the dataset Question 1.2 : Find the proportion of the word like in the song In Your Eyes Question 1.3: Set expected_row_sum to the number that you expect will result from summing all proportions in each row, excluding the first three columns. # Set row_sum to a number that's the (approximate) sum of each row of word proportions. expected_row_sum = ... Verify your answer by doing sum along the columns for each row","title":"1. The Dataset (20 points)"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#word-stemming","text":"The columns other than Title, Artist, and Genre in the lyrics table are all words that appear in some of the songs in our dataset. Some of those names have been stemmed , or abbreviated heuristically, in an attempt to make different inflected forms of the same base word into the same string. For example, the column \"manag\" is the sum of proportions of the words \"manage\", \"manager\", \"managed\", and \"managerial\" (and perhaps others) in each song. Stemming makes it a little tricky to search for the words you want to use, so we have provided another dataframe that will let you see examples of unstemmed versions of each stemmed word. Run the code below to load it. Question 1.4 : Read the vocabulary from the given file mxm_reverse_mapping_safe.csv and store it into a variale vocab_mapping vocab_mapping = ... vocab_mapping Question 1.5 : Compare if the number of stemmed words in the vocabulary is the same with one in the song lyrics dataset. Question 1.6: Assign unchanged to the percentage of words in vocab_table that are the same as their stemmed form. Question 1.7: Assign stemmed_message to the stemmed version of the word \"message\". # Set stemmed_message to the stemmed version of \"message\" (which # should be a string). Question 1.8: Assign unstemmed_singl to the word in vocab_table that has \"singl\" as its stemmed form. ( Note that multiple English words may stem to \"singl\", but only one example appears in vocab_table . ) # Set unstemmed_singl to the unstemmed version of \"single\" (which # should be a string). Question 1.9: What word in vocab_table was shortened the most by this stemming process? Assign most_shortened to the word. hint: function len(str) will return the length of the input string str . You will do a loop over rows of the vocabulary to compute the length of each word.","title":"Word Stemming"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#splitting-the-dataset","text":"We're going to use our lyrics dataset for three purposes. First, we want to train various song genre classifiers. Second, we want to validate which classifier is most effective. Finally, we want to test the performance of our final classifier. Hence, we need three different datasets: training , validation , and test . The purpose of a classifier is to generalize to unseen data that is similar to the training data. Therefore, we must ensure that there are no songs that appear in two different sets. We do so by splitting the dataset randomly. The dataset has already been permuted randomly, so it's easy to split. We just take the top for training, the next part for validation, and the last for test. Question 1.10 : Split the data with the ratio 80% for training and 20% for testing. Question 1.11 : Draw a horizontal bar chart with three bars that shows the proportion of Country songs in each of the training and testing datasets.","title":"Splitting the dataset"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#2-k-nearest-neighbors-20-points","text":"K-Nearest Neighbors (k-NN) is a classification algorithm. Given some features of an unseen example, it decides whether that example belongs to one or the other of two categories based on its similarity to previously seen examples. A feature we have about each song is the proportion of times a particular word appears in the lyrics , and the categories are two music genres: hip-hop and country. The algorithm requires many previously seen examples for which both the features and categories are known: that's the train_lyrics table. We're going to visualize the algorithm, instead of just describing it. To get started, let's pick colors for the genres. # Just run this cell to define genre_color. def genre_color(genre): \"\"\"Assign a color to each genre.\"\"\" if genre == 'Country': return 'gold' elif genre == 'Hip-hop': return 'blue' else: return 'green' genre_color('Country') 'gold' genre_color('Hip-hop') 'blue'","title":"2. K-Nearest Neighbors (20 points)"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#classifying-a-song","text":"In k-NN, we classify a song by finding the k songs in the training set that are most similar according to the features we choose. We call those songs with similar features the \"neighbors\". The k-NN algorithm assigns the song to the most common category among its k neighbors. Let's limit ourselves to just 2 features for now, so we can plot each song. The features we will use are the proportions of the words \"like\" and \"love\" in the lyrics. Taking the song \"In Your Eyes\" (in the test set), 0.0119 of its words are \"like\" and 0.0595 are \"love\". This song appears in the test set, so let's imagine that we don't yet know its genre. First, we need to make our notion of similarity more precise. We will say that the dissimilarity , or distance between two songs is the straight-line distance between them when we plot their features in a scatter diagram. This distance is called the Euclidean (\"yoo-KLID-ee-un\") distance. For example, in the song Insane in the Brain (in the training set), 0.0203 of all the words in the song are \"like\" and 0 are \"love\". Its distance from In Your Eyes on this 2-word feature set is \\sqrt{(0.0119 - 0.0203)^2 + (0.0595 - 0)^2} \\approx 0.06 . (If we included more or different features, the distance could be different.) A third song, Sangria Wine (in the training set), is 0.0044 \"like\" and 0.0925 \"love\". Question 2.1 : Define a function that creates a plot to display a test song and some training songs in a two-dimensional space defined by two features. Utilize the function to visualize the songs In Your Eyes , Sangria Wine , and Insane in the Brain . hint: the function has four arguments and it does not return anything but it plots the songs in 2D space: test_song: has string datatype, is the name of a song training_songs: has list datatype, is a list of songs x_feature: has string datatype, is the name of a feature. y_feature: has string datatype, is the name of another feature. import matplotlib.pyplot as plt def plot_with_two_features(test_song, training_songs, x_feature, y_feature): \"\"\"Plot a test song and training songs using two features.\"\"\" # visualize the distances of the songs In Your Eyes, Sangria Wine, and Insane in the Brain. training = [\"Sangria Wine\", \"Insane In The Brain\"] test_song = \"In Your Eyes\" plot_with_two_features(test_song, training, \"like\", \"love\") Question 2.2 : Utilize the plot_with_two_features function and plot the positions of the three songs Sangria Wine , Lookin' for Love , Insane In The Brain together with the song In Your Eyes . Which one is closer to In Your Eyes and what is its genre? Question 2.3. Complete the function distance_two_features that computes the Euclidean distance between any two songs, using two features. Utilize the function distance_two_features to show that Lookin' for Love is closer to In Your Eyes than Insane In The Brain . import math as m def distance_two_features(title0, title1, x_feature, y_feature): \"\"\"Compute the distance between two songs, represented as rows.\"\"\" return ... The nearest neighbor to a song is the example in the training set that has the smallest distance from that song. Question 2.4. What are the names and genres of the 7 closest songs to \"In Your Eyes\" in train_lyrics , by Euclidean distance for the 2 features \"like\" and \"love\"? To answer this question, make a dataframe named close_songs containing those 7 songs with columns \"Title\", \"Artist\", \"Genre\", \"like\", and \"love\" from the lyrics dataframe, as well as a column called distance that contains the distance from \"In Your Eyes\" sorted in ascending order . Question 2.5 . Find the most common value in the column Genre of the dataframe close_songs . In case of a tie, it can return any of the most common values. Congratulations are in order -- you've classified your first song!","title":"Classifying a  song"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#3-features-20-points","text":"Now, we're going to extend our classifier to consider more than two features at a time. Euclidean distance still makes sense with more than two features. For n different features, we compute the difference between corresponding feature values for two songs, square each of the n differences, sum up the resulting numbers, and take the square root of the sum.","title":"3. Features (20 points)"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#question-31","text":"Write a function to compute the Euclidean distance between two arrays of features of arbitrary (but equal) length. Use it to compute the distance between the first song in the training set and the first song in the test set, using all of the features . (Remember that the title, artist, and genre of the songs are not features.) Hint: The function has two arguments which are two arrays representing the two lists of features: import numpy as np def distance(features1, features2): \"\"\"The Euclidean distance between two arrays of feature values.\"\"\" return ...","title":"Question 3.1"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#creating-your-own-feature-set","text":"Unfortunately, using all of the features has some downsides. One clear downside is computational -- computing Euclidean distances just takes a long time when we have lots of features. You might have noticed that in the last question! So we're going to select just 20. We'd like to choose features that are very discriminative . That is, features which lead us to correctly classify as much of the test set as possible. This process of choosing features that will make a classifier work well is sometimes called feature selection , or more broadly feature engineering .","title":"Creating your own feature set"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#question-32","text":"Look through the list of features (the labels of the lyrics table after the first three). Choose 20 common words that you think might let you distinguish between country and hip-hop songs. Make sure to choose words that are frequent enough that every song contains at least one of them. Don't just choose the 20 most frequent, though... you can do much better. The first time you answer this question, spend some time looking through the features, but not more than 15 minutes.","title":"Question 3.2"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#question-33","text":"In two sentences or less, describe how you selected your features.","title":"Question 3.3"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#question-34","text":"Use the distance function developed above to compute the distance from the first song in the test set to all the songs in the training set, using your set of 20 features . Make a new dataframe called genre_and_distances with one row for each song in the training set and two columns: * The \"Genre\" of the training song * The \"Distance\" from the first song in the test set Ensure that genre_and_distances is sorted in increasing order by distance to the first test song .","title":"Question 3.4"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#question-35","text":"Now compute the 5-nearest neighbors classification of the first song in the test set. That is, decide on its genre by finding the most common genre among its 5 nearest neighbors, according to the distances you've calculated. Then check whether your classifier chose the right genre. (Depending on the features you chose, your classifier might not get this song right, and that's okay.)","title":"Question 3.5"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#a-classifier-function","text":"Now it's time to write a single function that encapsulates this whole process of classification. Question 3.6. Write a function called classify . It should take the following arguments: * An array of features for a song to classify , * A dataframe has similar structure of the original dataset, * k , the number of neighbors to use in classification. It should return the class your classifier picks for the given row of features (e.g., 'Country' or 'Hip-hop' ). Test if the function works by classifying the first song in the test set using k=5. def classify(test_features, train_dataframe, k): \"\"\"Return the most common class among k nearest neigbors to test_row.\"\"\" return ... Question 3.7. Assign grandpa_genre to the genre predicted by your classifier for the song \"Grandpa Got Runned Over By A John Deere\", using 9 neigbors.","title":"A classifier function"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#evaluating-your-classifier","text":"Now that it's easy to use the classifier, let's see how accurate it is on the whole test set. But we will reduce the test set to 20 songs only to save computing power. Question 3.8. Generate a new test set of 20 songs from your current test set Question 3.9. Classify every song in the newly generated test set, then compute the proportion of correct classifications. (It may take some minutes to complete the classification of these 20 songs) At this point, you've gone through one cycle of classifier design. Let's summarize the steps: 1. From available data, select test and training sets. 2. Choose an algorithm you're going to use for classification. 3. Identify some features. 4. Define a classifier function using your features and the training set. 5. Evaluate its performance (the proportion of correct classifications) on the test set.","title":"Evaluating your classifier"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#4-feature-design-15-points","text":"One way to interpret the accuracy of a classifier is to compare it to another classifier. Question 4.1. Below we've provided 10 features selected by the staff [\"come\", \"do\", \"have\", \"heart\", \"make\", \"never\", \"now\", \"wanna\", \"with\", \"yo\"] . Build a 5-nearest-neighbor classifier using these features and compute its accuracy on the test set. Question 4.2. Are the features you chose better or worse than the staff features at classifying the test set? Why do you think this is so? Question 4.3. Is there anything random about a classifier's accuracy measured in this way? Is it possible that the difference in classifier performance is due to chance? If so, describe (in 2-3 sentences) how you would investigate that.","title":"4. Feature design (15 points)"},{"location":"6-Projects/P-MusicClassification/genre-classification-project-problem/#5-computational-thinking-15-points","text":"The following questions are answered via a video of no more than 5 minutes. Everybody must speak. You will provide the link to that video in the answer box. Question 5.1 : Specifically refer to some lines of code, or the thought processes that you made in all the above solutions to elaborate computational concepts which are used in solving the project. Question 5.2 : How did you work as a team to complete the project? Question 5.3 (Optional - no credit): Draw a picture (or better yet, a data visualization) of life before, during, and/or after taking Computational Thinking with Data Science course.","title":"5. Computational thinking (15 points)"},{"location":"8-Labs/Lab0/Lab0_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Laboratory 0 Laboratory 0: Yes, That's how we count in python! Welcome to your first Jupyter Notebook . This is a medium that we will be using throughout the semester. Why is this called a notebook? Because you can write stuff in it! Is that it? Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!). How do we get this? There are online services that allow you create, modify, and export Jupyter notebooks. However, to have this on your local machines (computers), you can install Anaconda . Anaconda is a package of different software suits including \"Jupyter Notebook\". You can find videos on how to install Anaconda on your devices on BlackBoard: Go to Anaconda.com Scroll down to the bottom of the page or click on products > individual edition Download the right version for your system: Windows, MacOS, and Linux- This may take a while depending on your connection speed Once the installer file is downloaded, run it and install Anaconda on your machine. Anaconda requires almost 3 GB of free space Install it in a separate folder- Preferably on a drive with lots of free memory! BE PATIENT!- It will take a while. The Environment - Let's have a look around this window! The tabs File Edit View Insert Cell Kernel The Icons Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list) The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type. There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially). Code Cells: A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code. When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability. Markdown Cells: You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied: If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks: # title ## major headings ### subheadings #### 4th level subheadings ##### 5th level subheadings These codes are also quite useful: Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format: Raw Cells: Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook. Let's meet world's most popular python! What is python? \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language) How to have access to it? There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers: a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3 We can do the exact same thing in this notebook. But we need a CODE cell. print(\"Hello World\") Hello World This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens: print(\"This is my first notebook!\") This is my first notebook! How to save a notebook? As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE Exercise: Let's see who you are! Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!","title":"Lab0 Dev"},{"location":"8-Labs/Lab0/Lab0_Dev/#laboratory-0-yes-thats-how-we-count-in-python","text":"","title":"Laboratory 0: Yes, That's how we count in python!"},{"location":"8-Labs/Lab0/Lab0_Dev/#welcome-to-your-first-jupyter-notebook-this-is-a-medium-that-we-will-be-using-throughout-the-semester","text":"","title":"Welcome to your first Jupyter Notebook. This is a medium that we will be using throughout the semester."},{"location":"8-Labs/Lab0/Lab0_Dev/#why-is-this-called-a-notebook","text":"","title":"Why is this called a notebook?"},{"location":"8-Labs/Lab0/Lab0_Dev/#because-you-can-write-stuff-in-it","text":"","title":"Because you can write stuff in it!"},{"location":"8-Labs/Lab0/Lab0_Dev/#is-that-it","text":"","title":"Is that it?"},{"location":"8-Labs/Lab0/Lab0_Dev/#nope-you-can-write-and-run-code-in-this-notebook-plus-a-bunch-of-other-cool-stuff-such-as-making-graphs-running-tests-and-simulations-adding-images-and-prepare-documents-such-as-this-one","text":"","title":"Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!)."},{"location":"8-Labs/Lab0/Lab0_Dev/#how-do-we-get-this","text":"","title":"How do we get this?"},{"location":"8-Labs/Lab0/Lab0_Dev/#there-are-online-services-that-allow-you-create-modify-and-export-jupyter-notebooks-however-to-have-this-on-your-local-machines-computers-you-can-install-anaconda-anaconda-is-a-package-of-different-software-suits-including-jupyter-notebook-you-can-find-videos-on-how-to-install-anaconda-on-your-devices-on-blackboard","text":"Go to Anaconda.com Scroll down to the bottom of the page or click on products > individual edition Download the right version for your system: Windows, MacOS, and Linux- This may take a while depending on your connection speed Once the installer file is downloaded, run it and install Anaconda on your machine. Anaconda requires almost 3 GB of free space Install it in a separate folder- Preferably on a drive with lots of free memory! BE PATIENT!- It will take a while.","title":"There are online services that allow you create, modify, and export Jupyter notebooks. However, to have this on your local machines (computers), you can install Anaconda. Anaconda is a package of different software suits including \"Jupyter Notebook\". You can find videos on how to install Anaconda on your devices on BlackBoard:"},{"location":"8-Labs/Lab0/Lab0_Dev/#the-environment-lets-have-a-look-around-this-window","text":"The tabs File Edit View Insert Cell Kernel The Icons Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list)","title":"The Environment - Let's have a look around this window!"},{"location":"8-Labs/Lab0/Lab0_Dev/#the-notebook-consists-of-a-sequence-of-cells-a-cell-is-a-multiline-text-input-field-and-its-contents-can-be-executed-by-using-shift-enter-or-by-clicking-run-in-the-menu-bar-the-execution-behavior-of-a-cell-is-determined-by-the-cells-type","text":"","title":"The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type."},{"location":"8-Labs/Lab0/Lab0_Dev/#there-are-three-types-of-cells-code-cells-markdown-cells-and-raw-cells-every-cell-starts-off-being-a-code-cell-but-its-type-can-be-changed-by-using-a-drop-down-on-the-toolbar-which-will-be-code-initially","text":"","title":"There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially)."},{"location":"8-Labs/Lab0/Lab0_Dev/#code-cells","text":"","title":"Code Cells:"},{"location":"8-Labs/Lab0/Lab0_Dev/#a-code-cell-allows-you-to-edit-and-write-new-code-with-full-syntax-highlighting-and-tab-completion-the-programming-language-you-use-depends-on-the-kernel-what-we-will-use-for-this-course-and-the-default-kernel-ipython-runs-is-python-code","text":"","title":"A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code."},{"location":"8-Labs/Lab0/Lab0_Dev/#when-a-code-cell-is-executed-code-that-it-contains-is-sent-to-the-kernel-associated-with-the-notebook-the-results-that-are-returned-from-this-computation-are-then-displayed-in-the-notebook-as-the-cells-output-the-output-is-not-limited-to-text-with-many-other-possible-forms-of-output-are-also-possible-including-matplotlib-figures-and-html-tables-this-is-known-as-ipythons-rich-display-capability","text":"","title":"When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability."},{"location":"8-Labs/Lab0/Lab0_Dev/#markdown-cells","text":"","title":"Markdown Cells:"},{"location":"8-Labs/Lab0/Lab0_Dev/#you-can-document-the-computational-process-in-a-literate-way-alternating-descriptive-text-with-code-using-rich-text-in-ipython-this-is-accomplished-by-marking-up-text-with-the-markdown-language-the-corresponding-cells-are-called-markdown-cells-the-markdown-language-provides-a-simple-way-to-perform-this-text-markup-that-is-to-specify-which-parts-of-the-text-should-be-emphasized-italics-bold-form-lists-etc-in-fact-markdown-cells-allow-a-variety-of-cool-modifications-to-be-applied","text":"","title":"You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied:"},{"location":"8-Labs/Lab0/Lab0_Dev/#if-you-want-to-provide-structure-for-your-document-you-can-use-markdown-headings-markdown-headings-consist-of-1-to-5-hash-signs-followed-by-a-space-and-the-title-of-your-section-the-markdown-heading-will-be-converted-to-a-clickable-link-for-a-section-of-the-notebook-it-is-also-used-as-a-hint-when-exporting-to-other-document-formats-like-pdf-here-is-how-it-looks","text":"","title":"If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks:"},{"location":"8-Labs/Lab0/Lab0_Dev/#title","text":"","title":"# title"},{"location":"8-Labs/Lab0/Lab0_Dev/#major-headings","text":"","title":"## major headings"},{"location":"8-Labs/Lab0/Lab0_Dev/#subheadings","text":"","title":"### subheadings"},{"location":"8-Labs/Lab0/Lab0_Dev/#4th-level-subheadings","text":"","title":"#### 4th level subheadings"},{"location":"8-Labs/Lab0/Lab0_Dev/#5th-level-subheadings","text":"","title":"##### 5th level subheadings"},{"location":"8-Labs/Lab0/Lab0_Dev/#these-codes-are-also-quite-useful","text":"Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format:","title":"These codes are also quite useful:"},{"location":"8-Labs/Lab0/Lab0_Dev/#raw-cells","text":"","title":"Raw Cells:"},{"location":"8-Labs/Lab0/Lab0_Dev/#raw-cells-provide-a-place-in-which-you-can-write-output-directly-raw-cells-are-not-evaluated-by-the-notebook","text":"","title":"Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook."},{"location":"8-Labs/Lab0/Lab0_Dev/#lets-meet-worlds-most-popular-python","text":"","title":"Let's meet world's most popular python!"},{"location":"8-Labs/Lab0/Lab0_Dev/#what-is-python","text":"\"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language)","title":"What is python?"},{"location":"8-Labs/Lab0/Lab0_Dev/#how-to-have-access-to-it","text":"","title":"How to have access to it?"},{"location":"8-Labs/Lab0/Lab0_Dev/#there-are-plenty-of-ways-from-online-compilers-to-our-beloved-jupyter-notebook-on-your-local-machines-here-are-a-few-examples-of-online-compilers","text":"a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3","title":"There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers:"},{"location":"8-Labs/Lab0/Lab0_Dev/#we-can-do-the-exact-same-thing-in-this-notebook-but-we-need-a-code-cell","text":"print(\"Hello World\") Hello World","title":"We can do the exact same thing in this notebook. But we need a CODE cell."},{"location":"8-Labs/Lab0/Lab0_Dev/#this-is-the-classic-first-program-of-many-languages-the-script-input-is-quite-simple-we-instruct-the-computer-to-print-the-literal-string-hello-world-to-standard-inputoutput-device-which-is-the-console-lets-change-it-and-see-what-happens","text":"print(\"This is my first notebook!\") This is my first notebook!","title":"This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens:"},{"location":"8-Labs/Lab0/Lab0_Dev/#how-to-save-a-notebook","text":"As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE","title":"How to save a notebook?"},{"location":"8-Labs/Lab0/Lab0_Dev/#exercise-lets-see-who-you-are","text":"","title":"Exercise: Let's see who you are! "},{"location":"8-Labs/Lab0/Lab0_Dev/#similar-to-the-example-use-a-code-cell-and-print-a-paragraph-about-you-you-can-introduce-yourselves-and-write-about-interesting-things-to-and-about-you","text":"","title":"Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab0 Laboratory 0: Yes, That's how we count in python! Welcome to your first Jupyter Notebook . This is a medium that we will be using throughout the semester. Why is this called a notebook? Because you can write stuff in it! Is that it? Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!). How do we get this? There are online services that allow you create, modify, and export Jupyter notebooks. However, to have this on your local machines (computers), you can install Anaconda . Anaconda is a package of different software suits including \"Jupyter Notebook\". You can find videos on how to install Anaconda on your devices on BlackBoard: Go to Anaconda.com Scroll down to the bottom of the page or click on products > individual edition Download the right version for your system: Windows, MacOS, and Linux- This may take a while depending on your connection speed Once the installer file is downloaded, run it and install Anaconda on your machine. Anaconda requires almost 3 GB of free space Install it in a separate folder- Preferably on a drive with lots of free memory! BE PATIENT!- It will take a while. The Environment - Let's have a look around this window! Rami Malek in Mr. Robot The tabs: File Edit View Insert Cell Kernel The Icons: Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list) The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type. There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially). Code Cells: A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code. When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability. Markdown Cells: You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied: If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks: # title ## major headings ### subheadings #### 4th level subheadings ##### 5th level subheadings These codes are also quite useful: Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format: Raw Cells: Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook. Let's meet world's most popular python! What is python? \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language) How to have access to it? There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers: a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3 We can do the exact same thing in this notebook. But we need a CODE cell. print(\"Hello World\") Hello World This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens: print(\"This is my first notebook!\") This is my first notebook! How to save a notebook? As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE Exercise: Let's see who you are! Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!","title":"Lab0 Dev S21"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#laboratory-0-yes-thats-how-we-count-in-python","text":"","title":"Laboratory 0: Yes, That's how we count in python!"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#welcome-to-your-first-jupyter-notebook-this-is-a-medium-that-we-will-be-using-throughout-the-semester","text":"","title":"Welcome to your first Jupyter Notebook. This is a medium that we will be using throughout the semester."},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#why-is-this-called-a-notebook","text":"","title":"Why is this called a notebook?"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#because-you-can-write-stuff-in-it","text":"","title":"Because you can write stuff in it!"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#is-that-it","text":"","title":"Is that it?"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#nope-you-can-write-and-run-code-in-this-notebook-plus-a-bunch-of-other-cool-stuff-such-as-making-graphs-running-tests-and-simulations-adding-images-and-prepare-documents-such-as-this-one","text":"","title":"Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!)."},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#how-do-we-get-this","text":"","title":"How do we get this?"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#there-are-online-services-that-allow-you-create-modify-and-export-jupyter-notebooks-however-to-have-this-on-your-local-machines-computers-you-can-install-anaconda-anaconda-is-a-package-of-different-software-suits-including-jupyter-notebook-you-can-find-videos-on-how-to-install-anaconda-on-your-devices-on-blackboard","text":"Go to Anaconda.com Scroll down to the bottom of the page or click on products > individual edition Download the right version for your system: Windows, MacOS, and Linux- This may take a while depending on your connection speed Once the installer file is downloaded, run it and install Anaconda on your machine. Anaconda requires almost 3 GB of free space Install it in a separate folder- Preferably on a drive with lots of free memory! BE PATIENT!- It will take a while.","title":"There are online services that allow you create, modify, and export Jupyter notebooks. However, to have this on your local machines (computers), you can install Anaconda. Anaconda is a package of different software suits including \"Jupyter Notebook\". You can find videos on how to install Anaconda on your devices on BlackBoard:"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#the-environment-lets-have-a-look-around-this-window","text":"Rami Malek in Mr. Robot The tabs: File Edit View Insert Cell Kernel The Icons: Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list)","title":"The Environment - Let's have a look around this window!"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#the-notebook-consists-of-a-sequence-of-cells-a-cell-is-a-multiline-text-input-field-and-its-contents-can-be-executed-by-using-shift-enter-or-by-clicking-run-in-the-menu-bar-the-execution-behavior-of-a-cell-is-determined-by-the-cells-type","text":"","title":"The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type."},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#there-are-three-types-of-cells-code-cells-markdown-cells-and-raw-cells-every-cell-starts-off-being-a-code-cell-but-its-type-can-be-changed-by-using-a-drop-down-on-the-toolbar-which-will-be-code-initially","text":"","title":"There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially)."},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#code-cells","text":"","title":"Code Cells:"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#a-code-cell-allows-you-to-edit-and-write-new-code-with-full-syntax-highlighting-and-tab-completion-the-programming-language-you-use-depends-on-the-kernel-what-we-will-use-for-this-course-and-the-default-kernel-ipython-runs-is-python-code","text":"","title":"A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code."},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#when-a-code-cell-is-executed-code-that-it-contains-is-sent-to-the-kernel-associated-with-the-notebook-the-results-that-are-returned-from-this-computation-are-then-displayed-in-the-notebook-as-the-cells-output-the-output-is-not-limited-to-text-with-many-other-possible-forms-of-output-are-also-possible-including-matplotlib-figures-and-html-tables-this-is-known-as-ipythons-rich-display-capability","text":"","title":"When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability."},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#markdown-cells","text":"","title":"Markdown Cells:"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#you-can-document-the-computational-process-in-a-literate-way-alternating-descriptive-text-with-code-using-rich-text-in-ipython-this-is-accomplished-by-marking-up-text-with-the-markdown-language-the-corresponding-cells-are-called-markdown-cells-the-markdown-language-provides-a-simple-way-to-perform-this-text-markup-that-is-to-specify-which-parts-of-the-text-should-be-emphasized-italics-bold-form-lists-etc-in-fact-markdown-cells-allow-a-variety-of-cool-modifications-to-be-applied","text":"","title":"You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied:"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#if-you-want-to-provide-structure-for-your-document-you-can-use-markdown-headings-markdown-headings-consist-of-1-to-5-hash-signs-followed-by-a-space-and-the-title-of-your-section-the-markdown-heading-will-be-converted-to-a-clickable-link-for-a-section-of-the-notebook-it-is-also-used-as-a-hint-when-exporting-to-other-document-formats-like-pdf-here-is-how-it-looks","text":"","title":"If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks:"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#title","text":"","title":"# title"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#major-headings","text":"","title":"## major headings"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#subheadings","text":"","title":"### subheadings"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#4th-level-subheadings","text":"","title":"#### 4th level subheadings"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#5th-level-subheadings","text":"","title":"##### 5th level subheadings"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#these-codes-are-also-quite-useful","text":"Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format:","title":"These codes are also quite useful:"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#raw-cells","text":"","title":"Raw Cells:"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#raw-cells-provide-a-place-in-which-you-can-write-output-directly-raw-cells-are-not-evaluated-by-the-notebook","text":"","title":"Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook."},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#lets-meet-worlds-most-popular-python","text":"","title":"Let's meet world's most popular python!"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#what-is-python","text":"\"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language)","title":"What is python?"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#how-to-have-access-to-it","text":"","title":"How to have access to it?"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#there-are-plenty-of-ways-from-online-compilers-to-our-beloved-jupyter-notebook-on-your-local-machines-here-are-a-few-examples-of-online-compilers","text":"a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3","title":"There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers:"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#we-can-do-the-exact-same-thing-in-this-notebook-but-we-need-a-code-cell","text":"print(\"Hello World\") Hello World","title":"We can do the exact same thing in this notebook. But we need a CODE cell."},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#this-is-the-classic-first-program-of-many-languages-the-script-input-is-quite-simple-we-instruct-the-computer-to-print-the-literal-string-hello-world-to-standard-inputoutput-device-which-is-the-console-lets-change-it-and-see-what-happens","text":"print(\"This is my first notebook!\") This is my first notebook!","title":"This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens:"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#how-to-save-a-notebook","text":"As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE","title":"How to save a notebook?"},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#exercise-lets-see-who-you-are","text":"","title":"Exercise: Let's see who you are! "},{"location":"8-Labs/Lab0/Lab0_Dev_S21/#similar-to-the-example-use-a-code-cell-and-print-a-paragraph-about-you-you-can-introduce-yourselves-and-write-about-interesting-things-to-and-about-you","text":"","title":"Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!"},{"location":"8-Labs/Lab0/Lab1_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Laboratory 1 Laboratory 1: A Notebook Like No Other! Welcome to your first Jupyter Notebook . This is a medium that we will be using throughout the semester. Why is this called a notebook? Because you can write stuff in it! Is that it? Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!). The Environment - Let's have a look around this window! Rami Malek in Mr. Robot The tabs: File Edit View Insert Cell Kernel The Icons: Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list) The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type. There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially). Code Cells: A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code. When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability. Markdown Cells: You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied: If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks: # title ## major headings ### subheadings #### 4th level subheadings ##### 5th level subheadings These codes are also quite useful: Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format: Raw Cells: Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook. Let's meet world's most popular python! What is python? \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language) How to have access to it? There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers: a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3 We can do the exact same thing in this notebook. But we need a CODE cell. print(\"Hello World\") Hello World This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens: print(\"This is my first notebook!\") This is my first notebook! How to save a notebook? As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE Exercise: Let's see who you are! Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!","title":"Lab1 Dev"},{"location":"8-Labs/Lab0/Lab1_Dev/#laboratory-1-a-notebook-like-no-other","text":"","title":"Laboratory 1: A Notebook Like No Other!"},{"location":"8-Labs/Lab0/Lab1_Dev/#welcome-to-your-first-jupyter-notebook-this-is-a-medium-that-we-will-be-using-throughout-the-semester","text":"","title":"Welcome to your first Jupyter Notebook. This is a medium that we will be using throughout the semester."},{"location":"8-Labs/Lab0/Lab1_Dev/#why-is-this-called-a-notebook","text":"","title":"Why is this called a notebook?"},{"location":"8-Labs/Lab0/Lab1_Dev/#because-you-can-write-stuff-in-it","text":"","title":"Because you can write stuff in it!"},{"location":"8-Labs/Lab0/Lab1_Dev/#is-that-it","text":"","title":"Is that it?"},{"location":"8-Labs/Lab0/Lab1_Dev/#nope-you-can-write-and-run-code-in-this-notebook-plus-a-bunch-of-other-cool-stuff-such-as-making-graphs-running-tests-and-simulations-adding-images-and-prepare-documents-such-as-this-one","text":"","title":"Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!)."},{"location":"8-Labs/Lab0/Lab1_Dev/#the-environment-lets-have-a-look-around-this-window","text":"Rami Malek in Mr. Robot The tabs: File Edit View Insert Cell Kernel The Icons: Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list)","title":"The Environment - Let's have a look around this window!"},{"location":"8-Labs/Lab0/Lab1_Dev/#the-notebook-consists-of-a-sequence-of-cells-a-cell-is-a-multiline-text-input-field-and-its-contents-can-be-executed-by-using-shift-enter-or-by-clicking-run-in-the-menu-bar-the-execution-behavior-of-a-cell-is-determined-by-the-cells-type","text":"","title":"The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type."},{"location":"8-Labs/Lab0/Lab1_Dev/#there-are-three-types-of-cells-code-cells-markdown-cells-and-raw-cells-every-cell-starts-off-being-a-code-cell-but-its-type-can-be-changed-by-using-a-drop-down-on-the-toolbar-which-will-be-code-initially","text":"","title":"There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially)."},{"location":"8-Labs/Lab0/Lab1_Dev/#code-cells","text":"","title":"Code Cells:"},{"location":"8-Labs/Lab0/Lab1_Dev/#a-code-cell-allows-you-to-edit-and-write-new-code-with-full-syntax-highlighting-and-tab-completion-the-programming-language-you-use-depends-on-the-kernel-what-we-will-use-for-this-course-and-the-default-kernel-ipython-runs-is-python-code","text":"","title":"A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code."},{"location":"8-Labs/Lab0/Lab1_Dev/#when-a-code-cell-is-executed-code-that-it-contains-is-sent-to-the-kernel-associated-with-the-notebook-the-results-that-are-returned-from-this-computation-are-then-displayed-in-the-notebook-as-the-cells-output-the-output-is-not-limited-to-text-with-many-other-possible-forms-of-output-are-also-possible-including-matplotlib-figures-and-html-tables-this-is-known-as-ipythons-rich-display-capability","text":"","title":"When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability."},{"location":"8-Labs/Lab0/Lab1_Dev/#markdown-cells","text":"","title":"Markdown Cells:"},{"location":"8-Labs/Lab0/Lab1_Dev/#you-can-document-the-computational-process-in-a-literate-way-alternating-descriptive-text-with-code-using-rich-text-in-ipython-this-is-accomplished-by-marking-up-text-with-the-markdown-language-the-corresponding-cells-are-called-markdown-cells-the-markdown-language-provides-a-simple-way-to-perform-this-text-markup-that-is-to-specify-which-parts-of-the-text-should-be-emphasized-italics-bold-form-lists-etc-in-fact-markdown-cells-allow-a-variety-of-cool-modifications-to-be-applied","text":"","title":"You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied:"},{"location":"8-Labs/Lab0/Lab1_Dev/#if-you-want-to-provide-structure-for-your-document-you-can-use-markdown-headings-markdown-headings-consist-of-1-to-5-hash-signs-followed-by-a-space-and-the-title-of-your-section-the-markdown-heading-will-be-converted-to-a-clickable-link-for-a-section-of-the-notebook-it-is-also-used-as-a-hint-when-exporting-to-other-document-formats-like-pdf-here-is-how-it-looks","text":"","title":"If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks:"},{"location":"8-Labs/Lab0/Lab1_Dev/#title","text":"","title":"# title"},{"location":"8-Labs/Lab0/Lab1_Dev/#major-headings","text":"","title":"## major headings"},{"location":"8-Labs/Lab0/Lab1_Dev/#subheadings","text":"","title":"### subheadings"},{"location":"8-Labs/Lab0/Lab1_Dev/#4th-level-subheadings","text":"","title":"#### 4th level subheadings"},{"location":"8-Labs/Lab0/Lab1_Dev/#5th-level-subheadings","text":"","title":"##### 5th level subheadings"},{"location":"8-Labs/Lab0/Lab1_Dev/#these-codes-are-also-quite-useful","text":"Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format:","title":"These codes are also quite useful:"},{"location":"8-Labs/Lab0/Lab1_Dev/#raw-cells","text":"","title":"Raw Cells:"},{"location":"8-Labs/Lab0/Lab1_Dev/#raw-cells-provide-a-place-in-which-you-can-write-output-directly-raw-cells-are-not-evaluated-by-the-notebook","text":"","title":"Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook."},{"location":"8-Labs/Lab0/Lab1_Dev/#lets-meet-worlds-most-popular-python","text":"","title":"Let's meet world's most popular python!"},{"location":"8-Labs/Lab0/Lab1_Dev/#what-is-python","text":"\"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language)","title":"What is python?"},{"location":"8-Labs/Lab0/Lab1_Dev/#how-to-have-access-to-it","text":"","title":"How to have access to it?"},{"location":"8-Labs/Lab0/Lab1_Dev/#there-are-plenty-of-ways-from-online-compilers-to-our-beloved-jupyter-notebook-on-your-local-machines-here-are-a-few-examples-of-online-compilers","text":"a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3","title":"There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers:"},{"location":"8-Labs/Lab0/Lab1_Dev/#we-can-do-the-exact-same-thing-in-this-notebook-but-we-need-a-code-cell","text":"print(\"Hello World\") Hello World","title":"We can do the exact same thing in this notebook. But we need a CODE cell."},{"location":"8-Labs/Lab0/Lab1_Dev/#this-is-the-classic-first-program-of-many-languages-the-script-input-is-quite-simple-we-instruct-the-computer-to-print-the-literal-string-hello-world-to-standard-inputoutput-device-which-is-the-console-lets-change-it-and-see-what-happens","text":"print(\"This is my first notebook!\") This is my first notebook!","title":"This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens:"},{"location":"8-Labs/Lab0/Lab1_Dev/#how-to-save-a-notebook","text":"As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE","title":"How to save a notebook?"},{"location":"8-Labs/Lab0/Lab1_Dev/#exercise-lets-see-who-you-are","text":"","title":"Exercise: Let's see who you are! "},{"location":"8-Labs/Lab0/Lab1_Dev/#similar-to-the-example-use-a-code-cell-and-print-a-paragraph-about-you-you-can-introduce-yourselves-and-write-about-interesting-things-to-and-about-you","text":"","title":"Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!"},{"location":"8-Labs/Lab1/Lab1_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab1 Laboratory 1: First Steps... Notice the code cell below! From this notebook forward please include and run the script in the cell, it will help in debugging a notebook. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Also, from now on, please make sure that you have the following markdown cell, filled with your own information, on top of your notebooks: Full name: R#: Title of the notebook: Date: Now, let's get to work! Variables Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float). TimeOfConcentration + 5 5.0 Naming Rules Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print, input, if, while, and for. There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables. Operators The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below: # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10. What's it with # ? Comments are added by writing a hashtag symbol (#) followed by any text of your choice. Any text that follows the hashtag symbol on the same line is ignored by the Python interpreter. Arithmetic Operators In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x ** y Raises value in x by value in y. ( e.g. xy) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0 Data Type In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary Integer Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309 Real (Float) A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427 String(Alphanumeric) A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) print(MyName[0:4]) # Notice how the string is sliced- This is Python: ALWAYS start counting from zero! All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Theo Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting. Changing Types A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens! Expressions Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15 Example: Simple Input/Output Get two floating point numbers via the input() function and store them under the variable names float1 and float2 . Then, compare them, and try a few operations on them! float1 = input(\"Please enter float1: \") float1 = float(float1) ... Print float1 and float2 to the output screen. print(\"float1:\", float1) ... Then check whether float1 is greater than or equal to float2 . float1 = input(\"Please enter float1: \") float2 = input(\"Please enter float2: \") Please enter float1: 2.5 Please enter float2: 5 print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5 float1 = float(float1) float2 = float(float2) print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5.0 float1>float2 False float1+float2 7.5 float1/float2 0.5 Here are some great reads on this topic: - \"Variables in Python\" by John Sturtz available at https://realpython.com/python-variables/ - \"A Beginner\u2019s Guide To Python Variables\" by Avijeet Biswal available at https://www.simplilearn.com/tutorials/python-tutorial/python-variables - \"A Very Basic Introduction to Variables in Python\" by Dr. Python available at *https://medium.com/@doctorsmonsters/a-very-basic-introduction-to-variables-in-python-4231e36dac52 Here are some great videos on these topics: - \"Python Tutorial for Absolute Beginners #1 - What Are Variables?\" by CS Dojo available at https://www.youtube.com/watch?v=Z1Yd7upQsXY - \"#4 Python Tutorial for Beginners | Variables in Python\" by Telusko available at https://www.youtube.com/watch?v=TqPzwenhMj0 - \"Variables and Types in Python\" by DataCamp available at *https://www.youtube.com/watch?v=OH86oLzVzzw Exercise: Integer or Float? Think of a few cases where one might need to convert a float into an integer. * Make sure to cite any resources that you may use.","title":"Lab1 Dev"},{"location":"8-Labs/Lab1/Lab1_Dev/#laboratory-1-first-steps","text":"","title":"Laboratory 1: First Steps... "},{"location":"8-Labs/Lab1/Lab1_Dev/#notice-the-code-cell-below","text":"","title":"Notice the code cell below!"},{"location":"8-Labs/Lab1/Lab1_Dev/#from-this-notebook-forward-please-include-and-run-the-script-in-the-cell-it-will-help-in-debugging-a-notebook","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"From this notebook forward please include and run the script in the cell, it will help in debugging a notebook."},{"location":"8-Labs/Lab1/Lab1_Dev/#also-from-now-on-please-make-sure-that-you-have-the-following-markdown-cell-filled-with-your-own-information-on-top-of-your-notebooks","text":"","title":"Also, from now on, please make sure that you have the following markdown cell, filled with your own information, on top of your notebooks:"},{"location":"8-Labs/Lab1/Lab1_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab1/Lab1_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab1/Lab1_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab1/Lab1_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab1/Lab1_Dev/#now-lets-get-to-work","text":"","title":"Now, let's get to work!"},{"location":"8-Labs/Lab1/Lab1_Dev/#variables","text":"Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float). TimeOfConcentration + 5 5.0","title":"Variables"},{"location":"8-Labs/Lab1/Lab1_Dev/#naming-rules","text":"Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print, input, if, while, and for. There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables.","title":"Naming Rules"},{"location":"8-Labs/Lab1/Lab1_Dev/#operators","text":"The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below: # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10.","title":"Operators"},{"location":"8-Labs/Lab1/Lab1_Dev/#whats-it-with","text":"Comments are added by writing a hashtag symbol (#) followed by any text of your choice. Any text that follows the hashtag symbol on the same line is ignored by the Python interpreter.","title":"What's it with # ?"},{"location":"8-Labs/Lab1/Lab1_Dev/#arithmetic-operators","text":"In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x ** y Raises value in x by value in y. ( e.g. xy) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0","title":"Arithmetic Operators"},{"location":"8-Labs/Lab1/Lab1_Dev/#data-type","text":"In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary","title":"Data Type"},{"location":"8-Labs/Lab1/Lab1_Dev/#integer","text":"Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309","title":"Integer"},{"location":"8-Labs/Lab1/Lab1_Dev/#real-float","text":"A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427","title":"Real (Float)"},{"location":"8-Labs/Lab1/Lab1_Dev/#stringalphanumeric","text":"A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) print(MyName[0:4]) # Notice how the string is sliced- This is Python: ALWAYS start counting from zero! All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Theo Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting.","title":"String(Alphanumeric)"},{"location":"8-Labs/Lab1/Lab1_Dev/#changing-types","text":"A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens!","title":"Changing Types"},{"location":"8-Labs/Lab1/Lab1_Dev/#expressions","text":"Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15","title":"Expressions"},{"location":"8-Labs/Lab1/Lab1_Dev/#example-simple-inputoutput","text":"Get two floating point numbers via the input() function and store them under the variable names float1 and float2 . Then, compare them, and try a few operations on them! float1 = input(\"Please enter float1: \") float1 = float(float1) ... Print float1 and float2 to the output screen. print(\"float1:\", float1) ... Then check whether float1 is greater than or equal to float2 . float1 = input(\"Please enter float1: \") float2 = input(\"Please enter float2: \") Please enter float1: 2.5 Please enter float2: 5 print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5 float1 = float(float1) float2 = float(float2) print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5.0 float1>float2 False float1+float2 7.5 float1/float2 0.5 Here are some great reads on this topic: - \"Variables in Python\" by John Sturtz available at https://realpython.com/python-variables/ - \"A Beginner\u2019s Guide To Python Variables\" by Avijeet Biswal available at https://www.simplilearn.com/tutorials/python-tutorial/python-variables - \"A Very Basic Introduction to Variables in Python\" by Dr. Python available at *https://medium.com/@doctorsmonsters/a-very-basic-introduction-to-variables-in-python-4231e36dac52 Here are some great videos on these topics: - \"Python Tutorial for Absolute Beginners #1 - What Are Variables?\" by CS Dojo available at https://www.youtube.com/watch?v=Z1Yd7upQsXY - \"#4 Python Tutorial for Beginners | Variables in Python\" by Telusko available at https://www.youtube.com/watch?v=TqPzwenhMj0 - \"Variables and Types in Python\" by DataCamp available at *https://www.youtube.com/watch?v=OH86oLzVzzw","title":"Example: Simple Input/Output"},{"location":"8-Labs/Lab1/Lab1_Dev/#exercise-integer-or-float","text":"","title":"Exercise: Integer or Float? "},{"location":"8-Labs/Lab1/Lab1_Dev/#think-of-a-few-cases-where-one-might-need-to-convert-a-float-into-an-integer","text":"","title":"Think of a few cases where one might need to convert a float into an integer."},{"location":"8-Labs/Lab1/Lab1_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab1/Lab2_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab2 Laboratory 2: First Steps... Notice the code cell below! From this notebook forward please include and run the script in the cell, it will help in debugging a notebook. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Also, from now on, please make sure that you have the following markdown cell, filled with your own information, on top of your notebooks: Full name: R#: Title of the notebook: Date: Now, let's get to work! Variables Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float). TimeOfConcentration + 5 5.0 Naming Rules Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print, input, if, while, and for. There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables. Operators The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below: # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10. What's it with # ? Comments are added by writing a hashtag symbol (#) followed by any text of your choice. Any text that follows the hashtag symbol on the same line is ignored by the Python interpreter. Arithmetic Operators In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x ** y Raises value in x by value in y. ( e.g. xy) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0 Data Type In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary Integer Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309 Real (Float) A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427 String(Alphanumeric) A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) print(MyName[0:4]) # Notice how the string is sliced- This is Python: ALWAYS start counting from zero! All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Theo Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting. Changing Types A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens! Expressions Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15 Example: Simple Input/Output Get two floating point numbers via the input() function and store them under the variable names float1 and float2 . Then, compare them, and try a few operations on them! float1 = input(\"Please enter float1: \") float1 = float(float1) ... Print float1 and float2 to the output screen. print(\"float1:\", float1) ... Then check whether float1 is greater than or equal to float2 . float1 = input(\"Please enter float1: \") float2 = input(\"Please enter float2: \") Please enter float1: 2.5 Please enter float2: 5 print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5 float1 = float(float1) float2 = float(float2) print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5.0 float1>float2 False float1+float2 7.5 float1/float2 0.5 Here are some great reads on this topic: - \"Variables in Python\" by John Sturtz available at https://realpython.com/python-variables/ - \"A Beginner\u2019s Guide To Python Variables\" by Avijeet Biswal available at https://www.simplilearn.com/tutorials/python-tutorial/python-variables - \"A Very Basic Introduction to Variables in Python\" by Dr. Python available at *https://medium.com/@doctorsmonsters/a-very-basic-introduction-to-variables-in-python-4231e36dac52 Here are some great videos on these topics: - \"Python Tutorial for Absolute Beginners #1 - What Are Variables?\" by CS Dojo available at https://www.youtube.com/watch?v=Z1Yd7upQsXY - \"#4 Python Tutorial for Beginners | Variables in Python\" by Telusko available at https://www.youtube.com/watch?v=TqPzwenhMj0 - \"Variables and Types in Python\" by DataCamp available at *https://www.youtube.com/watch?v=OH86oLzVzzw Exercise: Integer or Float? Think of a few cases where one might need to convert a float into an integer. * Make sure to cite any resources that you may use.","title":"Lab2 Dev"},{"location":"8-Labs/Lab1/Lab2_Dev/#laboratory-2-first-steps","text":"","title":"Laboratory 2: First Steps... "},{"location":"8-Labs/Lab1/Lab2_Dev/#notice-the-code-cell-below","text":"","title":"Notice the code cell below!"},{"location":"8-Labs/Lab1/Lab2_Dev/#from-this-notebook-forward-please-include-and-run-the-script-in-the-cell-it-will-help-in-debugging-a-notebook","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"From this notebook forward please include and run the script in the cell, it will help in debugging a notebook."},{"location":"8-Labs/Lab1/Lab2_Dev/#also-from-now-on-please-make-sure-that-you-have-the-following-markdown-cell-filled-with-your-own-information-on-top-of-your-notebooks","text":"","title":"Also, from now on, please make sure that you have the following markdown cell, filled with your own information, on top of your notebooks:"},{"location":"8-Labs/Lab1/Lab2_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab1/Lab2_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab1/Lab2_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab1/Lab2_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab1/Lab2_Dev/#now-lets-get-to-work","text":"","title":"Now, let's get to work!"},{"location":"8-Labs/Lab1/Lab2_Dev/#variables","text":"Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float). TimeOfConcentration + 5 5.0","title":"Variables"},{"location":"8-Labs/Lab1/Lab2_Dev/#naming-rules","text":"Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print, input, if, while, and for. There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables.","title":"Naming Rules"},{"location":"8-Labs/Lab1/Lab2_Dev/#operators","text":"The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below: # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10.","title":"Operators"},{"location":"8-Labs/Lab1/Lab2_Dev/#whats-it-with","text":"Comments are added by writing a hashtag symbol (#) followed by any text of your choice. Any text that follows the hashtag symbol on the same line is ignored by the Python interpreter.","title":"What's it with # ?"},{"location":"8-Labs/Lab1/Lab2_Dev/#arithmetic-operators","text":"In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x ** y Raises value in x by value in y. ( e.g. xy) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0","title":"Arithmetic Operators"},{"location":"8-Labs/Lab1/Lab2_Dev/#data-type","text":"In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary","title":"Data Type"},{"location":"8-Labs/Lab1/Lab2_Dev/#integer","text":"Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309","title":"Integer"},{"location":"8-Labs/Lab1/Lab2_Dev/#real-float","text":"A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427","title":"Real (Float)"},{"location":"8-Labs/Lab1/Lab2_Dev/#stringalphanumeric","text":"A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) print(MyName[0:4]) # Notice how the string is sliced- This is Python: ALWAYS start counting from zero! All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Theo Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting.","title":"String(Alphanumeric)"},{"location":"8-Labs/Lab1/Lab2_Dev/#changing-types","text":"A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens!","title":"Changing Types"},{"location":"8-Labs/Lab1/Lab2_Dev/#expressions","text":"Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15","title":"Expressions"},{"location":"8-Labs/Lab1/Lab2_Dev/#example-simple-inputoutput","text":"Get two floating point numbers via the input() function and store them under the variable names float1 and float2 . Then, compare them, and try a few operations on them! float1 = input(\"Please enter float1: \") float1 = float(float1) ... Print float1 and float2 to the output screen. print(\"float1:\", float1) ... Then check whether float1 is greater than or equal to float2 . float1 = input(\"Please enter float1: \") float2 = input(\"Please enter float2: \") Please enter float1: 2.5 Please enter float2: 5 print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5 float1 = float(float1) float2 = float(float2) print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5.0 float1>float2 False float1+float2 7.5 float1/float2 0.5 Here are some great reads on this topic: - \"Variables in Python\" by John Sturtz available at https://realpython.com/python-variables/ - \"A Beginner\u2019s Guide To Python Variables\" by Avijeet Biswal available at https://www.simplilearn.com/tutorials/python-tutorial/python-variables - \"A Very Basic Introduction to Variables in Python\" by Dr. Python available at *https://medium.com/@doctorsmonsters/a-very-basic-introduction-to-variables-in-python-4231e36dac52 Here are some great videos on these topics: - \"Python Tutorial for Absolute Beginners #1 - What Are Variables?\" by CS Dojo available at https://www.youtube.com/watch?v=Z1Yd7upQsXY - \"#4 Python Tutorial for Beginners | Variables in Python\" by Telusko available at https://www.youtube.com/watch?v=TqPzwenhMj0 - \"Variables and Types in Python\" by DataCamp available at *https://www.youtube.com/watch?v=OH86oLzVzzw","title":"Example: Simple Input/Output"},{"location":"8-Labs/Lab1/Lab2_Dev/#exercise-integer-or-float","text":"","title":"Exercise: Integer or Float? "},{"location":"8-Labs/Lab1/Lab2_Dev/#think-of-a-few-cases-where-one-might-need-to-convert-a-float-into-an-integer","text":"","title":"Think of a few cases where one might need to convert a float into an integer."},{"location":"8-Labs/Lab1/Lab2_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab13/Lab13_Dev/","text":"Laboratory 13: Title. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, May 27 2021, 13:30:53) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date:","title":"<font color=darkred>Laboratory 13: Title. </font>"},{"location":"8-Labs/Lab13/Lab13_Dev/#laboratory-13-title","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, May 27 2021, 13:30:53) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Laboratory 13: Title. "},{"location":"8-Labs/Lab13/Lab13_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab13/Lab13_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab13/Lab13_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab13/Lab13_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab15/Lab15_Dev/","text":"Download (right-click, save target as ...) this jupyterlab notebook from: https://3.137.111.182/engr-1330-webbook/8-Labs/Lab15/Lab15_Dev.ipynb Download (right-click, save target as ...) AirTraffic database file from: https://3.137.111.182/engr-1330-webbook/8-Labs/Lab15/AirTraffic.csv Download (right-click, save target as ...) Lubbock_Oct_T&P database file from: https://3.137.111.182/engr-1330-webbook/8-Labs/Lab15/Lubbock_Oct_T&P.csv Laboratory 15: \"Avoiding Data Alexithymia\" or \"Perks of Using Descriptive Statistics # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Descriptive Statistics with Python # Let's import the necessary libraries: import numpy as np import pandas as pd import statistics import scipy.stats import matplotlib.pyplot as plt Example1: 1. Read the \"AirTraffic.csv\" file as a dataframe and check its first few rows. 2. Use descriptive functions of the Pandas library to learn more about the dataframe 3. Compute the arithmetic and harmonic mean of 'Distance'. 4. Find the median of 'Distance'. 5. Find the range of 'Distance'. 6. Find the IQR of 'Distance'. 7. Use descriptive functions of the Pandas library to get a 5-number summary of 'Distance'. Plot a box plot without outliers. 8. Find the variance and standard deviation of 'Distance'. 9. Find the skewness and kurtosis 'Distance'. AT = pd.read_csv(\"AirTraffic.csv\") #Read the .csv file a data frame AT.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DISTANCE ORIGIN_COUNTRY_NAME DEST_COUNTRY_NAME AIRCRAFT_GROUP CLASS DATA_SOURCE 0 3855 United States Germany 6 G IF 1 5805 United States Germany 6 G IF 2 801 United States United States 6 G DF 3 4343 United States Germany 6 G IF 4 5142 United States Germany 6 G IF AT.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 118279 entries, 0 to 118278 Data columns (total 6 columns): DISTANCE 118279 non-null int64 ORIGIN_COUNTRY_NAME 118279 non-null object DEST_COUNTRY_NAME 118279 non-null object AIRCRAFT_GROUP 118279 non-null int64 CLASS 118279 non-null object DATA_SOURCE 118279 non-null object dtypes: int64(2), object(4) memory usage: 5.4+ MB Distance = AT['DISTANCE'] # Use the mean function from the statistics library mean = statistics.mean(Distance) print(\"The arithmetic mean distance of the 2020 flights is \",round(mean,2),\"miles\") hmean = statistics.harmonic_mean(Distance) print(\"The harmonic mean distance of the 2020 flights is \",round(hmean,2),\"miles\") The arithmetic mean distance of the 2020 flights is 1101.12 miles The harmonic mean distance of the 2020 flights is 220.81 miles Distance = AT['DISTANCE'] # Use the mean function from the statistics library median = statistics.median(Distance) print(\"The median of distance of the 2020 flights is \",median,\"miles\") The median of distance of the 2020 flights is 740 miles Distance = AT['DISTANCE'] Range = np.ptp(Distance) #ptp stands for Peak To Peak print(\"The range of distance of the 2020 flights is \",Range,\"miles\") The range of distance of the 2020 flights is 9778 miles C:\\Users\\Farha\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead. return ptp(axis=axis, out=out, **kwargs) Distance = AT['DISTANCE'] IQR = scipy.stats.iqr(Distance) print(\"The IQR of distance of the 2020 flights is \",IQR,\"miles\") The IQR of distance of the 2020 flights is 968.0 miles Distance = AT['DISTANCE'] Distance.describe() count 118279.000000 mean 1101.120165 std 1260.423448 min 1.000000 25% 335.000000 50% 740.000000 75% 1303.000000 max 9779.000000 Name: DISTANCE, dtype: float64 fig = plt.figure(figsize =(7, 5)) plt.boxplot(Distance,medianprops={'linewidth': 1, 'color': 'purple'},showfliers=False) plt.show() Distance = AT['DISTANCE'] var = statistics.variance(Distance) sd = statistics.stdev(Distance) print(\"The variance and standard deviation of distance of the 2020 flights is \",round(var,2),\" and \",round(sd,2),\" respectively\") The variance and standard deviation of distance of the 2020 flights is 1588667.27 and 1260.42 respectively Distance = AT['DISTANCE'] skew = scipy.stats.skew(Distance) kurtosis = scipy.stats.kurtosis(Distance) print(\"The skewness and kurtosis of distance of the 2020 flights is \",round(skew,2),\" and \",round(kurtosis,2),\" respectively\") The skewness and kurtosis of distance of the 2020 flights is 2.63 and 8.13 respectively Example: 1. Read the \"Lubbock_Oct_T&P.csv\" file as a dataframe and check its first few rows. 2. Use descriptive functions of the Pandas library and explain the format of the dataframe 3. Compute the arithmetic and harmonic mean of 'temperature'. 4. Find the median of 'precipitation' and 'temperature'. 5. Find the range and IQR of 'precipitation'. 6. Find the 10th,40th, and 70th percentile of 'temperature'. 7. Provide a 5-number summary of 'precipitation'. Plot a box plot without outliers. Interpret it in your own words 8. Find the variance and standard deviation of 'precipitation'. 9. Find the skewness and kurtosis 'precipitation'. Here are some great reads on this topic: - \"Python Statistics Fundamentals: How to Describe Your Data\" by Mirko Stojiljkovi\u0107 available at https://realpython.com/python-statistics/ - \"Introduction to Descriptive Statistics and Probability for Data Science\" by Abhishek Kumar available at https://towardsdatascience.com/intro-to-descriptive-statistics-and-probability-for-data-science-8effec826488 - \"Statistics for Data Science \u2014 A beginners guide to Descriptive Statistics in Python\" by Angel Das available at https://towardsdatascience.com/statistics-for-data-science-a-beginners-guide-to-descriptive-statistics-in-python-9e0daa30809a - \"Interpreting Data Using Descriptive Statistics with Python\" by Deepika Singh available at https://www.pluralsight.com/guides/interpreting-data-using-descriptive-statistics-python Here are some great videos on these topics: - \"Descriptive Statistics Using Scipy , Numpy and Pandas in Python - Tutorial 13\" by TheEngineeringWorld available at https://www.youtube.com/watch?v=mWIwXqtZmd8 - \"Python for Data Analysis: Descriptive Statistics\" by DataDaft available at https://www.youtube.com/watch?v=3mELSEnGBvA - \"Introduction to Descriptive Statistics\" by Teresa Johnson available at https://www.youtube.com/watch?v=QoQbR4lVLrs Exercise: Why Descriptive Statistics? What is the importance and application of descriptive statistics, especially in Engineering. Make sure to cite any resources that you may use.","title":"Lab15 Dev"},{"location":"8-Labs/Lab15/Lab15_Dev/#laboratory-15-avoiding-data-alexithymia-or-perks-of-using-descriptive-statistics","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 15: \"Avoiding Data Alexithymia\" or \"Perks of Using Descriptive Statistics"},{"location":"8-Labs/Lab15/Lab15_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab15/Lab15_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab15/Lab15_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab15/Lab15_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab15/Lab15_Dev/#descriptive-statistics-with-python","text":"# Let's import the necessary libraries: import numpy as np import pandas as pd import statistics import scipy.stats import matplotlib.pyplot as plt","title":"Descriptive Statistics with Python"},{"location":"8-Labs/Lab15/Lab15_Dev/#example1","text":"1. Read the \"AirTraffic.csv\" file as a dataframe and check its first few rows. 2. Use descriptive functions of the Pandas library to learn more about the dataframe 3. Compute the arithmetic and harmonic mean of 'Distance'. 4. Find the median of 'Distance'. 5. Find the range of 'Distance'. 6. Find the IQR of 'Distance'. 7. Use descriptive functions of the Pandas library to get a 5-number summary of 'Distance'. Plot a box plot without outliers. 8. Find the variance and standard deviation of 'Distance'. 9. Find the skewness and kurtosis 'Distance'. AT = pd.read_csv(\"AirTraffic.csv\") #Read the .csv file a data frame AT.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } DISTANCE ORIGIN_COUNTRY_NAME DEST_COUNTRY_NAME AIRCRAFT_GROUP CLASS DATA_SOURCE 0 3855 United States Germany 6 G IF 1 5805 United States Germany 6 G IF 2 801 United States United States 6 G DF 3 4343 United States Germany 6 G IF 4 5142 United States Germany 6 G IF AT.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 118279 entries, 0 to 118278 Data columns (total 6 columns): DISTANCE 118279 non-null int64 ORIGIN_COUNTRY_NAME 118279 non-null object DEST_COUNTRY_NAME 118279 non-null object AIRCRAFT_GROUP 118279 non-null int64 CLASS 118279 non-null object DATA_SOURCE 118279 non-null object dtypes: int64(2), object(4) memory usage: 5.4+ MB Distance = AT['DISTANCE'] # Use the mean function from the statistics library mean = statistics.mean(Distance) print(\"The arithmetic mean distance of the 2020 flights is \",round(mean,2),\"miles\") hmean = statistics.harmonic_mean(Distance) print(\"The harmonic mean distance of the 2020 flights is \",round(hmean,2),\"miles\") The arithmetic mean distance of the 2020 flights is 1101.12 miles The harmonic mean distance of the 2020 flights is 220.81 miles Distance = AT['DISTANCE'] # Use the mean function from the statistics library median = statistics.median(Distance) print(\"The median of distance of the 2020 flights is \",median,\"miles\") The median of distance of the 2020 flights is 740 miles Distance = AT['DISTANCE'] Range = np.ptp(Distance) #ptp stands for Peak To Peak print(\"The range of distance of the 2020 flights is \",Range,\"miles\") The range of distance of the 2020 flights is 9778 miles C:\\Users\\Farha\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead. return ptp(axis=axis, out=out, **kwargs) Distance = AT['DISTANCE'] IQR = scipy.stats.iqr(Distance) print(\"The IQR of distance of the 2020 flights is \",IQR,\"miles\") The IQR of distance of the 2020 flights is 968.0 miles Distance = AT['DISTANCE'] Distance.describe() count 118279.000000 mean 1101.120165 std 1260.423448 min 1.000000 25% 335.000000 50% 740.000000 75% 1303.000000 max 9779.000000 Name: DISTANCE, dtype: float64 fig = plt.figure(figsize =(7, 5)) plt.boxplot(Distance,medianprops={'linewidth': 1, 'color': 'purple'},showfliers=False) plt.show() Distance = AT['DISTANCE'] var = statistics.variance(Distance) sd = statistics.stdev(Distance) print(\"The variance and standard deviation of distance of the 2020 flights is \",round(var,2),\" and \",round(sd,2),\" respectively\") The variance and standard deviation of distance of the 2020 flights is 1588667.27 and 1260.42 respectively Distance = AT['DISTANCE'] skew = scipy.stats.skew(Distance) kurtosis = scipy.stats.kurtosis(Distance) print(\"The skewness and kurtosis of distance of the 2020 flights is \",round(skew,2),\" and \",round(kurtosis,2),\" respectively\") The skewness and kurtosis of distance of the 2020 flights is 2.63 and 8.13 respectively","title":"Example1:"},{"location":"8-Labs/Lab15/Lab15_Dev/#example","text":"1. Read the \"Lubbock_Oct_T&P.csv\" file as a dataframe and check its first few rows. 2. Use descriptive functions of the Pandas library and explain the format of the dataframe 3. Compute the arithmetic and harmonic mean of 'temperature'. 4. Find the median of 'precipitation' and 'temperature'. 5. Find the range and IQR of 'precipitation'. 6. Find the 10th,40th, and 70th percentile of 'temperature'. 7. Provide a 5-number summary of 'precipitation'. Plot a box plot without outliers. Interpret it in your own words 8. Find the variance and standard deviation of 'precipitation'. 9. Find the skewness and kurtosis 'precipitation'. Here are some great reads on this topic: - \"Python Statistics Fundamentals: How to Describe Your Data\" by Mirko Stojiljkovi\u0107 available at https://realpython.com/python-statistics/ - \"Introduction to Descriptive Statistics and Probability for Data Science\" by Abhishek Kumar available at https://towardsdatascience.com/intro-to-descriptive-statistics-and-probability-for-data-science-8effec826488 - \"Statistics for Data Science \u2014 A beginners guide to Descriptive Statistics in Python\" by Angel Das available at https://towardsdatascience.com/statistics-for-data-science-a-beginners-guide-to-descriptive-statistics-in-python-9e0daa30809a - \"Interpreting Data Using Descriptive Statistics with Python\" by Deepika Singh available at https://www.pluralsight.com/guides/interpreting-data-using-descriptive-statistics-python Here are some great videos on these topics: - \"Descriptive Statistics Using Scipy , Numpy and Pandas in Python - Tutorial 13\" by TheEngineeringWorld available at https://www.youtube.com/watch?v=mWIwXqtZmd8 - \"Python for Data Analysis: Descriptive Statistics\" by DataDaft available at https://www.youtube.com/watch?v=3mELSEnGBvA - \"Introduction to Descriptive Statistics\" by Teresa Johnson available at https://www.youtube.com/watch?v=QoQbR4lVLrs","title":"Example:"},{"location":"8-Labs/Lab15/Lab15_Dev/#exercise-why-descriptive-statistics","text":"","title":"Exercise: Why Descriptive Statistics?  "},{"location":"8-Labs/Lab15/Lab15_Dev/#what-is-the-importance-and-application-of-descriptive-statistics-especially-in-engineering","text":"","title":"What is the importance and application of descriptive statistics, especially in Engineering."},{"location":"8-Labs/Lab15/Lab15_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab16/Lab16_Dev/","text":"Download (right-click, save target as ...) this jupyterlab notebook from: https://3.137.111.182/engr-1330-webbook/8-Labs/Lab16/Lab16_Dev.ipynb Download (right-click, save target as ...) sta08068500 database file from: https://3.137.111.182/engr-1330-webbook/8-Labs/Lab16/sta08068500.pkf Laboratory 16: \"Functions for Probability Modeling\" # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Important Terminology: Population: In statistics, a population is the entire pool from which a statistical sample is drawn. A population may refer to an entire group of people, objects, events, hospital visits, or measurements. Sample: In statistics and quantitative research methodology, a sample is a set of individuals or objects collected or selected from a statistical population by a defined procedure. The elements of a sample are known as sample points, sampling units or observations. Distribution (Data Model): A data distribution is a special function or a listing which shows all the possible values (or intervals) of the data. It also (and this is important) tells you how often each value occurs. From https://www.investopedia.com/terms https://www.statisticshowto.com/data-distribution/ Important Steps in going from samples to population: Get descriptive statistics- mean, variance, std. dev. Use plotting position formulas (e.g., weibull, gringorten, cunnane) and plot the SAMPLES (data you already have) Use different data models (e.g., normal, log-normal, Gumbell) and find the one that better FITs your samples- Visual or Numerical Use the data model that provides the best fit to infer about the POPULATION Estimate the magnitude of the annual peak flow at Spring Ck near Spring, TX. The file 08068500.pkf is an actual WATSTORE formatted file for a USGS gage at Spring Creek, Texas. The first few lines of the file look like: Z08068500 USGS H08068500 3006370952610004848339SW12040102409 409 72.6 N08068500 Spring Ck nr Spring, TX Y08068500 308068500 19290530 483007 34.30 1879 308068500 19390603 838 13.75 308068500 19400612 3420 21.42 308068500 19401125 42700 33.60 308068500 19420409 14200 27.78 308068500 19430730 8000 25.09 308068500 19440319 5260 23.15 308068500 19450830 31100 32.79 308068500 19460521 12200 27.97 The first column are some agency codes that identify the station , the second column after the fourth row is a date in YYYYMMDD format, the third column is a discharge in CFS, the fourth and fifth column are not relevant for this laboratory exercise. The file was downloadef from https://nwis.waterdata.usgs.gov/tx/nwis/peak?site_no=08068500&agency_cd=USGS&format=hn2 In the original file there are a couple of codes that are manually removed: 19290530 483007; the trailing 7 is a code identifying a break in the series (non-sequential) 20170828 784009; the trailing 9 identifies the historical peak The laboratory task is to fit the data models to this data, decide the best model from visual perspective, and report from that data model the magnitudes of peak flow associated with the probebilitiess below (i.e. populate the table) Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) The first step is to read the file, skipping the first part, then build a dataframe: # Read the data file amatrix = [] # null list to store matrix reads rowNumA = 0 matrix1=[] col0=[] col1=[] col2=[] with open('08068500.pkf','r') as afile: lines_after_4 = afile.readlines()[4:] afile.close() # Disconnect the file howmanyrows = len(lines_after_4) for i in range(howmanyrows): matrix1.append(lines_after_4[i].strip().split()) for i in range(howmanyrows): col0.append(int(matrix1[i][0])) col1.append(matrix1[i][1]) col2.append(int(matrix1[i][2])) #We need to conver it to integers # col2 is date, col3 is peak flow #now build a datafranem import pandas df = pandas.DataFrame(col0) df['date']= col1 df['flow']= col2 df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 date flow 0 308068500 19290530 48300 1 308068500 19390603 838 2 308068500 19400612 3420 3 308068500 19401125 42700 4 308068500 19420409 14200 Now explore if you can plot the dataframe as a plot of peaks versus date. import matplotlib.pyplot # the python plotting library import numpy as np date = df['date'] flow = df['flow'] # Plot here myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate an object from the figure class, set aspect ratio matplotlib.pyplot.plot(date,flow, color ='blue') [<matplotlib.lines.Line2D at 0x2289ccf1cc8>] From here on you can proceede using the lecture notebook as a go-by, although you should use functions as much as practical to keep your work concise # Descriptive Statistics flow.describe() count 80.000000 mean 11197.800000 std 15022.831582 min 381.000000 25% 3360.000000 50% 7190.000000 75% 11500.000000 max 78800.000000 Name: flow, dtype: float64 # Weibull Plotting Position Function # sort the flow in place! flow1 = np.array(flow) flow1.sort() # built a relative frequency approximation to probability, assume each pick is equally likely weibull_pp = [] for i in range(0,len(flow1),1): weibull_pp.append((i+1)/(len(flow1)+1)) What is Weibull function? The Weibull plotting position for the rth ranked (from largest to smallest) datum from a sample of size n is the quotient r / (n+1) It is recommended for use when the form of the underlying distribution is unknown and when unbiased exceedance probabilities are desired. From Glossary of Meteorology @ https://glossary.ametsoc.org/wiki/Weibull_plotting_position#:~:text=The%20Weibull%20plotting%20position%20for,unbiased%20exceedance%20probabilities%20are%20desired # Normal Quantile Function import math def normdist(x,mu,sigma): argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist mu = flow1.mean() # Fitted Model sigma = flow1.std() x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(flow1) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Fitting Data to Normal Data Model # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, flow1 ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of Flow\") mytitle = \"Normal Distribution Data Model sample mean = : \" + str(mu)+ \" sample variance =:\" + str(sigma**2) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Normal Distribution Data Model Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) from scipy.optimize import newton myguess = 1200 def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.25 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) ############################# def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.50 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) ############################# def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.75 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) ############################# def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.90 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) ############################# def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.998 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) ############################# def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.999 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) 1128.5828928044123 0.2499999999999999 11197.800000000001 0.5 21267.01710719559 0.7500000000000001 30329.626604901816 0.9 54164.850888691755 0.998 57330.776807175724 0.9990000000000001 Exceedence Probability Flow Value Remarks 25% 1128.58 75% chance of greater value 50% 11197.80 50% chance of greater value 75% 30329.63 25% chance of greater value 90% 54164.85 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% 54164.85 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% 57330.78 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # Log-Normal Quantile Function def loggit(x): # A prototype function to log transform x return(math.log(x)) flow2 = df['flow'].apply(loggit).tolist() # put the peaks into a list flow2_mean = np.array(flow2).mean() flow2_variance = np.array(flow2).std()**2 flow2.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(flow2),1): weibull_pp.append((i+1)/(len(flow2)+1)) ################ mu = flow2_mean # Fitted Model in Log Space sigma = math.sqrt(flow2_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(flow2) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Fitting Data to Log-Normal Data Model # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, flow2 ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"log of Flow\") mytitle = \"Log Normal Data Model log sample mean = : \" + str(flow2_mean)+ \" log sample variance =:\" + str(flow2_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) flow3 = df['flow'].tolist() # pull original list flow3.sort() # sort in place ################ mu = flow2_mean # Fitted Model in Log Space sigma = math.sqrt(flow2_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(flow2) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(antiloggit(xlow + i*xstep)) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, flow3 ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of Flow\") mytitle = \"Log Normal Data Model sample log mean = : \" + str((flow2_mean))+ \" sample log variance =:\" + str((flow2_variance)) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Log-Normal Distribution Data Model Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) from scipy.optimize import newton myguess = 4000 def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.25 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.50 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.75 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.90 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.99 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.998 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.999 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## 2930.9043006567786 0.2500000000000001 6214.9579846906345 0.5 13178.766274563899 0.7500000000000001 25922.5572160778 0.9000000000000002 83048.86984807049 0.99 153602.4541787375 0.998 194551.74365124086 0.9990000000000001 # Gumbell EV1 Quantile Function def ev1dist(x,alpha,beta): argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist flow4 = df['flow'].tolist() # put the peaks into a list flow4_mean = np.array(flow4).mean() flow4_variance = np.array(flow4).std()**2 alpha_mom = flow4_mean*math.sqrt(6)/math.pi beta_mom = math.sqrt(flow4_variance)*0.45 flow4.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(flow4),1): weibull_pp.append((i+1)/(len(flow4)+1)) ################ mu = flow4_mean # Fitted Model sigma = math.sqrt(flow4_variance) x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(flow4) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = ev1dist(xlow + i*xstep,alpha_mom,beta_mom) ycdf.append(yvalue) # Fitting Data to Gumbell EV1 Data Model # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, flow4 ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of Flow\") mytitle = \"Extreme Value Type 1 Distribution Data Model sample mean = : \" + str(flow4_mean)+ \" sample variance =:\" + str(flow4_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Gumbell Double Exponential (EV1) Distribution Data Model Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) from scipy.optimize import newton myguess= 4000 def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.25 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.50 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.75 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.90 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.99 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.998 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.999 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## 6536.595933014118 0.25 11193.082189211951 0.5 17100.702987342494 0.7499999999999999 23848.60817221199 0.9 39634.18362687675 0.99 50473.21664381971 0.998 55133.066049399946 0.999 # Gamma (Pearson Type III) Quantile Function import scipy.stats # import scipy stats package import math # import math package import numpy # import numpy package # log and antilog def loggit(x): # A prototype function to log transform x return(math.log(x)) def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) def weibull_pp(sample): # plotting position function # returns a list of plotting positions; sample must be a numeric list weibull_pp = [] # null list to return after fill sample.sort() # sort the sample list in place for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) return weibull_pp def gammacdf(x,tau,alpha,beta): # Gamma Cumulative Density function - with three parameter to one parameter convert xhat = x-tau lamda = 1.0/beta gammacdf = scipy.stats.gamma.cdf(lamda*xhat, alpha) return gammacdf flow5 = df['flow'].tolist() # put the log peaks into a list flow5_mean = np.array(flow5).mean() flow5_stdev = np.array(flow5).std() flow5_skew = scipy.stats.skew(flow5) flow5_alpha = 4.0/(flow5_skew**2) flow5_beta = np.sign(flow5_skew)*math.sqrt(flow5_stdev**2/flow5_alpha) flow5_tau = flow5_mean - flow5_alpha*flow5_beta # plotting = weibull_pp(flow5) # x = []; ycdf = [] xlow = (0.9*min(flow5)); xhigh = (1.1*max(flow5)) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = gammacdf(xlow + i*xstep,flow5_tau,flow5_alpha,flow5_beta) ycdf.append(yvalue) # Fitting Data to Pearson (Gamma) III Data Model # This is new, in lecture the fit was to log-Pearson, same procedure, but not log transformed myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(plotting, flow5 ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of Flow\") mytitle = \"Pearson (Gamma) Type III Distribution Data Model\\n \" mytitle += \"Mean = \" + str(flow5_mean) + \"\\n\" mytitle += \"SD = \" + str(flow5_stdev) + \"\\n\" mytitle += \"Skew = \" + str(flow5_skew) + \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Pearson III Distribution Data Model Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) print(flow5_tau) print(flow5_alpha) print(flow5_beta) 1356.6347332854966 0.43456260236933913 22646.13938948754 from scipy.optimize import newton myguess = 4000 def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.25 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.50 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.75 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.90 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.99 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.998 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.999 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### 2077.9070960392946 0.25 5267.02594598091 0.5 14029.404138740409 0.75 28734.914752838442 0.9 71869.9637302461 0.99 104294.440621576 0.998 118564.83951972787 0.999 # Fitting Data to Log-Pearson (Log-Gamma) III Data Model flow6 = df['flow'].apply(loggit).tolist() # put the log peaks into a list flow6_mean = np.array(flow6).mean() flow6_stdev = np.array(flow6).std() flow6_skew = scipy.stats.skew(flow6) flow6_alpha = 4.0/(flow6_skew**2) flow6_beta = np.sign(flow6_skew)*math.sqrt(flow6_stdev**2/flow6_alpha) flow6_tau = flow6_mean - flow6_alpha*flow6_beta # plotting = weibull_pp(flow6) # x = []; ycdf = [] xlow = (0.9*min(flow6)); xhigh = (1.1*max(flow6)) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = gammacdf(xlow + i*xstep,flow6_tau,flow6_alpha,flow6_beta) ycdf.append(yvalue) # reverse transform the peaks, and the data model peaks for i in range(len(flow6)): flow6[i] = antiloggit(flow6[i]) for i in range(len(x)): x[i] = antiloggit(x[i]) rycdf = ycdf[::-1] myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(plotting, flow6 ,color ='blue') matplotlib.pyplot.plot(rycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Pearson Type III Distribution Data Model\\n \" mytitle += \"Mean = \" + str(antiloggit(flow6_mean)) + \"\\n\" mytitle += \"SD = \" + str(antiloggit(flow6_stdev)) + \"\\n\" mytitle += \"Skew = \" + str(antiloggit(flow6_skew)) + \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Here are some great reads on this topic: - \"Introduction to Probability distributions\" by Megha Singhal available at https://medium.datadriveninvestor.com/introduction-to-probability-distributions-56b5253344b8 - \"A Gentle Introduction to Probability Distributions\" by Jason Brownlee available at https://machinelearningmastery.com/what-are-probability-distributions/ - \"Python Probability Distributions \u2013 Normal, Binomial, Poisson, Bernoulli\" available at https://data-flair.training/blogs/python-probability-distributions/ Here are some great videos on these topics: - \"Python for Data Analysis: Probability Distributions\" by DataDaft available at https://www.youtube.com/watch?v=uial-2girHQ - \"Python Tutorial: Probability distributions\" by DataCamp available at https://www.youtube.com/watch?v=HR59jIi5tFE - \"Statistics basics. Working with probability distributions in SciPy\" by Hidrolog\u00edaUC available at https://www.youtube.com/watch?v=biuz0yS8Z5Y Exercise: Extreme Value Distributions Why do we use EV distributions? What are some of the applications? Make sure to cite any resources that you may use.","title":"Lab16 Dev"},{"location":"8-Labs/Lab16/Lab16_Dev/#laboratory-16-functions-for-probability-modeling","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 16: \"Functions for Probability Modeling\""},{"location":"8-Labs/Lab16/Lab16_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab16/Lab16_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab16/Lab16_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab16/Lab16_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab16/Lab16_Dev/#important-terminology","text":"Population: In statistics, a population is the entire pool from which a statistical sample is drawn. A population may refer to an entire group of people, objects, events, hospital visits, or measurements. Sample: In statistics and quantitative research methodology, a sample is a set of individuals or objects collected or selected from a statistical population by a defined procedure. The elements of a sample are known as sample points, sampling units or observations. Distribution (Data Model): A data distribution is a special function or a listing which shows all the possible values (or intervals) of the data. It also (and this is important) tells you how often each value occurs. From https://www.investopedia.com/terms https://www.statisticshowto.com/data-distribution/","title":"Important Terminology:"},{"location":"8-Labs/Lab16/Lab16_Dev/#important-steps-in-going-from-samples-to-population","text":"Get descriptive statistics- mean, variance, std. dev. Use plotting position formulas (e.g., weibull, gringorten, cunnane) and plot the SAMPLES (data you already have) Use different data models (e.g., normal, log-normal, Gumbell) and find the one that better FITs your samples- Visual or Numerical Use the data model that provides the best fit to infer about the POPULATION","title":"Important Steps in going from samples to population:"},{"location":"8-Labs/Lab16/Lab16_Dev/#estimate-the-magnitude-of-the-annual-peak-flow-at-spring-ck-near-spring-tx","text":"The file 08068500.pkf is an actual WATSTORE formatted file for a USGS gage at Spring Creek, Texas. The first few lines of the file look like: Z08068500 USGS H08068500 3006370952610004848339SW12040102409 409 72.6 N08068500 Spring Ck nr Spring, TX Y08068500 308068500 19290530 483007 34.30 1879 308068500 19390603 838 13.75 308068500 19400612 3420 21.42 308068500 19401125 42700 33.60 308068500 19420409 14200 27.78 308068500 19430730 8000 25.09 308068500 19440319 5260 23.15 308068500 19450830 31100 32.79 308068500 19460521 12200 27.97 The first column are some agency codes that identify the station , the second column after the fourth row is a date in YYYYMMDD format, the third column is a discharge in CFS, the fourth and fifth column are not relevant for this laboratory exercise. The file was downloadef from https://nwis.waterdata.usgs.gov/tx/nwis/peak?site_no=08068500&agency_cd=USGS&format=hn2 In the original file there are a couple of codes that are manually removed: 19290530 483007; the trailing 7 is a code identifying a break in the series (non-sequential) 20170828 784009; the trailing 9 identifies the historical peak The laboratory task is to fit the data models to this data, decide the best model from visual perspective, and report from that data model the magnitudes of peak flow associated with the probebilitiess below (i.e. populate the table) Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) The first step is to read the file, skipping the first part, then build a dataframe: # Read the data file amatrix = [] # null list to store matrix reads rowNumA = 0 matrix1=[] col0=[] col1=[] col2=[] with open('08068500.pkf','r') as afile: lines_after_4 = afile.readlines()[4:] afile.close() # Disconnect the file howmanyrows = len(lines_after_4) for i in range(howmanyrows): matrix1.append(lines_after_4[i].strip().split()) for i in range(howmanyrows): col0.append(int(matrix1[i][0])) col1.append(matrix1[i][1]) col2.append(int(matrix1[i][2])) #We need to conver it to integers # col2 is date, col3 is peak flow #now build a datafranem import pandas df = pandas.DataFrame(col0) df['date']= col1 df['flow']= col2 df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 date flow 0 308068500 19290530 48300 1 308068500 19390603 838 2 308068500 19400612 3420 3 308068500 19401125 42700 4 308068500 19420409 14200 Now explore if you can plot the dataframe as a plot of peaks versus date. import matplotlib.pyplot # the python plotting library import numpy as np date = df['date'] flow = df['flow'] # Plot here myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate an object from the figure class, set aspect ratio matplotlib.pyplot.plot(date,flow, color ='blue') [<matplotlib.lines.Line2D at 0x2289ccf1cc8>] From here on you can proceede using the lecture notebook as a go-by, although you should use functions as much as practical to keep your work concise # Descriptive Statistics flow.describe() count 80.000000 mean 11197.800000 std 15022.831582 min 381.000000 25% 3360.000000 50% 7190.000000 75% 11500.000000 max 78800.000000 Name: flow, dtype: float64 # Weibull Plotting Position Function # sort the flow in place! flow1 = np.array(flow) flow1.sort() # built a relative frequency approximation to probability, assume each pick is equally likely weibull_pp = [] for i in range(0,len(flow1),1): weibull_pp.append((i+1)/(len(flow1)+1)) What is Weibull function? The Weibull plotting position for the rth ranked (from largest to smallest) datum from a sample of size n is the quotient r / (n+1) It is recommended for use when the form of the underlying distribution is unknown and when unbiased exceedance probabilities are desired. From Glossary of Meteorology @ https://glossary.ametsoc.org/wiki/Weibull_plotting_position#:~:text=The%20Weibull%20plotting%20position%20for,unbiased%20exceedance%20probabilities%20are%20desired # Normal Quantile Function import math def normdist(x,mu,sigma): argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist mu = flow1.mean() # Fitted Model sigma = flow1.std() x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(flow1) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Fitting Data to Normal Data Model # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, flow1 ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of Flow\") mytitle = \"Normal Distribution Data Model sample mean = : \" + str(mu)+ \" sample variance =:\" + str(sigma**2) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show()","title":"Estimate the magnitude of the annual peak flow at Spring Ck near Spring, TX."},{"location":"8-Labs/Lab16/Lab16_Dev/#normal-distribution-data-model","text":"Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) from scipy.optimize import newton myguess = 1200 def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.25 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) ############################# def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.50 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) ############################# def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.75 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) ############################# def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.90 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) ############################# def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.998 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) ############################# def f(x): mu = flow1.mean() sigma = flow1.std() quantile = 0.999 argument = (x - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(newton(f, myguess),mu,sigma)) 1128.5828928044123 0.2499999999999999 11197.800000000001 0.5 21267.01710719559 0.7500000000000001 30329.626604901816 0.9 54164.850888691755 0.998 57330.776807175724 0.9990000000000001 Exceedence Probability Flow Value Remarks 25% 1128.58 75% chance of greater value 50% 11197.80 50% chance of greater value 75% 30329.63 25% chance of greater value 90% 54164.85 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% 54164.85 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% 57330.78 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # Log-Normal Quantile Function def loggit(x): # A prototype function to log transform x return(math.log(x)) flow2 = df['flow'].apply(loggit).tolist() # put the peaks into a list flow2_mean = np.array(flow2).mean() flow2_variance = np.array(flow2).std()**2 flow2.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(flow2),1): weibull_pp.append((i+1)/(len(flow2)+1)) ################ mu = flow2_mean # Fitted Model in Log Space sigma = math.sqrt(flow2_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(flow2) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Fitting Data to Log-Normal Data Model # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, flow2 ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"log of Flow\") mytitle = \"Log Normal Data Model log sample mean = : \" + str(flow2_mean)+ \" log sample variance =:\" + str(flow2_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) flow3 = df['flow'].tolist() # pull original list flow3.sort() # sort in place ################ mu = flow2_mean # Fitted Model in Log Space sigma = math.sqrt(flow2_variance) x = []; ycdf = [] xlow = 1; xhigh = 1.05*max(flow2) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(antiloggit(xlow + i*xstep)) yvalue = normdist(xlow + i*xstep,mu,sigma) ycdf.append(yvalue) # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,9)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, flow3 ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of Flow\") mytitle = \"Log Normal Data Model sample log mean = : \" + str((flow2_mean))+ \" sample log variance =:\" + str((flow2_variance)) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show()","title":"Normal Distribution Data Model"},{"location":"8-Labs/Lab16/Lab16_Dev/#log-normal-distribution-data-model","text":"Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) from scipy.optimize import newton myguess = 4000 def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.25 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.50 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.75 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.90 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.99 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.998 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## def f(x): mu = 8.734714243614741 sigma = 1.1143949244101454 quantile = 0.999 argument = (loggit(x) - mu)/(math.sqrt(2.0)*sigma) normdist = (1.0 + math.erf(argument))/2.0 return normdist - quantile print(newton(f, myguess)) print(normdist(loggit(newton(f, myguess)),mu,sigma)) ################################################## 2930.9043006567786 0.2500000000000001 6214.9579846906345 0.5 13178.766274563899 0.7500000000000001 25922.5572160778 0.9000000000000002 83048.86984807049 0.99 153602.4541787375 0.998 194551.74365124086 0.9990000000000001 # Gumbell EV1 Quantile Function def ev1dist(x,alpha,beta): argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist flow4 = df['flow'].tolist() # put the peaks into a list flow4_mean = np.array(flow4).mean() flow4_variance = np.array(flow4).std()**2 alpha_mom = flow4_mean*math.sqrt(6)/math.pi beta_mom = math.sqrt(flow4_variance)*0.45 flow4.sort() # sort the sample in place! weibull_pp = [] # built a relative frequency approximation to probability, assume each pick is equally likely for i in range(0,len(flow4),1): weibull_pp.append((i+1)/(len(flow4)+1)) ################ mu = flow4_mean # Fitted Model sigma = math.sqrt(flow4_variance) x = []; ycdf = [] xlow = 0; xhigh = 1.2*max(flow4) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = ev1dist(xlow + i*xstep,alpha_mom,beta_mom) ycdf.append(yvalue) # Fitting Data to Gumbell EV1 Data Model # Now plot the sample values and plotting position myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(weibull_pp, flow4 ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of Flow\") mytitle = \"Extreme Value Type 1 Distribution Data Model sample mean = : \" + str(flow4_mean)+ \" sample variance =:\" + str(flow4_variance) matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show()","title":"Log-Normal Distribution Data Model"},{"location":"8-Labs/Lab16/Lab16_Dev/#gumbell-double-exponential-ev1-distribution-data-model","text":"Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) from scipy.optimize import newton myguess= 4000 def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.25 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.50 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.75 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.90 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.99 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.998 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## def f(x): alpha = 8730.888840854457 beta = 6717.889629784229 quantile = 0.999 argument = (x - alpha)/beta constant = 1.0/beta ev1dist = math.exp(-1.0*math.exp(-1.0*argument)) return ev1dist - quantile print(newton(f, myguess)) print(ev1dist(newton(f, myguess),alpha_mom,beta_mom)) ############################################################## 6536.595933014118 0.25 11193.082189211951 0.5 17100.702987342494 0.7499999999999999 23848.60817221199 0.9 39634.18362687675 0.99 50473.21664381971 0.998 55133.066049399946 0.999 # Gamma (Pearson Type III) Quantile Function import scipy.stats # import scipy stats package import math # import math package import numpy # import numpy package # log and antilog def loggit(x): # A prototype function to log transform x return(math.log(x)) def antiloggit(x): # A prototype function to log transform x return(math.exp(x)) def weibull_pp(sample): # plotting position function # returns a list of plotting positions; sample must be a numeric list weibull_pp = [] # null list to return after fill sample.sort() # sort the sample list in place for i in range(0,len(sample),1): weibull_pp.append((i+1)/(len(sample)+1)) return weibull_pp def gammacdf(x,tau,alpha,beta): # Gamma Cumulative Density function - with three parameter to one parameter convert xhat = x-tau lamda = 1.0/beta gammacdf = scipy.stats.gamma.cdf(lamda*xhat, alpha) return gammacdf flow5 = df['flow'].tolist() # put the log peaks into a list flow5_mean = np.array(flow5).mean() flow5_stdev = np.array(flow5).std() flow5_skew = scipy.stats.skew(flow5) flow5_alpha = 4.0/(flow5_skew**2) flow5_beta = np.sign(flow5_skew)*math.sqrt(flow5_stdev**2/flow5_alpha) flow5_tau = flow5_mean - flow5_alpha*flow5_beta # plotting = weibull_pp(flow5) # x = []; ycdf = [] xlow = (0.9*min(flow5)); xhigh = (1.1*max(flow5)) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = gammacdf(xlow + i*xstep,flow5_tau,flow5_alpha,flow5_beta) ycdf.append(yvalue) # Fitting Data to Pearson (Gamma) III Data Model # This is new, in lecture the fit was to log-Pearson, same procedure, but not log transformed myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(plotting, flow5 ,color ='blue') matplotlib.pyplot.plot(ycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of Flow\") mytitle = \"Pearson (Gamma) Type III Distribution Data Model\\n \" mytitle += \"Mean = \" + str(flow5_mean) + \"\\n\" mytitle += \"SD = \" + str(flow5_stdev) + \"\\n\" mytitle += \"Skew = \" + str(flow5_skew) + \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show()","title":"Gumbell Double Exponential (EV1) Distribution Data Model"},{"location":"8-Labs/Lab16/Lab16_Dev/#pearson-iii-distribution-data-model","text":"Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) print(flow5_tau) print(flow5_alpha) print(flow5_beta) 1356.6347332854966 0.43456260236933913 22646.13938948754 from scipy.optimize import newton myguess = 4000 def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.25 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.50 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.75 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.90 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.99 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.998 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### def f(x): sample_tau = 1356.6347332854966 sample_alpha = 0.43456260236933913 sample_beta = 22646.13938948754 quantile = 0.999 #argument = loggit(x) gammavalue = gammacdf(x,sample_tau,sample_alpha,sample_beta) return gammavalue - quantile print(newton(f, myguess)) print(round(gammacdf(newton(f, myguess),flow5_tau,flow5_alpha,flow5_beta),4)) ##################################################################################### 2077.9070960392946 0.25 5267.02594598091 0.5 14029.404138740409 0.75 28734.914752838442 0.9 71869.9637302461 0.99 104294.440621576 0.998 118564.83951972787 0.999 # Fitting Data to Log-Pearson (Log-Gamma) III Data Model flow6 = df['flow'].apply(loggit).tolist() # put the log peaks into a list flow6_mean = np.array(flow6).mean() flow6_stdev = np.array(flow6).std() flow6_skew = scipy.stats.skew(flow6) flow6_alpha = 4.0/(flow6_skew**2) flow6_beta = np.sign(flow6_skew)*math.sqrt(flow6_stdev**2/flow6_alpha) flow6_tau = flow6_mean - flow6_alpha*flow6_beta # plotting = weibull_pp(flow6) # x = []; ycdf = [] xlow = (0.9*min(flow6)); xhigh = (1.1*max(flow6)) ; howMany = 100 xstep = (xhigh - xlow)/howMany for i in range(0,howMany+1,1): x.append(xlow + i*xstep) yvalue = gammacdf(xlow + i*xstep,flow6_tau,flow6_alpha,flow6_beta) ycdf.append(yvalue) # reverse transform the peaks, and the data model peaks for i in range(len(flow6)): flow6[i] = antiloggit(flow6[i]) for i in range(len(x)): x[i] = antiloggit(x[i]) rycdf = ycdf[::-1] myfigure = matplotlib.pyplot.figure(figsize = (7,8)) # generate a object from the figure class, set aspect ratio matplotlib.pyplot.scatter(plotting, flow6 ,color ='blue') matplotlib.pyplot.plot(rycdf, x, color ='red') matplotlib.pyplot.xlabel(\"Quantile Value\") matplotlib.pyplot.ylabel(\"Value of RV\") mytitle = \"Log Pearson Type III Distribution Data Model\\n \" mytitle += \"Mean = \" + str(antiloggit(flow6_mean)) + \"\\n\" mytitle += \"SD = \" + str(antiloggit(flow6_stdev)) + \"\\n\" mytitle += \"Skew = \" + str(antiloggit(flow6_skew)) + \"\\n\" matplotlib.pyplot.title(mytitle) matplotlib.pyplot.show() Here are some great reads on this topic: - \"Introduction to Probability distributions\" by Megha Singhal available at https://medium.datadriveninvestor.com/introduction-to-probability-distributions-56b5253344b8 - \"A Gentle Introduction to Probability Distributions\" by Jason Brownlee available at https://machinelearningmastery.com/what-are-probability-distributions/ - \"Python Probability Distributions \u2013 Normal, Binomial, Poisson, Bernoulli\" available at https://data-flair.training/blogs/python-probability-distributions/ Here are some great videos on these topics: - \"Python for Data Analysis: Probability Distributions\" by DataDaft available at https://www.youtube.com/watch?v=uial-2girHQ - \"Python Tutorial: Probability distributions\" by DataCamp available at https://www.youtube.com/watch?v=HR59jIi5tFE - \"Statistics basics. Working with probability distributions in SciPy\" by Hidrolog\u00edaUC available at https://www.youtube.com/watch?v=biuz0yS8Z5Y","title":"Pearson III Distribution Data Model"},{"location":"8-Labs/Lab16/Lab16_Dev/#exercise-extreme-value-distributions","text":"","title":"Exercise: Extreme Value Distributions  "},{"location":"8-Labs/Lab16/Lab16_Dev/#why-do-we-use-ev-distributions-what-are-some-of-the-applications","text":"","title":"Why do we use EV distributions? What are some of the applications?"},{"location":"8-Labs/Lab16/Lab16_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab17/Lab17_Dev/","text":"Laboratory 17 Data Modeling using Probability Functions Full name: R#: HEX: Title of the notebook Date: Important Terminology: Population: In statistics, a population is the entire pool from which a statistical sample is drawn. A population may refer to an entire group of people, objects, events, hospital visits, or measurements. Sample: In statistics and quantitative research methodology, a sample is a set of individuals or objects collected or selected from a statistical population by a defined procedure. The elements of a sample are known as sample points, sampling units or observations. Distribution (Data Model): A data distribution is a function or a listing which shows all the possible values (or intervals) of the data. It also (and this is important) tells you how often each value occurs. From https://www.investopedia.com/terms https://www.statisticshowto.com/data-distribution/ ### Important Steps: 1. __Get descriptive statistics- mean, variance, std. dev.__ 2. __Use plotting position formulas (e.g., weibull, gringorten, cunnane) and plot the SAMPLES (data you already have)__ 3. __Use different data models (e.g., normal, log-normal, Gumbell) and find the one that better FITs your samples- Visual or Numerical__ 4. __Use the data model that provides the best fit to infer about the POPULATION__ Estimate the magnitude of the annual peak flow at Spring Ck near Spring, TX. The file 08068500.pkf is an actual WATSTORE formatted file for a USGS gage at Spring Creek, Texas. The first few lines of the file look like: Z08068500 USGS H08068500 3006370952610004848339SW12040102409 409 72.6 N08068500 Spring Ck nr Spring, TX Y08068500 308068500 19290530 483007 34.30 1879 308068500 19390603 838 13.75 308068500 19400612 3420 21.42 308068500 19401125 42700 33.60 308068500 19420409 14200 27.78 308068500 19430730 8000 25.09 308068500 19440319 5260 23.15 308068500 19450830 31100 32.79 308068500 19460521 12200 27.97 The first column are some agency codes that identify the station , the second column after the fourth row is a date in YYYYMMDD format, the third column is a discharge in CFS, the fourth and fifth column are not relevant for this laboratory exercise. The file was downloadef from https://nwis.waterdata.usgs.gov/tx/nwis/peak?site_no=08068500&agency_cd=USGS&format=hn2 In the original file there are a couple of codes that are manually removed: 19290530 483007; the trailing 7 is a code identifying a break in the series (non-sequential) 20170828 784009; the trailing 9 identifies the historical peak The laboratory task is to fit the data models to this data, decide the best model from visual perspective, and report from that data model the magnitudes of peak flow associated with the probebilitiess below (i.e. populate the table) Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) The first step is to read the file, skipping the first part, then build a dataframe: # Read the data file amatrix = [] # null list to store matrix reads rowNumA = 0 matrix1=[] col0=[] col1=[] col2=[] with open('08068500.pkf','r') as afile: lines_after_4 = afile.readlines()[4:] afile.close() # Disconnect the file howmanyrows = len(lines_after_4) for i in range(howmanyrows): matrix1.append(lines_after_4[i].strip().split()) for i in range(howmanyrows): col0.append(matrix1[i][0]) col1.append(matrix1[i][1]) col2.append(matrix1[i][2]) # col2 is date, col3 is peak flow #now build a datafranem import pandas df = pandas.DataFrame(col0) df['date']= col1 df['flow']= col2 df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 date flow 0 308068500 19290530 48300 1 308068500 19390603 838 2 308068500 19400612 3420 3 308068500 19401125 42700 4 308068500 19420409 14200 Now explore if you can plot the dataframe as a plot of peaks versus date. # Plot here From here on you can proceede using the lecture notebook as a go-by, although you should use functions as much as practical to keep your work concise # Descriptive Statistics # Weibull Plotting Position Function # Normal Quantile Function # Fitting Data to Normal Data Model Normal Distribution Data Model Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # Log-Normal Quantile Function # Fitting Data to Normal Data Model Log-Normal Distribution Data Model Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # Gumbell EV1 Quantile Function # Fitting Data to Gumbell EV1 Data Model Gumbell Double Exponential (EV1) Distribution Data Model Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # Gamma (Pearson Type III) Quantile Function # Fitting Data to Pearson (Gamma) III Data Model # This is new, in lecture the fit was to log-Pearson, same procedure, but not log transformed Pearson III Distribution Data Model Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # Fitting Data to Log-Pearson (Log-Gamma) III Data Model Log-Pearson III Distribution Data Model Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) Summary of \"Best\" Data Model based on Graphical Fit","title":"Laboratory 17 Data Modeling using Probability Functions"},{"location":"8-Labs/Lab17/Lab17_Dev/#laboratory-17-data-modeling-using-probability-functions","text":"","title":"Laboratory 17 Data Modeling using Probability Functions"},{"location":"8-Labs/Lab17/Lab17_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab17/Lab17_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab17/Lab17_Dev/#hex","text":"","title":"HEX:"},{"location":"8-Labs/Lab17/Lab17_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook"},{"location":"8-Labs/Lab17/Lab17_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab17/Lab17_Dev/#important-terminology","text":"Population: In statistics, a population is the entire pool from which a statistical sample is drawn. A population may refer to an entire group of people, objects, events, hospital visits, or measurements. Sample: In statistics and quantitative research methodology, a sample is a set of individuals or objects collected or selected from a statistical population by a defined procedure. The elements of a sample are known as sample points, sampling units or observations. Distribution (Data Model): A data distribution is a function or a listing which shows all the possible values (or intervals) of the data. It also (and this is important) tells you how often each value occurs. From https://www.investopedia.com/terms https://www.statisticshowto.com/data-distribution/ ### Important Steps: 1. __Get descriptive statistics- mean, variance, std. dev.__ 2. __Use plotting position formulas (e.g., weibull, gringorten, cunnane) and plot the SAMPLES (data you already have)__ 3. __Use different data models (e.g., normal, log-normal, Gumbell) and find the one that better FITs your samples- Visual or Numerical__ 4. __Use the data model that provides the best fit to infer about the POPULATION__","title":"Important Terminology:"},{"location":"8-Labs/Lab17/Lab17_Dev/#estimate-the-magnitude-of-the-annual-peak-flow-at-spring-ck-near-spring-tx","text":"The file 08068500.pkf is an actual WATSTORE formatted file for a USGS gage at Spring Creek, Texas. The first few lines of the file look like: Z08068500 USGS H08068500 3006370952610004848339SW12040102409 409 72.6 N08068500 Spring Ck nr Spring, TX Y08068500 308068500 19290530 483007 34.30 1879 308068500 19390603 838 13.75 308068500 19400612 3420 21.42 308068500 19401125 42700 33.60 308068500 19420409 14200 27.78 308068500 19430730 8000 25.09 308068500 19440319 5260 23.15 308068500 19450830 31100 32.79 308068500 19460521 12200 27.97 The first column are some agency codes that identify the station , the second column after the fourth row is a date in YYYYMMDD format, the third column is a discharge in CFS, the fourth and fifth column are not relevant for this laboratory exercise. The file was downloadef from https://nwis.waterdata.usgs.gov/tx/nwis/peak?site_no=08068500&agency_cd=USGS&format=hn2 In the original file there are a couple of codes that are manually removed: 19290530 483007; the trailing 7 is a code identifying a break in the series (non-sequential) 20170828 784009; the trailing 9 identifies the historical peak The laboratory task is to fit the data models to this data, decide the best model from visual perspective, and report from that data model the magnitudes of peak flow associated with the probebilitiess below (i.e. populate the table) Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) The first step is to read the file, skipping the first part, then build a dataframe: # Read the data file amatrix = [] # null list to store matrix reads rowNumA = 0 matrix1=[] col0=[] col1=[] col2=[] with open('08068500.pkf','r') as afile: lines_after_4 = afile.readlines()[4:] afile.close() # Disconnect the file howmanyrows = len(lines_after_4) for i in range(howmanyrows): matrix1.append(lines_after_4[i].strip().split()) for i in range(howmanyrows): col0.append(matrix1[i][0]) col1.append(matrix1[i][1]) col2.append(matrix1[i][2]) # col2 is date, col3 is peak flow #now build a datafranem import pandas df = pandas.DataFrame(col0) df['date']= col1 df['flow']= col2 df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 date flow 0 308068500 19290530 48300 1 308068500 19390603 838 2 308068500 19400612 3420 3 308068500 19401125 42700 4 308068500 19420409 14200 Now explore if you can plot the dataframe as a plot of peaks versus date. # Plot here From here on you can proceede using the lecture notebook as a go-by, although you should use functions as much as practical to keep your work concise # Descriptive Statistics # Weibull Plotting Position Function # Normal Quantile Function # Fitting Data to Normal Data Model","title":"Estimate the magnitude of the annual peak flow at Spring Ck near Spring, TX."},{"location":"8-Labs/Lab17/Lab17_Dev/#normal-distribution-data-model","text":"Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # Log-Normal Quantile Function # Fitting Data to Normal Data Model","title":"Normal Distribution Data Model"},{"location":"8-Labs/Lab17/Lab17_Dev/#log-normal-distribution-data-model","text":"Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # Gumbell EV1 Quantile Function # Fitting Data to Gumbell EV1 Data Model","title":"Log-Normal Distribution Data Model"},{"location":"8-Labs/Lab17/Lab17_Dev/#gumbell-double-exponential-ev1-distribution-data-model","text":"Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # Gamma (Pearson Type III) Quantile Function # Fitting Data to Pearson (Gamma) III Data Model # This is new, in lecture the fit was to log-Pearson, same procedure, but not log transformed","title":"Gumbell Double Exponential (EV1) Distribution Data Model"},{"location":"8-Labs/Lab17/Lab17_Dev/#pearson-iii-distribution-data-model","text":"Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event) # Fitting Data to Log-Pearson (Log-Gamma) III Data Model","title":"Pearson III Distribution Data Model"},{"location":"8-Labs/Lab17/Lab17_Dev/#log-pearson-iii-distribution-data-model","text":"Exceedence Probability Flow Value Remarks 25% ???? 75% chance of greater value 50% ???? 50% chance of greater value 75% ???? 25% chance of greater value 90% ???? 10% chance of greater value 99% ???? 1% chance of greater value (in flood statistics, this is the 1 in 100-yr chance event) 99.8% ???? 0.002% chance of greater value (in flood statistics, this is the 1 in 500-yr chance event) 99.9% ???? 0.001% chance of greater value (in flood statistics, this is the 1 in 1000-yr chance event)","title":"Log-Pearson III Distribution Data Model"},{"location":"8-Labs/Lab17/Lab17_Dev/#summary-of-best-data-model-based-on-graphical-fit","text":"","title":"Summary of \"Best\" Data Model based on Graphical Fit"},{"location":"8-Labs/Lab2/Lab2_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab2 Laboratory 2: Structures and Conditions. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Data Structures: List (Array) A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\ x_1= 11 \\ x_2= 5 \\ x_3= 9 \\ x_4= 13 \\ ... \\ x_N= 223 \\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO. Alot of other lnguages start at ONE. Its just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9. MyOtherList = [] #Create an empty list MyOtherList.append(765) #Add one item to the list print(MyOtherList) MyList = [7,11,5,9,13,66,99,223] #Define a list print(MyList) sublist = MyList[3:6] #slice a sublist print(\"sublist is: \", sublist) mysum = sum(sublist) #sum the numbers in the sublist print(\"Sum: \", mysum) mylength = len(sublist) #get the length of the sublist print(\"Length: \", mylength) [765] [7, 11, 5, 9, 13, 66, 99, 223] sublist is: [9, 13, 66] Sum: 88 Length: 3 Data Structures: Special List | Tuple A tuple is a special kind of list where the values cannot be changed after the list is created. It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") Data Structures: Special List | Dictionary A dictionary is a special kind of list where the items are related data PAIRS. It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Some examples follow: MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") MyTupleName ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec') MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} print(MyPetsNamesAndMass) MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print(MyPetsNamesAndMassToo) {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} # Tuples MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") # Access a Tuple print (\"5th element of the tuple:\", MyTupleName[4]) # Dictionary MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} # Access the Dictionary print (\"Aspen's mass = \", MyPetsNamesAndMass[\"Aspen\"]) # Change a value in a dictionary print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"]) MyPetsNamesAndMass[\"Merrimee\"] = 0.01 print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"], \"She lost weight !\") # Alternate dictionary MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print (\"Merrimee's mass\" , MyPetsNamesAndMassToo[\"Merrimee\"]) # Attempt to change a Tuple #MyTupleName[3]=(\"Fred\") # Activate this line and see what happens! 5th element of the tuple: May Aspen's mass = 6.3 Merrimee's mass 0.03 Merrimee's mass 0.01 She lost weight ! Merrimee's mass 0.03 Example: Nested Dictionary From the dictionary below, print \"Pandemic\" and \"Tokyo\": FD = {\"Quentin\":\"Tarantino\",\"2020\":[2020,\"COVID\",19,\"Pandemic\"],\"Bond\":[\"James\",\"Gun\",(\"Paris\",\"Tokyo\",\"London\")]} #A nested dictionary print(FD) {'Quentin': 'Tarantino', '2020': [2020, 'COVID', 19, 'Pandemic'], 'Bond': ['James', 'Gun', ('Paris', 'Tokyo', 'London')]} FD['2020'][3] 'Pandemic' FD['Bond'][2][1] 'Tokyo' Conditional Execution Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, butmore recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs. Conditional Execution: Comparison The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal. # Compare x = 7 y = 10 print(\"x =: \",x,\"y =: \",y) print(\"x is equal to y : \",x==y) print(\"x is not equal to y : \",x!=y) print(\"x is greater than y : \",x>y) print(\"x is less than y : \",x<y) x =: 7 y =: 10 x is equal to y : False x is not equal to y : True x is greater than y : False x is less than y : True # Logical operators print(\"5 == 5 and 5 < 6 ? \",5 == 5 and 5 < 6) print(\"4 > 3 or 17 > 20 \",4 > 3 or 17 > 20) print(\"not 5 == 5\",not 5 == 5) 5 == 5 and 5 < 6 ? True 4 > 3 or 17 > 20 True not 5 == 5 False Conditional Execution: Block if statement The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter. The next code fragment illustrates illustrates how the if statements work. The program asks the user for input. The use of raw_input() will let the program read any input as a string so non-numeric results will not throw an error. The input is stored in the variable named userInput . Next the statement if userInput == \"1\": compares the value of userInput with the string \"1\" . If the value in the variable is indeed \\1\", then the program will execute the block of code in the indentation after the colon. In this case it will execute print \"Hello World\" print \"How do you do? \" Alternatively, if the value of userInput is the string '2' , then the program will execute print \"Snakes on a plane \" For all other values the program will execute print \"You did not enter a valid number\" # Block if example userInput = input('Enter the number 1 or 2') # Use block if structure if userInput == '1': print(\"Hello World\") print(\"How do you do? \") elif userInput == '2': print(\"Snakes on a plane \") else: print(\"You did not enter a valid number\") Enter the number 1 or 21 Hello World How do you do? Conditional Execution: Inline if statement An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops. myInt = 0 num1 = 12 if (myInt == 0) else 13 num1 12 Example: Pass or Fail? Take the following inputs from the user: 1. Grade for Lesson 1 (from 0 to 5) 2. Grade for Lesson 2 (from 0 to 5) 3. Grade for Lesson 3 (from 0 to 5) Compute the average of the three grades. Use the result to decide whether the student will pass or fail. Lesson1 = int(input('Enter the grade for Lesson 1')) Lesson2 = int(input('Enter the grade for Lesson 2')) Lesson3 = int(input('Enter the grade for Lesson 3')) Average = int(Lesson1+Lesson2+Lesson3)/3 print('Average Course Grade:',Average) if Average >= 5: print(\"Passed\") else: print(\"Failed\") Enter the grade for Lesson 12 Enter the grade for Lesson 25 Enter the grade for Lesson 31 Average Course Grade: 2.6666666666666665 Failed Here are some great reads on this topic: - \"Common Python Data Structures (Guide)\" by Dan Bader available at https://realpython.com/python-data-structures/ - \"Data Structures You Need To Learn In Python\" by Akash available at https://www.edureka.co/blog/data-structures-in-python/ - \"Data Structures in Python\u2014 A Brief Introduction\" by Sowmya Krishnan available at https://towardsdatascience.com/data-structures-in-python-a-brief-introduction-b4135d7a9b7d - \"Everything you Should Know About Data Structures in Python\" by ANIRUDDHA BHANDARI available at https://www.analyticsvidhya.com/blog/2020/06/data-structures-python/ - \"Conditional Statements in Python\" by John Sturtz available at https://realpython.com/python-conditional-statements/ - \"Python If Statement explained with examples\" by CHAITANYA SINGH available at https://beginnersbook.com/2018/01/python-if-statement-example/ Here are some great videos on these topics: - \"Python: Data Structures - Lists, Tuples, Sets & Dictionaries tutorial\" by Joe James available at https://www.youtube.com/watch?v=R-HLU9Fl5ug&t=92s - \"Python Tutorial for Beginners 5: Dictionaries - Working with Key-Value Pairs\" by Corey Schafer available at https://www.youtube.com/watch?v=daefaLgNkw0 - \"How to Use If Else Statements in Python (Python Tutorial #2)\" by CS Dojo available at https://www.youtube.com/watch?v=AWek49wXGzI - \"Python If Statements | Python Tutorial #10\" by Amigoscode available at https://www.youtube.com/watch?v=wKQRmXR3jhc Exercise: Why dictionaries? Why do we need to use dictionaries in python? * Make sure to cite any resources that you may use.","title":"Lab2 Dev"},{"location":"8-Labs/Lab2/Lab2_Dev/#laboratory-2-structures-and-conditions","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 2: Structures and Conditions. "},{"location":"8-Labs/Lab2/Lab2_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab2/Lab2_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab2/Lab2_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab2/Lab2_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab2/Lab2_Dev/#data-structures-list-array","text":"A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\ x_1= 11 \\ x_2= 5 \\ x_3= 9 \\ x_4= 13 \\ ... \\ x_N= 223 \\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO. Alot of other lnguages start at ONE. Its just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9. MyOtherList = [] #Create an empty list MyOtherList.append(765) #Add one item to the list print(MyOtherList) MyList = [7,11,5,9,13,66,99,223] #Define a list print(MyList) sublist = MyList[3:6] #slice a sublist print(\"sublist is: \", sublist) mysum = sum(sublist) #sum the numbers in the sublist print(\"Sum: \", mysum) mylength = len(sublist) #get the length of the sublist print(\"Length: \", mylength) [765] [7, 11, 5, 9, 13, 66, 99, 223] sublist is: [9, 13, 66] Sum: 88 Length: 3","title":"Data Structures: List (Array)"},{"location":"8-Labs/Lab2/Lab2_Dev/#data-structures-special-list-tuple","text":"A tuple is a special kind of list where the values cannot be changed after the list is created. It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")","title":"Data Structures: Special List | Tuple"},{"location":"8-Labs/Lab2/Lab2_Dev/#data-structures-special-list-dictionary","text":"A dictionary is a special kind of list where the items are related data PAIRS. It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Some examples follow: MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") MyTupleName ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec') MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} print(MyPetsNamesAndMass) MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print(MyPetsNamesAndMassToo) {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} # Tuples MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") # Access a Tuple print (\"5th element of the tuple:\", MyTupleName[4]) # Dictionary MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} # Access the Dictionary print (\"Aspen's mass = \", MyPetsNamesAndMass[\"Aspen\"]) # Change a value in a dictionary print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"]) MyPetsNamesAndMass[\"Merrimee\"] = 0.01 print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"], \"She lost weight !\") # Alternate dictionary MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print (\"Merrimee's mass\" , MyPetsNamesAndMassToo[\"Merrimee\"]) # Attempt to change a Tuple #MyTupleName[3]=(\"Fred\") # Activate this line and see what happens! 5th element of the tuple: May Aspen's mass = 6.3 Merrimee's mass 0.03 Merrimee's mass 0.01 She lost weight ! Merrimee's mass 0.03","title":"Data Structures: Special List | Dictionary"},{"location":"8-Labs/Lab2/Lab2_Dev/#example-nested-dictionary","text":"From the dictionary below, print \"Pandemic\" and \"Tokyo\": FD = {\"Quentin\":\"Tarantino\",\"2020\":[2020,\"COVID\",19,\"Pandemic\"],\"Bond\":[\"James\",\"Gun\",(\"Paris\",\"Tokyo\",\"London\")]} #A nested dictionary print(FD) {'Quentin': 'Tarantino', '2020': [2020, 'COVID', 19, 'Pandemic'], 'Bond': ['James', 'Gun', ('Paris', 'Tokyo', 'London')]} FD['2020'][3] 'Pandemic' FD['Bond'][2][1] 'Tokyo'","title":"Example: Nested Dictionary"},{"location":"8-Labs/Lab2/Lab2_Dev/#conditional-execution","text":"Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, butmore recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs.","title":"Conditional Execution"},{"location":"8-Labs/Lab2/Lab2_Dev/#conditional-execution-comparison","text":"The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal. # Compare x = 7 y = 10 print(\"x =: \",x,\"y =: \",y) print(\"x is equal to y : \",x==y) print(\"x is not equal to y : \",x!=y) print(\"x is greater than y : \",x>y) print(\"x is less than y : \",x<y) x =: 7 y =: 10 x is equal to y : False x is not equal to y : True x is greater than y : False x is less than y : True # Logical operators print(\"5 == 5 and 5 < 6 ? \",5 == 5 and 5 < 6) print(\"4 > 3 or 17 > 20 \",4 > 3 or 17 > 20) print(\"not 5 == 5\",not 5 == 5) 5 == 5 and 5 < 6 ? True 4 > 3 or 17 > 20 True not 5 == 5 False","title":"Conditional Execution: Comparison"},{"location":"8-Labs/Lab2/Lab2_Dev/#conditional-execution-block-if-statement","text":"The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter. The next code fragment illustrates illustrates how the if statements work. The program asks the user for input. The use of raw_input() will let the program read any input as a string so non-numeric results will not throw an error. The input is stored in the variable named userInput . Next the statement if userInput == \"1\": compares the value of userInput with the string \"1\" . If the value in the variable is indeed \\1\", then the program will execute the block of code in the indentation after the colon. In this case it will execute print \"Hello World\" print \"How do you do? \" Alternatively, if the value of userInput is the string '2' , then the program will execute print \"Snakes on a plane \" For all other values the program will execute print \"You did not enter a valid number\" # Block if example userInput = input('Enter the number 1 or 2') # Use block if structure if userInput == '1': print(\"Hello World\") print(\"How do you do? \") elif userInput == '2': print(\"Snakes on a plane \") else: print(\"You did not enter a valid number\") Enter the number 1 or 21 Hello World How do you do?","title":"Conditional Execution:  Block if statement"},{"location":"8-Labs/Lab2/Lab2_Dev/#conditional-execution-inline-if-statement","text":"An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops. myInt = 0 num1 = 12 if (myInt == 0) else 13 num1 12","title":"Conditional Execution:  Inline if statement"},{"location":"8-Labs/Lab2/Lab2_Dev/#example-pass-or-fail","text":"Take the following inputs from the user: 1. Grade for Lesson 1 (from 0 to 5) 2. Grade for Lesson 2 (from 0 to 5) 3. Grade for Lesson 3 (from 0 to 5) Compute the average of the three grades. Use the result to decide whether the student will pass or fail. Lesson1 = int(input('Enter the grade for Lesson 1')) Lesson2 = int(input('Enter the grade for Lesson 2')) Lesson3 = int(input('Enter the grade for Lesson 3')) Average = int(Lesson1+Lesson2+Lesson3)/3 print('Average Course Grade:',Average) if Average >= 5: print(\"Passed\") else: print(\"Failed\") Enter the grade for Lesson 12 Enter the grade for Lesson 25 Enter the grade for Lesson 31 Average Course Grade: 2.6666666666666665 Failed Here are some great reads on this topic: - \"Common Python Data Structures (Guide)\" by Dan Bader available at https://realpython.com/python-data-structures/ - \"Data Structures You Need To Learn In Python\" by Akash available at https://www.edureka.co/blog/data-structures-in-python/ - \"Data Structures in Python\u2014 A Brief Introduction\" by Sowmya Krishnan available at https://towardsdatascience.com/data-structures-in-python-a-brief-introduction-b4135d7a9b7d - \"Everything you Should Know About Data Structures in Python\" by ANIRUDDHA BHANDARI available at https://www.analyticsvidhya.com/blog/2020/06/data-structures-python/ - \"Conditional Statements in Python\" by John Sturtz available at https://realpython.com/python-conditional-statements/ - \"Python If Statement explained with examples\" by CHAITANYA SINGH available at https://beginnersbook.com/2018/01/python-if-statement-example/ Here are some great videos on these topics: - \"Python: Data Structures - Lists, Tuples, Sets & Dictionaries tutorial\" by Joe James available at https://www.youtube.com/watch?v=R-HLU9Fl5ug&t=92s - \"Python Tutorial for Beginners 5: Dictionaries - Working with Key-Value Pairs\" by Corey Schafer available at https://www.youtube.com/watch?v=daefaLgNkw0 - \"How to Use If Else Statements in Python (Python Tutorial #2)\" by CS Dojo available at https://www.youtube.com/watch?v=AWek49wXGzI - \"Python If Statements | Python Tutorial #10\" by Amigoscode available at https://www.youtube.com/watch?v=wKQRmXR3jhc","title":"Example: Pass or Fail?"},{"location":"8-Labs/Lab2/Lab2_Dev/#exercise-why-dictionaries","text":"","title":"Exercise: Why dictionaries? "},{"location":"8-Labs/Lab2/Lab2_Dev/#why-do-we-need-to-use-dictionaries-in-python","text":"","title":"Why do we need to use dictionaries in python?"},{"location":"8-Labs/Lab2/Lab2_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab2/Lab3_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab3 Laboratory 3: Structures and Conditions. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Data Structures: List (Array) A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\ x_1= 11 \\ x_2= 5 \\ x_3= 9 \\ x_4= 13 \\ ... \\ x_N= 223 \\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO. Alot of other lnguages start at ONE. Its just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9. MyOtherList = [] #Create an empty list MyOtherList.append(765) #Add one item to the list print(MyOtherList) MyList = [7,11,5,9,13,66,99,223] #Define a list print(MyList) sublist = MyList[3:6] #slice a sublist print(\"sublist is: \", sublist) mysum = sum(sublist) #sum the numbers in the sublist print(\"Sum: \", mysum) mylength = len(sublist) #get the length of the sublist print(\"Length: \", mylength) [765] [7, 11, 5, 9, 13, 66, 99, 223] sublist is: [9, 13, 66] Sum: 88 Length: 3 Data Structures: Special List | Tuple A tuple is a special kind of list where the values cannot be changed after the list is created. It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") Data Structures: Special List | Dictionary A dictionary is a special kind of list where the items are related data PAIRS. It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Some examples follow: MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") MyTupleName ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec') MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} print(MyPetsNamesAndMass) MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print(MyPetsNamesAndMassToo) {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} # Tuples MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") # Access a Tuple print (\"5th element of the tuple:\", MyTupleName[4]) # Dictionary MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} # Access the Dictionary print (\"Aspen's mass = \", MyPetsNamesAndMass[\"Aspen\"]) # Change a value in a dictionary print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"]) MyPetsNamesAndMass[\"Merrimee\"] = 0.01 print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"], \"She lost weight !\") # Alternate dictionary MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print (\"Merrimee's mass\" , MyPetsNamesAndMassToo[\"Merrimee\"]) # Attempt to change a Tuple #MyTupleName[3]=(\"Fred\") # Activate this line and see what happens! 5th element of the tuple: May Aspen's mass = 6.3 Merrimee's mass 0.03 Merrimee's mass 0.01 She lost weight ! Merrimee's mass 0.03 Example: Nested Dictionary From the dictionary below, print \"Pandemic\" and \"Tokyo\": FD = {\"Quentin\":\"Tarantino\",\"2020\":[2020,\"COVID\",19,\"Pandemic\"],\"Bond\":[\"James\",\"Gun\",(\"Paris\",\"Tokyo\",\"London\")]} #A nested dictionary print(FD) {'Quentin': 'Tarantino', '2020': [2020, 'COVID', 19, 'Pandemic'], 'Bond': ['James', 'Gun', ('Paris', 'Tokyo', 'London')]} FD['2020'][3] 'Pandemic' FD['Bond'][2][1] 'Tokyo' Conditional Execution Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, butmore recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs. Conditional Execution: Comparison The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal. # Compare x = 7 y = 10 print(\"x =: \",x,\"y =: \",y) print(\"x is equal to y : \",x==y) print(\"x is not equal to y : \",x!=y) print(\"x is greater than y : \",x>y) print(\"x is less than y : \",x<y) x =: 7 y =: 10 x is equal to y : False x is not equal to y : True x is greater than y : False x is less than y : True # Logical operators print(\"5 == 5 and 5 < 6 ? \",5 == 5 and 5 < 6) print(\"4 > 3 or 17 > 20 \",4 > 3 or 17 > 20) print(\"not 5 == 5\",not 5 == 5) 5 == 5 and 5 < 6 ? True 4 > 3 or 17 > 20 True not 5 == 5 False Conditional Execution: Block if statement The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter. The next code fragment illustrates illustrates how the if statements work. The program asks the user for input. The use of raw_input() will let the program read any input as a string so non-numeric results will not throw an error. The input is stored in the variable named userInput . Next the statement if userInput == \"1\": compares the value of userInput with the string \"1\" . If the value in the variable is indeed \\1\", then the program will execute the block of code in the indentation after the colon. In this case it will execute print \"Hello World\" print \"How do you do? \" Alternatively, if the value of userInput is the string '2' , then the program will execute print \"Snakes on a plane \" For all other values the program will execute print \"You did not enter a valid number\" # Block if example userInput = input('Enter the number 1 or 2') # Use block if structure if userInput == '1': print(\"Hello World\") print(\"How do you do? \") elif userInput == '2': print(\"Snakes on a plane \") else: print(\"You did not enter a valid number\") Enter the number 1 or 21 Hello World How do you do? Conditional Execution: Inline if statement An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops. myInt = 0 num1 = 12 if (myInt == 0) else 13 num1 12 Example: Pass or Fail? Take the following inputs from the user: 1. Grade for Lesson 1 (from 0 to 5) 2. Grade for Lesson 2 (from 0 to 5) 3. Grade for Lesson 3 (from 0 to 5) Compute the average of the three grades. Use the result to decide whether the student will pass or fail. Lesson1 = int(input('Enter the grade for Lesson 1')) Lesson2 = int(input('Enter the grade for Lesson 2')) Lesson3 = int(input('Enter the grade for Lesson 3')) Average = int(Lesson1+Lesson2+Lesson3)/3 print('Average Course Grade:',Average) if Average >= 5: print(\"Passed\") else: print(\"Failed\") Enter the grade for Lesson 12 Enter the grade for Lesson 25 Enter the grade for Lesson 31 Average Course Grade: 2.6666666666666665 Failed Here are some great reads on this topic: - \"Common Python Data Structures (Guide)\" by Dan Bader available at https://realpython.com/python-data-structures/ - \"Data Structures You Need To Learn In Python\" by Akash available at https://www.edureka.co/blog/data-structures-in-python/ - \"Data Structures in Python\u2014 A Brief Introduction\" by Sowmya Krishnan available at https://towardsdatascience.com/data-structures-in-python-a-brief-introduction-b4135d7a9b7d - \"Everything you Should Know About Data Structures in Python\" by ANIRUDDHA BHANDARI available at https://www.analyticsvidhya.com/blog/2020/06/data-structures-python/ - \"Conditional Statements in Python\" by John Sturtz available at https://realpython.com/python-conditional-statements/ - \"Python If Statement explained with examples\" by CHAITANYA SINGH available at https://beginnersbook.com/2018/01/python-if-statement-example/ Here are some great videos on these topics: - \"Python: Data Structures - Lists, Tuples, Sets & Dictionaries tutorial\" by Joe James available at https://www.youtube.com/watch?v=R-HLU9Fl5ug&t=92s - \"Python Tutorial for Beginners 5: Dictionaries - Working with Key-Value Pairs\" by Corey Schafer available at https://www.youtube.com/watch?v=daefaLgNkw0 - \"How to Use If Else Statements in Python (Python Tutorial #2)\" by CS Dojo available at https://www.youtube.com/watch?v=AWek49wXGzI - \"Python If Statements | Python Tutorial #10\" by Amigoscode available at https://www.youtube.com/watch?v=wKQRmXR3jhc Exercise: Why dictionaries? Why do we need to use dictionaries in python? * Make sure to cite any resources that you may use.","title":"Lab3 Dev"},{"location":"8-Labs/Lab2/Lab3_Dev/#laboratory-3-structures-and-conditions","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 3: Structures and Conditions. "},{"location":"8-Labs/Lab2/Lab3_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab2/Lab3_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab2/Lab3_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab2/Lab3_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab2/Lab3_Dev/#data-structures-list-array","text":"A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\ x_1= 11 \\ x_2= 5 \\ x_3= 9 \\ x_4= 13 \\ ... \\ x_N= 223 \\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO. Alot of other lnguages start at ONE. Its just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9. MyOtherList = [] #Create an empty list MyOtherList.append(765) #Add one item to the list print(MyOtherList) MyList = [7,11,5,9,13,66,99,223] #Define a list print(MyList) sublist = MyList[3:6] #slice a sublist print(\"sublist is: \", sublist) mysum = sum(sublist) #sum the numbers in the sublist print(\"Sum: \", mysum) mylength = len(sublist) #get the length of the sublist print(\"Length: \", mylength) [765] [7, 11, 5, 9, 13, 66, 99, 223] sublist is: [9, 13, 66] Sum: 88 Length: 3","title":"Data Structures: List (Array)"},{"location":"8-Labs/Lab2/Lab3_Dev/#data-structures-special-list-tuple","text":"A tuple is a special kind of list where the values cannot be changed after the list is created. It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")","title":"Data Structures: Special List | Tuple"},{"location":"8-Labs/Lab2/Lab3_Dev/#data-structures-special-list-dictionary","text":"A dictionary is a special kind of list where the items are related data PAIRS. It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Some examples follow: MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") MyTupleName ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec') MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} print(MyPetsNamesAndMass) MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print(MyPetsNamesAndMassToo) {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} # Tuples MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") # Access a Tuple print (\"5th element of the tuple:\", MyTupleName[4]) # Dictionary MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} # Access the Dictionary print (\"Aspen's mass = \", MyPetsNamesAndMass[\"Aspen\"]) # Change a value in a dictionary print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"]) MyPetsNamesAndMass[\"Merrimee\"] = 0.01 print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"], \"She lost weight !\") # Alternate dictionary MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print (\"Merrimee's mass\" , MyPetsNamesAndMassToo[\"Merrimee\"]) # Attempt to change a Tuple #MyTupleName[3]=(\"Fred\") # Activate this line and see what happens! 5th element of the tuple: May Aspen's mass = 6.3 Merrimee's mass 0.03 Merrimee's mass 0.01 She lost weight ! Merrimee's mass 0.03","title":"Data Structures: Special List | Dictionary"},{"location":"8-Labs/Lab2/Lab3_Dev/#example-nested-dictionary","text":"From the dictionary below, print \"Pandemic\" and \"Tokyo\": FD = {\"Quentin\":\"Tarantino\",\"2020\":[2020,\"COVID\",19,\"Pandemic\"],\"Bond\":[\"James\",\"Gun\",(\"Paris\",\"Tokyo\",\"London\")]} #A nested dictionary print(FD) {'Quentin': 'Tarantino', '2020': [2020, 'COVID', 19, 'Pandemic'], 'Bond': ['James', 'Gun', ('Paris', 'Tokyo', 'London')]} FD['2020'][3] 'Pandemic' FD['Bond'][2][1] 'Tokyo'","title":"Example: Nested Dictionary"},{"location":"8-Labs/Lab2/Lab3_Dev/#conditional-execution","text":"Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, butmore recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs.","title":"Conditional Execution"},{"location":"8-Labs/Lab2/Lab3_Dev/#conditional-execution-comparison","text":"The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal. # Compare x = 7 y = 10 print(\"x =: \",x,\"y =: \",y) print(\"x is equal to y : \",x==y) print(\"x is not equal to y : \",x!=y) print(\"x is greater than y : \",x>y) print(\"x is less than y : \",x<y) x =: 7 y =: 10 x is equal to y : False x is not equal to y : True x is greater than y : False x is less than y : True # Logical operators print(\"5 == 5 and 5 < 6 ? \",5 == 5 and 5 < 6) print(\"4 > 3 or 17 > 20 \",4 > 3 or 17 > 20) print(\"not 5 == 5\",not 5 == 5) 5 == 5 and 5 < 6 ? True 4 > 3 or 17 > 20 True not 5 == 5 False","title":"Conditional Execution: Comparison"},{"location":"8-Labs/Lab2/Lab3_Dev/#conditional-execution-block-if-statement","text":"The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter. The next code fragment illustrates illustrates how the if statements work. The program asks the user for input. The use of raw_input() will let the program read any input as a string so non-numeric results will not throw an error. The input is stored in the variable named userInput . Next the statement if userInput == \"1\": compares the value of userInput with the string \"1\" . If the value in the variable is indeed \\1\", then the program will execute the block of code in the indentation after the colon. In this case it will execute print \"Hello World\" print \"How do you do? \" Alternatively, if the value of userInput is the string '2' , then the program will execute print \"Snakes on a plane \" For all other values the program will execute print \"You did not enter a valid number\" # Block if example userInput = input('Enter the number 1 or 2') # Use block if structure if userInput == '1': print(\"Hello World\") print(\"How do you do? \") elif userInput == '2': print(\"Snakes on a plane \") else: print(\"You did not enter a valid number\") Enter the number 1 or 21 Hello World How do you do?","title":"Conditional Execution:  Block if statement"},{"location":"8-Labs/Lab2/Lab3_Dev/#conditional-execution-inline-if-statement","text":"An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops. myInt = 0 num1 = 12 if (myInt == 0) else 13 num1 12","title":"Conditional Execution:  Inline if statement"},{"location":"8-Labs/Lab2/Lab3_Dev/#example-pass-or-fail","text":"Take the following inputs from the user: 1. Grade for Lesson 1 (from 0 to 5) 2. Grade for Lesson 2 (from 0 to 5) 3. Grade for Lesson 3 (from 0 to 5) Compute the average of the three grades. Use the result to decide whether the student will pass or fail. Lesson1 = int(input('Enter the grade for Lesson 1')) Lesson2 = int(input('Enter the grade for Lesson 2')) Lesson3 = int(input('Enter the grade for Lesson 3')) Average = int(Lesson1+Lesson2+Lesson3)/3 print('Average Course Grade:',Average) if Average >= 5: print(\"Passed\") else: print(\"Failed\") Enter the grade for Lesson 12 Enter the grade for Lesson 25 Enter the grade for Lesson 31 Average Course Grade: 2.6666666666666665 Failed Here are some great reads on this topic: - \"Common Python Data Structures (Guide)\" by Dan Bader available at https://realpython.com/python-data-structures/ - \"Data Structures You Need To Learn In Python\" by Akash available at https://www.edureka.co/blog/data-structures-in-python/ - \"Data Structures in Python\u2014 A Brief Introduction\" by Sowmya Krishnan available at https://towardsdatascience.com/data-structures-in-python-a-brief-introduction-b4135d7a9b7d - \"Everything you Should Know About Data Structures in Python\" by ANIRUDDHA BHANDARI available at https://www.analyticsvidhya.com/blog/2020/06/data-structures-python/ - \"Conditional Statements in Python\" by John Sturtz available at https://realpython.com/python-conditional-statements/ - \"Python If Statement explained with examples\" by CHAITANYA SINGH available at https://beginnersbook.com/2018/01/python-if-statement-example/ Here are some great videos on these topics: - \"Python: Data Structures - Lists, Tuples, Sets & Dictionaries tutorial\" by Joe James available at https://www.youtube.com/watch?v=R-HLU9Fl5ug&t=92s - \"Python Tutorial for Beginners 5: Dictionaries - Working with Key-Value Pairs\" by Corey Schafer available at https://www.youtube.com/watch?v=daefaLgNkw0 - \"How to Use If Else Statements in Python (Python Tutorial #2)\" by CS Dojo available at https://www.youtube.com/watch?v=AWek49wXGzI - \"Python If Statements | Python Tutorial #10\" by Amigoscode available at https://www.youtube.com/watch?v=wKQRmXR3jhc","title":"Example: Pass or Fail?"},{"location":"8-Labs/Lab2/Lab3_Dev/#exercise-why-dictionaries","text":"","title":"Exercise: Why dictionaries? "},{"location":"8-Labs/Lab2/Lab3_Dev/#why-do-we-need-to-use-dictionaries-in-python","text":"","title":"Why do we need to use dictionaries in python?"},{"location":"8-Labs/Lab2/Lab3_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab3/Lab3_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab3 Laboratory 3: Loops, Looops, Loooooops # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Program flow control (Loops) Controlled repetition Structured FOR Loop Structured WHILE Loop Count controlled repetition Count-controlled repetition is also called definite repetition because the number of repetitions is known before the loop begins executing. When we do not know in advance the number of times we want to execute a statement, we cannot use count-controlled repetition. In such an instance, we would use sentinel-controlled repetition. A count-controlled repetition will exit after running a certain number of times. The count is kept in a variable called an index or counter. When the index reaches a certain value (the loop bound) the loop will end. Count-controlled repetition requires control variable (or loop counter) initial value of the control variable increment (or decrement) by which the control variable is modified each iteration through the loop condition that tests for the final value of the control variable We can use both for and while loops, for count controlled repetition, but the for loop in combination with the range() function is more common. Structured FOR loop We have seen the for loop already, but we will formally introduce it here. The for loop executes a block of code repeatedly until the condition in the for statement is no longer true. Looping through an iterable An iterable is anything that can be looped over - typically a list, string, or tuple. The syntax for looping through an iterable is illustrated by an example. First a generic syntax for a in iterable: print(a) Notice the colon : and the indentation. Now a specific example: Example: A Loop to Begin With! Make a list with \"Walter\", \"Jesse\", \"Gus, \"Hank\". Then, write a loop that prints all the elements of your lisk. # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for AllStrings in BB: print(AllStrings) Walter Jesse Gus Hank The range() function to create an iterable The range(begin,end,increment) function will create an iterable starting at a value of begin, in steps defined by increment ( begin += increment ), ending at end . So a generic syntax becomes for a in range(begin,end,increment): print(a) The example that follows is count-controlled repetition (increment skip if greater) # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for i in range(0,4,1): # Change the numbers, what happens? print(BB[i]) Walter Jesse Gus Hank Example: That's odd! Write a loop to print all the odd numbers between 0 and 10. # For loop with range for x in range(1,10,2): # a sequence from 2 to 5 with steps of 1 print(x) 1 3 5 7 9 Sentinel-controlled repetition When loop control is based on the value of what we are processing, sentinel-controlled repetition is used. Sentinel-controlled repetition is also called indefinite repetition because it is not known in advance how many times the loop will be executed. It is a repetition procedure for solving a problem by using a sentinel value (also called a signal value, a dummy value or a flag value) to indicate \"end of process\". The sentinel value itself need not be a part of the processed data. One common example of using sentinel-controlled repetition is when we are processing data from a file and we do not know in advance when we would reach the end of the file. We can use both for and while loops, for Sentinel controlled repetition, but the while loop is more common. Structured WHILE loop The while loop repeats a block of instructions inside the loop while a condition remainsvtrue. First a generic syntax while condition is true: execute a execute b .... Notice our friend, the colon : and the indentation again. # set a counter counter = 5 # while loop while counter > 0: print(\"Counter = \",counter) counter = counter -1 Counter = 5 Counter = 4 Counter = 3 Counter = 2 Counter = 1 The while loop structure just depicted is a \"decrement, skip if equal\" in lower level languages. The next structure, also a while loop is an \"increment, skip if greater\" structure. # set a counter counter = 0 # while loop while counter <= 5: # change this line to: while counter <= 5: what happens? print (\"Counter = \",counter) counter = counter +1 # change this line to: counter +=1 what happens? Counter = 0 Counter = 1 Counter = 2 Counter = 3 Counter = 4 Counter = 5 Nested Repetition | Loops within Loops Round like a circle in a spiral, like a wheel within a wheel Never ending or beginning on an ever spinning reel Like a snowball down a mountain, or a carnival balloon Like a carousel that's turning running rings around the moon Like a clock whose hands are sweeping past the minutes of its face And the world is like an apple whirling silently in space Like the circles that you find in the windmills of your mind! Windmills of Your Mind lyrics \u00a9 Sony/ATV Music Publishing LLC, BMG Rights Management Songwriters: Marilyn Bergman / Michel Legrand / Alan Bergman Recommended versions: Neil Diamond | Dusty Springfield | Farhad Mehrad \"Like the circles that you find in the windmills of your mind\", Nested repetition is when a control structure is placed inside of the body or main part of another control structure. break to exit out of a loop Sometimes you may want to exit the loop when a certain condition different from the counting condition is met. Perhaps you are looping through a list and want to exit when you find the first element in the list that matches some criterion. The break keyword is useful for such an operation. For example run the following program: # j = 0 for i in range(0,5,1): j += 2 print (\"i = \",i,\"j = \",j) if j == 6: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 # One Small Change j = 0 for i in range(0,5,1): j += 2 print( \"i = \",i,\"j = \",j) if j == 7: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 i = 3 j = 8 i = 4 j = 10 In the first case, the for loop only executes 3 times before the condition j == 6 is TRUE and the loop is exited. In the second case, j == 7 never happens so the loop completes all its anticipated traverses. In both cases an if statement was used within a for loop. Such \"mixed\" control structures are quite common (and pretty necessary). A while loop contained within a for loop, with several if statements would be very common and such a structure is called nested control. There is typically an upper limit to nesting but the limit is pretty large - easily in the hundreds. It depends on the language and the system architecture ; suffice to say it is not a practical limit except possibly for general-domain AI applications. We can also do mundane activities and leverage loops, arithmetic, and format codes to make useful tables like Example: Cosines in the loop! Write a loop to print a table of the cosines of numbers between 0 and 0.01 with steps of 0.001. import math # package that contains cosine print(\" Cosines \") print(\" x \",\"|\",\" cos(x) \") print(\"--------|--------\") for i in range(0,100,1): x = float(i)*0.001 print(\"%.3f\" % x, \" |\", \" %.4f \" % math.cos(x)) # note the format code and the placeholder % and syntax of using package Example: Getting the hang of it! Write a Python script that takes a real input value (a float) for x and returns the y value according to the rules below \\begin{gather} y = x~for~0 <= x < 1 \\\\ y = x^2~for~1 <= x < 2 \\\\ y = x + 2~for~2 <= x < 3 \\\\ \\end{gather} Test the script with x values of 0.0, 1.0, 1.1, and 2.1. add functionality to automaticaly populate the table below: x y(x) 0.0 1.0 2.0 3.0 4.0 5.0 userInput = input('Enter enter a float') #ask for user's input x = float(userInput) print(\"x:\", x) if x >= 0 and x < 1: y = x print(\"y is equal to\",y) elif x >= 1 and x < 2: y = x*x print(\"y is equal to\",y) else: y = x+2 print(\"y is equal to\",y) # without pretty table print(\"---x---\",\"|\",\"---y---\") print(\"--------|--------\") for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) elif x >= 1 and x < 2: y = x*x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) else: y = x+2 print(\"%4.f\" % x, \" |\", \" %4.f \" % y) # with pretty table from prettytable import PrettyTable #Required to create tables t = PrettyTable(['x', 'y']) #Define an empty table for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) #will add a row to the table \"t\" elif x >= 1 and x < 2: y = x*x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) else: y = x+2 print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) print(t) The continue statement The continue instruction skips the block of code after it is executed for that iteration. It is best illustrated by an example. j = 0 for i in range(0,5,1): j += 2 print (\"\\n i = \", i , \", j = \", j) #here the \\n is a newline command if j == 6: continue print(\" this message will be skipped over if j = 6 \") # still within the loop, so the skip is implemented #When j ==6 the line after the continue keyword is not printed. #Other than that one difference the rest of the script runs normally. i = 0 , j = 2 this message will be skipped over if j = 6 i = 1 , j = 4 this message will be skipped over if j = 6 i = 2 , j = 6 i = 3 , j = 8 this message will be skipped over if j = 6 i = 4 , j = 10 this message will be skipped over if j = 6 The try , except structure An important control structure (and a pretty cool one for error trapping) is the try , except statement. The statement controls how the program proceeds when an error occurs in an instruction. The structure is really useful to trap likely errors (divide by zero, wrong kind of input) yet let the program keep running or at least issue a meaningful message to the user. The syntax is: try: do something except: do something else if ``do something'' returns an error Here is a really simple, but hugely important example: #MyErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition try: print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) except: print (\"error divide by zero\") y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 error divide by zero x = 12.0 y = -1.0 x/y = -12.0 x = 12.0 y = -2.0 x/y = -6.0 x = 12.0 y = -3.0 x/y = -4.0 x = 12.0 y = -4.0 x/y = -3.0 x = 12.0 y = -5.0 x/y = -2.4 x = 12.0 y = -6.0 x/y = -2.0 x = 12.0 y = -7.0 x/y = -1.7142857142857142 x = 12.0 y = -8.0 x/y = -1.5 x = 12.0 y = -9.0 x/y = -1.3333333333333333 x = 12.0 y = -10.0 x/y = -1.2 x = 12.0 y = -11.0 x/y = -1.0909090909090908 x = 12.0 y = -12.0 x/y = -1.0 So this silly code starts with x fixed at a value of 12, and y starting at 12 and decreasing by 1 until y equals -1. The code returns the ratio of x to y and at one point y is equal to zero and the division would be undefined. By trapping the error the code can issue us a measure and keep running. Modify the script as shown below,Run, and see what happens #NoErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) <ipython-input-31-82eeaceb9a12> in <module> 3 y = 12. 4 while y >= -12.: # sentinel controlled repetition ----> 5 print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) 6 y -= 1 ZeroDivisionError: float division by zero Here are some great reads on this topic: - \"Python for Loop\" available at https://www.programiz.com/python-programming/for-loop/ - \"Python \"for\" Loops (Definite Iteration)\" by John Sturtz available at https://realpython.com/python-for-loop/ - \"Python \"while\" Loops (Indefinite Iteration)\" by John Sturtz available at https://realpython.com/python-while-loop/ - \"loops in python\" available at https://www.geeksforgeeks.org/loops-in-python/ - \"Python Exceptions: An Introduction\" by Said van de Klundert available at *https://realpython.com/python-exceptions/ Here are some great videos on these topics: - \"Python For Loops - Python Tutorial for Absolute Beginners\" by Programming with Mosh available at https://www.youtube.com/watch?v=94UHCEmprCY - \"Python Tutorial for Beginners 7: Loops and Iterations - For/While Loops\" by Corey Schafer available at https://www.youtube.com/watch?v=6iF8Xb7Z3wQ - \"Python 3 Programming Tutorial - For loop\" by sentdex available at *https://www.youtube.com/watch?v=xtXexPSfcZg Exercise: FOR or WHILE? 1000 people have asked to be enlisted to take the first dose of COVID-19 vaccine. You are asked to write a loop and allow the ones who meet the requirements to take the shot. What kind of loop will you use? a FOR loop or a WHILE loop? Explain the logic behind your choice briefly. * Make sure to cite any resources that you may use.","title":"Lab3 Dev"},{"location":"8-Labs/Lab3/Lab3_Dev/#laboratory-3-loops-looops-loooooops","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 3: Loops, Looops, Loooooops "},{"location":"8-Labs/Lab3/Lab3_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab3/Lab3_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab3/Lab3_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab3/Lab3_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab3/Lab3_Dev/#program-flow-control-loops","text":"Controlled repetition Structured FOR Loop Structured WHILE Loop","title":"Program flow control (Loops)"},{"location":"8-Labs/Lab3/Lab3_Dev/#count-controlled-repetition","text":"Count-controlled repetition is also called definite repetition because the number of repetitions is known before the loop begins executing. When we do not know in advance the number of times we want to execute a statement, we cannot use count-controlled repetition. In such an instance, we would use sentinel-controlled repetition. A count-controlled repetition will exit after running a certain number of times. The count is kept in a variable called an index or counter. When the index reaches a certain value (the loop bound) the loop will end. Count-controlled repetition requires control variable (or loop counter) initial value of the control variable increment (or decrement) by which the control variable is modified each iteration through the loop condition that tests for the final value of the control variable We can use both for and while loops, for count controlled repetition, but the for loop in combination with the range() function is more common.","title":"Count controlled repetition"},{"location":"8-Labs/Lab3/Lab3_Dev/#structured-for-loop","text":"We have seen the for loop already, but we will formally introduce it here. The for loop executes a block of code repeatedly until the condition in the for statement is no longer true.","title":"Structured FOR loop"},{"location":"8-Labs/Lab3/Lab3_Dev/#looping-through-an-iterable","text":"An iterable is anything that can be looped over - typically a list, string, or tuple. The syntax for looping through an iterable is illustrated by an example. First a generic syntax for a in iterable: print(a) Notice the colon : and the indentation. Now a specific example:","title":"Looping through an iterable"},{"location":"8-Labs/Lab3/Lab3_Dev/#example-a-loop-to-begin-with","text":"Make a list with \"Walter\", \"Jesse\", \"Gus, \"Hank\". Then, write a loop that prints all the elements of your lisk. # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for AllStrings in BB: print(AllStrings) Walter Jesse Gus Hank","title":"Example: A Loop to Begin With!"},{"location":"8-Labs/Lab3/Lab3_Dev/#the-range-function-to-create-an-iterable","text":"The range(begin,end,increment) function will create an iterable starting at a value of begin, in steps defined by increment ( begin += increment ), ending at end . So a generic syntax becomes for a in range(begin,end,increment): print(a) The example that follows is count-controlled repetition (increment skip if greater) # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for i in range(0,4,1): # Change the numbers, what happens? print(BB[i]) Walter Jesse Gus Hank","title":"The range() function to create an iterable"},{"location":"8-Labs/Lab3/Lab3_Dev/#example-thats-odd","text":"Write a loop to print all the odd numbers between 0 and 10. # For loop with range for x in range(1,10,2): # a sequence from 2 to 5 with steps of 1 print(x) 1 3 5 7 9","title":"Example: That's odd!"},{"location":"8-Labs/Lab3/Lab3_Dev/#sentinel-controlled-repetition","text":"When loop control is based on the value of what we are processing, sentinel-controlled repetition is used. Sentinel-controlled repetition is also called indefinite repetition because it is not known in advance how many times the loop will be executed. It is a repetition procedure for solving a problem by using a sentinel value (also called a signal value, a dummy value or a flag value) to indicate \"end of process\". The sentinel value itself need not be a part of the processed data. One common example of using sentinel-controlled repetition is when we are processing data from a file and we do not know in advance when we would reach the end of the file. We can use both for and while loops, for Sentinel controlled repetition, but the while loop is more common.","title":"Sentinel-controlled repetition"},{"location":"8-Labs/Lab3/Lab3_Dev/#structured-while-loop","text":"The while loop repeats a block of instructions inside the loop while a condition remainsvtrue. First a generic syntax while condition is true: execute a execute b .... Notice our friend, the colon : and the indentation again. # set a counter counter = 5 # while loop while counter > 0: print(\"Counter = \",counter) counter = counter -1 Counter = 5 Counter = 4 Counter = 3 Counter = 2 Counter = 1 The while loop structure just depicted is a \"decrement, skip if equal\" in lower level languages. The next structure, also a while loop is an \"increment, skip if greater\" structure. # set a counter counter = 0 # while loop while counter <= 5: # change this line to: while counter <= 5: what happens? print (\"Counter = \",counter) counter = counter +1 # change this line to: counter +=1 what happens? Counter = 0 Counter = 1 Counter = 2 Counter = 3 Counter = 4 Counter = 5","title":"Structured WHILE loop"},{"location":"8-Labs/Lab3/Lab3_Dev/#nested-repetition-loops-within-loops","text":"Round like a circle in a spiral, like a wheel within a wheel Never ending or beginning on an ever spinning reel Like a snowball down a mountain, or a carnival balloon Like a carousel that's turning running rings around the moon Like a clock whose hands are sweeping past the minutes of its face And the world is like an apple whirling silently in space Like the circles that you find in the windmills of your mind! Windmills of Your Mind lyrics \u00a9 Sony/ATV Music Publishing LLC, BMG Rights Management Songwriters: Marilyn Bergman / Michel Legrand / Alan Bergman Recommended versions: Neil Diamond | Dusty Springfield | Farhad Mehrad \"Like the circles that you find in the windmills of your mind\", Nested repetition is when a control structure is placed inside of the body or main part of another control structure.","title":"Nested Repetition | Loops within Loops"},{"location":"8-Labs/Lab3/Lab3_Dev/#break-to-exit-out-of-a-loop","text":"Sometimes you may want to exit the loop when a certain condition different from the counting condition is met. Perhaps you are looping through a list and want to exit when you find the first element in the list that matches some criterion. The break keyword is useful for such an operation. For example run the following program: # j = 0 for i in range(0,5,1): j += 2 print (\"i = \",i,\"j = \",j) if j == 6: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 # One Small Change j = 0 for i in range(0,5,1): j += 2 print( \"i = \",i,\"j = \",j) if j == 7: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 i = 3 j = 8 i = 4 j = 10 In the first case, the for loop only executes 3 times before the condition j == 6 is TRUE and the loop is exited. In the second case, j == 7 never happens so the loop completes all its anticipated traverses. In both cases an if statement was used within a for loop. Such \"mixed\" control structures are quite common (and pretty necessary). A while loop contained within a for loop, with several if statements would be very common and such a structure is called nested control. There is typically an upper limit to nesting but the limit is pretty large - easily in the hundreds. It depends on the language and the system architecture ; suffice to say it is not a practical limit except possibly for general-domain AI applications. We can also do mundane activities and leverage loops, arithmetic, and format codes to make useful tables like","title":"break to exit out of a loop"},{"location":"8-Labs/Lab3/Lab3_Dev/#example-cosines-in-the-loop","text":"Write a loop to print a table of the cosines of numbers between 0 and 0.01 with steps of 0.001. import math # package that contains cosine print(\" Cosines \") print(\" x \",\"|\",\" cos(x) \") print(\"--------|--------\") for i in range(0,100,1): x = float(i)*0.001 print(\"%.3f\" % x, \" |\", \" %.4f \" % math.cos(x)) # note the format code and the placeholder % and syntax of using package","title":"Example: Cosines in the loop!"},{"location":"8-Labs/Lab3/Lab3_Dev/#example-getting-the-hang-of-it","text":"Write a Python script that takes a real input value (a float) for x and returns the y value according to the rules below \\begin{gather} y = x~for~0 <= x < 1 \\\\ y = x^2~for~1 <= x < 2 \\\\ y = x + 2~for~2 <= x < 3 \\\\ \\end{gather} Test the script with x values of 0.0, 1.0, 1.1, and 2.1. add functionality to automaticaly populate the table below: x y(x) 0.0 1.0 2.0 3.0 4.0 5.0 userInput = input('Enter enter a float') #ask for user's input x = float(userInput) print(\"x:\", x) if x >= 0 and x < 1: y = x print(\"y is equal to\",y) elif x >= 1 and x < 2: y = x*x print(\"y is equal to\",y) else: y = x+2 print(\"y is equal to\",y) # without pretty table print(\"---x---\",\"|\",\"---y---\") print(\"--------|--------\") for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) elif x >= 1 and x < 2: y = x*x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) else: y = x+2 print(\"%4.f\" % x, \" |\", \" %4.f \" % y) # with pretty table from prettytable import PrettyTable #Required to create tables t = PrettyTable(['x', 'y']) #Define an empty table for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) #will add a row to the table \"t\" elif x >= 1 and x < 2: y = x*x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) else: y = x+2 print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) print(t)","title":"Example: Getting the hang of it!"},{"location":"8-Labs/Lab3/Lab3_Dev/#the-continue-statement","text":"The continue instruction skips the block of code after it is executed for that iteration. It is best illustrated by an example. j = 0 for i in range(0,5,1): j += 2 print (\"\\n i = \", i , \", j = \", j) #here the \\n is a newline command if j == 6: continue print(\" this message will be skipped over if j = 6 \") # still within the loop, so the skip is implemented #When j ==6 the line after the continue keyword is not printed. #Other than that one difference the rest of the script runs normally. i = 0 , j = 2 this message will be skipped over if j = 6 i = 1 , j = 4 this message will be skipped over if j = 6 i = 2 , j = 6 i = 3 , j = 8 this message will be skipped over if j = 6 i = 4 , j = 10 this message will be skipped over if j = 6","title":"The continue statement"},{"location":"8-Labs/Lab3/Lab3_Dev/#the-try-except-structure","text":"An important control structure (and a pretty cool one for error trapping) is the try , except statement. The statement controls how the program proceeds when an error occurs in an instruction. The structure is really useful to trap likely errors (divide by zero, wrong kind of input) yet let the program keep running or at least issue a meaningful message to the user. The syntax is: try: do something except: do something else if ``do something'' returns an error Here is a really simple, but hugely important example: #MyErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition try: print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) except: print (\"error divide by zero\") y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 error divide by zero x = 12.0 y = -1.0 x/y = -12.0 x = 12.0 y = -2.0 x/y = -6.0 x = 12.0 y = -3.0 x/y = -4.0 x = 12.0 y = -4.0 x/y = -3.0 x = 12.0 y = -5.0 x/y = -2.4 x = 12.0 y = -6.0 x/y = -2.0 x = 12.0 y = -7.0 x/y = -1.7142857142857142 x = 12.0 y = -8.0 x/y = -1.5 x = 12.0 y = -9.0 x/y = -1.3333333333333333 x = 12.0 y = -10.0 x/y = -1.2 x = 12.0 y = -11.0 x/y = -1.0909090909090908 x = 12.0 y = -12.0 x/y = -1.0 So this silly code starts with x fixed at a value of 12, and y starting at 12 and decreasing by 1 until y equals -1. The code returns the ratio of x to y and at one point y is equal to zero and the division would be undefined. By trapping the error the code can issue us a measure and keep running. Modify the script as shown below,Run, and see what happens #NoErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) <ipython-input-31-82eeaceb9a12> in <module> 3 y = 12. 4 while y >= -12.: # sentinel controlled repetition ----> 5 print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) 6 y -= 1 ZeroDivisionError: float division by zero Here are some great reads on this topic: - \"Python for Loop\" available at https://www.programiz.com/python-programming/for-loop/ - \"Python \"for\" Loops (Definite Iteration)\" by John Sturtz available at https://realpython.com/python-for-loop/ - \"Python \"while\" Loops (Indefinite Iteration)\" by John Sturtz available at https://realpython.com/python-while-loop/ - \"loops in python\" available at https://www.geeksforgeeks.org/loops-in-python/ - \"Python Exceptions: An Introduction\" by Said van de Klundert available at *https://realpython.com/python-exceptions/ Here are some great videos on these topics: - \"Python For Loops - Python Tutorial for Absolute Beginners\" by Programming with Mosh available at https://www.youtube.com/watch?v=94UHCEmprCY - \"Python Tutorial for Beginners 7: Loops and Iterations - For/While Loops\" by Corey Schafer available at https://www.youtube.com/watch?v=6iF8Xb7Z3wQ - \"Python 3 Programming Tutorial - For loop\" by sentdex available at *https://www.youtube.com/watch?v=xtXexPSfcZg","title":"The try, except structure"},{"location":"8-Labs/Lab3/Lab3_Dev/#exercise-for-or-while","text":"","title":"Exercise: FOR or WHILE? "},{"location":"8-Labs/Lab3/Lab3_Dev/#1000-people-have-asked-to-be-enlisted-to-take-the-first-dose-of-covid-19-vaccine-you-are-asked-to-write-a-loop-and-allow-the-ones-who-meet-the-requirements-to-take-the-shot-what-kind-of-loop-will-you-use-a-for-loop-or-a-while-loop-explain-the-logic-behind-your-choice-briefly","text":"","title":"1000 people have asked to be enlisted to take the first dose of COVID-19 vaccine. You are asked to write a loop and allow the ones who meet the requirements to take the shot. What kind of loop will you use? a FOR loop or a WHILE loop? Explain the logic behind your choice briefly."},{"location":"8-Labs/Lab3/Lab3_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab3/Lab4_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab4 Laboratory 4: Loops, Looops, Loooooops # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Program flow control (Loops) Controlled repetition Structured FOR Loop Structured WHILE Loop Count controlled repetition Count-controlled repetition is also called definite repetition because the number of repetitions is known before the loop begins executing. When we do not know in advance the number of times we want to execute a statement, we cannot use count-controlled repetition. In such an instance, we would use sentinel-controlled repetition. A count-controlled repetition will exit after running a certain number of times. The count is kept in a variable called an index or counter. When the index reaches a certain value (the loop bound) the loop will end. Count-controlled repetition requires control variable (or loop counter) initial value of the control variable increment (or decrement) by which the control variable is modified each iteration through the loop condition that tests for the final value of the control variable We can use both for and while loops, for count controlled repetition, but the for loop in combination with the range() function is more common. Structured FOR loop We have seen the for loop already, but we will formally introduce it here. The for loop executes a block of code repeatedly until the condition in the for statement is no longer true. Looping through an iterable An iterable is anything that can be looped over - typically a list, string, or tuple. The syntax for looping through an iterable is illustrated by an example. First a generic syntax for a in iterable: print(a) Notice the colon : and the indentation. Now a specific example: Example: A Loop to Begin With! Make a list with \"Walter\", \"Jesse\", \"Gus, \"Hank\". Then, write a loop that prints all the elements of your lisk. # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for AllStrings in BB: print(AllStrings) Walter Jesse Gus Hank The range() function to create an iterable The range(begin,end,increment) function will create an iterable starting at a value of begin, in steps defined by increment ( begin += increment ), ending at end . So a generic syntax becomes for a in range(begin,end,increment): print(a) The example that follows is count-controlled repetition (increment skip if greater) # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for i in range(0,4,1): # Change the numbers, what happens? print(BB[i]) Walter Jesse Gus Hank Example: That's odd! Write a loop to print all the odd numbers between 0 and 10. # For loop with range for x in range(1,10,2): # a sequence from 2 to 5 with steps of 1 print(x) 1 3 5 7 9 Sentinel-controlled repetition When loop control is based on the value of what we are processing, sentinel-controlled repetition is used. Sentinel-controlled repetition is also called indefinite repetition because it is not known in advance how many times the loop will be executed. It is a repetition procedure for solving a problem by using a sentinel value (also called a signal value, a dummy value or a flag value) to indicate \"end of process\". The sentinel value itself need not be a part of the processed data. One common example of using sentinel-controlled repetition is when we are processing data from a file and we do not know in advance when we would reach the end of the file. We can use both for and while loops, for Sentinel controlled repetition, but the while loop is more common. Structured WHILE loop The while loop repeats a block of instructions inside the loop while a condition remainsvtrue. First a generic syntax while condition is true: execute a execute b .... Notice our friend, the colon : and the indentation again. # set a counter counter = 5 # while loop while counter > 0: print(\"Counter = \",counter) counter = counter -1 Counter = 5 Counter = 4 Counter = 3 Counter = 2 Counter = 1 The while loop structure just depicted is a \"decrement, skip if equal\" in lower level languages. The next structure, also a while loop is an \"increment, skip if greater\" structure. # set a counter counter = 0 # while loop while counter <= 5: # change this line to: while counter <= 5: what happens? print (\"Counter = \",counter) counter = counter +1 # change this line to: counter +=1 what happens? Counter = 0 Counter = 1 Counter = 2 Counter = 3 Counter = 4 Counter = 5 Nested Repetition | Loops within Loops Round like a circle in a spiral, like a wheel within a wheel Never ending or beginning on an ever spinning reel Like a snowball down a mountain, or a carnival balloon Like a carousel that's turning running rings around the moon Like a clock whose hands are sweeping past the minutes of its face And the world is like an apple whirling silently in space Like the circles that you find in the windmills of your mind! Windmills of Your Mind lyrics \u00a9 Sony/ATV Music Publishing LLC, BMG Rights Management Songwriters: Marilyn Bergman / Michel Legrand / Alan Bergman Recommended versions: Neil Diamond | Dusty Springfield | Farhad Mehrad \"Like the circles that you find in the windmills of your mind\", Nested repetition is when a control structure is placed inside of the body or main part of another control structure. break to exit out of a loop Sometimes you may want to exit the loop when a certain condition different from the counting condition is met. Perhaps you are looping through a list and want to exit when you find the first element in the list that matches some criterion. The break keyword is useful for such an operation. For example run the following program: # j = 0 for i in range(0,5,1): j += 2 print (\"i = \",i,\"j = \",j) if j == 6: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 # One Small Change j = 0 for i in range(0,5,1): j += 2 print( \"i = \",i,\"j = \",j) if j == 7: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 i = 3 j = 8 i = 4 j = 10 In the first case, the for loop only executes 3 times before the condition j == 6 is TRUE and the loop is exited. In the second case, j == 7 never happens so the loop completes all its anticipated traverses. In both cases an if statement was used within a for loop. Such \"mixed\" control structures are quite common (and pretty necessary). A while loop contained within a for loop, with several if statements would be very common and such a structure is called nested control. There is typically an upper limit to nesting but the limit is pretty large - easily in the hundreds. It depends on the language and the system architecture ; suffice to say it is not a practical limit except possibly for general-domain AI applications. We can also do mundane activities and leverage loops, arithmetic, and format codes to make useful tables like Example: Cosines in the loop! Write a loop to print a table of the cosines of numbers between 0 and 0.01 with steps of 0.001. import math # package that contains cosine print(\" Cosines \") print(\" x \",\"|\",\" cos(x) \") print(\"--------|--------\") for i in range(0,100,1): x = float(i)*0.001 print(\"%.3f\" % x, \" |\", \" %.4f \" % math.cos(x)) # note the format code and the placeholder % and syntax of using package Example: Getting the hang of it! Write a Python script that takes a real input value (a float) for x and returns the y value according to the rules below \\begin{gather} y = x~for~0 <= x < 1 \\\\ y = x^2~for~1 <= x < 2 \\\\ y = x + 2~for~2 <= x < 3 \\\\ \\end{gather} Test the script with x values of 0.0, 1.0, 1.1, and 2.1. add functionality to automaticaly populate the table below: x y(x) 0.0 1.0 2.0 3.0 4.0 5.0 userInput = input('Enter enter a float') #ask for user's input x = float(userInput) print(\"x:\", x) if x >= 0 and x < 1: y = x print(\"y is equal to\",y) elif x >= 1 and x < 2: y = x*x print(\"y is equal to\",y) else: y = x+2 print(\"y is equal to\",y) # without pretty table print(\"---x---\",\"|\",\"---y---\") print(\"--------|--------\") for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) elif x >= 1 and x < 2: y = x*x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) else: y = x+2 print(\"%4.f\" % x, \" |\", \" %4.f \" % y) # with pretty table from prettytable import PrettyTable #Required to create tables t = PrettyTable(['x', 'y']) #Define an empty table for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) #will add a row to the table \"t\" elif x >= 1 and x < 2: y = x*x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) else: y = x+2 print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) print(t) The continue statement The continue instruction skips the block of code after it is executed for that iteration. It is best illustrated by an example. j = 0 for i in range(0,5,1): j += 2 print (\"\\n i = \", i , \", j = \", j) #here the \\n is a newline command if j == 6: continue print(\" this message will be skipped over if j = 6 \") # still within the loop, so the skip is implemented #When j ==6 the line after the continue keyword is not printed. #Other than that one difference the rest of the script runs normally. i = 0 , j = 2 this message will be skipped over if j = 6 i = 1 , j = 4 this message will be skipped over if j = 6 i = 2 , j = 6 i = 3 , j = 8 this message will be skipped over if j = 6 i = 4 , j = 10 this message will be skipped over if j = 6 The try , except structure An important control structure (and a pretty cool one for error trapping) is the try , except statement. The statement controls how the program proceeds when an error occurs in an instruction. The structure is really useful to trap likely errors (divide by zero, wrong kind of input) yet let the program keep running or at least issue a meaningful message to the user. The syntax is: try: do something except: do something else if ``do something'' returns an error Here is a really simple, but hugely important example: #MyErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition try: print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) except: print (\"error divide by zero\") y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 error divide by zero x = 12.0 y = -1.0 x/y = -12.0 x = 12.0 y = -2.0 x/y = -6.0 x = 12.0 y = -3.0 x/y = -4.0 x = 12.0 y = -4.0 x/y = -3.0 x = 12.0 y = -5.0 x/y = -2.4 x = 12.0 y = -6.0 x/y = -2.0 x = 12.0 y = -7.0 x/y = -1.7142857142857142 x = 12.0 y = -8.0 x/y = -1.5 x = 12.0 y = -9.0 x/y = -1.3333333333333333 x = 12.0 y = -10.0 x/y = -1.2 x = 12.0 y = -11.0 x/y = -1.0909090909090908 x = 12.0 y = -12.0 x/y = -1.0 So this silly code starts with x fixed at a value of 12, and y starting at 12 and decreasing by 1 until y equals -1. The code returns the ratio of x to y and at one point y is equal to zero and the division would be undefined. By trapping the error the code can issue us a measure and keep running. Modify the script as shown below,Run, and see what happens #NoErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) <ipython-input-31-82eeaceb9a12> in <module> 3 y = 12. 4 while y >= -12.: # sentinel controlled repetition ----> 5 print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) 6 y -= 1 ZeroDivisionError: float division by zero Here are some great reads on this topic: - \"Python for Loop\" available at https://www.programiz.com/python-programming/for-loop/ - \"Python \"for\" Loops (Definite Iteration)\" by John Sturtz available at https://realpython.com/python-for-loop/ - \"Python \"while\" Loops (Indefinite Iteration)\" by John Sturtz available at https://realpython.com/python-while-loop/ - \"loops in python\" available at https://www.geeksforgeeks.org/loops-in-python/ - \"Python Exceptions: An Introduction\" by Said van de Klundert available at *https://realpython.com/python-exceptions/ Here are some great videos on these topics: - \"Python For Loops - Python Tutorial for Absolute Beginners\" by Programming with Mosh available at https://www.youtube.com/watch?v=94UHCEmprCY - \"Python Tutorial for Beginners 7: Loops and Iterations - For/While Loops\" by Corey Schafer available at https://www.youtube.com/watch?v=6iF8Xb7Z3wQ - \"Python 3 Programming Tutorial - For loop\" by sentdex available at *https://www.youtube.com/watch?v=xtXexPSfcZg Exercise: FOR or WHILE? 1000 people have asked to be enlisted to take the first dose of COVID-19 vaccine. You are asked to write a loop and allow the ones who meet the requirements to take the shot. What kind of loop will you use? a FOR loop or a WHILE loop? Explain the logic behind your choice briefly. * Make sure to cite any resources that you may use.","title":"Lab4 Dev"},{"location":"8-Labs/Lab3/Lab4_Dev/#laboratory-4-loops-looops-loooooops","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 4: Loops, Looops, Loooooops "},{"location":"8-Labs/Lab3/Lab4_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab3/Lab4_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab3/Lab4_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab3/Lab4_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab3/Lab4_Dev/#program-flow-control-loops","text":"Controlled repetition Structured FOR Loop Structured WHILE Loop","title":"Program flow control (Loops)"},{"location":"8-Labs/Lab3/Lab4_Dev/#count-controlled-repetition","text":"Count-controlled repetition is also called definite repetition because the number of repetitions is known before the loop begins executing. When we do not know in advance the number of times we want to execute a statement, we cannot use count-controlled repetition. In such an instance, we would use sentinel-controlled repetition. A count-controlled repetition will exit after running a certain number of times. The count is kept in a variable called an index or counter. When the index reaches a certain value (the loop bound) the loop will end. Count-controlled repetition requires control variable (or loop counter) initial value of the control variable increment (or decrement) by which the control variable is modified each iteration through the loop condition that tests for the final value of the control variable We can use both for and while loops, for count controlled repetition, but the for loop in combination with the range() function is more common.","title":"Count controlled repetition"},{"location":"8-Labs/Lab3/Lab4_Dev/#structured-for-loop","text":"We have seen the for loop already, but we will formally introduce it here. The for loop executes a block of code repeatedly until the condition in the for statement is no longer true.","title":"Structured FOR loop"},{"location":"8-Labs/Lab3/Lab4_Dev/#looping-through-an-iterable","text":"An iterable is anything that can be looped over - typically a list, string, or tuple. The syntax for looping through an iterable is illustrated by an example. First a generic syntax for a in iterable: print(a) Notice the colon : and the indentation. Now a specific example:","title":"Looping through an iterable"},{"location":"8-Labs/Lab3/Lab4_Dev/#example-a-loop-to-begin-with","text":"Make a list with \"Walter\", \"Jesse\", \"Gus, \"Hank\". Then, write a loop that prints all the elements of your lisk. # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for AllStrings in BB: print(AllStrings) Walter Jesse Gus Hank","title":"Example: A Loop to Begin With!"},{"location":"8-Labs/Lab3/Lab4_Dev/#the-range-function-to-create-an-iterable","text":"The range(begin,end,increment) function will create an iterable starting at a value of begin, in steps defined by increment ( begin += increment ), ending at end . So a generic syntax becomes for a in range(begin,end,increment): print(a) The example that follows is count-controlled repetition (increment skip if greater) # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for i in range(0,4,1): # Change the numbers, what happens? print(BB[i]) Walter Jesse Gus Hank","title":"The range() function to create an iterable"},{"location":"8-Labs/Lab3/Lab4_Dev/#example-thats-odd","text":"Write a loop to print all the odd numbers between 0 and 10. # For loop with range for x in range(1,10,2): # a sequence from 2 to 5 with steps of 1 print(x) 1 3 5 7 9","title":"Example: That's odd!"},{"location":"8-Labs/Lab3/Lab4_Dev/#sentinel-controlled-repetition","text":"When loop control is based on the value of what we are processing, sentinel-controlled repetition is used. Sentinel-controlled repetition is also called indefinite repetition because it is not known in advance how many times the loop will be executed. It is a repetition procedure for solving a problem by using a sentinel value (also called a signal value, a dummy value or a flag value) to indicate \"end of process\". The sentinel value itself need not be a part of the processed data. One common example of using sentinel-controlled repetition is when we are processing data from a file and we do not know in advance when we would reach the end of the file. We can use both for and while loops, for Sentinel controlled repetition, but the while loop is more common.","title":"Sentinel-controlled repetition"},{"location":"8-Labs/Lab3/Lab4_Dev/#structured-while-loop","text":"The while loop repeats a block of instructions inside the loop while a condition remainsvtrue. First a generic syntax while condition is true: execute a execute b .... Notice our friend, the colon : and the indentation again. # set a counter counter = 5 # while loop while counter > 0: print(\"Counter = \",counter) counter = counter -1 Counter = 5 Counter = 4 Counter = 3 Counter = 2 Counter = 1 The while loop structure just depicted is a \"decrement, skip if equal\" in lower level languages. The next structure, also a while loop is an \"increment, skip if greater\" structure. # set a counter counter = 0 # while loop while counter <= 5: # change this line to: while counter <= 5: what happens? print (\"Counter = \",counter) counter = counter +1 # change this line to: counter +=1 what happens? Counter = 0 Counter = 1 Counter = 2 Counter = 3 Counter = 4 Counter = 5","title":"Structured WHILE loop"},{"location":"8-Labs/Lab3/Lab4_Dev/#nested-repetition-loops-within-loops","text":"Round like a circle in a spiral, like a wheel within a wheel Never ending or beginning on an ever spinning reel Like a snowball down a mountain, or a carnival balloon Like a carousel that's turning running rings around the moon Like a clock whose hands are sweeping past the minutes of its face And the world is like an apple whirling silently in space Like the circles that you find in the windmills of your mind! Windmills of Your Mind lyrics \u00a9 Sony/ATV Music Publishing LLC, BMG Rights Management Songwriters: Marilyn Bergman / Michel Legrand / Alan Bergman Recommended versions: Neil Diamond | Dusty Springfield | Farhad Mehrad \"Like the circles that you find in the windmills of your mind\", Nested repetition is when a control structure is placed inside of the body or main part of another control structure.","title":"Nested Repetition | Loops within Loops"},{"location":"8-Labs/Lab3/Lab4_Dev/#break-to-exit-out-of-a-loop","text":"Sometimes you may want to exit the loop when a certain condition different from the counting condition is met. Perhaps you are looping through a list and want to exit when you find the first element in the list that matches some criterion. The break keyword is useful for such an operation. For example run the following program: # j = 0 for i in range(0,5,1): j += 2 print (\"i = \",i,\"j = \",j) if j == 6: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 # One Small Change j = 0 for i in range(0,5,1): j += 2 print( \"i = \",i,\"j = \",j) if j == 7: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 i = 3 j = 8 i = 4 j = 10 In the first case, the for loop only executes 3 times before the condition j == 6 is TRUE and the loop is exited. In the second case, j == 7 never happens so the loop completes all its anticipated traverses. In both cases an if statement was used within a for loop. Such \"mixed\" control structures are quite common (and pretty necessary). A while loop contained within a for loop, with several if statements would be very common and such a structure is called nested control. There is typically an upper limit to nesting but the limit is pretty large - easily in the hundreds. It depends on the language and the system architecture ; suffice to say it is not a practical limit except possibly for general-domain AI applications. We can also do mundane activities and leverage loops, arithmetic, and format codes to make useful tables like","title":"break to exit out of a loop"},{"location":"8-Labs/Lab3/Lab4_Dev/#example-cosines-in-the-loop","text":"Write a loop to print a table of the cosines of numbers between 0 and 0.01 with steps of 0.001. import math # package that contains cosine print(\" Cosines \") print(\" x \",\"|\",\" cos(x) \") print(\"--------|--------\") for i in range(0,100,1): x = float(i)*0.001 print(\"%.3f\" % x, \" |\", \" %.4f \" % math.cos(x)) # note the format code and the placeholder % and syntax of using package","title":"Example: Cosines in the loop!"},{"location":"8-Labs/Lab3/Lab4_Dev/#example-getting-the-hang-of-it","text":"Write a Python script that takes a real input value (a float) for x and returns the y value according to the rules below \\begin{gather} y = x~for~0 <= x < 1 \\\\ y = x^2~for~1 <= x < 2 \\\\ y = x + 2~for~2 <= x < 3 \\\\ \\end{gather} Test the script with x values of 0.0, 1.0, 1.1, and 2.1. add functionality to automaticaly populate the table below: x y(x) 0.0 1.0 2.0 3.0 4.0 5.0 userInput = input('Enter enter a float') #ask for user's input x = float(userInput) print(\"x:\", x) if x >= 0 and x < 1: y = x print(\"y is equal to\",y) elif x >= 1 and x < 2: y = x*x print(\"y is equal to\",y) else: y = x+2 print(\"y is equal to\",y) # without pretty table print(\"---x---\",\"|\",\"---y---\") print(\"--------|--------\") for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) elif x >= 1 and x < 2: y = x*x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) else: y = x+2 print(\"%4.f\" % x, \" |\", \" %4.f \" % y) # with pretty table from prettytable import PrettyTable #Required to create tables t = PrettyTable(['x', 'y']) #Define an empty table for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) #will add a row to the table \"t\" elif x >= 1 and x < 2: y = x*x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) else: y = x+2 print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) print(t)","title":"Example: Getting the hang of it!"},{"location":"8-Labs/Lab3/Lab4_Dev/#the-continue-statement","text":"The continue instruction skips the block of code after it is executed for that iteration. It is best illustrated by an example. j = 0 for i in range(0,5,1): j += 2 print (\"\\n i = \", i , \", j = \", j) #here the \\n is a newline command if j == 6: continue print(\" this message will be skipped over if j = 6 \") # still within the loop, so the skip is implemented #When j ==6 the line after the continue keyword is not printed. #Other than that one difference the rest of the script runs normally. i = 0 , j = 2 this message will be skipped over if j = 6 i = 1 , j = 4 this message will be skipped over if j = 6 i = 2 , j = 6 i = 3 , j = 8 this message will be skipped over if j = 6 i = 4 , j = 10 this message will be skipped over if j = 6","title":"The continue statement"},{"location":"8-Labs/Lab3/Lab4_Dev/#the-try-except-structure","text":"An important control structure (and a pretty cool one for error trapping) is the try , except statement. The statement controls how the program proceeds when an error occurs in an instruction. The structure is really useful to trap likely errors (divide by zero, wrong kind of input) yet let the program keep running or at least issue a meaningful message to the user. The syntax is: try: do something except: do something else if ``do something'' returns an error Here is a really simple, but hugely important example: #MyErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition try: print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) except: print (\"error divide by zero\") y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 error divide by zero x = 12.0 y = -1.0 x/y = -12.0 x = 12.0 y = -2.0 x/y = -6.0 x = 12.0 y = -3.0 x/y = -4.0 x = 12.0 y = -4.0 x/y = -3.0 x = 12.0 y = -5.0 x/y = -2.4 x = 12.0 y = -6.0 x/y = -2.0 x = 12.0 y = -7.0 x/y = -1.7142857142857142 x = 12.0 y = -8.0 x/y = -1.5 x = 12.0 y = -9.0 x/y = -1.3333333333333333 x = 12.0 y = -10.0 x/y = -1.2 x = 12.0 y = -11.0 x/y = -1.0909090909090908 x = 12.0 y = -12.0 x/y = -1.0 So this silly code starts with x fixed at a value of 12, and y starting at 12 and decreasing by 1 until y equals -1. The code returns the ratio of x to y and at one point y is equal to zero and the division would be undefined. By trapping the error the code can issue us a measure and keep running. Modify the script as shown below,Run, and see what happens #NoErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) <ipython-input-31-82eeaceb9a12> in <module> 3 y = 12. 4 while y >= -12.: # sentinel controlled repetition ----> 5 print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) 6 y -= 1 ZeroDivisionError: float division by zero Here are some great reads on this topic: - \"Python for Loop\" available at https://www.programiz.com/python-programming/for-loop/ - \"Python \"for\" Loops (Definite Iteration)\" by John Sturtz available at https://realpython.com/python-for-loop/ - \"Python \"while\" Loops (Indefinite Iteration)\" by John Sturtz available at https://realpython.com/python-while-loop/ - \"loops in python\" available at https://www.geeksforgeeks.org/loops-in-python/ - \"Python Exceptions: An Introduction\" by Said van de Klundert available at *https://realpython.com/python-exceptions/ Here are some great videos on these topics: - \"Python For Loops - Python Tutorial for Absolute Beginners\" by Programming with Mosh available at https://www.youtube.com/watch?v=94UHCEmprCY - \"Python Tutorial for Beginners 7: Loops and Iterations - For/While Loops\" by Corey Schafer available at https://www.youtube.com/watch?v=6iF8Xb7Z3wQ - \"Python 3 Programming Tutorial - For loop\" by sentdex available at *https://www.youtube.com/watch?v=xtXexPSfcZg","title":"The try, except structure"},{"location":"8-Labs/Lab3/Lab4_Dev/#exercise-for-or-while","text":"","title":"Exercise: FOR or WHILE? "},{"location":"8-Labs/Lab3/Lab4_Dev/#1000-people-have-asked-to-be-enlisted-to-take-the-first-dose-of-covid-19-vaccine-you-are-asked-to-write-a-loop-and-allow-the-ones-who-meet-the-requirements-to-take-the-shot-what-kind-of-loop-will-you-use-a-for-loop-or-a-while-loop-explain-the-logic-behind-your-choice-briefly","text":"","title":"1000 people have asked to be enlisted to take the first dose of COVID-19 vaccine. You are asked to write a loop and allow the ones who meet the requirements to take the shot. What kind of loop will you use? a FOR loop or a WHILE loop? Explain the logic behind your choice briefly."},{"location":"8-Labs/Lab3/Lab4_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab4/Lab4_Dev/","text":"Laboratory 4: FUNctions # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: What is a function in Python? Functions are simply pre-written code fragments that perform a certain task. In older procedural languages functions and subroutines are similar, but a function returns a value whereas a subroutine operates on data. The difference is subtle but important. More recent thinking has functions being able to operate on data (they always could) and the value returned may be simply an exit code. An analogy are the functions in MS Excel . To add numbers, we can use the sum(range) function and type =sum(A1:A5) instead of typing =A1+A2+A3+A4+A5 Calling the Function We call a function simply by typing the name of the function or by using the dot notation. Whether we can use the dot notation or not depends on how the function is written, whether it is part of a class, and how it is imported into a program. Some functions expect us to pass data to them to perform their tasks. These data are known as parameters( older terminology is arguments, or argument list) and we pass them to the function by enclosing their values in parenthesis ( ) separated by commas. For instance, the print() function for displaying text on the screen is \\called\" by typing print('Hello World') where print is the name of the function and the literal (a string) 'Hello World' is the argument. Program flow A function, whether built-in, or added must be defined before it is called, otherwise the script will fail. Certain built-in functions \"self define\" upon start (such as print() and type() and we need not worry about those funtions). The diagram below illustrates the requesite flow control for functions that need to be defined before use. An example below will illustrate, change the cell to code and run it, you should get an error. Then fix the indicated line (remove the leading \"#\" in the import math ... line) and rerun, should get a functioning script. reset the notebook using a magic function in JupyterLab %reset -f An example, run once as is then activate indicated line, run again - what happens? x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic ) import math # import the math package ## activate and rerun sqrt_by_math = math.sqrt(x) # note the dot notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) # Here is an alternative way: We just load the function that we want: # reset the notebook using a magic function in JupyterLab %reset -f # An example, run once as is then activate indicated line, run again - what happens? x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic ) from math import sqrt # import sqrt from the math package ## activate and rerun sqrt_by_math = sqrt(x) # note the notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) Using arithmetic square root of 4.0 is 2.0 Using math package square root of 4.0 is 2.0 Built-In in Primitive Python (Base install) The base Python functions and types built into it that are always available, the figure below lists those functions. Notice all have the structure of function_name() , except __import__() which has a constructor type structure, and is not intended for routine use. We will learn about constructors later. Added-In using External Packages/Modules and Libaries (e.g. math) Python is also distributed with a large number of external functions. These functions are saved in files known as modules. To use the built-in codes in Python modules, we have to import them into our programs first. We do that by using the import keyword. There are three ways to import: 1. Import the entire module by writing import moduleName; For instance, to import the random module, we write import random. To use the randrange() function in the random module, we write random.randrange( 1, 10);28 2. Import and rename the module by writing import random as r (where r is any name of your choice). Now to use the randrange() function, you simply write r.randrange(1, 10); and 3. Import specific functions from the module by writing from moduleName import name1[,name2[, ... nameN]]. For instance, to import the randrange() function from the random module, we write from random import randrange. To import multiple functions, we separate them with a comma. To import the randrange() and randint() functions, we write from random import randrange, randint. To use the function now, we do not have to use the dot notation anymore. Just write randrange( 1, 10). # Example 1 of import %reset -f import random low = 1 ; high = 10 random.randrange(low,high) #generate random number in range low to high 4 # Example 2 of import %reset -f import random as r low = 1 ; high = 10 r.randrange(low,high) 7 # Example 3 of import %reset -f from random import randrange low = 1 ; high = 10 randrange(low,high) 5 The modules that come with Python are extensive and listed at https://docs.python.org/3/py-modindex.html. There are also other modules that can be downloaded and used (just like user defined modules below). In these labs we are building primitive codes to learn how to code and how to create algorithms. For many practical cases you will want to load a well-tested package to accomplish the tasks. That exercise is saved for the end of the document. User-Built We can define our own functions in Python and reuse them throughout the program. The syntax for defining a function is: def functionName( argument ): code detailing what the function should do note the colon above and indentation ... ... return [expression] The keyword def tells the program that the indented code from the next line onwards is part of the function. The keyword return tells the program to return an answer from the function. There can be multiple return statements in a function. Once the function executes a return statement, the program exits the function and continues with its next executable statement. If the function does not need to return any value, you can omit the return statement. Functions can be pretty elaborate; they can search for things in a list, determine variable types, open and close files, read and write to files. To get started we will build a few really simple mathematical functions; we will need this skill in the future anyway, especially in scientific programming contexts. User-built within a Code Block For our first function we will code f(x) = x\\sqrt{1 + x} into a function named dusty() . When you run the next cell, all it does is prototype the function (defines it), nothing happens until we use the function. def dusty(x) : temp = x * ((1.0+x)**(0.5)) # don't need the math package return temp # the function should make the evaluation # store in the local variable temp # return contents of temp # wrapper to run the dusty function yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = dusty(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589 Example: The Average Function Create the AVERAGE function for three values and test it for these values: - 3,4,5 - 10,100,1000 - -5,15,5 def AVERAGE3(x,y,z) : #define the function \"AVERAGE3\" Ave = (x+y+z)/3 #computes the average return Ave print(AVERAGE3(3,4,5)) print(AVERAGE3(10,100,1000)) print(AVERAGE3(-5,15,5)) 4.0 370.0 5.0 Example: The KATANA Function Create the Katana function for rounding off to the nearest hundredths (to 2 decimal places) and test it for these values: - 25.33694 - 15.753951 - 3.14159265359 def Katana(x) : #define the function \"Katana\" newX = round(x, 2) return newX print(Katana(25.33694)) print(Katana(15.753951)) print(Katana(3.14159265359)) 25.34 15.75 3.14 Variable Scope An important concept when defining a function is the concept of variable scope. Variables defined inside a function are treated differently from variables defined outside. Firstly, any variable declared within a function is only accessible within the function. These are known as local variables. In the dusty() function, the variables x and temp are local to the function. Any variable declared outside a function in a main program is known as a program variable and is accessible anywhere in the program. In the example, the variables xvalue and yvalue are program variables (global to the program; if they are addressed within a function, they could be operated on.) Generally we want to protect the program variables from the function unless the intent is to change their values. The way the function is written in the example, the function cannot damage xvalue or yvalue . If a local variable shares the same name as a program variable, any code inside the function is accessing the local variable. Any code outside is accessing the program variable As Separate Module/File In this section we will invent the neko() function, export it to a file, so we can reuse it in later notebooks without having to retype or cut-and-paste. The neko() function evaluates: f(x) = x\\sqrt{|(1 + x)|} Its the same as the dusty() function, except operates on the absolute value in the wadical. Create a text file named \"mylibrary.txt\" Copy the neko() function script below into that file. def neko(input_argument) : import math #ok to import into a function local_variable = input_argument * math.sqrt(abs(1.0+input_argument)) return local_variable rename mylibrary.txt to mylibrary.py modify the wrapper script to use the neko function as an external module # wrapper to run the neko function import mylibrary yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = mylibrary.neko(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589 In JupyterHub environments, you may discover that changes you make to your external python file are not reflected when you re-run your script; you need to restart the kernel to get the changes to actually update. The figure below depicts the notebook, external file relatonship Rudimentary Graphics Graphing values is part of the broader field of data visualization, which has two main goals: To explore data, and To communicate data. In this subsection we will concentrate on introducing skills to start exploring data and to produce meaningful visualizations we can use throughout the rest of this notebook. Data visualization is a rich field of study that fills entire books. The reason to start visualization here instead of elsewhere is that with functions plotting is a natural activity and we have to import the matplotlib module to make the plots. The example below is code adapted from Grus (2015) that illustrates simple generic plots. I added a single line (label the x-axis), and corrected some transcription errors (not the original author's mistake, just the consequence of how the API handled the cut-and-paste), but otherwise the code is unchanged. # python script to illustrate plotting # CODE BELOW IS ADAPTED FROM: # Grus, Joel (2015-04-14). Data Science from Scratch: First Principles with Python # (Kindle Locations 1190-1191). O'Reilly Media. Kindle Edition. # from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define one list for years gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] # and another one for Gross Domestic Product (GDP) plt.plot( years, gdp, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis # what if \"^\", \"P\", \"*\" for marker? # what if \"red\" for color? # what if \"dashdot\", '--' for linestyle? plt.title(\"Nominal GDP\")# add a title plt.ylabel(\"Billions of $\")# add a label to the x and y-axes plt.xlabel(\"Year\") plt.show() # display the plot # Now lets put the plotting script into a function so we can make line charts of any two numeric lists def plotAline(list1,list2,strx,stry,strtitle): # plot list1 on x, list2 on y, xlabel, ylabel, title from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() plt.plot( list1, list2, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis plt.title(strtitle)# add a title plt.ylabel(stry)# add a label to the x and y-axes plt.xlabel(strx) plt.show() # display the plot return #null return # wrapper years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define two lists years and gdp gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] print(type(years[0])) print(type(gdp[0])) plotAline(years,gdp,\"Year\",\"Billions of $\",\"Nominal GDP\") <class 'int'> <class 'float'> Example- The Hopeless Romantic! Copy the wrapper script for the plotAline() function, and modify the copy to create a plot of x = 16sin^3(t) y = 13cos(t) - 5cos(2t) - 2cos(3t) - cos(4t) for t raging from [0,2 \\Pi ] (inclusive). Label the plot and the plot axes. from matplotlib import pyplot as plt # import the plotting library import numpy as np # import NumPy: for large, multi-dimensional arrays and matrices, along with high-level mathematical functions to operate on these arrays. pi = np.pi #pi value from the np package t= np.linspace(0,2*pi,360)# the NumPy function np.linspace is similar to the range() x = 16*np.sin(t)**3 y = 13*np.cos(t) - 5*np.cos(2*t) - 2*np.cos(3*t) - np.cos(4*t) plt.plot( x, y, color ='purple', marker ='.', linestyle ='solid') plt.ylabel(\"Y-axis\")# add a label to the x and y-axes plt.xlabel(\"X-axis\") plt.axis('equal') #sets equal axis ratios plt.title(\"A Hopeless Romantic's Curve\")# add a title plt.show() # display the plot Here are some great reads on this topic: - \"Functions in Python\" available at https://www.geeksforgeeks.org/functions-in-python/ - \"Defining Your Own Python Function\" by John Sturtz available at https://realpython.com/defining-your-own-python-function/ - \"Graph Plotting in Python | Set 1\" available at https://www.geeksforgeeks.org/graph-plotting-in-python-set-1/ - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"How To Use Functions In Python (Python Tutorial #3)\" by CS Dojo available at https://www.youtube.com/watch?v=NSbOtYzIQI0 - \"Python Tutorial for Beginners 8: Functions\" by Corey Schafer available at https://www.youtube.com/watch?v=9Os0o3wzS_I - \"Python 3 Programming Tutorial - Functions\" by sentdex available at *https://www.youtube.com/watch?v=owglNL1KQf0 Exercise: A Function for Coffee. Because Coffee is life! Write a pseudo-code for a function that asks for user's preferences on their coffee (e.g., type of brew), follows certain steps to prepare that coffee, and delivers that coffee with an appropriate message. You can be as imaginative as you like, but make sure to provide logical justification for your script. * Make sure to cite any resources that you may use.","title":"<font color=darkred>Laboratory 4: FUNctions </font>"},{"location":"8-Labs/Lab4/Lab4_Dev/#laboratory-4-functions","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 4: FUNctions "},{"location":"8-Labs/Lab4/Lab4_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab4/Lab4_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab4/Lab4_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab4/Lab4_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab4/Lab4_Dev/#what-is-a-function-in-python","text":"Functions are simply pre-written code fragments that perform a certain task. In older procedural languages functions and subroutines are similar, but a function returns a value whereas a subroutine operates on data. The difference is subtle but important. More recent thinking has functions being able to operate on data (they always could) and the value returned may be simply an exit code. An analogy are the functions in MS Excel . To add numbers, we can use the sum(range) function and type =sum(A1:A5) instead of typing =A1+A2+A3+A4+A5","title":"What is a function in Python?"},{"location":"8-Labs/Lab4/Lab4_Dev/#calling-the-function","text":"We call a function simply by typing the name of the function or by using the dot notation. Whether we can use the dot notation or not depends on how the function is written, whether it is part of a class, and how it is imported into a program. Some functions expect us to pass data to them to perform their tasks. These data are known as parameters( older terminology is arguments, or argument list) and we pass them to the function by enclosing their values in parenthesis ( ) separated by commas. For instance, the print() function for displaying text on the screen is \\called\" by typing print('Hello World') where print is the name of the function and the literal (a string) 'Hello World' is the argument.","title":"Calling the Function"},{"location":"8-Labs/Lab4/Lab4_Dev/#program-flow","text":"A function, whether built-in, or added must be defined before it is called, otherwise the script will fail. Certain built-in functions \"self define\" upon start (such as print() and type() and we need not worry about those funtions). The diagram below illustrates the requesite flow control for functions that need to be defined before use. An example below will illustrate, change the cell to code and run it, you should get an error. Then fix the indicated line (remove the leading \"#\" in the import math ... line) and rerun, should get a functioning script.","title":"Program flow"},{"location":"8-Labs/Lab4/Lab4_Dev/#reset-the-notebook-using-a-magic-function-in-jupyterlab","text":"%reset -f","title":"reset the notebook using a magic function in JupyterLab"},{"location":"8-Labs/Lab4/Lab4_Dev/#an-example-run-once-as-is-then-activate-indicated-line-run-again-what-happens","text":"x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic )","title":"An example, run once as is then activate indicated line, run again - what happens?"},{"location":"8-Labs/Lab4/Lab4_Dev/#import-math-import-the-math-package-activate-and-rerun","text":"sqrt_by_math = math.sqrt(x) # note the dot notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) # Here is an alternative way: We just load the function that we want: # reset the notebook using a magic function in JupyterLab %reset -f # An example, run once as is then activate indicated line, run again - what happens? x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic ) from math import sqrt # import sqrt from the math package ## activate and rerun sqrt_by_math = sqrt(x) # note the notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) Using arithmetic square root of 4.0 is 2.0 Using math package square root of 4.0 is 2.0","title":"import math # import the math package ## activate and rerun"},{"location":"8-Labs/Lab4/Lab4_Dev/#built-in-in-primitive-python-base-install","text":"The base Python functions and types built into it that are always available, the figure below lists those functions. Notice all have the structure of function_name() , except __import__() which has a constructor type structure, and is not intended for routine use. We will learn about constructors later.","title":"Built-In in Primitive Python (Base install)"},{"location":"8-Labs/Lab4/Lab4_Dev/#added-in-using-external-packagesmodules-and-libaries-eg-math","text":"Python is also distributed with a large number of external functions. These functions are saved in files known as modules. To use the built-in codes in Python modules, we have to import them into our programs first. We do that by using the import keyword. There are three ways to import: 1. Import the entire module by writing import moduleName; For instance, to import the random module, we write import random. To use the randrange() function in the random module, we write random.randrange( 1, 10);28 2. Import and rename the module by writing import random as r (where r is any name of your choice). Now to use the randrange() function, you simply write r.randrange(1, 10); and 3. Import specific functions from the module by writing from moduleName import name1[,name2[, ... nameN]]. For instance, to import the randrange() function from the random module, we write from random import randrange. To import multiple functions, we separate them with a comma. To import the randrange() and randint() functions, we write from random import randrange, randint. To use the function now, we do not have to use the dot notation anymore. Just write randrange( 1, 10). # Example 1 of import %reset -f import random low = 1 ; high = 10 random.randrange(low,high) #generate random number in range low to high 4 # Example 2 of import %reset -f import random as r low = 1 ; high = 10 r.randrange(low,high) 7 # Example 3 of import %reset -f from random import randrange low = 1 ; high = 10 randrange(low,high) 5 The modules that come with Python are extensive and listed at https://docs.python.org/3/py-modindex.html. There are also other modules that can be downloaded and used (just like user defined modules below). In these labs we are building primitive codes to learn how to code and how to create algorithms. For many practical cases you will want to load a well-tested package to accomplish the tasks. That exercise is saved for the end of the document.","title":"Added-In using External Packages/Modules and Libaries (e.g. math)"},{"location":"8-Labs/Lab4/Lab4_Dev/#user-built","text":"We can define our own functions in Python and reuse them throughout the program. The syntax for defining a function is: def functionName( argument ): code detailing what the function should do note the colon above and indentation ... ... return [expression] The keyword def tells the program that the indented code from the next line onwards is part of the function. The keyword return tells the program to return an answer from the function. There can be multiple return statements in a function. Once the function executes a return statement, the program exits the function and continues with its next executable statement. If the function does not need to return any value, you can omit the return statement. Functions can be pretty elaborate; they can search for things in a list, determine variable types, open and close files, read and write to files. To get started we will build a few really simple mathematical functions; we will need this skill in the future anyway, especially in scientific programming contexts.","title":"User-Built"},{"location":"8-Labs/Lab4/Lab4_Dev/#user-built-within-a-code-block","text":"For our first function we will code f(x) = x\\sqrt{1 + x} into a function named dusty() . When you run the next cell, all it does is prototype the function (defines it), nothing happens until we use the function. def dusty(x) : temp = x * ((1.0+x)**(0.5)) # don't need the math package return temp # the function should make the evaluation # store in the local variable temp # return contents of temp # wrapper to run the dusty function yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = dusty(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589","title":"User-built within a Code Block"},{"location":"8-Labs/Lab4/Lab4_Dev/#example-the-average-function","text":"Create the AVERAGE function for three values and test it for these values: - 3,4,5 - 10,100,1000 - -5,15,5 def AVERAGE3(x,y,z) : #define the function \"AVERAGE3\" Ave = (x+y+z)/3 #computes the average return Ave print(AVERAGE3(3,4,5)) print(AVERAGE3(10,100,1000)) print(AVERAGE3(-5,15,5)) 4.0 370.0 5.0","title":"Example: The Average Function"},{"location":"8-Labs/Lab4/Lab4_Dev/#example-the-katana-function","text":"Create the Katana function for rounding off to the nearest hundredths (to 2 decimal places) and test it for these values: - 25.33694 - 15.753951 - 3.14159265359 def Katana(x) : #define the function \"Katana\" newX = round(x, 2) return newX print(Katana(25.33694)) print(Katana(15.753951)) print(Katana(3.14159265359)) 25.34 15.75 3.14","title":"Example: The KATANA Function"},{"location":"8-Labs/Lab4/Lab4_Dev/#variable-scope","text":"An important concept when defining a function is the concept of variable scope. Variables defined inside a function are treated differently from variables defined outside. Firstly, any variable declared within a function is only accessible within the function. These are known as local variables. In the dusty() function, the variables x and temp are local to the function. Any variable declared outside a function in a main program is known as a program variable and is accessible anywhere in the program. In the example, the variables xvalue and yvalue are program variables (global to the program; if they are addressed within a function, they could be operated on.) Generally we want to protect the program variables from the function unless the intent is to change their values. The way the function is written in the example, the function cannot damage xvalue or yvalue . If a local variable shares the same name as a program variable, any code inside the function is accessing the local variable. Any code outside is accessing the program variable","title":"Variable Scope"},{"location":"8-Labs/Lab4/Lab4_Dev/#as-separate-modulefile","text":"In this section we will invent the neko() function, export it to a file, so we can reuse it in later notebooks without having to retype or cut-and-paste. The neko() function evaluates: f(x) = x\\sqrt{|(1 + x)|} Its the same as the dusty() function, except operates on the absolute value in the wadical. Create a text file named \"mylibrary.txt\" Copy the neko() function script below into that file. def neko(input_argument) : import math #ok to import into a function local_variable = input_argument * math.sqrt(abs(1.0+input_argument)) return local_variable rename mylibrary.txt to mylibrary.py modify the wrapper script to use the neko function as an external module # wrapper to run the neko function import mylibrary yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = mylibrary.neko(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589 In JupyterHub environments, you may discover that changes you make to your external python file are not reflected when you re-run your script; you need to restart the kernel to get the changes to actually update. The figure below depicts the notebook, external file relatonship","title":"As Separate Module/File"},{"location":"8-Labs/Lab4/Lab4_Dev/#rudimentary-graphics","text":"Graphing values is part of the broader field of data visualization, which has two main goals: To explore data, and To communicate data. In this subsection we will concentrate on introducing skills to start exploring data and to produce meaningful visualizations we can use throughout the rest of this notebook. Data visualization is a rich field of study that fills entire books. The reason to start visualization here instead of elsewhere is that with functions plotting is a natural activity and we have to import the matplotlib module to make the plots. The example below is code adapted from Grus (2015) that illustrates simple generic plots. I added a single line (label the x-axis), and corrected some transcription errors (not the original author's mistake, just the consequence of how the API handled the cut-and-paste), but otherwise the code is unchanged. # python script to illustrate plotting # CODE BELOW IS ADAPTED FROM: # Grus, Joel (2015-04-14). Data Science from Scratch: First Principles with Python # (Kindle Locations 1190-1191). O'Reilly Media. Kindle Edition. # from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define one list for years gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] # and another one for Gross Domestic Product (GDP) plt.plot( years, gdp, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis # what if \"^\", \"P\", \"*\" for marker? # what if \"red\" for color? # what if \"dashdot\", '--' for linestyle? plt.title(\"Nominal GDP\")# add a title plt.ylabel(\"Billions of $\")# add a label to the x and y-axes plt.xlabel(\"Year\") plt.show() # display the plot # Now lets put the plotting script into a function so we can make line charts of any two numeric lists def plotAline(list1,list2,strx,stry,strtitle): # plot list1 on x, list2 on y, xlabel, ylabel, title from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() plt.plot( list1, list2, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis plt.title(strtitle)# add a title plt.ylabel(stry)# add a label to the x and y-axes plt.xlabel(strx) plt.show() # display the plot return #null return # wrapper years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define two lists years and gdp gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] print(type(years[0])) print(type(gdp[0])) plotAline(years,gdp,\"Year\",\"Billions of $\",\"Nominal GDP\") <class 'int'> <class 'float'>","title":"Rudimentary Graphics"},{"location":"8-Labs/Lab4/Lab4_Dev/#example-the-hopeless-romantic","text":"Copy the wrapper script for the plotAline() function, and modify the copy to create a plot of x = 16sin^3(t) y = 13cos(t) - 5cos(2t) - 2cos(3t) - cos(4t) for t raging from [0,2 \\Pi ] (inclusive). Label the plot and the plot axes. from matplotlib import pyplot as plt # import the plotting library import numpy as np # import NumPy: for large, multi-dimensional arrays and matrices, along with high-level mathematical functions to operate on these arrays. pi = np.pi #pi value from the np package t= np.linspace(0,2*pi,360)# the NumPy function np.linspace is similar to the range() x = 16*np.sin(t)**3 y = 13*np.cos(t) - 5*np.cos(2*t) - 2*np.cos(3*t) - np.cos(4*t) plt.plot( x, y, color ='purple', marker ='.', linestyle ='solid') plt.ylabel(\"Y-axis\")# add a label to the x and y-axes plt.xlabel(\"X-axis\") plt.axis('equal') #sets equal axis ratios plt.title(\"A Hopeless Romantic's Curve\")# add a title plt.show() # display the plot Here are some great reads on this topic: - \"Functions in Python\" available at https://www.geeksforgeeks.org/functions-in-python/ - \"Defining Your Own Python Function\" by John Sturtz available at https://realpython.com/defining-your-own-python-function/ - \"Graph Plotting in Python | Set 1\" available at https://www.geeksforgeeks.org/graph-plotting-in-python-set-1/ - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"How To Use Functions In Python (Python Tutorial #3)\" by CS Dojo available at https://www.youtube.com/watch?v=NSbOtYzIQI0 - \"Python Tutorial for Beginners 8: Functions\" by Corey Schafer available at https://www.youtube.com/watch?v=9Os0o3wzS_I - \"Python 3 Programming Tutorial - Functions\" by sentdex available at *https://www.youtube.com/watch?v=owglNL1KQf0","title":"Example- The Hopeless Romantic!"},{"location":"8-Labs/Lab4/Lab4_Dev/#exercise-a-function-for-coffee-because-coffee-is-life","text":"","title":"Exercise: A Function for Coffee. Because Coffee is life! "},{"location":"8-Labs/Lab4/Lab4_Dev/#write-a-pseudo-code-for-a-function-that-asks-for-users-preferences-on-their-coffee-eg-type-of-brew-follows-certain-steps-to-prepare-that-coffee-and-delivers-that-coffee-with-an-appropriate-message-you-can-be-as-imaginative-as-you-like-but-make-sure-to-provide-logical-justification-for-your-script","text":"","title":"Write a pseudo-code for a function that asks for user's preferences on their coffee (e.g., type of brew), follows certain steps to prepare that coffee, and delivers that coffee with an appropriate message. You can be as imaginative as you like, but make sure to provide logical justification for your script."},{"location":"8-Labs/Lab4/Lab4_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab4/Lab6_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab6 Laboratory 6: FUNctions # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: What is a function in Python? Functions are simply pre-written code fragments that perform a certain task. In older procedural languages functions and subroutines are similar, but a function returns a value whereas a subroutine operates on data. The difference is subtle but important. More recent thinking has functions being able to operate on data (they always could) and the value returned may be simply an exit code. An analogy are the functions in MS Excel . To add numbers, we can use the sum(range) function and type =sum(A1:A5) instead of typing =A1+A2+A3+A4+A5 Calling the Function We call a function simply by typing the name of the function or by using the dot notation. Whether we can use the dot notation or not depends on how the function is written, whether it is part of a class, and how it is imported into a program. Some functions expect us to pass data to them to perform their tasks. These data are known as parameters( older terminology is arguments, or argument list) and we pass them to the function by enclosing their values in parenthesis ( ) separated by commas. For instance, the print() function for displaying text on the screen is \\called\" by typing print('Hello World') where print is the name of the function and the literal (a string) 'Hello World' is the argument. Program flow A function, whether built-in, or added must be defined before it is called, otherwise the script will fail. Certain built-in functions \"self define\" upon start (such as print() and type() and we need not worry about those funtions). The diagram below illustrates the requesite flow control for functions that need to be defined before use. An example below will illustrate, change the cell to code and run it, you should get an error. Then fix the indicated line (remove the leading \"#\" in the import math ... line) and rerun, should get a functioning script. # reset the notebook using a magic function in JupyterLab %reset -f # An example, run once as is then activate indicated line, run again - what happens? x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic ) #import math # import the math package ## activate and rerun sqrt_by_math = math.sqrt(x) # note the dot notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) # Here is an alternative way: We just load the function that we want: # reset the notebook using a magic function in JupyterLab %reset -f # An example, run once as is then activate indicated line, run again - what happens? x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic ) from math import sqrt # import sqrt from the math package ## activate and rerun sqrt_by_math = sqrt(x) # note the notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) Using arithmetic square root of 4.0 is 2.0 Using math package square root of 4.0 is 2.0 Built-In in Primitive Python (Base install) The base Python functions and types built into it that are always available, the figure below lists those functions. Notice all have the structure of function_name() , except __import__() which has a constructor type structure, and is not intended for routine use. We will learn about constructors later. Added-In using External Packages/Modules and Libaries (e.g. math) Python is also distributed with a large number of external functions. These functions are saved in files known as modules. To use the built-in codes in Python modules, we have to import them into our programs first. We do that by using the import keyword. There are three ways to import: 1. Import the entire module by writing import moduleName; For instance, to import the random module, we write import random. To use the randrange() function in the random module, we write random.randrange( 1, 10);28 2. Import and rename the module by writing import random as r (where r is any name of your choice). Now to use the randrange() function, you simply write r.randrange(1, 10); and 3. Import specific functions from the module by writing from moduleName import name1[,name2[, ... nameN]]. For instance, to import the randrange() function from the random module, we write from random import randrange. To import multiple functions, we separate them with a comma. To import the randrange() and randint() functions, we write from random import randrange, randint. To use the function now, we do not have to use the dot notation anymore. Just write randrange( 1, 10). # Example 1 of import %reset -f import random low = 1 ; high = 10 random.randrange(low,high) #generate random number in range low to high 4 # Example 2 of import %reset -f import random as r low = 1 ; high = 10 r.randrange(low,high) 7 # Example 3 of import %reset -f from random import randrange low = 1 ; high = 10 randrange(low,high) 5 The modules that come with Python are extensive and listed at https://docs.python.org/3/py-modindex.html. There are also other modules that can be downloaded and used (just like user defined modules below). In these labs we are building primitive codes to learn how to code and how to create algorithms. For many practical cases you will want to load a well-tested package to accomplish the tasks. That exercise is saved for the end of the document. User-Built We can define our own functions in Python and reuse them throughout the program. The syntax for defining a function is: def functionName( argument ): code detailing what the function should do note the colon above and indentation ... ... return [expression] The keyword def tells the program that the indented code from the next line onwards is part of the function. The keyword return tells the program to return an answer from the function. There can be multiple return statements in a function. Once the function executes a return statement, the program exits the function and continues with its next executable statement. If the function does not need to return any value, you can omit the return statement. Functions can be pretty elaborate; they can search for things in a list, determine variable types, open and close files, read and write to files. To get started we will build a few really simple mathematical functions; we will need this skill in the future anyway, especially in scientific programming contexts. User-built within a Code Block For our first function we will code f(x) = x\\sqrt{1 + x} into a function named dusty() . When you run the next cell, all it does is prototype the function (defines it), nothing happens until we use the function. def dusty(x) : temp = x * ((1.0+x)**(0.5)) # don't need the math package return temp # the function should make the evaluation # store in the local variable temp # return contents of temp # wrapper to run the dusty function yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = dusty(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589 Example: The Average Function Create the AVERAGE function for three values and test it for these values: - 3,4,5 - 10,100,1000 - -5,15,5 def AVERAGE3(x,y,z) : #define the function \"AVERAGE3\" Ave = (x+y+z)/3 #computes the average return Ave print(AVERAGE3(3,4,5)) print(AVERAGE3(10,100,1000)) print(AVERAGE3(-5,15,5)) 4.0 370.0 5.0 Example: The KATANA Function Create the Katana function for rounding off to the nearest hundredths (to 2 decimal places) and test it for these values: - 25.33694 - 15.753951 - 3.14159265359 def Katana(x) : #define the function \"Katana\" newX = round(x, 2) return newX print(Katana(25.33694)) print(Katana(15.753951)) print(Katana(3.14159265359)) 25.34 15.75 3.14 Variable Scope An important concept when defining a function is the concept of variable scope. Variables defined inside a function are treated differently from variables defined outside. Firstly, any variable declared within a function is only accessible within the function. These are known as local variables. In the dusty() function, the variables x and temp are local to the function. Any variable declared outside a function in a main program is known as a program variable and is accessible anywhere in the program. In the example, the variables xvalue and yvalue are program variables (global to the program; if they are addressed within a function, they could be operated on.) Generally we want to protect the program variables from the function unless the intent is to change their values. The way the function is written in the example, the function cannot damage xvalue or yvalue . If a local variable shares the same name as a program variable, any code inside the function is accessing the local variable. Any code outside is accessing the program variable As Separate Module/File In this section we will invent the neko() function, export it to a file, so we can reuse it in later notebooks without having to retype or cut-and-paste. The neko() function evaluates: f(x) = x\\sqrt{|(1 + x)|} Its the same as the dusty() function, except operates on the absolute value in the wadical. Create a text file named \"mylibrary.txt\" Copy the neko() function script below into that file. def neko(input_argument) : import math #ok to import into a function local_variable = input_argument * math.sqrt(abs(1.0+input_argument)) return local_variable rename mylibrary.txt to mylibrary.py modify the wrapper script to use the neko function as an external module # wrapper to run the neko function import mylibrary yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = mylibrary.neko(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589 In JupyterHub environments, you may discover that changes you make to your external python file are not reflected when you re-run your script; you need to restart the kernel to get the changes to actually update. The figure below depicts the notebook, external file relatonship Rudimentary Graphics Graphing values is part of the broader field of data visualization, which has two main goals: To explore data, and To communicate data. In this subsection we will concentrate on introducing skills to start exploring data and to produce meaningful visualizations we can use throughout the rest of this notebook. Data visualization is a rich field of study that fills entire books. The reason to start visualization here instead of elsewhere is that with functions plotting is a natural activity and we have to import the matplotlib module to make the plots. The example below is code adapted from Grus (2015) that illustrates simple generic plots. I added a single line (label the x-axis), and corrected some transcription errors (not the original author's mistake, just the consequence of how the API handled the cut-and-paste), but otherwise the code is unchanged. # python script to illustrate plotting # CODE BELOW IS ADAPTED FROM: # Grus, Joel (2015-04-14). Data Science from Scratch: First Principles with Python # (Kindle Locations 1190-1191). O'Reilly Media. Kindle Edition. # from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define one list for years gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] # and another one for Gross Domestic Product (GDP) plt.plot( years, gdp, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis # what if \"^\", \"P\", \"*\" for marker? # what if \"red\" for color? # what if \"dashdot\", '--' for linestyle? plt.title(\"Nominal GDP\")# add a title plt.ylabel(\"Billions of $\")# add a label to the x and y-axes plt.xlabel(\"Year\") plt.show() # display the plot # Now lets put the plotting script into a function so we can make line charts of any two numeric lists def plotAline(list1,list2,strx,stry,strtitle): # plot list1 on x, list2 on y, xlabel, ylabel, title from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() plt.plot( list1, list2, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis plt.title(strtitle)# add a title plt.ylabel(stry)# add a label to the x and y-axes plt.xlabel(strx) plt.show() # display the plot return #null return # wrapper years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define two lists years and gdp gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] print(type(years[0])) print(type(gdp[0])) plotAline(years,gdp,\"Year\",\"Billions of $\",\"Nominal GDP\") <class 'int'> <class 'float'> Example- The Hopeless Romantic! Copy the wrapper script for the plotAline() function, and modify the copy to create a plot of x = 16sin^3(t) y = 13cos(t) - 5cos(2t) - 2cos(3t) - cos(4t) for t raging from [0,2 \\Pi ] (inclusive). Label the plot and the plot axes. from matplotlib import pyplot as plt # import the plotting library import numpy as np # import NumPy: for large, multi-dimensional arrays and matrices, along with high-level mathematical functions to operate on these arrays. pi = np.pi #pi value from the np package t= np.linspace(0,2*pi,360)# the NumPy function np.linspace is similar to the range() x = 16*np.sin(t)**3 y = 13*np.cos(t) - 5*np.cos(2*t) - 2*np.cos(3*t) - np.cos(4*t) plt.plot( x, y, color ='purple', marker ='.', linestyle ='solid') plt.ylabel(\"Y-axis\")# add a label to the x and y-axes plt.xlabel(\"X-axis\") plt.axis('equal') #sets equal axis ratios plt.title(\"A Hopeless Romantic's Curve\")# add a title plt.show() # display the plot Here are some great reads on this topic: - \"Functions in Python\" available at https://www.geeksforgeeks.org/functions-in-python/ - \"Defining Your Own Python Function\" by John Sturtz available at https://realpython.com/defining-your-own-python-function/ - \"Graph Plotting in Python | Set 1\" available at https://www.geeksforgeeks.org/graph-plotting-in-python-set-1/ - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"How To Use Functions In Python (Python Tutorial #3)\" by CS Dojo available at https://www.youtube.com/watch?v=NSbOtYzIQI0 - \"Python Tutorial for Beginners 8: Functions\" by Corey Schafer available at https://www.youtube.com/watch?v=9Os0o3wzS_I - \"Python 3 Programming Tutorial - Functions\" by sentdex available at *https://www.youtube.com/watch?v=owglNL1KQf0 Exercise: A Function for Coffee. Because Coffee is life! Write a pseudo-code for a function that asks for user's preferences on their coffee (e.g., type of brew), follows certain steps to prepare that coffee, and delivers that coffee with an appropriate message. You can be as imaginative as you like, but make sure to provide logical justification for your script. * Make sure to cite any resources that you may use.","title":"Lab6 Dev"},{"location":"8-Labs/Lab4/Lab6_Dev/#laboratory-6-functions","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 6: FUNctions "},{"location":"8-Labs/Lab4/Lab6_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab4/Lab6_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab4/Lab6_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab4/Lab6_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab4/Lab6_Dev/#what-is-a-function-in-python","text":"Functions are simply pre-written code fragments that perform a certain task. In older procedural languages functions and subroutines are similar, but a function returns a value whereas a subroutine operates on data. The difference is subtle but important. More recent thinking has functions being able to operate on data (they always could) and the value returned may be simply an exit code. An analogy are the functions in MS Excel . To add numbers, we can use the sum(range) function and type =sum(A1:A5) instead of typing =A1+A2+A3+A4+A5","title":"What is a function in Python?"},{"location":"8-Labs/Lab4/Lab6_Dev/#calling-the-function","text":"We call a function simply by typing the name of the function or by using the dot notation. Whether we can use the dot notation or not depends on how the function is written, whether it is part of a class, and how it is imported into a program. Some functions expect us to pass data to them to perform their tasks. These data are known as parameters( older terminology is arguments, or argument list) and we pass them to the function by enclosing their values in parenthesis ( ) separated by commas. For instance, the print() function for displaying text on the screen is \\called\" by typing print('Hello World') where print is the name of the function and the literal (a string) 'Hello World' is the argument.","title":"Calling the Function"},{"location":"8-Labs/Lab4/Lab6_Dev/#program-flow","text":"A function, whether built-in, or added must be defined before it is called, otherwise the script will fail. Certain built-in functions \"self define\" upon start (such as print() and type() and we need not worry about those funtions). The diagram below illustrates the requesite flow control for functions that need to be defined before use. An example below will illustrate, change the cell to code and run it, you should get an error. Then fix the indicated line (remove the leading \"#\" in the import math ... line) and rerun, should get a functioning script. # reset the notebook using a magic function in JupyterLab %reset -f # An example, run once as is then activate indicated line, run again - what happens? x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic ) #import math # import the math package ## activate and rerun sqrt_by_math = math.sqrt(x) # note the dot notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) # Here is an alternative way: We just load the function that we want: # reset the notebook using a magic function in JupyterLab %reset -f # An example, run once as is then activate indicated line, run again - what happens? x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic ) from math import sqrt # import sqrt from the math package ## activate and rerun sqrt_by_math = sqrt(x) # note the notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) Using arithmetic square root of 4.0 is 2.0 Using math package square root of 4.0 is 2.0","title":"Program flow"},{"location":"8-Labs/Lab4/Lab6_Dev/#built-in-in-primitive-python-base-install","text":"The base Python functions and types built into it that are always available, the figure below lists those functions. Notice all have the structure of function_name() , except __import__() which has a constructor type structure, and is not intended for routine use. We will learn about constructors later.","title":"Built-In in Primitive Python (Base install)"},{"location":"8-Labs/Lab4/Lab6_Dev/#added-in-using-external-packagesmodules-and-libaries-eg-math","text":"Python is also distributed with a large number of external functions. These functions are saved in files known as modules. To use the built-in codes in Python modules, we have to import them into our programs first. We do that by using the import keyword. There are three ways to import: 1. Import the entire module by writing import moduleName; For instance, to import the random module, we write import random. To use the randrange() function in the random module, we write random.randrange( 1, 10);28 2. Import and rename the module by writing import random as r (where r is any name of your choice). Now to use the randrange() function, you simply write r.randrange(1, 10); and 3. Import specific functions from the module by writing from moduleName import name1[,name2[, ... nameN]]. For instance, to import the randrange() function from the random module, we write from random import randrange. To import multiple functions, we separate them with a comma. To import the randrange() and randint() functions, we write from random import randrange, randint. To use the function now, we do not have to use the dot notation anymore. Just write randrange( 1, 10). # Example 1 of import %reset -f import random low = 1 ; high = 10 random.randrange(low,high) #generate random number in range low to high 4 # Example 2 of import %reset -f import random as r low = 1 ; high = 10 r.randrange(low,high) 7 # Example 3 of import %reset -f from random import randrange low = 1 ; high = 10 randrange(low,high) 5 The modules that come with Python are extensive and listed at https://docs.python.org/3/py-modindex.html. There are also other modules that can be downloaded and used (just like user defined modules below). In these labs we are building primitive codes to learn how to code and how to create algorithms. For many practical cases you will want to load a well-tested package to accomplish the tasks. That exercise is saved for the end of the document.","title":"Added-In using External Packages/Modules and Libaries (e.g. math)"},{"location":"8-Labs/Lab4/Lab6_Dev/#user-built","text":"We can define our own functions in Python and reuse them throughout the program. The syntax for defining a function is: def functionName( argument ): code detailing what the function should do note the colon above and indentation ... ... return [expression] The keyword def tells the program that the indented code from the next line onwards is part of the function. The keyword return tells the program to return an answer from the function. There can be multiple return statements in a function. Once the function executes a return statement, the program exits the function and continues with its next executable statement. If the function does not need to return any value, you can omit the return statement. Functions can be pretty elaborate; they can search for things in a list, determine variable types, open and close files, read and write to files. To get started we will build a few really simple mathematical functions; we will need this skill in the future anyway, especially in scientific programming contexts.","title":"User-Built"},{"location":"8-Labs/Lab4/Lab6_Dev/#user-built-within-a-code-block","text":"For our first function we will code f(x) = x\\sqrt{1 + x} into a function named dusty() . When you run the next cell, all it does is prototype the function (defines it), nothing happens until we use the function. def dusty(x) : temp = x * ((1.0+x)**(0.5)) # don't need the math package return temp # the function should make the evaluation # store in the local variable temp # return contents of temp # wrapper to run the dusty function yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = dusty(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589","title":"User-built within a Code Block"},{"location":"8-Labs/Lab4/Lab6_Dev/#example-the-average-function","text":"Create the AVERAGE function for three values and test it for these values: - 3,4,5 - 10,100,1000 - -5,15,5 def AVERAGE3(x,y,z) : #define the function \"AVERAGE3\" Ave = (x+y+z)/3 #computes the average return Ave print(AVERAGE3(3,4,5)) print(AVERAGE3(10,100,1000)) print(AVERAGE3(-5,15,5)) 4.0 370.0 5.0","title":"Example: The Average Function"},{"location":"8-Labs/Lab4/Lab6_Dev/#example-the-katana-function","text":"Create the Katana function for rounding off to the nearest hundredths (to 2 decimal places) and test it for these values: - 25.33694 - 15.753951 - 3.14159265359 def Katana(x) : #define the function \"Katana\" newX = round(x, 2) return newX print(Katana(25.33694)) print(Katana(15.753951)) print(Katana(3.14159265359)) 25.34 15.75 3.14","title":"Example: The KATANA Function"},{"location":"8-Labs/Lab4/Lab6_Dev/#variable-scope","text":"An important concept when defining a function is the concept of variable scope. Variables defined inside a function are treated differently from variables defined outside. Firstly, any variable declared within a function is only accessible within the function. These are known as local variables. In the dusty() function, the variables x and temp are local to the function. Any variable declared outside a function in a main program is known as a program variable and is accessible anywhere in the program. In the example, the variables xvalue and yvalue are program variables (global to the program; if they are addressed within a function, they could be operated on.) Generally we want to protect the program variables from the function unless the intent is to change their values. The way the function is written in the example, the function cannot damage xvalue or yvalue . If a local variable shares the same name as a program variable, any code inside the function is accessing the local variable. Any code outside is accessing the program variable","title":"Variable Scope"},{"location":"8-Labs/Lab4/Lab6_Dev/#as-separate-modulefile","text":"In this section we will invent the neko() function, export it to a file, so we can reuse it in later notebooks without having to retype or cut-and-paste. The neko() function evaluates: f(x) = x\\sqrt{|(1 + x)|} Its the same as the dusty() function, except operates on the absolute value in the wadical. Create a text file named \"mylibrary.txt\" Copy the neko() function script below into that file. def neko(input_argument) : import math #ok to import into a function local_variable = input_argument * math.sqrt(abs(1.0+input_argument)) return local_variable rename mylibrary.txt to mylibrary.py modify the wrapper script to use the neko function as an external module # wrapper to run the neko function import mylibrary yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = mylibrary.neko(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589 In JupyterHub environments, you may discover that changes you make to your external python file are not reflected when you re-run your script; you need to restart the kernel to get the changes to actually update. The figure below depicts the notebook, external file relatonship","title":"As Separate Module/File"},{"location":"8-Labs/Lab4/Lab6_Dev/#rudimentary-graphics","text":"Graphing values is part of the broader field of data visualization, which has two main goals: To explore data, and To communicate data. In this subsection we will concentrate on introducing skills to start exploring data and to produce meaningful visualizations we can use throughout the rest of this notebook. Data visualization is a rich field of study that fills entire books. The reason to start visualization here instead of elsewhere is that with functions plotting is a natural activity and we have to import the matplotlib module to make the plots. The example below is code adapted from Grus (2015) that illustrates simple generic plots. I added a single line (label the x-axis), and corrected some transcription errors (not the original author's mistake, just the consequence of how the API handled the cut-and-paste), but otherwise the code is unchanged. # python script to illustrate plotting # CODE BELOW IS ADAPTED FROM: # Grus, Joel (2015-04-14). Data Science from Scratch: First Principles with Python # (Kindle Locations 1190-1191). O'Reilly Media. Kindle Edition. # from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define one list for years gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] # and another one for Gross Domestic Product (GDP) plt.plot( years, gdp, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis # what if \"^\", \"P\", \"*\" for marker? # what if \"red\" for color? # what if \"dashdot\", '--' for linestyle? plt.title(\"Nominal GDP\")# add a title plt.ylabel(\"Billions of $\")# add a label to the x and y-axes plt.xlabel(\"Year\") plt.show() # display the plot # Now lets put the plotting script into a function so we can make line charts of any two numeric lists def plotAline(list1,list2,strx,stry,strtitle): # plot list1 on x, list2 on y, xlabel, ylabel, title from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() plt.plot( list1, list2, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis plt.title(strtitle)# add a title plt.ylabel(stry)# add a label to the x and y-axes plt.xlabel(strx) plt.show() # display the plot return #null return # wrapper years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define two lists years and gdp gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] print(type(years[0])) print(type(gdp[0])) plotAline(years,gdp,\"Year\",\"Billions of $\",\"Nominal GDP\") <class 'int'> <class 'float'>","title":"Rudimentary Graphics"},{"location":"8-Labs/Lab4/Lab6_Dev/#example-the-hopeless-romantic","text":"Copy the wrapper script for the plotAline() function, and modify the copy to create a plot of x = 16sin^3(t) y = 13cos(t) - 5cos(2t) - 2cos(3t) - cos(4t) for t raging from [0,2 \\Pi ] (inclusive). Label the plot and the plot axes. from matplotlib import pyplot as plt # import the plotting library import numpy as np # import NumPy: for large, multi-dimensional arrays and matrices, along with high-level mathematical functions to operate on these arrays. pi = np.pi #pi value from the np package t= np.linspace(0,2*pi,360)# the NumPy function np.linspace is similar to the range() x = 16*np.sin(t)**3 y = 13*np.cos(t) - 5*np.cos(2*t) - 2*np.cos(3*t) - np.cos(4*t) plt.plot( x, y, color ='purple', marker ='.', linestyle ='solid') plt.ylabel(\"Y-axis\")# add a label to the x and y-axes plt.xlabel(\"X-axis\") plt.axis('equal') #sets equal axis ratios plt.title(\"A Hopeless Romantic's Curve\")# add a title plt.show() # display the plot Here are some great reads on this topic: - \"Functions in Python\" available at https://www.geeksforgeeks.org/functions-in-python/ - \"Defining Your Own Python Function\" by John Sturtz available at https://realpython.com/defining-your-own-python-function/ - \"Graph Plotting in Python | Set 1\" available at https://www.geeksforgeeks.org/graph-plotting-in-python-set-1/ - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"How To Use Functions In Python (Python Tutorial #3)\" by CS Dojo available at https://www.youtube.com/watch?v=NSbOtYzIQI0 - \"Python Tutorial for Beginners 8: Functions\" by Corey Schafer available at https://www.youtube.com/watch?v=9Os0o3wzS_I - \"Python 3 Programming Tutorial - Functions\" by sentdex available at *https://www.youtube.com/watch?v=owglNL1KQf0","title":"Example- The Hopeless Romantic!"},{"location":"8-Labs/Lab4/Lab6_Dev/#exercise-a-function-for-coffee-because-coffee-is-life","text":"","title":"Exercise: A Function for Coffee. Because Coffee is life! "},{"location":"8-Labs/Lab4/Lab6_Dev/#write-a-pseudo-code-for-a-function-that-asks-for-users-preferences-on-their-coffee-eg-type-of-brew-follows-certain-steps-to-prepare-that-coffee-and-delivers-that-coffee-with-an-appropriate-message-you-can-be-as-imaginative-as-you-like-but-make-sure-to-provide-logical-justification-for-your-script","text":"","title":"Write a pseudo-code for a function that asks for user's preferences on their coffee (e.g., type of brew), follows certain steps to prepare that coffee, and delivers that coffee with an appropriate message. You can be as imaginative as you like, but make sure to provide logical justification for your script."},{"location":"8-Labs/Lab4/Lab6_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab5/Lab5_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab5 Laboratory 5: Working with Files # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 sensei /opt/jupyterhub/bin/python3 3.8.5 (default, May 27 2021, 13:30:53) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Background A computer file is a computer resource for recording data discretely (not in the secretive context, but specifically somewhere on a piece of hardware) in a computer storage device. Just as words can be written to paper, so can information be written to a computer file. Files can be edited and transferred through the internet on that particular computer system. There are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once. By using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times. Typically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access. File system In computing, a file system or filesystem, controls how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stops and the next begins. By separating the data into pieces and giving each piece a name, the data is isolated and identified. Taking its name from the way paper-based data management system is named, each group of data is called a \u201cfile\u201d. The structure and logic rules used to manage the groups of data and their names is called a \u201cfile system\u201d. Path A path, the general form of the name of a file or directory, specifies a unique location in a file system. A path points to a file system location by following the directory tree hierarchy expressed in a string of characters in which path components, separated by a delimiting character, represent each directory. The delimiting character is most commonly the slash (\u201d/\u201d), the backslash character (\u201d\\\u201d), or colon (\u201d:\u201d), though some operating systems may use a different delimiter. Paths are used extensively in computer science to represent the directory/file relationships common in modern operating systems, and are essential in the construction of Uniform Resource Locators (URLs). Resources can be represented by either absolute or relative paths. As an example consider the following two files: /Users/theodore/MyGit/@atomickitty/hurri-sensors/.git/Guest.conf /etc/apache2/users/Guest.conf They both have the same file name, but are located on different paths. Failure to provide the path when addressing the file can be a problem. Another way to interpret is that the two unique files actually have different names, and only part of those names is common (Guest.conf) The two names above (including the path) are called fully qualified filenames (or absolute names), a relative path (usually relative to the file or program of interest depends on where in the directory structure the file lives. If we are currently in the .git directory (the first file) the path to the file is just the filename. We have experienced path issues with dependencies on .png files - in general your JupyterLab notebooks on CoCalc can only look at the local directory which is why we have to copy files into the directory for things to work. File Types Text Files. Text files are regular files that contain information readable by the user. This information is stored in ASCII. You can display and print these files. The lines of a text file must not contain NULL characters, and none can exceed a prescribed (by architecture) length, including the new-line character. The term text file does not prevent the inclusion of control or other nonprintable characters (other than NUL). Therefore, standard utilities that list text files as inputs or outputs are either able to process the special characters gracefully or they explicitly describe their limitations within their individual sections. Binary Files. Binary files are regular files that contain information readable by the computer. Binary files may be executable files that instruct the system to accomplish a job. Commands and programs are stored in executable, binary files. Special compiling programs translate ASCII text into binary code. The only difference between text and binary files is that text files have lines of less than some length, with no NULL characters, each terminated by a new-line character. Directory Files. Directory files contain information the system needs to access all types of files, but they do not contain the actual file data. As a result, directories occupy less space than a regular file and give the file system structure flexibility and depth. Each directory entry represents either a file or a subdirectory. Each entry contains the name of the file and the file's index node reference number (i-node). The i-node points to the unique index node assigned to the file. The i-node describes the location of the data associated with the file. Directories are created and controlled by a separate set of commands. File Manipulation For this laboratory we will learn just a handfull of file manipulations which are quite useful. Files can be \"created\",\"read\",\"updated\", or \"deleted\" (CRUD). Example: Create a file, write to it. Below is an example of creating a file that does not yet exist. The script is a bit pendandic on purpose. First will use some system commands to view the contents of the local directory import sys ! del myfirstfile.txt # delete file if it exists, Use rm -f on Mac %pwd # list name of working directory, note it includes path, so it is an absolute path 'C:\\\\Users\\\\Farha' # create file example externalfile = open(\"myfirstfile.txt\",'w') # create connection to file, set to write (w), file does not need to exist mymessage = 'message in a bottle' #some object to write, in this case a string externalfile.write(mymessage)# write the contents of mymessage to the file externalfile.close() # close the file connection At this point our new file should exist, lets list the directory and see if that is so Sure enough, its there, we will use a bash command cat to look at the contents of the file. ! type myfirstfile.txt message in a bottle Thats about it, use of system commands, of course depends on the system, the examples above should work OK on CoCalc or a Macintosh; on Winderz the shell commands are a little different. If you have the linux subsystem installed then these should work as is. Example: Read from an existing file. We will continue using the file we just made, and read from it the example is below # read file example externalfile = open(\"myfirstfile.txt\",'r') # create connection to file, set to read (r), file must exist silly_string = externalfile.read() # read the contents externalfile.close() # close the file connection print(silly_string) message in a bottle Example: Update a file. This example continues with our same file, but we will now add contents without destroying existing contents. The keyword is append externalfile = open(\"myfirstfile.txt\",'a') # create connection to file, set to append (a), file does not need to exist externalfile.write('\\n') # adds a newline character what_to_add = 'I love rock-and-roll, put another dime in the jukebox baby ... \\n' externalfile.write(what_to_add) # add a string including the linefeed what_to_add = '... the waiting is the hardest part \\n' externalfile.write(what_to_add) # add a string including the linefeed mylist = [1,2,3,4,5] # a list of numbers what_to_add = ','.join(map(repr, mylist)) + \"\\n\" # one way to write the list externalfile.write(what_to_add) what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" # another way to write the list externalfile.write(what_to_add) externalfile.close() As before we can examine the contents using a shell command sent from the notebook. ! type myfirstfile.txt message in a bottle I love rock-and-roll, put another dime in the jukebox baby ... ... the waiting is the hardest part 1,2,3,4,5 1,2,3,4,5 A little discussion on the part where we wrote numbers what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" Here are descriptions of the two functions map and repr map(function, iterable, ...) Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list. repr(object) Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse quotes). It is sometimes useful to be able to access this operation as an ordinary function. For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() , otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. A class can control what this function returns for its instances by defining a repr() method. What they do in this script is important. The statement: what_to_add = \u2019,\u2019.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" is building a string that will be comprised of elements of mylist[0:len(mylist)]. The repr() function gets these elements as they are represented in the computer, the delimiter a comma is added using the join method in Python, and because everything is now a string the ... + \"\\n\" puts a linefeed character at the end of the string so the output will start a new line the next time something is written. Example: Delete a file Delete can be done by a system call as we did above to clear the local directory In a JupyterLab notebook, we can either use import sys ! del myfirstfile.txt # delete file if it exists, Use rm -f on Mac or import os os.remove(\"myfirstfile.txt\") they both have same effect, both equally dangerous to your filesystem. Learn more about CRUD with text files at https://www.guru99.com/reading-and-writing-files-in-python.html Learn more about file delete at https://www.dummies.com/programming/python/how-to-delete-a-file-in-python/ # import os file2kill = \"myfirstfile.txt\" try: os.remove(file2kill) # file must exist or will generate an exception except: pass # example of using pass to improve readability print(file2kill, \" missing or deleted !\") myfirstfile.txt missing or deleted ! Here are some great reads on this topic: - \"Python Classes and Objects\" available at https://www.geeksforgeeks.org/python-classes-and-objects/ - \"Object-Oriented Programming (OOP) in Python 3\" by David Amos available at https://realpython.com/python3-object-oriented-programming/ - \"Python File Operations \u2013 Read and Write to files with Python\" available at *https://www.journaldev.com/14408/python-read-file-open-write-delete-copy Here are some great videos on these topics: - \"Python OOP Tutorial 1: Classes and Instances\" by Corey Schafer available at https://www.youtube.com/watch?v=ZDa-Z5JzLYM - \"Python Object Oriented Programming (OOP) - For Beginners\" by Tech With Tim available at https://www.youtube.com/watch?v=JeznW_7DlB0 - \"Python Classes and Objects || Python Tutorial || Learn Python Programming\" by Socratica available at https://www.youtube.com/watch?v=apACNr7DC_s - \"Python Tutorial: File Objects - Reading and Writing to Files\" by Corey Schafer available at https://www.youtube.com/watch?v=Uh2ebFW8OYM Exercise: Your Favorite Quotation! create a text file, name it \"MyFavoriteQuotation\" . Write your favorite quotation in the file. Read the file. Add this string to it in a new line : \"And that's something I wish I had said...\" Show the final outcome. # create the \"My Favorite Quotation\" file: externalfile = open(\"MyFavoriteQuotation.txt\",'w') # create connection to file, set to write (w) myquotation = 'The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.' #My choice: quotation from Pulp Fiction externalfile.write(myquotation)# write the contents of mymessage to the file externalfile.close() # close the file connection #Let's read the file #! type MyFavoriteQuotation.txt # Let's add the string externalfile = open(\"MyFavoriteQuotation.txt\",'a') #create connection to file, set to append (a) externalfile.write('\\n') # adds a newline character what_to_add = \"And that's something I wish I had said ... \\n\" externalfile.write(what_to_add) externalfile.close() #Let's read the file one last time ! type MyFavoriteQuotation.txt The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you. And that's something I wish I had said ...","title":"Lab5 Dev"},{"location":"8-Labs/Lab5/Lab5_Dev/#laboratory-5-working-with-files","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 sensei /opt/jupyterhub/bin/python3 3.8.5 (default, May 27 2021, 13:30:53) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Laboratory 5: Working with Files "},{"location":"8-Labs/Lab5/Lab5_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab5/Lab5_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab5/Lab5_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab5/Lab5_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab5/Lab5_Dev/#background","text":"A computer file is a computer resource for recording data discretely (not in the secretive context, but specifically somewhere on a piece of hardware) in a computer storage device. Just as words can be written to paper, so can information be written to a computer file. Files can be edited and transferred through the internet on that particular computer system. There are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once. By using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times. Typically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access.","title":"Background"},{"location":"8-Labs/Lab5/Lab5_Dev/#file-system","text":"In computing, a file system or filesystem, controls how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stops and the next begins. By separating the data into pieces and giving each piece a name, the data is isolated and identified. Taking its name from the way paper-based data management system is named, each group of data is called a \u201cfile\u201d. The structure and logic rules used to manage the groups of data and their names is called a \u201cfile system\u201d.","title":"File system"},{"location":"8-Labs/Lab5/Lab5_Dev/#path","text":"A path, the general form of the name of a file or directory, specifies a unique location in a file system. A path points to a file system location by following the directory tree hierarchy expressed in a string of characters in which path components, separated by a delimiting character, represent each directory. The delimiting character is most commonly the slash (\u201d/\u201d), the backslash character (\u201d\\\u201d), or colon (\u201d:\u201d), though some operating systems may use a different delimiter. Paths are used extensively in computer science to represent the directory/file relationships common in modern operating systems, and are essential in the construction of Uniform Resource Locators (URLs). Resources can be represented by either absolute or relative paths. As an example consider the following two files: /Users/theodore/MyGit/@atomickitty/hurri-sensors/.git/Guest.conf /etc/apache2/users/Guest.conf They both have the same file name, but are located on different paths. Failure to provide the path when addressing the file can be a problem. Another way to interpret is that the two unique files actually have different names, and only part of those names is common (Guest.conf) The two names above (including the path) are called fully qualified filenames (or absolute names), a relative path (usually relative to the file or program of interest depends on where in the directory structure the file lives. If we are currently in the .git directory (the first file) the path to the file is just the filename. We have experienced path issues with dependencies on .png files - in general your JupyterLab notebooks on CoCalc can only look at the local directory which is why we have to copy files into the directory for things to work.","title":"Path"},{"location":"8-Labs/Lab5/Lab5_Dev/#file-types","text":"Text Files. Text files are regular files that contain information readable by the user. This information is stored in ASCII. You can display and print these files. The lines of a text file must not contain NULL characters, and none can exceed a prescribed (by architecture) length, including the new-line character. The term text file does not prevent the inclusion of control or other nonprintable characters (other than NUL). Therefore, standard utilities that list text files as inputs or outputs are either able to process the special characters gracefully or they explicitly describe their limitations within their individual sections. Binary Files. Binary files are regular files that contain information readable by the computer. Binary files may be executable files that instruct the system to accomplish a job. Commands and programs are stored in executable, binary files. Special compiling programs translate ASCII text into binary code. The only difference between text and binary files is that text files have lines of less than some length, with no NULL characters, each terminated by a new-line character. Directory Files. Directory files contain information the system needs to access all types of files, but they do not contain the actual file data. As a result, directories occupy less space than a regular file and give the file system structure flexibility and depth. Each directory entry represents either a file or a subdirectory. Each entry contains the name of the file and the file's index node reference number (i-node). The i-node points to the unique index node assigned to the file. The i-node describes the location of the data associated with the file. Directories are created and controlled by a separate set of commands.","title":"File Types"},{"location":"8-Labs/Lab5/Lab5_Dev/#file-manipulation","text":"For this laboratory we will learn just a handfull of file manipulations which are quite useful. Files can be \"created\",\"read\",\"updated\", or \"deleted\" (CRUD).","title":"File Manipulation"},{"location":"8-Labs/Lab5/Lab5_Dev/#example-create-a-file-write-to-it","text":"Below is an example of creating a file that does not yet exist. The script is a bit pendandic on purpose. First will use some system commands to view the contents of the local directory import sys ! del myfirstfile.txt # delete file if it exists, Use rm -f on Mac %pwd # list name of working directory, note it includes path, so it is an absolute path 'C:\\\\Users\\\\Farha' # create file example externalfile = open(\"myfirstfile.txt\",'w') # create connection to file, set to write (w), file does not need to exist mymessage = 'message in a bottle' #some object to write, in this case a string externalfile.write(mymessage)# write the contents of mymessage to the file externalfile.close() # close the file connection At this point our new file should exist, lets list the directory and see if that is so Sure enough, its there, we will use a bash command cat to look at the contents of the file. ! type myfirstfile.txt message in a bottle Thats about it, use of system commands, of course depends on the system, the examples above should work OK on CoCalc or a Macintosh; on Winderz the shell commands are a little different. If you have the linux subsystem installed then these should work as is.","title":"Example: Create a file, write to it."},{"location":"8-Labs/Lab5/Lab5_Dev/#example-read-from-an-existing-file","text":"We will continue using the file we just made, and read from it the example is below # read file example externalfile = open(\"myfirstfile.txt\",'r') # create connection to file, set to read (r), file must exist silly_string = externalfile.read() # read the contents externalfile.close() # close the file connection print(silly_string) message in a bottle","title":"Example: Read from an existing file."},{"location":"8-Labs/Lab5/Lab5_Dev/#example-update-a-file","text":"This example continues with our same file, but we will now add contents without destroying existing contents. The keyword is append externalfile = open(\"myfirstfile.txt\",'a') # create connection to file, set to append (a), file does not need to exist externalfile.write('\\n') # adds a newline character what_to_add = 'I love rock-and-roll, put another dime in the jukebox baby ... \\n' externalfile.write(what_to_add) # add a string including the linefeed what_to_add = '... the waiting is the hardest part \\n' externalfile.write(what_to_add) # add a string including the linefeed mylist = [1,2,3,4,5] # a list of numbers what_to_add = ','.join(map(repr, mylist)) + \"\\n\" # one way to write the list externalfile.write(what_to_add) what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" # another way to write the list externalfile.write(what_to_add) externalfile.close() As before we can examine the contents using a shell command sent from the notebook. ! type myfirstfile.txt message in a bottle I love rock-and-roll, put another dime in the jukebox baby ... ... the waiting is the hardest part 1,2,3,4,5 1,2,3,4,5 A little discussion on the part where we wrote numbers what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" Here are descriptions of the two functions map and repr map(function, iterable, ...) Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list. repr(object) Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse quotes). It is sometimes useful to be able to access this operation as an ordinary function. For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() , otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. A class can control what this function returns for its instances by defining a repr() method. What they do in this script is important. The statement: what_to_add = \u2019,\u2019.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" is building a string that will be comprised of elements of mylist[0:len(mylist)]. The repr() function gets these elements as they are represented in the computer, the delimiter a comma is added using the join method in Python, and because everything is now a string the ... + \"\\n\" puts a linefeed character at the end of the string so the output will start a new line the next time something is written.","title":"Example: Update a file."},{"location":"8-Labs/Lab5/Lab5_Dev/#example-delete-a-file","text":"Delete can be done by a system call as we did above to clear the local directory In a JupyterLab notebook, we can either use import sys ! del myfirstfile.txt # delete file if it exists, Use rm -f on Mac or import os os.remove(\"myfirstfile.txt\") they both have same effect, both equally dangerous to your filesystem. Learn more about CRUD with text files at https://www.guru99.com/reading-and-writing-files-in-python.html Learn more about file delete at https://www.dummies.com/programming/python/how-to-delete-a-file-in-python/ # import os file2kill = \"myfirstfile.txt\" try: os.remove(file2kill) # file must exist or will generate an exception except: pass # example of using pass to improve readability print(file2kill, \" missing or deleted !\") myfirstfile.txt missing or deleted ! Here are some great reads on this topic: - \"Python Classes and Objects\" available at https://www.geeksforgeeks.org/python-classes-and-objects/ - \"Object-Oriented Programming (OOP) in Python 3\" by David Amos available at https://realpython.com/python3-object-oriented-programming/ - \"Python File Operations \u2013 Read and Write to files with Python\" available at *https://www.journaldev.com/14408/python-read-file-open-write-delete-copy Here are some great videos on these topics: - \"Python OOP Tutorial 1: Classes and Instances\" by Corey Schafer available at https://www.youtube.com/watch?v=ZDa-Z5JzLYM - \"Python Object Oriented Programming (OOP) - For Beginners\" by Tech With Tim available at https://www.youtube.com/watch?v=JeznW_7DlB0 - \"Python Classes and Objects || Python Tutorial || Learn Python Programming\" by Socratica available at https://www.youtube.com/watch?v=apACNr7DC_s - \"Python Tutorial: File Objects - Reading and Writing to Files\" by Corey Schafer available at https://www.youtube.com/watch?v=Uh2ebFW8OYM","title":"Example: Delete a file"},{"location":"8-Labs/Lab5/Lab5_Dev/#exercise-your-favorite-quotation","text":"create a text file, name it \"MyFavoriteQuotation\" . Write your favorite quotation in the file. Read the file. Add this string to it in a new line : \"And that's something I wish I had said...\" Show the final outcome. # create the \"My Favorite Quotation\" file: externalfile = open(\"MyFavoriteQuotation.txt\",'w') # create connection to file, set to write (w) myquotation = 'The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.' #My choice: quotation from Pulp Fiction externalfile.write(myquotation)# write the contents of mymessage to the file externalfile.close() # close the file connection #Let's read the file #! type MyFavoriteQuotation.txt # Let's add the string externalfile = open(\"MyFavoriteQuotation.txt\",'a') #create connection to file, set to append (a) externalfile.write('\\n') # adds a newline character what_to_add = \"And that's something I wish I had said ... \\n\" externalfile.write(what_to_add) externalfile.close() #Let's read the file one last time ! type MyFavoriteQuotation.txt The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you. And that's something I wish I had said ...","title":"Exercise: Your Favorite Quotation! "},{"location":"8-Labs/Lab6/Lab6_Dev/","text":"Laboratory 6: Numpy for Bread! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Numpy Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is short for \u201cNumeric Python\u201d or \u201cNumerical Python\u201d. If you are curious about NumPy, this cheat sheet is recommended: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf Arrays A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension. In other words, an array contains information about the raw data, how to locate an element and how to interpret an element.To make a numpy array, you can just use the np.array() function. All you need to do is pass a list to it. Don\u2019t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. If you want to read more about the differences between a Python list and NumPy array, this link is recommended: https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference Example- 1D Arrays Let's create a 1D array from the 2000s (2000-2009): import numpy as np #First, we need to impoty \"numpy\" mylist = [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009] #Create a list of the years print(mylist) #Check how it looks np.array(mylist) #Define it as a numpy array [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009] array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]) Example- n-Dimensional Arrays Let's create a 5x2 array from the 2000s (2000-2009): myotherlist = [[2000,2001],[2002,2003],[2004,2005],[2006,2007],[2008,2009]] #Since I want a 5x2 array, I should group the years two by two print(myotherlist) #See how it looks as a list np.array(myotherlist) #See how it looks as a numpy array [[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]] array([[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]]) Arrays Arithmetic Once you have created the arrays, you can do basic Numpy operations. Numpy offers a variety of operations applicable on arrays. From basic operations such as summation, subtraction, multiplication and division to more advanced and essential operations such as matrix multiplication and other elementwise operations. In the examples below, we will go over some of these: Example- 1D Array Arithmetic Define a 1D array with [0,12,24,36,48,60,72,84,96] Multiple all elements by 2 Take all elements to the power of 2 Find the maximum value of the array and its position Find the minimum value of the array and its position Define another 1D array with [-12,0,12,24,36,48,60,72,84] Find the summation and subtraction of these two arrays Find the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([0,12,24,36,48,60,72,84,96]) #Step1: Define Array1 print(Array1) print(Array1*2) #Step2: Multiple all elements by 2 print(Array1**2) #Step3: Take all elements to the power of 2 print(np.power(Array1,2)) #Another way to do the same thing, by using a function in numpy print(np.max(Array1)) #Step4: Find the maximum value of the array print(np.argmax(Array1)) ##Step4: Find the postition of the maximum value print(np.min(Array1)) #Step5: Find the minimum value of the array print(np.argmin(Array1)) ##Step5: Find the postition of the minimum value Array2 = np.array([-12,0,12,24,36,48,60,72,84]) #Step6: Define Array2 print(Array2) print(Array1+Array2) #Step7: Find the summation of these two arrays print(Array1-Array2) #Step7: Find the subtraction of these two arrays print(Array1*Array2) #Step8: Find the multiplication of these two arrays [ 0 12 24 36 48 60 72 84 96] [ 0 24 48 72 96 120 144 168 192] [ 0 144 576 1296 2304 3600 5184 7056 9216] [ 0 144 576 1296 2304 3600 5184 7056 9216] 96 8 0 0 [-12 0 12 24 36 48 60 72 84] [-12 12 36 60 84 108 132 156 180] [12 12 12 12 12 12 12 12 12] [ 0 0 288 864 1728 2880 4320 6048 8064] Example- n-Dimensional Array Arithmetic Define a 2x2 array with [5,10,15,20] Define another 2x2 array with [3,6,9,12] Find the summation and subtraction of these two arrays Find the minimum number in the multiplication of these two arrays Find the position of the maximum in the multiplication of these two arrays Find the mean of the multiplication of these two arrays Find the mean of the first row of the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([[5,10],[15,20]]) #Step1: Define Array1 print(Array1) Array2 = np.array([[3,6],[9,12]]) #Step2: Define Array2 print(Array2) print(Array1+Array2) #Step3: Find the summation print(Array1-Array2) #Step3: Find the subtraction MultArray = Array1@Array2 #Step4: To perform a typical matrix multiplication (or matrix product) MultArray1 = Array1.dot(Array2) #Step4: Another way To perform a matrix multiplication print(MultArray) print(MultArray1) print(np.min(MultArray)) #Step4: Find the minimum value of the multiplication print(np.argmax(MultArray)) ##Step5: Find the postition of the maximum value print(np.mean(MultArray)) ##Step6: Find the mean of the multiplication of these two arrays print(np.mean(MultArray[0,:])) ##Step7: Find the mean of the first row of the multiplication of these two arrays [[ 5 10] [15 20]] [[ 3 6] [ 9 12]] [[ 8 16] [24 32]] [[2 4] [6 8]] [[105 150] [225 330]] [[105 150] [225 330]] 105 3 202.5 127.5 Arrays Comparison Comparing two NumPy arrays determines whether they are equivalent by checking if every element at each corresponding index are the same. Example- 1D Array Comparison Define a 1D array with [1.0,2.5,3.4,7,7] Define another 1D array with [5.0/5.0,5.0/2,6.8/2,21/3,14/2] Compare and see if the two arrays are equal Define another 1D array with [6,1.4,2.2,7.5,7] Compare and see if the first array is greater than or equal to the third array import numpy as np #import numpy Array1 = np.array([1.0,2.5,3.4,7,7]) #Step1: Define Array1 print(Array1) Array2 = np.array([5.0/5.0,5.0/2,6.8/2,21/3,14/2]) #Step2: Define Array1 print(Array2) print(np.equal(Array1, Array2)) #Step3: Compare and see if the two arrays are equal Array3 = np.array([6,1.4,2.2,7.5,7]) #Step4: Define Array3 print(Array3) print(np.greater_equal(Array1, Array3)) #Step3: Compare and see if the two arrays are equal [1. 2.5 3.4 7. 7. ] [1. 2.5 3.4 7. 7. ] [ True True True True True] [6. 1.4 2.2 7.5 7. ] [False True True False True] Arrays Manipulation numpy.copy() allows us to create a copy of an array. This is particularly useful when we need to manipulate an array while keeping an original copy in memory. The numpy.delete() function returns a new array with sub-arrays along an axis deleted. Let's have a look at the examples. Example- Copying and Deleting Arrays and Elements Define a 1D array, named \"x\" with [1,2,3] Define \"y\" so that \"y=x\" Define \"z\" as a copy of \"x\" Discuss the difference between y and z Delete the second element of x import numpy as np #import numpy x = np.array([1,2,3]) #Step1: Define x print(x) y = x #Step2: Define y as y=x print(y) z = np.copy(x) #Step3: Define z as a copy of x print(z) # For Step4: They look similar but check this out: x[1] = 8 # If we change x ... print(x) print(y) print(z) # By modifying x, y changes but z remains as a copy of the initial version of x. x = np.delete(x, 1) #Step5: Delete the second element of x print(x) [1 2 3] [1 2 3] [1 2 3] [1 8 3] [1 8 3] [1 2 3] [1 3] Sorting Arrays Sorting means putting elements in an ordered sequence. Ordered sequence is any sequence that has an order corresponding to elements, like numeric or alphabetical, ascending or descending. If you use the sort() method on a 2-D array, both arrays will be sorted. Example- Sorting 1D Arrays Define a 1D array as ['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed'] and print it out. Then, sort the array alphabetically. import numpy as np #import numpy games = np.array(['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed']) print(games) print(np.sort(games)) ['FIFA 2020' 'Red Dead Redemption' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed'] ['FIFA 2020' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed' 'Red Dead Redemption'] Example- Sorting n-Dimensional Arrays Define a 3x3 array with 17,-6,2,86,-12,0,0,23,12 and print it out. Then, sort the array. import numpy as np #import numpy a = np.array([[17,-6,2],[86,-12,0],[0,23,12]]) print(a) print (\"Along columns : \\n\", np.sort(a,axis = 0) ) #This will be sorting in each column print (\"Along rows : \\n\", np.sort(a,axis = 1) ) #This will be sorting in each row print (\"Sorting by default : \\n\", np.sort(a) ) #Same as above print (\"Along None Axis : \\n\", np.sort(a,axis = None) ) #This will be sorted like a 1D array [[ 17 -6 2] [ 86 -12 0] [ 0 23 12]] Along columns : [[ 0 -12 0] [ 17 -6 2] [ 86 23 12]] Along rows : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Sorting by default : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Along None Axis : [-12 -6 0 0 2 12 17 23 86] Partitioning (Slice) Arrays Slicing in python means taking elements from one given index to another given index. We can do slicing like this: [start:end]. We can also define the step, like this: [start:end:step]. If we don't pass start its considered 0 If we don't pass end its considered length of array in that dimension If we don't pass step its considered 1 Example- Slicing 1D Arrays Define a 1D array as [1,3,5,7,9], slice out the [3,5,7] and print it out. import numpy as np #import numpy a = np.array([1,3,5,7,9]) #Define the array print(a) aslice = a[1:4] #slice the [3,5,7] print(aslice) #print it out [1 3 5 7 9] [3 5 7] Example- Slicing n-Dimensional Arrays Define a 5x5 array with \"Superman, Batman, Jim Hammond, Captain America, Green Arrow, Aquaman, Wonder Woman, Martian Manhunter, Barry Allen, Hal Jordan, Hawkman, Ray Palmer, Spider Man, Thor, Hank Pym, Solar, Iron Man, Dr. Strange, Daredevil, Ted Kord, Captian Marvel, Black Panther, Wolverine, Booster Gold, Spawn \" and print it out. Then: - Slice the first column and print it out - Slice the third row and print it out - Slice 'Wolverine' and print it out - Slice a 3x3 array with 'Wonder Woman, Ray Palmer, Iron Man, Martian Manhunter, Spider Man, Dr. Strange, Barry Allen, Thor, Daredevil' import numpy as np #import numpy Superheroes = np.array([['Superman', 'Batman', 'Jim Hammond', 'Captain America', 'Green Arrow'], ['Aquaman', 'Wonder Woman', 'Martian Manhunter', 'Barry Allen', 'Hal Jordan'], ['Hawkman', 'Ray Palmer', 'Spider Man', 'Thor', 'Hank Pym'], ['Solar', 'Iron Man', 'Dr. Strange', 'Daredevil', 'Ted Kord'], ['Captian Marvel', 'Black Panther', 'Wolverine', 'Booster Gold', 'Spawn']]) print(Superheroes) #Step1 print(Superheroes[:,0]) print(Superheroes[2,:]) print(Superheroes[4,2]) print(Superheroes[1:4,1:4]) [['Superman' 'Batman' 'Jim Hammond' 'Captain America' 'Green Arrow'] ['Aquaman' 'Wonder Woman' 'Martian Manhunter' 'Barry Allen' 'Hal Jordan'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] ['Solar' 'Iron Man' 'Dr. Strange' 'Daredevil' 'Ted Kord'] ['Captian Marvel' 'Black Panther' 'Wolverine' 'Booster Gold' 'Spawn']] ['Superman' 'Aquaman' 'Hawkman' 'Solar' 'Captian Marvel'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] Wolverine [['Wonder Woman' 'Martian Manhunter' 'Barry Allen'] ['Ray Palmer' 'Spider Man' 'Thor'] ['Iron Man' 'Dr. Strange' 'Daredevil']] This is a Numpy Cheat Sheet- similar to the one you had on top of this notebook! Check out this link for more: https://blog.finxter.com/collection-10-best-numpy-cheat-sheets-every-python-coder-must-own/ Here are some of the resources used for creating this notebook: - Johnson, J. (2020). Python Numpy Tutorial (with Jupyter and Colab). Retrieved September 15, 2020, from https://cs231n.github.io/python-numpy-tutorial/ - Willems, K. (2019). (Tutorial) Python NUMPY Array TUTORIAL. Retrieved September 15, 2020, from https://www.datacamp.com/community/tutorials/python-numpy-tutorial?utm_source=adwords_ppc - Willems, K. (2017). NumPy Cheat Sheet: Data Analysis in Python. Retrieved September 15, 2020, from https://www.datacamp.com/community/blog/python-numpy-cheat-sheet - W3resource. (2020). NumPy: Compare two given arrays. Retrieved September 15, 2020, from https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-28.php Here are some great reads on this topic: - \"Python NumPy Tutorial\" available at https://www.geeksforgeeks.org/python-numpy-tutorial/ - \"What Is NumPy?\" a collection of blogs, available at https://realpython.com/tutorials/numpy/ - \"Look Ma, No For-Loops: Array Programming With NumPy\" by Brad Solomon available at https://realpython.com/numpy-array-programming/ - \"The Ultimate Beginner\u2019s Guide to NumPy\" by Anne Bonner available at https://towardsdatascience.com/the-ultimate-beginners-guide-to-numpy-f5a2f99aef54 Here are some great videos on these topics: - \"Learn NUMPY in 5 minutes - BEST Python Library!\" by Python Programmer available at https://www.youtube.com/watch?v=xECXZ3tyONo - \"Python NumPy Tutorial for Beginners\" by freeCodeCamp.org available at https://www.youtube.com/watch?v=QUT1VHiLmmI - \"Complete Python NumPy Tutorial (Creating Arrays, Indexing, Math, Statistics, Reshaping)\" by Keith Galli available at https://www.youtube.com/watch?v=GB9ByFAIAH4 - \"Python NumPy Tutorial | NumPy Array | Python Tutorial For Beginners | Python Training | Edureka\" by edureka! available at https://www.youtube.com/watch?v=8JfDAm9y_7s Exercise: Python List vs. Numpy Arrays? What are some differences between Python lists and Numpy arrays? * Make sure to cite any resources that you may use.","title":"<font color=darkred>Laboratory 6: Numpy for Bread! </font>"},{"location":"8-Labs/Lab6/Lab6_Dev/#laboratory-6-numpy-for-bread","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 6: Numpy for Bread! "},{"location":"8-Labs/Lab6/Lab6_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab6/Lab6_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab6/Lab6_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab6/Lab6_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab6/Lab6_Dev/#numpy","text":"Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is short for \u201cNumeric Python\u201d or \u201cNumerical Python\u201d. If you are curious about NumPy, this cheat sheet is recommended: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf","title":"Numpy"},{"location":"8-Labs/Lab6/Lab6_Dev/#arrays","text":"A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension. In other words, an array contains information about the raw data, how to locate an element and how to interpret an element.To make a numpy array, you can just use the np.array() function. All you need to do is pass a list to it. Don\u2019t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. If you want to read more about the differences between a Python list and NumPy array, this link is recommended: https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference","title":"Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-1d-arrays","text":"Let's create a 1D array from the 2000s (2000-2009): import numpy as np #First, we need to impoty \"numpy\" mylist = [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009] #Create a list of the years print(mylist) #Check how it looks np.array(mylist) #Define it as a numpy array [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009] array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009])","title":"Example- 1D Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-n-dimensional-arrays","text":"Let's create a 5x2 array from the 2000s (2000-2009): myotherlist = [[2000,2001],[2002,2003],[2004,2005],[2006,2007],[2008,2009]] #Since I want a 5x2 array, I should group the years two by two print(myotherlist) #See how it looks as a list np.array(myotherlist) #See how it looks as a numpy array [[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]] array([[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]])","title":"Example- n-Dimensional Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#arrays-arithmetic","text":"Once you have created the arrays, you can do basic Numpy operations. Numpy offers a variety of operations applicable on arrays. From basic operations such as summation, subtraction, multiplication and division to more advanced and essential operations such as matrix multiplication and other elementwise operations. In the examples below, we will go over some of these:","title":"Arrays Arithmetic"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-1d-array-arithmetic","text":"Define a 1D array with [0,12,24,36,48,60,72,84,96] Multiple all elements by 2 Take all elements to the power of 2 Find the maximum value of the array and its position Find the minimum value of the array and its position Define another 1D array with [-12,0,12,24,36,48,60,72,84] Find the summation and subtraction of these two arrays Find the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([0,12,24,36,48,60,72,84,96]) #Step1: Define Array1 print(Array1) print(Array1*2) #Step2: Multiple all elements by 2 print(Array1**2) #Step3: Take all elements to the power of 2 print(np.power(Array1,2)) #Another way to do the same thing, by using a function in numpy print(np.max(Array1)) #Step4: Find the maximum value of the array print(np.argmax(Array1)) ##Step4: Find the postition of the maximum value print(np.min(Array1)) #Step5: Find the minimum value of the array print(np.argmin(Array1)) ##Step5: Find the postition of the minimum value Array2 = np.array([-12,0,12,24,36,48,60,72,84]) #Step6: Define Array2 print(Array2) print(Array1+Array2) #Step7: Find the summation of these two arrays print(Array1-Array2) #Step7: Find the subtraction of these two arrays print(Array1*Array2) #Step8: Find the multiplication of these two arrays [ 0 12 24 36 48 60 72 84 96] [ 0 24 48 72 96 120 144 168 192] [ 0 144 576 1296 2304 3600 5184 7056 9216] [ 0 144 576 1296 2304 3600 5184 7056 9216] 96 8 0 0 [-12 0 12 24 36 48 60 72 84] [-12 12 36 60 84 108 132 156 180] [12 12 12 12 12 12 12 12 12] [ 0 0 288 864 1728 2880 4320 6048 8064]","title":"Example- 1D Array Arithmetic"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-n-dimensional-array-arithmetic","text":"Define a 2x2 array with [5,10,15,20] Define another 2x2 array with [3,6,9,12] Find the summation and subtraction of these two arrays Find the minimum number in the multiplication of these two arrays Find the position of the maximum in the multiplication of these two arrays Find the mean of the multiplication of these two arrays Find the mean of the first row of the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([[5,10],[15,20]]) #Step1: Define Array1 print(Array1) Array2 = np.array([[3,6],[9,12]]) #Step2: Define Array2 print(Array2) print(Array1+Array2) #Step3: Find the summation print(Array1-Array2) #Step3: Find the subtraction MultArray = Array1@Array2 #Step4: To perform a typical matrix multiplication (or matrix product) MultArray1 = Array1.dot(Array2) #Step4: Another way To perform a matrix multiplication print(MultArray) print(MultArray1) print(np.min(MultArray)) #Step4: Find the minimum value of the multiplication print(np.argmax(MultArray)) ##Step5: Find the postition of the maximum value print(np.mean(MultArray)) ##Step6: Find the mean of the multiplication of these two arrays print(np.mean(MultArray[0,:])) ##Step7: Find the mean of the first row of the multiplication of these two arrays [[ 5 10] [15 20]] [[ 3 6] [ 9 12]] [[ 8 16] [24 32]] [[2 4] [6 8]] [[105 150] [225 330]] [[105 150] [225 330]] 105 3 202.5 127.5","title":"Example- n-Dimensional Array Arithmetic"},{"location":"8-Labs/Lab6/Lab6_Dev/#arrays-comparison","text":"Comparing two NumPy arrays determines whether they are equivalent by checking if every element at each corresponding index are the same.","title":"Arrays Comparison"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-1d-array-comparison","text":"Define a 1D array with [1.0,2.5,3.4,7,7] Define another 1D array with [5.0/5.0,5.0/2,6.8/2,21/3,14/2] Compare and see if the two arrays are equal Define another 1D array with [6,1.4,2.2,7.5,7] Compare and see if the first array is greater than or equal to the third array import numpy as np #import numpy Array1 = np.array([1.0,2.5,3.4,7,7]) #Step1: Define Array1 print(Array1) Array2 = np.array([5.0/5.0,5.0/2,6.8/2,21/3,14/2]) #Step2: Define Array1 print(Array2) print(np.equal(Array1, Array2)) #Step3: Compare and see if the two arrays are equal Array3 = np.array([6,1.4,2.2,7.5,7]) #Step4: Define Array3 print(Array3) print(np.greater_equal(Array1, Array3)) #Step3: Compare and see if the two arrays are equal [1. 2.5 3.4 7. 7. ] [1. 2.5 3.4 7. 7. ] [ True True True True True] [6. 1.4 2.2 7.5 7. ] [False True True False True]","title":"Example- 1D Array Comparison"},{"location":"8-Labs/Lab6/Lab6_Dev/#arrays-manipulation","text":"numpy.copy() allows us to create a copy of an array. This is particularly useful when we need to manipulate an array while keeping an original copy in memory. The numpy.delete() function returns a new array with sub-arrays along an axis deleted. Let's have a look at the examples.","title":"Arrays Manipulation"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-copying-and-deleting-arrays-and-elements","text":"Define a 1D array, named \"x\" with [1,2,3] Define \"y\" so that \"y=x\" Define \"z\" as a copy of \"x\" Discuss the difference between y and z Delete the second element of x import numpy as np #import numpy x = np.array([1,2,3]) #Step1: Define x print(x) y = x #Step2: Define y as y=x print(y) z = np.copy(x) #Step3: Define z as a copy of x print(z) # For Step4: They look similar but check this out: x[1] = 8 # If we change x ... print(x) print(y) print(z) # By modifying x, y changes but z remains as a copy of the initial version of x. x = np.delete(x, 1) #Step5: Delete the second element of x print(x) [1 2 3] [1 2 3] [1 2 3] [1 8 3] [1 8 3] [1 2 3] [1 3]","title":"Example- Copying and Deleting Arrays and Elements"},{"location":"8-Labs/Lab6/Lab6_Dev/#sorting-arrays","text":"Sorting means putting elements in an ordered sequence. Ordered sequence is any sequence that has an order corresponding to elements, like numeric or alphabetical, ascending or descending. If you use the sort() method on a 2-D array, both arrays will be sorted.","title":"Sorting Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-sorting-1d-arrays","text":"Define a 1D array as ['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed'] and print it out. Then, sort the array alphabetically. import numpy as np #import numpy games = np.array(['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed']) print(games) print(np.sort(games)) ['FIFA 2020' 'Red Dead Redemption' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed'] ['FIFA 2020' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed' 'Red Dead Redemption']","title":"Example- Sorting 1D Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-sorting-n-dimensional-arrays","text":"Define a 3x3 array with 17,-6,2,86,-12,0,0,23,12 and print it out. Then, sort the array. import numpy as np #import numpy a = np.array([[17,-6,2],[86,-12,0],[0,23,12]]) print(a) print (\"Along columns : \\n\", np.sort(a,axis = 0) ) #This will be sorting in each column print (\"Along rows : \\n\", np.sort(a,axis = 1) ) #This will be sorting in each row print (\"Sorting by default : \\n\", np.sort(a) ) #Same as above print (\"Along None Axis : \\n\", np.sort(a,axis = None) ) #This will be sorted like a 1D array [[ 17 -6 2] [ 86 -12 0] [ 0 23 12]] Along columns : [[ 0 -12 0] [ 17 -6 2] [ 86 23 12]] Along rows : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Sorting by default : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Along None Axis : [-12 -6 0 0 2 12 17 23 86]","title":"Example- Sorting n-Dimensional Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#partitioning-slice-arrays","text":"Slicing in python means taking elements from one given index to another given index. We can do slicing like this: [start:end]. We can also define the step, like this: [start:end:step]. If we don't pass start its considered 0 If we don't pass end its considered length of array in that dimension If we don't pass step its considered 1","title":"Partitioning (Slice) Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-slicing-1d-arrays","text":"Define a 1D array as [1,3,5,7,9], slice out the [3,5,7] and print it out. import numpy as np #import numpy a = np.array([1,3,5,7,9]) #Define the array print(a) aslice = a[1:4] #slice the [3,5,7] print(aslice) #print it out [1 3 5 7 9] [3 5 7]","title":"Example- Slicing 1D Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#example-slicing-n-dimensional-arrays","text":"Define a 5x5 array with \"Superman, Batman, Jim Hammond, Captain America, Green Arrow, Aquaman, Wonder Woman, Martian Manhunter, Barry Allen, Hal Jordan, Hawkman, Ray Palmer, Spider Man, Thor, Hank Pym, Solar, Iron Man, Dr. Strange, Daredevil, Ted Kord, Captian Marvel, Black Panther, Wolverine, Booster Gold, Spawn \" and print it out. Then: - Slice the first column and print it out - Slice the third row and print it out - Slice 'Wolverine' and print it out - Slice a 3x3 array with 'Wonder Woman, Ray Palmer, Iron Man, Martian Manhunter, Spider Man, Dr. Strange, Barry Allen, Thor, Daredevil' import numpy as np #import numpy Superheroes = np.array([['Superman', 'Batman', 'Jim Hammond', 'Captain America', 'Green Arrow'], ['Aquaman', 'Wonder Woman', 'Martian Manhunter', 'Barry Allen', 'Hal Jordan'], ['Hawkman', 'Ray Palmer', 'Spider Man', 'Thor', 'Hank Pym'], ['Solar', 'Iron Man', 'Dr. Strange', 'Daredevil', 'Ted Kord'], ['Captian Marvel', 'Black Panther', 'Wolverine', 'Booster Gold', 'Spawn']]) print(Superheroes) #Step1 print(Superheroes[:,0]) print(Superheroes[2,:]) print(Superheroes[4,2]) print(Superheroes[1:4,1:4]) [['Superman' 'Batman' 'Jim Hammond' 'Captain America' 'Green Arrow'] ['Aquaman' 'Wonder Woman' 'Martian Manhunter' 'Barry Allen' 'Hal Jordan'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] ['Solar' 'Iron Man' 'Dr. Strange' 'Daredevil' 'Ted Kord'] ['Captian Marvel' 'Black Panther' 'Wolverine' 'Booster Gold' 'Spawn']] ['Superman' 'Aquaman' 'Hawkman' 'Solar' 'Captian Marvel'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] Wolverine [['Wonder Woman' 'Martian Manhunter' 'Barry Allen'] ['Ray Palmer' 'Spider Man' 'Thor'] ['Iron Man' 'Dr. Strange' 'Daredevil']]","title":"Example- Slicing n-Dimensional Arrays"},{"location":"8-Labs/Lab6/Lab6_Dev/#this-is-a-numpy-cheat-sheet-similar-to-the-one-you-had-on-top-of-this-notebook","text":"","title":"This is a Numpy Cheat Sheet- similar to the one you had on top of this notebook!"},{"location":"8-Labs/Lab6/Lab6_Dev/#check-out-this-link-for-more","text":"https://blog.finxter.com/collection-10-best-numpy-cheat-sheets-every-python-coder-must-own/ Here are some of the resources used for creating this notebook: - Johnson, J. (2020). Python Numpy Tutorial (with Jupyter and Colab). Retrieved September 15, 2020, from https://cs231n.github.io/python-numpy-tutorial/ - Willems, K. (2019). (Tutorial) Python NUMPY Array TUTORIAL. Retrieved September 15, 2020, from https://www.datacamp.com/community/tutorials/python-numpy-tutorial?utm_source=adwords_ppc - Willems, K. (2017). NumPy Cheat Sheet: Data Analysis in Python. Retrieved September 15, 2020, from https://www.datacamp.com/community/blog/python-numpy-cheat-sheet - W3resource. (2020). NumPy: Compare two given arrays. Retrieved September 15, 2020, from https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-28.php Here are some great reads on this topic: - \"Python NumPy Tutorial\" available at https://www.geeksforgeeks.org/python-numpy-tutorial/ - \"What Is NumPy?\" a collection of blogs, available at https://realpython.com/tutorials/numpy/ - \"Look Ma, No For-Loops: Array Programming With NumPy\" by Brad Solomon available at https://realpython.com/numpy-array-programming/ - \"The Ultimate Beginner\u2019s Guide to NumPy\" by Anne Bonner available at https://towardsdatascience.com/the-ultimate-beginners-guide-to-numpy-f5a2f99aef54 Here are some great videos on these topics: - \"Learn NUMPY in 5 minutes - BEST Python Library!\" by Python Programmer available at https://www.youtube.com/watch?v=xECXZ3tyONo - \"Python NumPy Tutorial for Beginners\" by freeCodeCamp.org available at https://www.youtube.com/watch?v=QUT1VHiLmmI - \"Complete Python NumPy Tutorial (Creating Arrays, Indexing, Math, Statistics, Reshaping)\" by Keith Galli available at https://www.youtube.com/watch?v=GB9ByFAIAH4 - \"Python NumPy Tutorial | NumPy Array | Python Tutorial For Beginners | Python Training | Edureka\" by edureka! available at https://www.youtube.com/watch?v=8JfDAm9y_7s","title":"Check out this link for more: "},{"location":"8-Labs/Lab6/Lab6_Dev/#exercise-python-list-vs-numpy-arrays","text":"","title":"Exercise: Python List vs. Numpy Arrays? "},{"location":"8-Labs/Lab6/Lab6_Dev/#what-are-some-differences-between-python-lists-and-numpy-arrays","text":"","title":"What are some differences between Python lists and Numpy arrays?"},{"location":"8-Labs/Lab6/Lab6_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab6/Lab7_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab7 Laboratory 7: Numpy for Bread! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Numpy Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is short for \u201cNumeric Python\u201d or \u201cNumerical Python\u201d. If you are curious about NumPy, this cheat sheet is recommended: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf Arrays A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension. In other words, an array contains information about the raw data, how to locate an element and how to interpret an element.To make a numpy array, you can just use the np.array() function. All you need to do is pass a list to it. Don\u2019t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. If you want to read more about the differences between a Python list and NumPy array, this link is recommended: https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference Example- 1D Arrays Let's create a 1D array from the 2000s (2000-2009): import numpy as np #First, we need to impoty \"numpy\" mylist = [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009] #Create a list of the years print(mylist) #Check how it looks np.array(mylist) #Define it as a numpy array [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009] array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]) Example- n-Dimensional Arrays Let's create a 5x2 array from the 2000s (2000-2009): myotherlist = [[2000,2001],[2002,2003],[2004,2005],[2006,2007],[2008,2009]] #Since I want a 5x2 array, I should group the years two by two print(myotherlist) #See how it looks as a list np.array(myotherlist) #See how it looks as a numpy array [[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]] array([[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]]) Arrays Arithmetic Once you have created the arrays, you can do basic Numpy operations. Numpy offers a variety of operations applicable on arrays. From basic operations such as summation, subtraction, multiplication and division to more advanced and essential operations such as matrix multiplication and other elementwise operations. In the examples below, we will go over some of these: Example- 1D Array Arithmetic Define a 1D array with [0,12,24,36,48,60,72,84,96] Multiple all elements by 2 Take all elements to the power of 2 Find the maximum value of the array and its position Find the minimum value of the array and its position Define another 1D array with [-12,0,12,24,36,48,60,72,84] Find the summation and subtraction of these two arrays Find the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([0,12,24,36,48,60,72,84,96]) #Step1: Define Array1 print(Array1) print(Array1*2) #Step2: Multiple all elements by 2 print(Array1**2) #Step3: Take all elements to the power of 2 print(np.power(Array1,2)) #Another way to do the same thing, by using a function in numpy print(np.max(Array1)) #Step4: Find the maximum value of the array print(np.argmax(Array1)) ##Step4: Find the postition of the maximum value print(np.min(Array1)) #Step5: Find the minimum value of the array print(np.argmin(Array1)) ##Step5: Find the postition of the minimum value Array2 = np.array([-12,0,12,24,36,48,60,72,84]) #Step6: Define Array2 print(Array2) print(Array1+Array2) #Step7: Find the summation of these two arrays print(Array1-Array2) #Step7: Find the subtraction of these two arrays print(Array1*Array2) #Step8: Find the multiplication of these two arrays [ 0 12 24 36 48 60 72 84 96] [ 0 24 48 72 96 120 144 168 192] [ 0 144 576 1296 2304 3600 5184 7056 9216] [ 0 144 576 1296 2304 3600 5184 7056 9216] 96 8 0 0 [-12 0 12 24 36 48 60 72 84] [-12 12 36 60 84 108 132 156 180] [12 12 12 12 12 12 12 12 12] [ 0 0 288 864 1728 2880 4320 6048 8064] Example- n-Dimensional Array Arithmetic Define a 2x2 array with [5,10,15,20] Define another 2x2 array with [3,6,9,12] Find the summation and subtraction of these two arrays Find the minimum number in the multiplication of these two arrays Find the position of the maximum in the multiplication of these two arrays Find the mean of the multiplication of these two arrays Find the mean of the first row of the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([[5,10],[15,20]]) #Step1: Define Array1 print(Array1) Array2 = np.array([[3,6],[9,12]]) #Step2: Define Array2 print(Array2) print(Array1+Array2) #Step3: Find the summation print(Array1-Array2) #Step3: Find the subtraction MultArray = Array1@Array2 #Step4: To perform a typical matrix multiplication (or matrix product) MultArray1 = Array1.dot(Array2) #Step4: Another way To perform a matrix multiplication print(MultArray) print(MultArray1) print(np.min(MultArray)) #Step4: Find the minimum value of the multiplication print(np.argmax(MultArray)) ##Step5: Find the postition of the maximum value print(np.mean(MultArray)) ##Step6: Find the mean of the multiplication of these two arrays print(np.mean(MultArray[0,:])) ##Step7: Find the mean of the first row of the multiplication of these two arrays [[ 5 10] [15 20]] [[ 3 6] [ 9 12]] [[ 8 16] [24 32]] [[2 4] [6 8]] [[105 150] [225 330]] [[105 150] [225 330]] 105 3 202.5 127.5 Arrays Comparison Comparing two NumPy arrays determines whether they are equivalent by checking if every element at each corresponding index are the same. Example- 1D Array Comparison Define a 1D array with [1.0,2.5,3.4,7,7] Define another 1D array with [5.0/5.0,5.0/2,6.8/2,21/3,14/2] Compare and see if the two arrays are equal Define another 1D array with [6,1.4,2.2,7.5,7] Compare and see if the first array is greater than or equal to the third array import numpy as np #import numpy Array1 = np.array([1.0,2.5,3.4,7,7]) #Step1: Define Array1 print(Array1) Array2 = np.array([5.0/5.0,5.0/2,6.8/2,21/3,14/2]) #Step2: Define Array1 print(Array2) print(np.equal(Array1, Array2)) #Step3: Compare and see if the two arrays are equal Array3 = np.array([6,1.4,2.2,7.5,7]) #Step4: Define Array3 print(Array3) print(np.greater_equal(Array1, Array3)) #Step3: Compare and see if the two arrays are equal [1. 2.5 3.4 7. 7. ] [1. 2.5 3.4 7. 7. ] [ True True True True True] [6. 1.4 2.2 7.5 7. ] [False True True False True] Arrays Manipulation numpy.copy() allows us to create a copy of an array. This is particularly useful when we need to manipulate an array while keeping an original copy in memory. The numpy.delete() function returns a new array with sub-arrays along an axis deleted. Let's have a look at the examples. Example- Copying and Deleting Arrays and Elements Define a 1D array, named \"x\" with [1,2,3] Define \"y\" so that \"y=x\" Define \"z\" as a copy of \"x\" Discuss the difference between y and z Delete the second element of x import numpy as np #import numpy x = np.array([1,2,3]) #Step1: Define x print(x) y = x #Step2: Define y as y=x print(y) z = np.copy(x) #Step3: Define z as a copy of x print(z) # For Step4: They look similar but check this out: x[1] = 8 # If we change x ... print(x) print(y) print(z) # By modifying x, y changes but z remains as a copy of the initial version of x. x = np.delete(x, 1) #Step5: Delete the second element of x print(x) [1 2 3] [1 2 3] [1 2 3] [1 8 3] [1 8 3] [1 2 3] [1 3] Sorting Arrays Sorting means putting elements in an ordered sequence. Ordered sequence is any sequence that has an order corresponding to elements, like numeric or alphabetical, ascending or descending. If you use the sort() method on a 2-D array, both arrays will be sorted. Example- Sorting 1D Arrays Define a 1D array as ['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed'] and print it out. Then, sort the array alphabetically. import numpy as np #import numpy games = np.array(['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed']) print(games) print(np.sort(games)) ['FIFA 2020' 'Red Dead Redemption' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed'] ['FIFA 2020' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed' 'Red Dead Redemption'] Example- Sorting n-Dimensional Arrays Define a 3x3 array with 17,-6,2,86,-12,0,0,23,12 and print it out. Then, sort the array. import numpy as np #import numpy a = np.array([[17,-6,2],[86,-12,0],[0,23,12]]) print(a) print (\"Along columns : \\n\", np.sort(a,axis = 0) ) #This will be sorting in each column print (\"Along rows : \\n\", np.sort(a,axis = 1) ) #This will be sorting in each row print (\"Sorting by default : \\n\", np.sort(a) ) #Same as above print (\"Along None Axis : \\n\", np.sort(a,axis = None) ) #This will be sorted like a 1D array [[ 17 -6 2] [ 86 -12 0] [ 0 23 12]] Along columns : [[ 0 -12 0] [ 17 -6 2] [ 86 23 12]] Along rows : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Sorting by default : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Along None Axis : [-12 -6 0 0 2 12 17 23 86] Partitioning (Slice) Arrays Slicing in python means taking elements from one given index to another given index. We can do slicing like this: [start:end]. We can also define the step, like this: [start:end:step]. If we don't pass start its considered 0 If we don't pass end its considered length of array in that dimension If we don't pass step its considered 1 Example- Slicing 1D Arrays Define a 1D array as [1,3,5,7,9], slice out the [3,5,7] and print it out. import numpy as np #import numpy a = np.array([1,3,5,7,9]) #Define the array print(a) aslice = a[1:4] #slice the [3,5,7] print(aslice) #print it out [1 3 5 7 9] [3 5 7] Example- Slicing n-Dimensional Arrays Define a 5x5 array with \"Superman, Batman, Jim Hammond, Captain America, Green Arrow, Aquaman, Wonder Woman, Martian Manhunter, Barry Allen, Hal Jordan, Hawkman, Ray Palmer, Spider Man, Thor, Hank Pym, Solar, Iron Man, Dr. Strange, Daredevil, Ted Kord, Captian Marvel, Black Panther, Wolverine, Booster Gold, Spawn \" and print it out. Then: - Slice the first column and print it out - Slice the third row and print it out - Slice 'Wolverine' and print it out - Slice a 3x3 array with 'Wonder Woman, Ray Palmer, Iron Man, Martian Manhunter, Spider Man, Dr. Strange, Barry Allen, Thor, Daredevil' import numpy as np #import numpy Superheroes = np.array([['Superman', 'Batman', 'Jim Hammond', 'Captain America', 'Green Arrow'], ['Aquaman', 'Wonder Woman', 'Martian Manhunter', 'Barry Allen', 'Hal Jordan'], ['Hawkman', 'Ray Palmer', 'Spider Man', 'Thor', 'Hank Pym'], ['Solar', 'Iron Man', 'Dr. Strange', 'Daredevil', 'Ted Kord'], ['Captian Marvel', 'Black Panther', 'Wolverine', 'Booster Gold', 'Spawn']]) print(Superheroes) #Step1 print(Superheroes[:,0]) print(Superheroes[2,:]) print(Superheroes[4,2]) print(Superheroes[1:4,1:4]) [['Superman' 'Batman' 'Jim Hammond' 'Captain America' 'Green Arrow'] ['Aquaman' 'Wonder Woman' 'Martian Manhunter' 'Barry Allen' 'Hal Jordan'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] ['Solar' 'Iron Man' 'Dr. Strange' 'Daredevil' 'Ted Kord'] ['Captian Marvel' 'Black Panther' 'Wolverine' 'Booster Gold' 'Spawn']] ['Superman' 'Aquaman' 'Hawkman' 'Solar' 'Captian Marvel'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] Wolverine [['Wonder Woman' 'Martian Manhunter' 'Barry Allen'] ['Ray Palmer' 'Spider Man' 'Thor'] ['Iron Man' 'Dr. Strange' 'Daredevil']] This is a Numpy Cheat Sheet- similar to the one you had on top of this notebook! Check out this link for more: https://blog.finxter.com/collection-10-best-numpy-cheat-sheets-every-python-coder-must-own/ Here are some of the resources used for creating this notebook: - Johnson, J. (2020). Python Numpy Tutorial (with Jupyter and Colab). Retrieved September 15, 2020, from https://cs231n.github.io/python-numpy-tutorial/ - Willems, K. (2019). (Tutorial) Python NUMPY Array TUTORIAL. Retrieved September 15, 2020, from https://www.datacamp.com/community/tutorials/python-numpy-tutorial?utm_source=adwords_ppc - Willems, K. (2017). NumPy Cheat Sheet: Data Analysis in Python. Retrieved September 15, 2020, from https://www.datacamp.com/community/blog/python-numpy-cheat-sheet - W3resource. (2020). NumPy: Compare two given arrays. Retrieved September 15, 2020, from https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-28.php Here are some great reads on this topic: - \"Python NumPy Tutorial\" available at https://www.geeksforgeeks.org/python-numpy-tutorial/ - \"What Is NumPy?\" a collection of blogs, available at https://realpython.com/tutorials/numpy/ - \"Look Ma, No For-Loops: Array Programming With NumPy\" by Brad Solomon available at https://realpython.com/numpy-array-programming/ - \"The Ultimate Beginner\u2019s Guide to NumPy\" by Anne Bonner available at https://towardsdatascience.com/the-ultimate-beginners-guide-to-numpy-f5a2f99aef54 Here are some great videos on these topics: - \"Learn NUMPY in 5 minutes - BEST Python Library!\" by Python Programmer available at https://www.youtube.com/watch?v=xECXZ3tyONo - \"Python NumPy Tutorial for Beginners\" by freeCodeCamp.org available at https://www.youtube.com/watch?v=QUT1VHiLmmI - \"Complete Python NumPy Tutorial (Creating Arrays, Indexing, Math, Statistics, Reshaping)\" by Keith Galli available at https://www.youtube.com/watch?v=GB9ByFAIAH4 - \"Python NumPy Tutorial | NumPy Array | Python Tutorial For Beginners | Python Training | Edureka\" by edureka! available at https://www.youtube.com/watch?v=8JfDAm9y_7s Exercise: Python List vs. Numpy Arrays? What are some differences between Python lists and Numpy arrays? * Make sure to cite any resources that you may use.","title":"Lab7 Dev"},{"location":"8-Labs/Lab6/Lab7_Dev/#laboratory-7-numpy-for-bread","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 7: Numpy for Bread! "},{"location":"8-Labs/Lab6/Lab7_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab6/Lab7_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab6/Lab7_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab6/Lab7_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab6/Lab7_Dev/#numpy","text":"Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is short for \u201cNumeric Python\u201d or \u201cNumerical Python\u201d. If you are curious about NumPy, this cheat sheet is recommended: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf","title":"Numpy"},{"location":"8-Labs/Lab6/Lab7_Dev/#arrays","text":"A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension. In other words, an array contains information about the raw data, how to locate an element and how to interpret an element.To make a numpy array, you can just use the np.array() function. All you need to do is pass a list to it. Don\u2019t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. If you want to read more about the differences between a Python list and NumPy array, this link is recommended: https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference","title":"Arrays"},{"location":"8-Labs/Lab6/Lab7_Dev/#example-1d-arrays","text":"Let's create a 1D array from the 2000s (2000-2009): import numpy as np #First, we need to impoty \"numpy\" mylist = [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009] #Create a list of the years print(mylist) #Check how it looks np.array(mylist) #Define it as a numpy array [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009] array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009])","title":"Example- 1D Arrays"},{"location":"8-Labs/Lab6/Lab7_Dev/#example-n-dimensional-arrays","text":"Let's create a 5x2 array from the 2000s (2000-2009): myotherlist = [[2000,2001],[2002,2003],[2004,2005],[2006,2007],[2008,2009]] #Since I want a 5x2 array, I should group the years two by two print(myotherlist) #See how it looks as a list np.array(myotherlist) #See how it looks as a numpy array [[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]] array([[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]])","title":"Example- n-Dimensional Arrays"},{"location":"8-Labs/Lab6/Lab7_Dev/#arrays-arithmetic","text":"Once you have created the arrays, you can do basic Numpy operations. Numpy offers a variety of operations applicable on arrays. From basic operations such as summation, subtraction, multiplication and division to more advanced and essential operations such as matrix multiplication and other elementwise operations. In the examples below, we will go over some of these:","title":"Arrays Arithmetic"},{"location":"8-Labs/Lab6/Lab7_Dev/#example-1d-array-arithmetic","text":"Define a 1D array with [0,12,24,36,48,60,72,84,96] Multiple all elements by 2 Take all elements to the power of 2 Find the maximum value of the array and its position Find the minimum value of the array and its position Define another 1D array with [-12,0,12,24,36,48,60,72,84] Find the summation and subtraction of these two arrays Find the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([0,12,24,36,48,60,72,84,96]) #Step1: Define Array1 print(Array1) print(Array1*2) #Step2: Multiple all elements by 2 print(Array1**2) #Step3: Take all elements to the power of 2 print(np.power(Array1,2)) #Another way to do the same thing, by using a function in numpy print(np.max(Array1)) #Step4: Find the maximum value of the array print(np.argmax(Array1)) ##Step4: Find the postition of the maximum value print(np.min(Array1)) #Step5: Find the minimum value of the array print(np.argmin(Array1)) ##Step5: Find the postition of the minimum value Array2 = np.array([-12,0,12,24,36,48,60,72,84]) #Step6: Define Array2 print(Array2) print(Array1+Array2) #Step7: Find the summation of these two arrays print(Array1-Array2) #Step7: Find the subtraction of these two arrays print(Array1*Array2) #Step8: Find the multiplication of these two arrays [ 0 12 24 36 48 60 72 84 96] [ 0 24 48 72 96 120 144 168 192] [ 0 144 576 1296 2304 3600 5184 7056 9216] [ 0 144 576 1296 2304 3600 5184 7056 9216] 96 8 0 0 [-12 0 12 24 36 48 60 72 84] [-12 12 36 60 84 108 132 156 180] [12 12 12 12 12 12 12 12 12] [ 0 0 288 864 1728 2880 4320 6048 8064]","title":"Example- 1D Array Arithmetic"},{"location":"8-Labs/Lab6/Lab7_Dev/#example-n-dimensional-array-arithmetic","text":"Define a 2x2 array with [5,10,15,20] Define another 2x2 array with [3,6,9,12] Find the summation and subtraction of these two arrays Find the minimum number in the multiplication of these two arrays Find the position of the maximum in the multiplication of these two arrays Find the mean of the multiplication of these two arrays Find the mean of the first row of the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([[5,10],[15,20]]) #Step1: Define Array1 print(Array1) Array2 = np.array([[3,6],[9,12]]) #Step2: Define Array2 print(Array2) print(Array1+Array2) #Step3: Find the summation print(Array1-Array2) #Step3: Find the subtraction MultArray = Array1@Array2 #Step4: To perform a typical matrix multiplication (or matrix product) MultArray1 = Array1.dot(Array2) #Step4: Another way To perform a matrix multiplication print(MultArray) print(MultArray1) print(np.min(MultArray)) #Step4: Find the minimum value of the multiplication print(np.argmax(MultArray)) ##Step5: Find the postition of the maximum value print(np.mean(MultArray)) ##Step6: Find the mean of the multiplication of these two arrays print(np.mean(MultArray[0,:])) ##Step7: Find the mean of the first row of the multiplication of these two arrays [[ 5 10] [15 20]] [[ 3 6] [ 9 12]] [[ 8 16] [24 32]] [[2 4] [6 8]] [[105 150] [225 330]] [[105 150] [225 330]] 105 3 202.5 127.5","title":"Example- n-Dimensional Array Arithmetic"},{"location":"8-Labs/Lab6/Lab7_Dev/#arrays-comparison","text":"Comparing two NumPy arrays determines whether they are equivalent by checking if every element at each corresponding index are the same.","title":"Arrays Comparison"},{"location":"8-Labs/Lab6/Lab7_Dev/#example-1d-array-comparison","text":"Define a 1D array with [1.0,2.5,3.4,7,7] Define another 1D array with [5.0/5.0,5.0/2,6.8/2,21/3,14/2] Compare and see if the two arrays are equal Define another 1D array with [6,1.4,2.2,7.5,7] Compare and see if the first array is greater than or equal to the third array import numpy as np #import numpy Array1 = np.array([1.0,2.5,3.4,7,7]) #Step1: Define Array1 print(Array1) Array2 = np.array([5.0/5.0,5.0/2,6.8/2,21/3,14/2]) #Step2: Define Array1 print(Array2) print(np.equal(Array1, Array2)) #Step3: Compare and see if the two arrays are equal Array3 = np.array([6,1.4,2.2,7.5,7]) #Step4: Define Array3 print(Array3) print(np.greater_equal(Array1, Array3)) #Step3: Compare and see if the two arrays are equal [1. 2.5 3.4 7. 7. ] [1. 2.5 3.4 7. 7. ] [ True True True True True] [6. 1.4 2.2 7.5 7. ] [False True True False True]","title":"Example- 1D Array Comparison"},{"location":"8-Labs/Lab6/Lab7_Dev/#arrays-manipulation","text":"numpy.copy() allows us to create a copy of an array. This is particularly useful when we need to manipulate an array while keeping an original copy in memory. The numpy.delete() function returns a new array with sub-arrays along an axis deleted. Let's have a look at the examples.","title":"Arrays Manipulation"},{"location":"8-Labs/Lab6/Lab7_Dev/#example-copying-and-deleting-arrays-and-elements","text":"Define a 1D array, named \"x\" with [1,2,3] Define \"y\" so that \"y=x\" Define \"z\" as a copy of \"x\" Discuss the difference between y and z Delete the second element of x import numpy as np #import numpy x = np.array([1,2,3]) #Step1: Define x print(x) y = x #Step2: Define y as y=x print(y) z = np.copy(x) #Step3: Define z as a copy of x print(z) # For Step4: They look similar but check this out: x[1] = 8 # If we change x ... print(x) print(y) print(z) # By modifying x, y changes but z remains as a copy of the initial version of x. x = np.delete(x, 1) #Step5: Delete the second element of x print(x) [1 2 3] [1 2 3] [1 2 3] [1 8 3] [1 8 3] [1 2 3] [1 3]","title":"Example- Copying and Deleting Arrays and Elements"},{"location":"8-Labs/Lab6/Lab7_Dev/#sorting-arrays","text":"Sorting means putting elements in an ordered sequence. Ordered sequence is any sequence that has an order corresponding to elements, like numeric or alphabetical, ascending or descending. If you use the sort() method on a 2-D array, both arrays will be sorted.","title":"Sorting Arrays"},{"location":"8-Labs/Lab6/Lab7_Dev/#example-sorting-1d-arrays","text":"Define a 1D array as ['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed'] and print it out. Then, sort the array alphabetically. import numpy as np #import numpy games = np.array(['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed']) print(games) print(np.sort(games)) ['FIFA 2020' 'Red Dead Redemption' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed'] ['FIFA 2020' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed' 'Red Dead Redemption']","title":"Example- Sorting 1D Arrays"},{"location":"8-Labs/Lab6/Lab7_Dev/#example-sorting-n-dimensional-arrays","text":"Define a 3x3 array with 17,-6,2,86,-12,0,0,23,12 and print it out. Then, sort the array. import numpy as np #import numpy a = np.array([[17,-6,2],[86,-12,0],[0,23,12]]) print(a) print (\"Along columns : \\n\", np.sort(a,axis = 0) ) #This will be sorting in each column print (\"Along rows : \\n\", np.sort(a,axis = 1) ) #This will be sorting in each row print (\"Sorting by default : \\n\", np.sort(a) ) #Same as above print (\"Along None Axis : \\n\", np.sort(a,axis = None) ) #This will be sorted like a 1D array [[ 17 -6 2] [ 86 -12 0] [ 0 23 12]] Along columns : [[ 0 -12 0] [ 17 -6 2] [ 86 23 12]] Along rows : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Sorting by default : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Along None Axis : [-12 -6 0 0 2 12 17 23 86]","title":"Example- Sorting n-Dimensional Arrays"},{"location":"8-Labs/Lab6/Lab7_Dev/#partitioning-slice-arrays","text":"Slicing in python means taking elements from one given index to another given index. We can do slicing like this: [start:end]. We can also define the step, like this: [start:end:step]. If we don't pass start its considered 0 If we don't pass end its considered length of array in that dimension If we don't pass step its considered 1","title":"Partitioning (Slice) Arrays"},{"location":"8-Labs/Lab6/Lab7_Dev/#example-slicing-1d-arrays","text":"Define a 1D array as [1,3,5,7,9], slice out the [3,5,7] and print it out. import numpy as np #import numpy a = np.array([1,3,5,7,9]) #Define the array print(a) aslice = a[1:4] #slice the [3,5,7] print(aslice) #print it out [1 3 5 7 9] [3 5 7]","title":"Example- Slicing 1D Arrays"},{"location":"8-Labs/Lab6/Lab7_Dev/#example-slicing-n-dimensional-arrays","text":"Define a 5x5 array with \"Superman, Batman, Jim Hammond, Captain America, Green Arrow, Aquaman, Wonder Woman, Martian Manhunter, Barry Allen, Hal Jordan, Hawkman, Ray Palmer, Spider Man, Thor, Hank Pym, Solar, Iron Man, Dr. Strange, Daredevil, Ted Kord, Captian Marvel, Black Panther, Wolverine, Booster Gold, Spawn \" and print it out. Then: - Slice the first column and print it out - Slice the third row and print it out - Slice 'Wolverine' and print it out - Slice a 3x3 array with 'Wonder Woman, Ray Palmer, Iron Man, Martian Manhunter, Spider Man, Dr. Strange, Barry Allen, Thor, Daredevil' import numpy as np #import numpy Superheroes = np.array([['Superman', 'Batman', 'Jim Hammond', 'Captain America', 'Green Arrow'], ['Aquaman', 'Wonder Woman', 'Martian Manhunter', 'Barry Allen', 'Hal Jordan'], ['Hawkman', 'Ray Palmer', 'Spider Man', 'Thor', 'Hank Pym'], ['Solar', 'Iron Man', 'Dr. Strange', 'Daredevil', 'Ted Kord'], ['Captian Marvel', 'Black Panther', 'Wolverine', 'Booster Gold', 'Spawn']]) print(Superheroes) #Step1 print(Superheroes[:,0]) print(Superheroes[2,:]) print(Superheroes[4,2]) print(Superheroes[1:4,1:4]) [['Superman' 'Batman' 'Jim Hammond' 'Captain America' 'Green Arrow'] ['Aquaman' 'Wonder Woman' 'Martian Manhunter' 'Barry Allen' 'Hal Jordan'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] ['Solar' 'Iron Man' 'Dr. Strange' 'Daredevil' 'Ted Kord'] ['Captian Marvel' 'Black Panther' 'Wolverine' 'Booster Gold' 'Spawn']] ['Superman' 'Aquaman' 'Hawkman' 'Solar' 'Captian Marvel'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] Wolverine [['Wonder Woman' 'Martian Manhunter' 'Barry Allen'] ['Ray Palmer' 'Spider Man' 'Thor'] ['Iron Man' 'Dr. Strange' 'Daredevil']]","title":"Example- Slicing n-Dimensional Arrays"},{"location":"8-Labs/Lab6/Lab7_Dev/#this-is-a-numpy-cheat-sheet-similar-to-the-one-you-had-on-top-of-this-notebook","text":"","title":"This is a Numpy Cheat Sheet- similar to the one you had on top of this notebook!"},{"location":"8-Labs/Lab6/Lab7_Dev/#check-out-this-link-for-more","text":"https://blog.finxter.com/collection-10-best-numpy-cheat-sheets-every-python-coder-must-own/ Here are some of the resources used for creating this notebook: - Johnson, J. (2020). Python Numpy Tutorial (with Jupyter and Colab). Retrieved September 15, 2020, from https://cs231n.github.io/python-numpy-tutorial/ - Willems, K. (2019). (Tutorial) Python NUMPY Array TUTORIAL. Retrieved September 15, 2020, from https://www.datacamp.com/community/tutorials/python-numpy-tutorial?utm_source=adwords_ppc - Willems, K. (2017). NumPy Cheat Sheet: Data Analysis in Python. Retrieved September 15, 2020, from https://www.datacamp.com/community/blog/python-numpy-cheat-sheet - W3resource. (2020). NumPy: Compare two given arrays. Retrieved September 15, 2020, from https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-28.php Here are some great reads on this topic: - \"Python NumPy Tutorial\" available at https://www.geeksforgeeks.org/python-numpy-tutorial/ - \"What Is NumPy?\" a collection of blogs, available at https://realpython.com/tutorials/numpy/ - \"Look Ma, No For-Loops: Array Programming With NumPy\" by Brad Solomon available at https://realpython.com/numpy-array-programming/ - \"The Ultimate Beginner\u2019s Guide to NumPy\" by Anne Bonner available at https://towardsdatascience.com/the-ultimate-beginners-guide-to-numpy-f5a2f99aef54 Here are some great videos on these topics: - \"Learn NUMPY in 5 minutes - BEST Python Library!\" by Python Programmer available at https://www.youtube.com/watch?v=xECXZ3tyONo - \"Python NumPy Tutorial for Beginners\" by freeCodeCamp.org available at https://www.youtube.com/watch?v=QUT1VHiLmmI - \"Complete Python NumPy Tutorial (Creating Arrays, Indexing, Math, Statistics, Reshaping)\" by Keith Galli available at https://www.youtube.com/watch?v=GB9ByFAIAH4 - \"Python NumPy Tutorial | NumPy Array | Python Tutorial For Beginners | Python Training | Edureka\" by edureka! available at https://www.youtube.com/watch?v=8JfDAm9y_7s","title":"Check out this link for more: "},{"location":"8-Labs/Lab6/Lab7_Dev/#exercise-python-list-vs-numpy-arrays","text":"","title":"Exercise: Python List vs. Numpy Arrays? "},{"location":"8-Labs/Lab6/Lab7_Dev/#what-are-some-differences-between-python-lists-and-numpy-arrays","text":"","title":"What are some differences between Python lists and Numpy arrays?"},{"location":"8-Labs/Lab6/Lab7_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab7/Lab7_Dev/","text":"Laboratory 7: Pandas for Butter! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Pandas A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. To use pandas, we need to import the module, generally pandas has numpy as a dependency so it also must be imported import numpy as np #Importing NumPy library as \"np\" import pandas as pd #Importing Pandas library as \"pd\" Dataframe-structure using primative python First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. mytabular = np.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction print(mytabular) [[61 82 48 85] [45 36 97 72] [91 3 22 35] [18 65 30 63] [79 71 8 45]] The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 61, 82, 48, 85] ['B', 45, 36, 97, 72] ['C', 91, 3, 22, 35] ['D', 18, 65, 30, 63] ['E', 79, 71, 8, 45] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 91, 3, 22, 35] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 82 36 3 65 71 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 22 Create a proper dataframe We will now do the same using pandas df = pd.DataFrame(np.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 We can also turn our table into a dataframe, notice how the constructor adds header row and index column df1 = pd.DataFrame(mytable) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 61 82 48 85 2 B 45 36 97 72 3 C 91 3 22 35 4 D 18 65 30 63 5 E 79 71 8 45 To get proper behavior, we can just reuse our original objects df2 = pd.DataFrame(mytabular,myrowname,mycolname) df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 61 82 48 85 B 45 36 97 72 C 91 3 22 35 D 18 65 30 63 E 79 71 8 45 Getting the shape of dataframes The shape method will return the row and column rank (count) of a dataframe. df.shape (5, 4) df1.shape (6, 5) df2.shape (5, 4) Appending new columns To append a column simply assign a value to a new column name to the dataframe df['new']= 'NA' df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA Appending new rows A bit trickier but we can create a copy of a row and concatenate it back into the dataframe. newrow = df.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pd.concat([df,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA X 9 46 81 84 NA Removing Rows and Columns To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 Indexing We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 34 B 40 D 51 E 46 X 46 Name: X, dtype: int64 newtable[['X','W']] #Selecing multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 34 52 B 40 11 D 51 96 E 46 9 X 46 9 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 9 X 46 Y 81 Z 84 Name: E, dtype: int64 newtable.loc[['E','X','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 9 46 81 84 X 9 46 81 84 B 11 40 9 69 newtable.loc[['B','E','D'],['X','Y']] #Selecting elemens via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 40 9 E 46 81 D 51 89 Conditional Selection df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? df[df['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? df[df['col2']==df['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object Descriptor Functions #Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach head method Returns the first few rows, useful to infer structure #Returns only the first five rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit info method Returns the data model (data column count, names, data types) #Info about the dataframe df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes describe method Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000 Counting and Sum methods There are also methods for counts and sums by specific columns df['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) df['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values df['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) df['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64 Using functions in dataframes - symbolic apply The power of pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. Its pretty complicated but quite handy, best shown by an example def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(df) print('Apply the times2 function to col2') df['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64 Sorts df.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit Aggregating (Grouping Values) dataframe contents #Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } df1 = pd.DataFrame(data) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' df1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' df1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27 Filtering out missing values #Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach Reading a File into a Dataframe Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. import pandas as pd readfilecsv = pd.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 Writing a dataframe to file #Creating and writing to a .csv file readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pd.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pd.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') readfileexcel = pd.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 Unnamed: 0.1 a b c d 0 0 0 0 1 2 3 1 1 1 4 5 6 7 2 2 2 8 9 10 11 3 3 3 12 13 14 15 This is a Pandas Cheat Sheet Here are some of the resources used for creating this notebook: Pandas foundations. Retrieved February 15, 2021, from https://www.datacamp.com/courses/pandas-foundations Pandas tutorial. Retrieved February 15, 2021, from https://www.w3schools.com/python/pandas/default.asp Pandas tutorial: Dataframes in Python. Retrieved February 15, 2021, from https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python Here are some great reads on this topic: - \"Introduction to Pandas in Python\" available at https://www.geeksforgeeks.org/introduction-to-pandas-in-python/ - \"Pandas Introduction & Tutorials for Beginners\" by Walker Rowe , available at https://www.bmc.com/blogs/pandas-basics/ - \"Using Pandas and Python to Explore Your Dataset\" by Reka Horvath available at https://realpython.com/pandas-python-explore-dataset/ - \"Python Pandas Tutorial: A Complete Introduction for Beginners\" by George McIntire, Lauren Washington, and Brendan Martin available at https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/ Here are some great videos on these topics: - \"Python: Pandas Tutorial | Intro to DataFrames\" by Joe James available at https://www.youtube.com/watch?v=e60ItwlZTKM - \"Complete Python Pandas Data Science Tutorial! (Reading CSV/Excel files, Sorting, Filtering, Groupby)\" by Keith Galli available at https://www.youtube.com/watch?v=vmEHCJofslg - \"What is Pandas? Why and How to Use Pandas in Python\" by Python Programmer available at *https://www.youtube.com/watch?v=dcqPhpY7tWk Exercise: Pandas of Data Pandas library supports three major types of data structures: Series, DataFrames, and Panels. What are some differences between the three structures? * Make sure to cite any resources that you may use.","title":"<font color=darkred>Laboratory 7: Pandas for Butter! </font>"},{"location":"8-Labs/Lab7/Lab7_Dev/#laboratory-7-pandas-for-butter","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Laboratory 7: Pandas for Butter! "},{"location":"8-Labs/Lab7/Lab7_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab7/Lab7_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab7/Lab7_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab7/Lab7_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab7/Lab7_Dev/#pandas","text":"A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. To use pandas, we need to import the module, generally pandas has numpy as a dependency so it also must be imported import numpy as np #Importing NumPy library as \"np\" import pandas as pd #Importing Pandas library as \"pd\"","title":"Pandas"},{"location":"8-Labs/Lab7/Lab7_Dev/#dataframe-structure-using-primative-python","text":"First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. mytabular = np.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction print(mytabular) [[61 82 48 85] [45 36 97 72] [91 3 22 35] [18 65 30 63] [79 71 8 45]] The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 61, 82, 48, 85] ['B', 45, 36, 97, 72] ['C', 91, 3, 22, 35] ['D', 18, 65, 30, 63] ['E', 79, 71, 8, 45] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 91, 3, 22, 35] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 82 36 3 65 71 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 22","title":"Dataframe-structure using primative python"},{"location":"8-Labs/Lab7/Lab7_Dev/#create-a-proper-dataframe","text":"We will now do the same using pandas df = pd.DataFrame(np.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 We can also turn our table into a dataframe, notice how the constructor adds header row and index column df1 = pd.DataFrame(mytable) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 61 82 48 85 2 B 45 36 97 72 3 C 91 3 22 35 4 D 18 65 30 63 5 E 79 71 8 45 To get proper behavior, we can just reuse our original objects df2 = pd.DataFrame(mytabular,myrowname,mycolname) df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 61 82 48 85 B 45 36 97 72 C 91 3 22 35 D 18 65 30 63 E 79 71 8 45","title":"Create a proper dataframe"},{"location":"8-Labs/Lab7/Lab7_Dev/#getting-the-shape-of-dataframes","text":"The shape method will return the row and column rank (count) of a dataframe. df.shape (5, 4) df1.shape (6, 5) df2.shape (5, 4)","title":"Getting the shape of dataframes"},{"location":"8-Labs/Lab7/Lab7_Dev/#appending-new-columns","text":"To append a column simply assign a value to a new column name to the dataframe df['new']= 'NA' df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA","title":"Appending new columns"},{"location":"8-Labs/Lab7/Lab7_Dev/#appending-new-rows","text":"A bit trickier but we can create a copy of a row and concatenate it back into the dataframe. newrow = df.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pd.concat([df,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA X 9 46 81 84 NA","title":"Appending new rows"},{"location":"8-Labs/Lab7/Lab7_Dev/#removing-rows-and-columns","text":"To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84","title":"Removing Rows and Columns"},{"location":"8-Labs/Lab7/Lab7_Dev/#indexing","text":"We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 34 B 40 D 51 E 46 X 46 Name: X, dtype: int64 newtable[['X','W']] #Selecing multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 34 52 B 40 11 D 51 96 E 46 9 X 46 9 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 9 X 46 Y 81 Z 84 Name: E, dtype: int64 newtable.loc[['E','X','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 9 46 81 84 X 9 46 81 84 B 11 40 9 69 newtable.loc[['B','E','D'],['X','Y']] #Selecting elemens via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 40 9 E 46 81 D 51 89","title":"Indexing"},{"location":"8-Labs/Lab7/Lab7_Dev/#conditional-selection","text":"df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? df[df['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? df[df['col2']==df['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object","title":"Conditional Selection"},{"location":"8-Labs/Lab7/Lab7_Dev/#descriptor-functions","text":"#Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach","title":"Descriptor Functions"},{"location":"8-Labs/Lab7/Lab7_Dev/#head-method","text":"Returns the first few rows, useful to infer structure #Returns only the first five rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit","title":"head method"},{"location":"8-Labs/Lab7/Lab7_Dev/#info-method","text":"Returns the data model (data column count, names, data types) #Info about the dataframe df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes","title":"info method"},{"location":"8-Labs/Lab7/Lab7_Dev/#describe-method","text":"Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000","title":"describe method"},{"location":"8-Labs/Lab7/Lab7_Dev/#counting-and-sum-methods","text":"There are also methods for counts and sums by specific columns df['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) df['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values df['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) df['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64","title":"Counting and Sum methods"},{"location":"8-Labs/Lab7/Lab7_Dev/#using-functions-in-dataframes-symbolic-apply","text":"The power of pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. Its pretty complicated but quite handy, best shown by an example def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(df) print('Apply the times2 function to col2') df['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64","title":"Using functions in dataframes - symbolic apply"},{"location":"8-Labs/Lab7/Lab7_Dev/#sorts","text":"df.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit","title":"Sorts"},{"location":"8-Labs/Lab7/Lab7_Dev/#aggregating-grouping-values-dataframe-contents","text":"#Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } df1 = pd.DataFrame(data) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' df1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' df1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27","title":"Aggregating (Grouping Values) dataframe contents"},{"location":"8-Labs/Lab7/Lab7_Dev/#filtering-out-missing-values","text":"#Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach","title":"Filtering out missing values"},{"location":"8-Labs/Lab7/Lab7_Dev/#reading-a-file-into-a-dataframe","text":"Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. import pandas as pd readfilecsv = pd.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15","title":"Reading a File into a Dataframe"},{"location":"8-Labs/Lab7/Lab7_Dev/#writing-a-dataframe-to-file","text":"#Creating and writing to a .csv file readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pd.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pd.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') readfileexcel = pd.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 Unnamed: 0.1 a b c d 0 0 0 0 1 2 3 1 1 1 4 5 6 7 2 2 2 8 9 10 11 3 3 3 12 13 14 15","title":"Writing a dataframe to file"},{"location":"8-Labs/Lab7/Lab7_Dev/#this-is-a-pandas-cheat-sheet","text":"Here are some of the resources used for creating this notebook: Pandas foundations. Retrieved February 15, 2021, from https://www.datacamp.com/courses/pandas-foundations Pandas tutorial. Retrieved February 15, 2021, from https://www.w3schools.com/python/pandas/default.asp Pandas tutorial: Dataframes in Python. Retrieved February 15, 2021, from https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python Here are some great reads on this topic: - \"Introduction to Pandas in Python\" available at https://www.geeksforgeeks.org/introduction-to-pandas-in-python/ - \"Pandas Introduction & Tutorials for Beginners\" by Walker Rowe , available at https://www.bmc.com/blogs/pandas-basics/ - \"Using Pandas and Python to Explore Your Dataset\" by Reka Horvath available at https://realpython.com/pandas-python-explore-dataset/ - \"Python Pandas Tutorial: A Complete Introduction for Beginners\" by George McIntire, Lauren Washington, and Brendan Martin available at https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/ Here are some great videos on these topics: - \"Python: Pandas Tutorial | Intro to DataFrames\" by Joe James available at https://www.youtube.com/watch?v=e60ItwlZTKM - \"Complete Python Pandas Data Science Tutorial! (Reading CSV/Excel files, Sorting, Filtering, Groupby)\" by Keith Galli available at https://www.youtube.com/watch?v=vmEHCJofslg - \"What is Pandas? Why and How to Use Pandas in Python\" by Python Programmer available at *https://www.youtube.com/watch?v=dcqPhpY7tWk","title":"This is a Pandas Cheat Sheet"},{"location":"8-Labs/Lab7/Lab7_Dev/#exercise-pandas-of-data","text":"","title":"Exercise: Pandas of Data  "},{"location":"8-Labs/Lab7/Lab7_Dev/#pandas-library-supports-three-major-types-of-data-structures-series-dataframes-and-panels-what-are-some-differences-between-the-three-structures","text":"","title":"Pandas library supports three major types of data structures: Series, DataFrames, and Panels. What are some differences between the three structures?"},{"location":"8-Labs/Lab7/Lab7_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab7/Lab8_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab8 Laboratory 8: Pandas for Butter! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Pandas A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. To use pandas, we need to import the module, generally pandas has numpy as a dependency so it also must be imported import numpy as np #Importing NumPy library as \"np\" import pandas as pd #Importing Pandas library as \"pd\" Dataframe-structure using primative python First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. mytabular = np.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction print(mytabular) [[61 82 48 85] [45 36 97 72] [91 3 22 35] [18 65 30 63] [79 71 8 45]] The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 61, 82, 48, 85] ['B', 45, 36, 97, 72] ['C', 91, 3, 22, 35] ['D', 18, 65, 30, 63] ['E', 79, 71, 8, 45] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 91, 3, 22, 35] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 82 36 3 65 71 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 22 Create a proper dataframe We will now do the same using pandas df = pd.DataFrame(np.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 We can also turn our table into a dataframe, notice how the constructor adds header row and index column df1 = pd.DataFrame(mytable) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 61 82 48 85 2 B 45 36 97 72 3 C 91 3 22 35 4 D 18 65 30 63 5 E 79 71 8 45 To get proper behavior, we can just reuse our original objects df2 = pd.DataFrame(mytabular,myrowname,mycolname) df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 61 82 48 85 B 45 36 97 72 C 91 3 22 35 D 18 65 30 63 E 79 71 8 45 Getting the shape of dataframes The shape method will return the row and column rank (count) of a dataframe. df.shape (5, 4) df1.shape (6, 5) df2.shape (5, 4) Appending new columns To append a column simply assign a value to a new column name to the dataframe df['new']= 'NA' df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA Appending new rows A bit trickier but we can create a copy of a row and concatenate it back into the dataframe. newrow = df.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pd.concat([df,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA X 9 46 81 84 NA Removing Rows and Columns To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 Indexing We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 34 B 40 D 51 E 46 X 46 Name: X, dtype: int64 newtable[['X','W']] #Selecing multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 34 52 B 40 11 D 51 96 E 46 9 X 46 9 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 9 X 46 Y 81 Z 84 Name: E, dtype: int64 newtable.loc[['E','X','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 9 46 81 84 X 9 46 81 84 B 11 40 9 69 newtable.loc[['B','E','D'],['X','Y']] #Selecting elemens via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 40 9 E 46 81 D 51 89 Conditional Selection df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? df[df['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? df[df['col2']==df['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object Descriptor Functions #Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach head method Returns the first few rows, useful to infer structure #Returns only the first five rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit info method Returns the data model (data column count, names, data types) #Info about the dataframe df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes describe method Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000 Counting and Sum methods There are also methods for counts and sums by specific columns df['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) df['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values df['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) df['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64 Using functions in dataframes - symbolic apply The power of pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. Its pretty complicated but quite handy, best shown by an example def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(df) print('Apply the times2 function to col2') df['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64 Sorts df.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit Aggregating (Grouping Values) dataframe contents #Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } df1 = pd.DataFrame(data) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' df1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' df1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27 Filtering out missing values #Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach Reading a File into a Dataframe Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. import pandas as pd readfilecsv = pd.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 Writing a dataframe to file #Creating and writing to a .csv file readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pd.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pd.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') readfileexcel = pd.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 Unnamed: 0.1 a b c d 0 0 0 0 1 2 3 1 1 1 4 5 6 7 2 2 2 8 9 10 11 3 3 3 12 13 14 15 This is a Pandas Cheat Sheet Here are some of the resources used for creating this notebook: Pandas foundations. Retrieved February 15, 2021, from https://www.datacamp.com/courses/pandas-foundations Pandas tutorial. Retrieved February 15, 2021, from https://www.w3schools.com/python/pandas/default.asp Pandas tutorial: Dataframes in Python. Retrieved February 15, 2021, from https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python Here are some great reads on this topic: - \"Introduction to Pandas in Python\" available at https://www.geeksforgeeks.org/introduction-to-pandas-in-python/ - \"Pandas Introduction & Tutorials for Beginners\" by Walker Rowe , available at https://www.bmc.com/blogs/pandas-basics/ - \"Using Pandas and Python to Explore Your Dataset\" by Reka Horvath available at https://realpython.com/pandas-python-explore-dataset/ - \"Python Pandas Tutorial: A Complete Introduction for Beginners\" by George McIntire, Lauren Washington, and Brendan Martin available at https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/ Here are some great videos on these topics: - \"Python: Pandas Tutorial | Intro to DataFrames\" by Joe James available at https://www.youtube.com/watch?v=e60ItwlZTKM - \"Complete Python Pandas Data Science Tutorial! (Reading CSV/Excel files, Sorting, Filtering, Groupby)\" by Keith Galli available at https://www.youtube.com/watch?v=vmEHCJofslg - \"What is Pandas? Why and How to Use Pandas in Python\" by Python Programmer available at *https://www.youtube.com/watch?v=dcqPhpY7tWk Exercise: Pandas of Data Pandas library supports three major types of data structures: Series, DataFrames, and Panels. What are some differences between the three structures? * Make sure to cite any resources that you may use.","title":"Lab8 Dev"},{"location":"8-Labs/Lab7/Lab8_Dev/#laboratory-8-pandas-for-butter","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Laboratory 8: Pandas for Butter! "},{"location":"8-Labs/Lab7/Lab8_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab7/Lab8_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab7/Lab8_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab7/Lab8_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab7/Lab8_Dev/#pandas","text":"A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. To use pandas, we need to import the module, generally pandas has numpy as a dependency so it also must be imported import numpy as np #Importing NumPy library as \"np\" import pandas as pd #Importing Pandas library as \"pd\"","title":"Pandas"},{"location":"8-Labs/Lab7/Lab8_Dev/#dataframe-structure-using-primative-python","text":"First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. mytabular = np.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction print(mytabular) [[61 82 48 85] [45 36 97 72] [91 3 22 35] [18 65 30 63] [79 71 8 45]] The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 61, 82, 48, 85] ['B', 45, 36, 97, 72] ['C', 91, 3, 22, 35] ['D', 18, 65, 30, 63] ['E', 79, 71, 8, 45] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 91, 3, 22, 35] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 82 36 3 65 71 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 22","title":"Dataframe-structure using primative python"},{"location":"8-Labs/Lab7/Lab8_Dev/#create-a-proper-dataframe","text":"We will now do the same using pandas df = pd.DataFrame(np.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 We can also turn our table into a dataframe, notice how the constructor adds header row and index column df1 = pd.DataFrame(mytable) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 61 82 48 85 2 B 45 36 97 72 3 C 91 3 22 35 4 D 18 65 30 63 5 E 79 71 8 45 To get proper behavior, we can just reuse our original objects df2 = pd.DataFrame(mytabular,myrowname,mycolname) df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 61 82 48 85 B 45 36 97 72 C 91 3 22 35 D 18 65 30 63 E 79 71 8 45","title":"Create a proper dataframe"},{"location":"8-Labs/Lab7/Lab8_Dev/#getting-the-shape-of-dataframes","text":"The shape method will return the row and column rank (count) of a dataframe. df.shape (5, 4) df1.shape (6, 5) df2.shape (5, 4)","title":"Getting the shape of dataframes"},{"location":"8-Labs/Lab7/Lab8_Dev/#appending-new-columns","text":"To append a column simply assign a value to a new column name to the dataframe df['new']= 'NA' df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA","title":"Appending new columns"},{"location":"8-Labs/Lab7/Lab8_Dev/#appending-new-rows","text":"A bit trickier but we can create a copy of a row and concatenate it back into the dataframe. newrow = df.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pd.concat([df,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA X 9 46 81 84 NA","title":"Appending new rows"},{"location":"8-Labs/Lab7/Lab8_Dev/#removing-rows-and-columns","text":"To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84","title":"Removing Rows and Columns"},{"location":"8-Labs/Lab7/Lab8_Dev/#indexing","text":"We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 34 B 40 D 51 E 46 X 46 Name: X, dtype: int64 newtable[['X','W']] #Selecing multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 34 52 B 40 11 D 51 96 E 46 9 X 46 9 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 9 X 46 Y 81 Z 84 Name: E, dtype: int64 newtable.loc[['E','X','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 9 46 81 84 X 9 46 81 84 B 11 40 9 69 newtable.loc[['B','E','D'],['X','Y']] #Selecting elemens via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 40 9 E 46 81 D 51 89","title":"Indexing"},{"location":"8-Labs/Lab7/Lab8_Dev/#conditional-selection","text":"df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? df[df['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? df[df['col2']==df['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object","title":"Conditional Selection"},{"location":"8-Labs/Lab7/Lab8_Dev/#descriptor-functions","text":"#Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach","title":"Descriptor Functions"},{"location":"8-Labs/Lab7/Lab8_Dev/#head-method","text":"Returns the first few rows, useful to infer structure #Returns only the first five rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit","title":"head method"},{"location":"8-Labs/Lab7/Lab8_Dev/#info-method","text":"Returns the data model (data column count, names, data types) #Info about the dataframe df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes","title":"info method"},{"location":"8-Labs/Lab7/Lab8_Dev/#describe-method","text":"Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000","title":"describe method"},{"location":"8-Labs/Lab7/Lab8_Dev/#counting-and-sum-methods","text":"There are also methods for counts and sums by specific columns df['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) df['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values df['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) df['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64","title":"Counting and Sum methods"},{"location":"8-Labs/Lab7/Lab8_Dev/#using-functions-in-dataframes-symbolic-apply","text":"The power of pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. Its pretty complicated but quite handy, best shown by an example def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(df) print('Apply the times2 function to col2') df['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64","title":"Using functions in dataframes - symbolic apply"},{"location":"8-Labs/Lab7/Lab8_Dev/#sorts","text":"df.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit","title":"Sorts"},{"location":"8-Labs/Lab7/Lab8_Dev/#aggregating-grouping-values-dataframe-contents","text":"#Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } df1 = pd.DataFrame(data) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' df1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' df1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27","title":"Aggregating (Grouping Values) dataframe contents"},{"location":"8-Labs/Lab7/Lab8_Dev/#filtering-out-missing-values","text":"#Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach","title":"Filtering out missing values"},{"location":"8-Labs/Lab7/Lab8_Dev/#reading-a-file-into-a-dataframe","text":"Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. import pandas as pd readfilecsv = pd.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15","title":"Reading a File into a Dataframe"},{"location":"8-Labs/Lab7/Lab8_Dev/#writing-a-dataframe-to-file","text":"#Creating and writing to a .csv file readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pd.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pd.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') readfileexcel = pd.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 Unnamed: 0.1 a b c d 0 0 0 0 1 2 3 1 1 1 4 5 6 7 2 2 2 8 9 10 11 3 3 3 12 13 14 15","title":"Writing a dataframe to file"},{"location":"8-Labs/Lab7/Lab8_Dev/#this-is-a-pandas-cheat-sheet","text":"Here are some of the resources used for creating this notebook: Pandas foundations. Retrieved February 15, 2021, from https://www.datacamp.com/courses/pandas-foundations Pandas tutorial. Retrieved February 15, 2021, from https://www.w3schools.com/python/pandas/default.asp Pandas tutorial: Dataframes in Python. Retrieved February 15, 2021, from https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python Here are some great reads on this topic: - \"Introduction to Pandas in Python\" available at https://www.geeksforgeeks.org/introduction-to-pandas-in-python/ - \"Pandas Introduction & Tutorials for Beginners\" by Walker Rowe , available at https://www.bmc.com/blogs/pandas-basics/ - \"Using Pandas and Python to Explore Your Dataset\" by Reka Horvath available at https://realpython.com/pandas-python-explore-dataset/ - \"Python Pandas Tutorial: A Complete Introduction for Beginners\" by George McIntire, Lauren Washington, and Brendan Martin available at https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/ Here are some great videos on these topics: - \"Python: Pandas Tutorial | Intro to DataFrames\" by Joe James available at https://www.youtube.com/watch?v=e60ItwlZTKM - \"Complete Python Pandas Data Science Tutorial! (Reading CSV/Excel files, Sorting, Filtering, Groupby)\" by Keith Galli available at https://www.youtube.com/watch?v=vmEHCJofslg - \"What is Pandas? Why and How to Use Pandas in Python\" by Python Programmer available at *https://www.youtube.com/watch?v=dcqPhpY7tWk","title":"This is a Pandas Cheat Sheet"},{"location":"8-Labs/Lab7/Lab8_Dev/#exercise-pandas-of-data","text":"","title":"Exercise: Pandas of Data  "},{"location":"8-Labs/Lab7/Lab8_Dev/#pandas-library-supports-three-major-types-of-data-structures-series-dataframes-and-panels-what-are-some-differences-between-the-three-structures","text":"","title":"Pandas library supports three major types of data structures: Series, DataFrames, and Panels. What are some differences between the three structures?"},{"location":"8-Labs/Lab7/Lab8_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab8/Lab8_Dev/","text":"Laboratory 8: Matplotlib for Jam! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Matplotlip and Visual Display of Data This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib About matplotlib Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis). Background Data are not always numerical. Data can music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) They can also be categorical into which you can place individuals: - The individuals are cartons of ice-cream, and the category is the flavor in the carton - The individuals are professional basketball players, and the category is the player's team. Bar Graphs Bar charts (graphs) are good display tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='maroon', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='orange', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.bar(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:xlabel='Flavor'> Example- Language Bars! Consider the data set \"data\" defined as data = {'C':20, 'C++':15, 'Java':30, 'Python':35} which lists student count by programming language in some school. Produce a bar chart of number of students in each language, where language is the classification, and student count is the variable. # Code and run your solution here import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show() Plot it as a horizontal bar chart: # Code and run your solution here # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.barh(courses, values, color ='maroon', height = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show() Line Charts A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application. Example- Speed vs Time Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() From examination of the plot, estimate the speed at time t = 5.0 (eyeball estimate) Example- Add a linear fit Using the same series from Exercise 1, Plot the speed vs time (speed on y-axis, time on x-axis) using a line plot. Plot a second line based on the linear model y = mx + b , where b=0~\\text{and}~m=7.6 . # Code and run your solution here: def ymodel(xmodel,slope,intercept): ymodel = slope*xmodel+intercept return(ymodel) yseries = [] slope = 7.6 intercept = 0.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0.5) # basic line plot plt.plot(time, yseries, c='blue') plt.show() Example- Find a better fit Using trial and error try to improve the 'fit' of the model, by adjusting values of m~\\text{and}~b . # Code and run your solution here: yseries = [] slope = 7.6 intercept = -8.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0) # basic scatter plot plt.plot(time, yseries, c='blue') plt.show() Scatter Plots A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot Example- Examine the dataset with heights of fathers, mothers and sons df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists dad = df['father'] ; mom = df['mother'] ; son = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red' , label='Father') # one plot series plt.scatter(son, mom, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\") Histograms Quoting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson.[1] To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\" Example- Explore the \"top_movies\" dataset and draw histograms for Gross and Year. import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Year\"]].hist() array([[<AxesSubplot:title={'center':'Year'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) This is a Matplotlib Cheat Sheet Here are some of the resources used for creating this notebook: \"Discrete distribution as horizontal bar chart\" available at *https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html \"Bar Plot in Matplotlib\" available at *https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ Here are some great reads on this topic: - \"Python | Introduction to Matplotlib\" available at https://www.geeksforgeeks.org/python-introduction-matplotlib/ - \"Visualization with Matplotlib\" available at https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html - \"Introduction to Matplotlib \u2014 Data Visualization in Python\" by Ehi Aigiomawu available at https://heartbeat.fritz.ai/introduction-to-matplotlib-data-visualization-in-python-d9143287ae39 - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"Matplotlib Tutorial (Part 1): Creating and Customizing Our First Plots\" by Corey Schafer available at https://www.youtube.com/watch?v=UO98lJQ3QGI - \"Intro to Data Analysis / Visualization with Python, Matplotlib and Pandas | Matplotlib Tutorial\" by CS Dojo available at https://www.youtube.com/watch?v=a9UrKTVEeZA - \"Intro to Data Visualization in Python with Matplotlib! (line graph, bar chart, title, labels, size)\" by Keith Galli available at *https://www.youtube.com/watch?v=DAQNHzOcO5A Exercise: Bins, Bins, Bins! Selecting the number of bins is an important decision when working with histograms. Are there any rules or recommendations for choosing the number or width of bins? What happens if we use too many or too few bins? * Make sure to cite any resources that you may use.","title":"<font color=darkred>Laboratory 8: Matplotlib for Jam! </font>"},{"location":"8-Labs/Lab8/Lab8_Dev/#laboratory-8-matplotlib-for-jam","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Laboratory 8: Matplotlib for Jam! "},{"location":"8-Labs/Lab8/Lab8_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab8/Lab8_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab8/Lab8_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab8/Lab8_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab8/Lab8_Dev/#matplotlip-and-visual-display-of-data","text":"This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib","title":"Matplotlip and Visual Display of Data"},{"location":"8-Labs/Lab8/Lab8_Dev/#about-matplotlib","text":"Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis).","title":"About matplotlib"},{"location":"8-Labs/Lab8/Lab8_Dev/#background","text":"Data are not always numerical. Data can music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) They can also be categorical into which you can place individuals: - The individuals are cartons of ice-cream, and the category is the flavor in the carton - The individuals are professional basketball players, and the category is the player's team.","title":"Background"},{"location":"8-Labs/Lab8/Lab8_Dev/#bar-graphs","text":"Bar charts (graphs) are good display tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='maroon', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='orange', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.bar(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:xlabel='Flavor'>","title":"Bar Graphs"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-language-bars","text":"Consider the data set \"data\" defined as data = {'C':20, 'C++':15, 'Java':30, 'Python':35} which lists student count by programming language in some school. Produce a bar chart of number of students in each language, where language is the classification, and student count is the variable. # Code and run your solution here import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show()","title":"Example- Language Bars!"},{"location":"8-Labs/Lab8/Lab8_Dev/#plot-it-as-a-horizontal-bar-chart","text":"# Code and run your solution here # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.barh(courses, values, color ='maroon', height = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show()","title":"Plot it as a horizontal bar chart:"},{"location":"8-Labs/Lab8/Lab8_Dev/#line-charts","text":"A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application.","title":"Line Charts"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-speed-vs-time","text":"Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() From examination of the plot, estimate the speed at time t = 5.0 (eyeball estimate)","title":"Example- Speed vs Time"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-add-a-linear-fit","text":"Using the same series from Exercise 1, Plot the speed vs time (speed on y-axis, time on x-axis) using a line plot. Plot a second line based on the linear model y = mx + b , where b=0~\\text{and}~m=7.6 . # Code and run your solution here: def ymodel(xmodel,slope,intercept): ymodel = slope*xmodel+intercept return(ymodel) yseries = [] slope = 7.6 intercept = 0.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0.5) # basic line plot plt.plot(time, yseries, c='blue') plt.show()","title":"Example- Add a linear fit"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-find-a-better-fit","text":"Using trial and error try to improve the 'fit' of the model, by adjusting values of m~\\text{and}~b . # Code and run your solution here: yseries = [] slope = 7.6 intercept = -8.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0) # basic scatter plot plt.plot(time, yseries, c='blue') plt.show()","title":"Example- Find a better fit"},{"location":"8-Labs/Lab8/Lab8_Dev/#scatter-plots","text":"A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot","title":"Scatter Plots"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-examine-the-dataset-with-heights-of-fathers-mothers-and-sons","text":"df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists dad = df['father'] ; mom = df['mother'] ; son = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red' , label='Father') # one plot series plt.scatter(son, mom, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\")","title":"Example- Examine the dataset with heights of fathers, mothers and sons"},{"location":"8-Labs/Lab8/Lab8_Dev/#histograms","text":"Quoting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson.[1] To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\"","title":"Histograms"},{"location":"8-Labs/Lab8/Lab8_Dev/#example-explore-the-top_movies-dataset-and-draw-histograms-for-gross-and-year","text":"import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Year\"]].hist() array([[<AxesSubplot:title={'center':'Year'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100)","title":"Example- Explore the \"top_movies\" dataset and draw histograms for Gross and Year."},{"location":"8-Labs/Lab8/Lab8_Dev/#this-is-a-matplotlib-cheat-sheet","text":"Here are some of the resources used for creating this notebook: \"Discrete distribution as horizontal bar chart\" available at *https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html \"Bar Plot in Matplotlib\" available at *https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ Here are some great reads on this topic: - \"Python | Introduction to Matplotlib\" available at https://www.geeksforgeeks.org/python-introduction-matplotlib/ - \"Visualization with Matplotlib\" available at https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html - \"Introduction to Matplotlib \u2014 Data Visualization in Python\" by Ehi Aigiomawu available at https://heartbeat.fritz.ai/introduction-to-matplotlib-data-visualization-in-python-d9143287ae39 - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"Matplotlib Tutorial (Part 1): Creating and Customizing Our First Plots\" by Corey Schafer available at https://www.youtube.com/watch?v=UO98lJQ3QGI - \"Intro to Data Analysis / Visualization with Python, Matplotlib and Pandas | Matplotlib Tutorial\" by CS Dojo available at https://www.youtube.com/watch?v=a9UrKTVEeZA - \"Intro to Data Visualization in Python with Matplotlib! (line graph, bar chart, title, labels, size)\" by Keith Galli available at *https://www.youtube.com/watch?v=DAQNHzOcO5A","title":"This is a Matplotlib Cheat Sheet"},{"location":"8-Labs/Lab8/Lab8_Dev/#exercise-bins-bins-bins","text":"","title":"Exercise: Bins, Bins, Bins!  "},{"location":"8-Labs/Lab8/Lab8_Dev/#selecting-the-number-of-bins-is-an-important-decision-when-working-with-histograms-are-there-any-rules-or-recommendations-for-choosing-the-number-or-width-of-bins-what-happens-if-we-use-too-many-or-too-few-bins","text":"","title":"Selecting the number of bins is an important decision when working with histograms. Are there any rules or recommendations for choosing the number or width of bins? What happens if we use too many or too few bins?"},{"location":"8-Labs/Lab8/Lab8_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab8/Lab9_Dev/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab9 Laboratory 9: Matplotlib for Jam! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Matplotlip and Visual Display of Data This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib About matplotlib Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis). Background Data are not always numerical. Data can music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) They can also be categorical into which you can place individuals: - The individuals are cartons of ice-cream, and the category is the flavor in the carton - The individuals are professional basketball players, and the category is the player's team. Bar Graphs Bar charts (graphs) are good display tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='maroon', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='orange', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.bar(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:xlabel='Flavor'> Example- Language Bars! Consider the data set \"data\" defined as data = {'C':20, 'C++':15, 'Java':30, 'Python':35} which lists student count by programming language in some school. Produce a bar chart of number of students in each language, where language is the classification, and student count is the variable. # Code and run your solution here import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show() Plot it as a horizontal bar chart: # Code and run your solution here # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.barh(courses, values, color ='maroon', height = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show() Line Charts A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application. Example- Speed vs Time Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() From examination of the plot, estimate the speed at time t = 5.0 (eyeball estimate) Example- Add a linear fit Using the same series from Exercise 1, Plot the speed vs time (speed on y-axis, time on x-axis) using a line plot. Plot a second line based on the linear model y = mx + b , where b=0~\\text{and}~m=7.6 . # Code and run your solution here: def ymodel(xmodel,slope,intercept): ymodel = slope*xmodel+intercept return(ymodel) yseries = [] slope = 7.6 intercept = 0.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0.5) # basic line plot plt.plot(time, yseries, c='blue') plt.show() Example- Find a better fit Using trial and error try to improve the 'fit' of the model, by adjusting values of m~\\text{and}~b . # Code and run your solution here: yseries = [] slope = 7.6 intercept = -8.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0) # basic scatter plot plt.plot(time, yseries, c='blue') plt.show() Scatter Plots A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot Example- Examine the dataset with heights of fathers, mothers and sons df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists dad = df['father'] ; mom = df['mother'] ; son = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red' , label='Father') # one plot series plt.scatter(son, mom, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\") Histograms Quoting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson.[1] To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\" Example- Explore the \"top_movies\" dataset and draw histograms for Gross and Year. import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Year\"]].hist() array([[<AxesSubplot:title={'center':'Year'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) This is a Matplotlib Cheat Sheet Here are some of the resources used for creating this notebook: \"Discrete distribution as horizontal bar chart\" available at *https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html \"Bar Plot in Matplotlib\" available at *https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ Here are some great reads on this topic: - \"Python | Introduction to Matplotlib\" available at https://www.geeksforgeeks.org/python-introduction-matplotlib/ - \"Visualization with Matplotlib\" available at https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html - \"Introduction to Matplotlib \u2014 Data Visualization in Python\" by Ehi Aigiomawu available at https://heartbeat.fritz.ai/introduction-to-matplotlib-data-visualization-in-python-d9143287ae39 - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"Matplotlib Tutorial (Part 1): Creating and Customizing Our First Plots\" by Corey Schafer available at https://www.youtube.com/watch?v=UO98lJQ3QGI - \"Intro to Data Analysis / Visualization with Python, Matplotlib and Pandas | Matplotlib Tutorial\" by CS Dojo available at https://www.youtube.com/watch?v=a9UrKTVEeZA - \"Intro to Data Visualization in Python with Matplotlib! (line graph, bar chart, title, labels, size)\" by Keith Galli available at *https://www.youtube.com/watch?v=DAQNHzOcO5A Exercise: Bins, Bins, Bins! Selecting the number of bins is an important decision when working with histograms. Are there any rules or recommendations for choosing the number or width of bins? What happens if we use too many or too few bins? * Make sure to cite any resources that you may use.","title":"Lab9 Dev"},{"location":"8-Labs/Lab8/Lab9_Dev/#laboratory-9-matplotlib-for-jam","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Laboratory 9: Matplotlib for Jam! "},{"location":"8-Labs/Lab8/Lab9_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab8/Lab9_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab8/Lab9_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Lab8/Lab9_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab8/Lab9_Dev/#matplotlip-and-visual-display-of-data","text":"This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib","title":"Matplotlip and Visual Display of Data"},{"location":"8-Labs/Lab8/Lab9_Dev/#about-matplotlib","text":"Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis).","title":"About matplotlib"},{"location":"8-Labs/Lab8/Lab9_Dev/#background","text":"Data are not always numerical. Data can music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) They can also be categorical into which you can place individuals: - The individuals are cartons of ice-cream, and the category is the flavor in the carton - The individuals are professional basketball players, and the category is the player's team.","title":"Background"},{"location":"8-Labs/Lab8/Lab9_Dev/#bar-graphs","text":"Bar charts (graphs) are good display tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='maroon', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='orange', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.bar(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:xlabel='Flavor'>","title":"Bar Graphs"},{"location":"8-Labs/Lab8/Lab9_Dev/#example-language-bars","text":"Consider the data set \"data\" defined as data = {'C':20, 'C++':15, 'Java':30, 'Python':35} which lists student count by programming language in some school. Produce a bar chart of number of students in each language, where language is the classification, and student count is the variable. # Code and run your solution here import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show()","title":"Example- Language Bars!"},{"location":"8-Labs/Lab8/Lab9_Dev/#plot-it-as-a-horizontal-bar-chart","text":"# Code and run your solution here # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.barh(courses, values, color ='maroon', height = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show()","title":"Plot it as a horizontal bar chart:"},{"location":"8-Labs/Lab8/Lab9_Dev/#line-charts","text":"A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application.","title":"Line Charts"},{"location":"8-Labs/Lab8/Lab9_Dev/#example-speed-vs-time","text":"Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() From examination of the plot, estimate the speed at time t = 5.0 (eyeball estimate)","title":"Example- Speed vs Time"},{"location":"8-Labs/Lab8/Lab9_Dev/#example-add-a-linear-fit","text":"Using the same series from Exercise 1, Plot the speed vs time (speed on y-axis, time on x-axis) using a line plot. Plot a second line based on the linear model y = mx + b , where b=0~\\text{and}~m=7.6 . # Code and run your solution here: def ymodel(xmodel,slope,intercept): ymodel = slope*xmodel+intercept return(ymodel) yseries = [] slope = 7.6 intercept = 0.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0.5) # basic line plot plt.plot(time, yseries, c='blue') plt.show()","title":"Example- Add a linear fit"},{"location":"8-Labs/Lab8/Lab9_Dev/#example-find-a-better-fit","text":"Using trial and error try to improve the 'fit' of the model, by adjusting values of m~\\text{and}~b . # Code and run your solution here: yseries = [] slope = 7.6 intercept = -8.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0) # basic scatter plot plt.plot(time, yseries, c='blue') plt.show()","title":"Example- Find a better fit"},{"location":"8-Labs/Lab8/Lab9_Dev/#scatter-plots","text":"A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot","title":"Scatter Plots"},{"location":"8-Labs/Lab8/Lab9_Dev/#example-examine-the-dataset-with-heights-of-fathers-mothers-and-sons","text":"df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists dad = df['father'] ; mom = df['mother'] ; son = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red' , label='Father') # one plot series plt.scatter(son, mom, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\")","title":"Example- Examine the dataset with heights of fathers, mothers and sons"},{"location":"8-Labs/Lab8/Lab9_Dev/#histograms","text":"Quoting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson.[1] To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\"","title":"Histograms"},{"location":"8-Labs/Lab8/Lab9_Dev/#example-explore-the-top_movies-dataset-and-draw-histograms-for-gross-and-year","text":"import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Year\"]].hist() array([[<AxesSubplot:title={'center':'Year'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object)","title":"Example- Explore the \"top_movies\" dataset and draw histograms for Gross and Year."},{"location":"8-Labs/Lab8/Lab9_Dev/#this-is-a-matplotlib-cheat-sheet","text":"Here are some of the resources used for creating this notebook: \"Discrete distribution as horizontal bar chart\" available at *https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html \"Bar Plot in Matplotlib\" available at *https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ Here are some great reads on this topic: - \"Python | Introduction to Matplotlib\" available at https://www.geeksforgeeks.org/python-introduction-matplotlib/ - \"Visualization with Matplotlib\" available at https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html - \"Introduction to Matplotlib \u2014 Data Visualization in Python\" by Ehi Aigiomawu available at https://heartbeat.fritz.ai/introduction-to-matplotlib-data-visualization-in-python-d9143287ae39 - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"Matplotlib Tutorial (Part 1): Creating and Customizing Our First Plots\" by Corey Schafer available at https://www.youtube.com/watch?v=UO98lJQ3QGI - \"Intro to Data Analysis / Visualization with Python, Matplotlib and Pandas | Matplotlib Tutorial\" by CS Dojo available at https://www.youtube.com/watch?v=a9UrKTVEeZA - \"Intro to Data Visualization in Python with Matplotlib! (line graph, bar chart, title, labels, size)\" by Keith Galli available at *https://www.youtube.com/watch?v=DAQNHzOcO5A","title":"This is a Matplotlib Cheat Sheet"},{"location":"8-Labs/Lab8/Lab9_Dev/#exercise-bins-bins-bins","text":"","title":"Exercise: Bins, Bins, Bins!  "},{"location":"8-Labs/Lab8/Lab9_Dev/#selecting-the-number-of-bins-is-an-important-decision-when-working-with-histograms-are-there-any-rules-or-recommendations-for-choosing-the-number-or-width-of-bins-what-happens-if-we-use-too-many-or-too-few-bins","text":"","title":"Selecting the number of bins is an important decision when working with histograms. Are there any rules or recommendations for choosing the number or width of bins? What happens if we use too many or too few bins?"},{"location":"8-Labs/Lab8/Lab9_Dev/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Lab9/Lab9_Dev/","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: HEX: Title of the notebook Date: Lab9: Simulation Example1: Simulate a game of Russian Roulette: For 2 rounds For 5 rounds For 10 rounds import numpy as np #import numpy revolver = np.array([1,0,0,0,0,0]) #create a numpy array with 1 bullet and 5 empty chambers print(np.random.choice(revolver,2)) #randomly select a value from revolver - simulation [0 0] print(np.random.choice(revolver,5)) [0 0 0 1 1] print(np.random.choice(revolver,10)) [0 0 0 0 0 0 0 0 0 0] Exercise 1: Simulate the results of throwing a D6 (regular dice) for 10 times. Example2: Assume the following rules: If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. Define a function to simulate a game with the above rules, assuming a D6, and compute the net gain of the player over any given number of rolls. Compute the net gain for 5, 50, and 500 rolls def D6game(nrolls): import numpy as np #import numpy dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 rolls = np.random.choice(dice,nrolls) #randomly selecting a value from dice for nrolls times- simulation gainlist =[] #create an empty list for gains|losses for i in np.arange(len(rolls)): #Apply the rules if rolls[i]<=2: gainlist.append(-1) elif rolls[i]<=4: gainlist.append(0) elif rolls[i]<=6: gainlist.append(+1) return (np.sum(gainlist)) #sum up all gains|losses # return (gainlist,\"The net gain is equal to:\",np.sum(gainlist)) D6game(5) -2 D6game(50) -4 D6game(500) -16 Exercise2: Assume the following rules: If the dice shows 1 or 2 spots, my net gain is (-2*value of dice) dollars. If the dice shows 3 or 4 spots, my net gain is 1 dollars. If the dice shows 5 spots, my net gain is (2*value of dice) dollars. If the dice shows 6 spots, my net gain is -5 dollars. Define a function to simulate a game with the above rules, assuming a D6, and compute the net gain of the player over any given number of rolls. Compute the net gain for 5, 50, and 500 rolls # Define the function # Run for 5 rounds # Run for 50 rounds # Run for 500 rounds Example3: Simulate Monty Hall Game for 1000 times. Use a barplot and discuss whether players are better off sticking to their initial choice, or switching doors? def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 1', 'Goat 2', 'Car'] Goat 1 Goat 2 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining for i in np.arange(1000): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list import pandas as pd #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Car Goat 1 Goat 2 1 Car Goat 2 Goat 1 2 Car Goat 2 Goat 1 3 Goat 2 Goat 1 Car 4 Goat 2 Goat 1 Car ... ... ... ... 995 Car Goat 2 Goat 1 996 Car Goat 2 Goat 1 997 Goat 2 Goat 1 Car 998 Goat 2 Goat 1 Car 999 Goat 2 Goat 1 Car 1000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot import matplotlib.pyplot as plt # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show() According to the plot, it is statitically beneficial for the players to switch doors because the initial chance for being correct is only 1/3 Example4: What if there were 4 doors and 3 goats? import numpy as np import pandas as pd import matplotlib.pyplot as plt Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\",\"Goat 3\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\",\"Goat 3\"]) #Define a list for goats! def othergoat12(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" def othergoat23(x): #Define a function to return \"the other goat\"! if x == \"Goat 2\": return \"Goat 3\" elif x == \"Goat 3\": return \"Goat 2\" def othergoat13(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 3\" elif x == \"Goat 3\": return \"Goat 1\" ##################################### def othergoat123(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return np.random.choice([\"Goat 2\",\"Goat 3\"]) elif x == \"Goat 2\": return np.random.choice([\"Goat 1\",\"Goat 3\"]) elif x == \"Goat 3\": return np.random.choice([\"Goat 1\",\"Goat 2\"]) def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"unrevealed1\", \"unrevealed2\"] goats = np.array([\"Goat 1\" , \"Goat 2\",\"Goat 3\"]) userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": #If the user chooses Goat 1 revealed = np.random.choice(goats[np.arange(len(goats))!=0]) unrevealed1 = othergoat23(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Goat 2\": #If the user chooses Goat 2 revealed = np.random.choice(goats[np.arange(len(goats))!=1]) unrevealed1 = othergoat13(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Goat 3\": #If the user chooses Goat 3 revealed = np.random.choice(goats[np.arange(len(goats))!=2]) unrevealed1 = othergoat12(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Car\": #If the user chooses Car revealed = np.random.choice(goats) newgoat = goats[goats != revealed] unrevealed1 = newgoat[0] unrevealed2 = newgoat[1] return [userguess, revealed,unrevealed1,unrevealed2] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) print(a[3]) ['Car', 'Goat 1', 'Goat 2', 'Goat 3'] Car Goat 1 Goat 2 Goat 3 c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining1 c4 = [] #Create an empty list for the remaining2 for i in np.arange(1000): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list1 c4.append(game[3]) #In each round, add the fourth element to the remaining list2 import pandas as pd #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining1':c3, 'Remaining2':c4}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining1 Remaining2 0 Goat 3 Goat 2 Goat 1 Car 1 Goat 3 Goat 2 Goat 1 Car 2 Goat 1 Goat 3 Goat 2 Car 3 Goat 1 Goat 2 Goat 3 Car 4 Goat 2 Goat 1 Goat 3 Car ... ... ... ... ... 995 Goat 1 Goat 2 Goat 3 Car 996 Car Goat 1 Goat 2 Goat 3 997 Goat 2 Goat 1 Goat 3 Car 998 Goat 3 Goat 1 Goat 2 Car 999 Goat 2 Goat 1 Goat 3 Car 1000 rows \u00d7 4 columns # Get the count of each item in the first and (3rd+4th) column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining1 == 'Car'].shape[0] + gamedf[gamedf.Remaining2 == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining1 == 'Goat 1'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining1 == 'Goat 2'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 2'].shape[0] original_g3 =gamedf[gamedf.Guess == 'Goat 3'].shape[0] remaining_g3 =gamedf[gamedf.Remaining1 == 'Goat 3'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 3'].shape[0] # Let's plot a grouped barplot import matplotlib.pyplot as plt # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2,original_g3] bars2 = [remaining_car,remaining_g1,remaining_g2,remaining_g3] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2','Goat 3']) # Create legend & Show graphic plt.legend() plt.show() Comparison of the plots show that as the number of doors (and goats) increases, it makes even more sense to switch! Exercise3: Run the modified Monty Hall game for 10,100, and 1000 rounds. Show the bar plots for each series and explain the difference. #Define necessary functions #Run and plot for 10 rounds #Run and plot for 100 rounds #Run and plot for 1000 rounds","title":"Lab9 Dev"},{"location":"8-Labs/Lab9/Lab9_Dev/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Lab9/Lab9_Dev/#r","text":"","title":"R#:"},{"location":"8-Labs/Lab9/Lab9_Dev/#hex","text":"","title":"HEX:"},{"location":"8-Labs/Lab9/Lab9_Dev/#title-of-the-notebook","text":"","title":"Title of the notebook"},{"location":"8-Labs/Lab9/Lab9_Dev/#date","text":"","title":"Date:"},{"location":"8-Labs/Lab9/Lab9_Dev/#lab9-simulation","text":"","title":"Lab9: Simulation"},{"location":"8-Labs/Lab9/Lab9_Dev/#example1-simulate-a-game-of-russian-roulette","text":"For 2 rounds For 5 rounds For 10 rounds import numpy as np #import numpy revolver = np.array([1,0,0,0,0,0]) #create a numpy array with 1 bullet and 5 empty chambers print(np.random.choice(revolver,2)) #randomly select a value from revolver - simulation [0 0] print(np.random.choice(revolver,5)) [0 0 0 1 1] print(np.random.choice(revolver,10)) [0 0 0 0 0 0 0 0 0 0]","title":"Example1: Simulate a game of Russian Roulette:"},{"location":"8-Labs/Lab9/Lab9_Dev/#exercise-1-simulate-the-results-of-throwing-a-d6-regular-dice-for-10-times","text":"","title":"Exercise 1: Simulate the results of throwing a D6 (regular dice) for 10 times."},{"location":"8-Labs/Lab9/Lab9_Dev/#example2-assume-the-following-rules","text":"If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. Define a function to simulate a game with the above rules, assuming a D6, and compute the net gain of the player over any given number of rolls. Compute the net gain for 5, 50, and 500 rolls def D6game(nrolls): import numpy as np #import numpy dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 rolls = np.random.choice(dice,nrolls) #randomly selecting a value from dice for nrolls times- simulation gainlist =[] #create an empty list for gains|losses for i in np.arange(len(rolls)): #Apply the rules if rolls[i]<=2: gainlist.append(-1) elif rolls[i]<=4: gainlist.append(0) elif rolls[i]<=6: gainlist.append(+1) return (np.sum(gainlist)) #sum up all gains|losses # return (gainlist,\"The net gain is equal to:\",np.sum(gainlist)) D6game(5) -2 D6game(50) -4 D6game(500) -16","title":"Example2: Assume the following rules:"},{"location":"8-Labs/Lab9/Lab9_Dev/#exercise2-assume-the-following-rules","text":"If the dice shows 1 or 2 spots, my net gain is (-2*value of dice) dollars. If the dice shows 3 or 4 spots, my net gain is 1 dollars. If the dice shows 5 spots, my net gain is (2*value of dice) dollars. If the dice shows 6 spots, my net gain is -5 dollars. Define a function to simulate a game with the above rules, assuming a D6, and compute the net gain of the player over any given number of rolls. Compute the net gain for 5, 50, and 500 rolls # Define the function # Run for 5 rounds # Run for 50 rounds # Run for 500 rounds","title":"Exercise2: Assume the following rules:"},{"location":"8-Labs/Lab9/Lab9_Dev/#example3-simulate-monty-hall-game-for-1000-times-use-a-barplot-and-discuss-whether-players-are-better-off-sticking-to-their-initial-choice-or-switching-doors","text":"def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 1', 'Goat 2', 'Car'] Goat 1 Goat 2 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining for i in np.arange(1000): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list import pandas as pd #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Car Goat 1 Goat 2 1 Car Goat 2 Goat 1 2 Car Goat 2 Goat 1 3 Goat 2 Goat 1 Car 4 Goat 2 Goat 1 Car ... ... ... ... 995 Car Goat 2 Goat 1 996 Car Goat 2 Goat 1 997 Goat 2 Goat 1 Car 998 Goat 2 Goat 1 Car 999 Goat 2 Goat 1 Car 1000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot import matplotlib.pyplot as plt # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show() According to the plot, it is statitically beneficial for the players to switch doors because the initial chance for being correct is only 1/3","title":"Example3: Simulate Monty Hall Game for 1000 times. Use a barplot and discuss whether players are better off sticking to their initial choice, or switching doors?"},{"location":"8-Labs/Lab9/Lab9_Dev/#example4-what-if-there-were-4-doors-and-3-goats","text":"import numpy as np import pandas as pd import matplotlib.pyplot as plt Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\",\"Goat 3\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\",\"Goat 3\"]) #Define a list for goats! def othergoat12(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" def othergoat23(x): #Define a function to return \"the other goat\"! if x == \"Goat 2\": return \"Goat 3\" elif x == \"Goat 3\": return \"Goat 2\" def othergoat13(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 3\" elif x == \"Goat 3\": return \"Goat 1\" ##################################### def othergoat123(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return np.random.choice([\"Goat 2\",\"Goat 3\"]) elif x == \"Goat 2\": return np.random.choice([\"Goat 1\",\"Goat 3\"]) elif x == \"Goat 3\": return np.random.choice([\"Goat 1\",\"Goat 2\"]) def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"unrevealed1\", \"unrevealed2\"] goats = np.array([\"Goat 1\" , \"Goat 2\",\"Goat 3\"]) userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": #If the user chooses Goat 1 revealed = np.random.choice(goats[np.arange(len(goats))!=0]) unrevealed1 = othergoat23(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Goat 2\": #If the user chooses Goat 2 revealed = np.random.choice(goats[np.arange(len(goats))!=1]) unrevealed1 = othergoat13(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Goat 3\": #If the user chooses Goat 3 revealed = np.random.choice(goats[np.arange(len(goats))!=2]) unrevealed1 = othergoat12(revealed) unrevealed2 = \"Car\" return [userguess, revealed,unrevealed1,unrevealed2] if userguess == \"Car\": #If the user chooses Car revealed = np.random.choice(goats) newgoat = goats[goats != revealed] unrevealed1 = newgoat[0] unrevealed2 = newgoat[1] return [userguess, revealed,unrevealed1,unrevealed2] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) print(a[3]) ['Car', 'Goat 1', 'Goat 2', 'Goat 3'] Car Goat 1 Goat 2 Goat 3 c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining1 c4 = [] #Create an empty list for the remaining2 for i in np.arange(1000): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list1 c4.append(game[3]) #In each round, add the fourth element to the remaining list2 import pandas as pd #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining1':c3, 'Remaining2':c4}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining1 Remaining2 0 Goat 3 Goat 2 Goat 1 Car 1 Goat 3 Goat 2 Goat 1 Car 2 Goat 1 Goat 3 Goat 2 Car 3 Goat 1 Goat 2 Goat 3 Car 4 Goat 2 Goat 1 Goat 3 Car ... ... ... ... ... 995 Goat 1 Goat 2 Goat 3 Car 996 Car Goat 1 Goat 2 Goat 3 997 Goat 2 Goat 1 Goat 3 Car 998 Goat 3 Goat 1 Goat 2 Car 999 Goat 2 Goat 1 Goat 3 Car 1000 rows \u00d7 4 columns # Get the count of each item in the first and (3rd+4th) column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining1 == 'Car'].shape[0] + gamedf[gamedf.Remaining2 == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining1 == 'Goat 1'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining1 == 'Goat 2'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 2'].shape[0] original_g3 =gamedf[gamedf.Guess == 'Goat 3'].shape[0] remaining_g3 =gamedf[gamedf.Remaining1 == 'Goat 3'].shape[0] + gamedf[gamedf.Remaining2 == 'Goat 3'].shape[0] # Let's plot a grouped barplot import matplotlib.pyplot as plt # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2,original_g3] bars2 = [remaining_car,remaining_g1,remaining_g2,remaining_g3] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2','Goat 3']) # Create legend & Show graphic plt.legend() plt.show() Comparison of the plots show that as the number of doors (and goats) increases, it makes even more sense to switch!","title":"Example4: What if there were 4 doors and 3 goats?"},{"location":"8-Labs/Lab9/Lab9_Dev/#exercise3-run-the-modified-monty-hall-game-for-10100-and-1000-rounds-show-the-bar-plots-for-each-series-and-explain-the-difference","text":"#Define necessary functions #Run and plot for 10 rounds #Run and plot for 100 rounds #Run and plot for 1000 rounds","title":"Exercise3: Run the modified Monty Hall game for 10,100, and 1000 rounds. Show the bar plots for each series and explain the difference."},{"location":"8-Labs/Newly Formatted/Lab0/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab0 Laboratory 0: Yes, That's how we count in python! Welcome to your first Jupyter Notebook . This is a medium that we will be using throughout the semester. Why is this called a notebook? Because you can write stuff in it! Is that it? Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!). How do we get this? There are online services that allow you create, modify, and export Jupyter notebooks. However, to have this on your local machines (computers), you can install Anaconda . Anaconda is a package of different software suits including \"Jupyter Notebook\". You can find videos on how to install Anaconda on your devices on BlackBoard: Go to Anaconda.com Scroll down to the bottom of the page or click on products > individual edition Download the right version for your system: Windows, MacOS, and Linux- This may take a while depending on your connection speed Once the installer file is downloaded, run it and install Anaconda on your machine. Anaconda requires almost 3 GB of free space Install it in a separate folder- Preferably on a drive with lots of free memory! BE PATIENT!- It will take a while. The Environment - Let's have a look around this window! Rami Malek in Mr. Robot The tabs: File Edit View Insert Cell Kernel The Icons: Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list) The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type. There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially). Code Cells: A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code. When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability. Markdown Cells: You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied: If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks: # title ## major headings ### subheadings #### 4th level subheadings ##### 5th level subheadings These codes are also quite useful: Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format: Raw Cells: Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook. Let's meet world's most popular python! What is python? \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language) How to have access to it? There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers: a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3 We can do the exact same thing in this notebook. But we need a CODE cell. print(\"Hello World\") Hello World This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens: print(\"This is my first notebook!\") This is my first notebook! How to save a notebook? As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE Exercise: Let's see who you are! Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!","title":"Lab0"},{"location":"8-Labs/Newly Formatted/Lab0/#laboratory-0-yes-thats-how-we-count-in-python","text":"","title":"Laboratory 0: Yes, That's how we count in python!"},{"location":"8-Labs/Newly Formatted/Lab0/#welcome-to-your-first-jupyter-notebook-this-is-a-medium-that-we-will-be-using-throughout-the-semester","text":"","title":"Welcome to your first Jupyter Notebook. This is a medium that we will be using throughout the semester."},{"location":"8-Labs/Newly Formatted/Lab0/#why-is-this-called-a-notebook","text":"","title":"Why is this called a notebook?"},{"location":"8-Labs/Newly Formatted/Lab0/#because-you-can-write-stuff-in-it","text":"","title":"Because you can write stuff in it!"},{"location":"8-Labs/Newly Formatted/Lab0/#is-that-it","text":"","title":"Is that it?"},{"location":"8-Labs/Newly Formatted/Lab0/#nope-you-can-write-and-run-code-in-this-notebook-plus-a-bunch-of-other-cool-stuff-such-as-making-graphs-running-tests-and-simulations-adding-images-and-prepare-documents-such-as-this-one","text":"","title":"Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!)."},{"location":"8-Labs/Newly Formatted/Lab0/#how-do-we-get-this","text":"","title":"How do we get this?"},{"location":"8-Labs/Newly Formatted/Lab0/#there-are-online-services-that-allow-you-create-modify-and-export-jupyter-notebooks-however-to-have-this-on-your-local-machines-computers-you-can-install-anaconda-anaconda-is-a-package-of-different-software-suits-including-jupyter-notebook-you-can-find-videos-on-how-to-install-anaconda-on-your-devices-on-blackboard","text":"Go to Anaconda.com Scroll down to the bottom of the page or click on products > individual edition Download the right version for your system: Windows, MacOS, and Linux- This may take a while depending on your connection speed Once the installer file is downloaded, run it and install Anaconda on your machine. Anaconda requires almost 3 GB of free space Install it in a separate folder- Preferably on a drive with lots of free memory! BE PATIENT!- It will take a while.","title":"There are online services that allow you create, modify, and export Jupyter notebooks. However, to have this on your local machines (computers), you can install Anaconda. Anaconda is a package of different software suits including \"Jupyter Notebook\". You can find videos on how to install Anaconda on your devices on BlackBoard:"},{"location":"8-Labs/Newly Formatted/Lab0/#the-environment-lets-have-a-look-around-this-window","text":"Rami Malek in Mr. Robot The tabs: File Edit View Insert Cell Kernel The Icons: Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list)","title":"The Environment - Let's have a look around this window!"},{"location":"8-Labs/Newly Formatted/Lab0/#the-notebook-consists-of-a-sequence-of-cells-a-cell-is-a-multiline-text-input-field-and-its-contents-can-be-executed-by-using-shift-enter-or-by-clicking-run-in-the-menu-bar-the-execution-behavior-of-a-cell-is-determined-by-the-cells-type","text":"","title":"The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type."},{"location":"8-Labs/Newly Formatted/Lab0/#there-are-three-types-of-cells-code-cells-markdown-cells-and-raw-cells-every-cell-starts-off-being-a-code-cell-but-its-type-can-be-changed-by-using-a-drop-down-on-the-toolbar-which-will-be-code-initially","text":"","title":"There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially)."},{"location":"8-Labs/Newly Formatted/Lab0/#code-cells","text":"","title":"Code Cells:"},{"location":"8-Labs/Newly Formatted/Lab0/#a-code-cell-allows-you-to-edit-and-write-new-code-with-full-syntax-highlighting-and-tab-completion-the-programming-language-you-use-depends-on-the-kernel-what-we-will-use-for-this-course-and-the-default-kernel-ipython-runs-is-python-code","text":"","title":"A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code."},{"location":"8-Labs/Newly Formatted/Lab0/#when-a-code-cell-is-executed-code-that-it-contains-is-sent-to-the-kernel-associated-with-the-notebook-the-results-that-are-returned-from-this-computation-are-then-displayed-in-the-notebook-as-the-cells-output-the-output-is-not-limited-to-text-with-many-other-possible-forms-of-output-are-also-possible-including-matplotlib-figures-and-html-tables-this-is-known-as-ipythons-rich-display-capability","text":"","title":"When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability."},{"location":"8-Labs/Newly Formatted/Lab0/#markdown-cells","text":"","title":"Markdown Cells:"},{"location":"8-Labs/Newly Formatted/Lab0/#you-can-document-the-computational-process-in-a-literate-way-alternating-descriptive-text-with-code-using-rich-text-in-ipython-this-is-accomplished-by-marking-up-text-with-the-markdown-language-the-corresponding-cells-are-called-markdown-cells-the-markdown-language-provides-a-simple-way-to-perform-this-text-markup-that-is-to-specify-which-parts-of-the-text-should-be-emphasized-italics-bold-form-lists-etc-in-fact-markdown-cells-allow-a-variety-of-cool-modifications-to-be-applied","text":"","title":"You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied:"},{"location":"8-Labs/Newly Formatted/Lab0/#if-you-want-to-provide-structure-for-your-document-you-can-use-markdown-headings-markdown-headings-consist-of-1-to-5-hash-signs-followed-by-a-space-and-the-title-of-your-section-the-markdown-heading-will-be-converted-to-a-clickable-link-for-a-section-of-the-notebook-it-is-also-used-as-a-hint-when-exporting-to-other-document-formats-like-pdf-here-is-how-it-looks","text":"","title":"If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks:"},{"location":"8-Labs/Newly Formatted/Lab0/#title","text":"","title":"# title"},{"location":"8-Labs/Newly Formatted/Lab0/#major-headings","text":"","title":"## major headings"},{"location":"8-Labs/Newly Formatted/Lab0/#subheadings","text":"","title":"### subheadings"},{"location":"8-Labs/Newly Formatted/Lab0/#4th-level-subheadings","text":"","title":"#### 4th level subheadings"},{"location":"8-Labs/Newly Formatted/Lab0/#5th-level-subheadings","text":"","title":"##### 5th level subheadings"},{"location":"8-Labs/Newly Formatted/Lab0/#these-codes-are-also-quite-useful","text":"Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format:","title":"These codes are also quite useful:"},{"location":"8-Labs/Newly Formatted/Lab0/#raw-cells","text":"","title":"Raw Cells:"},{"location":"8-Labs/Newly Formatted/Lab0/#raw-cells-provide-a-place-in-which-you-can-write-output-directly-raw-cells-are-not-evaluated-by-the-notebook","text":"","title":"Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook."},{"location":"8-Labs/Newly Formatted/Lab0/#lets-meet-worlds-most-popular-python","text":"","title":"Let's meet world's most popular python!"},{"location":"8-Labs/Newly Formatted/Lab0/#what-is-python","text":"\"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language)","title":"What is python?"},{"location":"8-Labs/Newly Formatted/Lab0/#how-to-have-access-to-it","text":"","title":"How to have access to it?"},{"location":"8-Labs/Newly Formatted/Lab0/#there-are-plenty-of-ways-from-online-compilers-to-our-beloved-jupyter-notebook-on-your-local-machines-here-are-a-few-examples-of-online-compilers","text":"a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3","title":"There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers:"},{"location":"8-Labs/Newly Formatted/Lab0/#we-can-do-the-exact-same-thing-in-this-notebook-but-we-need-a-code-cell","text":"print(\"Hello World\") Hello World","title":"We can do the exact same thing in this notebook. But we need a CODE cell."},{"location":"8-Labs/Newly Formatted/Lab0/#this-is-the-classic-first-program-of-many-languages-the-script-input-is-quite-simple-we-instruct-the-computer-to-print-the-literal-string-hello-world-to-standard-inputoutput-device-which-is-the-console-lets-change-it-and-see-what-happens","text":"print(\"This is my first notebook!\") This is my first notebook!","title":"This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens:"},{"location":"8-Labs/Newly Formatted/Lab0/#how-to-save-a-notebook","text":"As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE","title":"How to save a notebook?"},{"location":"8-Labs/Newly Formatted/Lab0/#exercise-lets-see-who-you-are","text":"","title":"Exercise: Let's see who you are! "},{"location":"8-Labs/Newly Formatted/Lab0/#similar-to-the-example-use-a-code-cell-and-print-a-paragraph-about-you-you-can-introduce-yourselves-and-write-about-interesting-things-to-and-about-you","text":"","title":"Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!"},{"location":"8-Labs/Newly Formatted/Lab1/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Laboratory 1 Laboratory 1: A Notebook Like No Other! Welcome to your first Jupyter Notebook . This is a medium that we will be using throughout the semester. Why is this called a notebook? Because you can write stuff in it! Is that it? Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!). The Environment - Let's have a look around this window! Rami Malek in Mr. Robot The tabs: File Edit View Insert Cell Kernel The Icons: Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list) The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type. There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially). Code Cells: A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code. When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability. Markdown Cells: You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied: If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks: # title ## major headings ### subheadings #### 4th level subheadings ##### 5th level subheadings These codes are also quite useful: Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format: Raw Cells: Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook. Let's meet world's most popular python! What is python? \"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language) How to have access to it? There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers: a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3 We can do the exact same thing in this notebook. But we need a CODE cell. print(\"Hello World\") Hello World This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens: print(\"This is my first notebook!\") This is my first notebook! How to save a notebook? As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE Exercise: Let's see who you are! Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!","title":"Lab1"},{"location":"8-Labs/Newly Formatted/Lab1/#laboratory-1-a-notebook-like-no-other","text":"","title":"Laboratory 1: A Notebook Like No Other!"},{"location":"8-Labs/Newly Formatted/Lab1/#welcome-to-your-first-jupyter-notebook-this-is-a-medium-that-we-will-be-using-throughout-the-semester","text":"","title":"Welcome to your first Jupyter Notebook. This is a medium that we will be using throughout the semester."},{"location":"8-Labs/Newly Formatted/Lab1/#why-is-this-called-a-notebook","text":"","title":"Why is this called a notebook?"},{"location":"8-Labs/Newly Formatted/Lab1/#because-you-can-write-stuff-in-it","text":"","title":"Because you can write stuff in it!"},{"location":"8-Labs/Newly Formatted/Lab1/#is-that-it","text":"","title":"Is that it?"},{"location":"8-Labs/Newly Formatted/Lab1/#nope-you-can-write-and-run-code-in-this-notebook-plus-a-bunch-of-other-cool-stuff-such-as-making-graphs-running-tests-and-simulations-adding-images-and-prepare-documents-such-as-this-one","text":"","title":"Nope! you can write and run CODE in this notebook! Plus a bunch of other cool stuff such as making graphs, running tests and simulations, adding images, and prepare documents (such as this one!)."},{"location":"8-Labs/Newly Formatted/Lab1/#the-environment-lets-have-a-look-around-this-window","text":"Rami Malek in Mr. Robot The tabs: File Edit View Insert Cell Kernel The Icons: Save Insert Cell Below Cut Copy Paste Cells Below Move Up Move Down Run Intruppt Kernel Restart Kernel Cell Type Selector (Dropdown list)","title":"The Environment - Let's have a look around this window!"},{"location":"8-Labs/Newly Formatted/Lab1/#the-notebook-consists-of-a-sequence-of-cells-a-cell-is-a-multiline-text-input-field-and-its-contents-can-be-executed-by-using-shift-enter-or-by-clicking-run-in-the-menu-bar-the-execution-behavior-of-a-cell-is-determined-by-the-cells-type","text":"","title":"The notebook consists of a sequence of cells. A cell is a multiline text input field, and its contents can be executed by using Shift-Enter, or by clicking Run in the menu bar. The execution behavior of a cell is determined by the cell\u2019s type."},{"location":"8-Labs/Newly Formatted/Lab1/#there-are-three-types-of-cells-code-cells-markdown-cells-and-raw-cells-every-cell-starts-off-being-a-code-cell-but-its-type-can-be-changed-by-using-a-drop-down-on-the-toolbar-which-will-be-code-initially","text":"","title":"There are three types of cells: code cells, markdown cells, and raw cells. Every cell starts off being a code cell, but its type can be changed by using a drop-down on the toolbar (which will be \u201cCode\u201d, initially)."},{"location":"8-Labs/Newly Formatted/Lab1/#code-cells","text":"","title":"Code Cells:"},{"location":"8-Labs/Newly Formatted/Lab1/#a-code-cell-allows-you-to-edit-and-write-new-code-with-full-syntax-highlighting-and-tab-completion-the-programming-language-you-use-depends-on-the-kernel-what-we-will-use-for-this-course-and-the-default-kernel-ipython-runs-is-python-code","text":"","title":"A code cell allows you to edit and write new code, with full syntax highlighting and tab completion. The programming language you use depends on the kernel. What we will use for this course and the default kernel IPython runs, is Python code."},{"location":"8-Labs/Newly Formatted/Lab1/#when-a-code-cell-is-executed-code-that-it-contains-is-sent-to-the-kernel-associated-with-the-notebook-the-results-that-are-returned-from-this-computation-are-then-displayed-in-the-notebook-as-the-cells-output-the-output-is-not-limited-to-text-with-many-other-possible-forms-of-output-are-also-possible-including-matplotlib-figures-and-html-tables-this-is-known-as-ipythons-rich-display-capability","text":"","title":"When a code cell is executed, code that it contains is sent to the kernel associated with the notebook. The results that are returned from this computation are then displayed in the notebook as the cell\u2019s output. The output is not limited to text, with many other possible forms of output are also possible, including matplotlib figures and HTML tables. This is known as IPython\u2019s rich display capability."},{"location":"8-Labs/Newly Formatted/Lab1/#markdown-cells","text":"","title":"Markdown Cells:"},{"location":"8-Labs/Newly Formatted/Lab1/#you-can-document-the-computational-process-in-a-literate-way-alternating-descriptive-text-with-code-using-rich-text-in-ipython-this-is-accomplished-by-marking-up-text-with-the-markdown-language-the-corresponding-cells-are-called-markdown-cells-the-markdown-language-provides-a-simple-way-to-perform-this-text-markup-that-is-to-specify-which-parts-of-the-text-should-be-emphasized-italics-bold-form-lists-etc-in-fact-markdown-cells-allow-a-variety-of-cool-modifications-to-be-applied","text":"","title":"You can document the computational process in a literate way, alternating descriptive text with code, using rich text. In IPython this is accomplished by marking up text with the Markdown language. The corresponding cells are called Markdown cells. The Markdown language provides a simple way to perform this text markup, that is, to specify which parts of the text should be emphasized (italics), bold, form lists, etc. In fact, markdown cells allow a variety of cool modifications to be applied:"},{"location":"8-Labs/Newly Formatted/Lab1/#if-you-want-to-provide-structure-for-your-document-you-can-use-markdown-headings-markdown-headings-consist-of-1-to-5-hash-signs-followed-by-a-space-and-the-title-of-your-section-the-markdown-heading-will-be-converted-to-a-clickable-link-for-a-section-of-the-notebook-it-is-also-used-as-a-hint-when-exporting-to-other-document-formats-like-pdf-here-is-how-it-looks","text":"","title":"If you want to provide structure for your document, you can use markdown headings. Markdown headings consist of 1 to 5 hash # signs followed by a space and the title of your section. (The markdown heading will be converted to a clickable link for a section of the notebook. It is also used as a hint when exporting to other document formats, like PDF.) Here is how it looks:"},{"location":"8-Labs/Newly Formatted/Lab1/#title","text":"","title":"# title"},{"location":"8-Labs/Newly Formatted/Lab1/#major-headings","text":"","title":"## major headings"},{"location":"8-Labs/Newly Formatted/Lab1/#subheadings","text":"","title":"### subheadings"},{"location":"8-Labs/Newly Formatted/Lab1/#4th-level-subheadings","text":"","title":"#### 4th level subheadings"},{"location":"8-Labs/Newly Formatted/Lab1/#5th-level-subheadings","text":"","title":"##### 5th level subheadings"},{"location":"8-Labs/Newly Formatted/Lab1/#these-codes-are-also-quite-useful","text":"Use triple \" * \" before and after a word (without spacing) to make the word bold and italic B&I: string __ or before and after a word (without spacing) to make the word bold Bold: string or string** _ or * before and after a word (without spacing to make the word italic Italic: string or string Double ~ before and after a word (without spacing to make the word scratched Scratched: ~~string~~ For line breaks use \"br\" in the middle of <> For colors use this code: Text Text Text For indented quoting, use a greater than sign (>) and then a space, then type the text. The text is indented and has a gray horizontal line to the left of it until the next carriage return. here is an example of how it works! For bullets, use the dash sign (- ) with a space after it, or a space, a dash, and a space ( - ), to create a circular bullet. To create a sub bullet, use a tab followed a dash and a space. You can also use an asterisk instead of a dash, and it works the same. For numbered lists, start with 1. followed by a space, then it starts numbering for you. Start each line with some number and a period, then a space. Tab to indent to get subnumbering. first second third ... For horizontal lines: Use three asterisks: *** For graphics, you can attach image files directly to a notebook only in Markdown cells. Drag and drop your images to the Mardown cell to attach it to the notebook. You can also use images from online sources be using this format:","title":"These codes are also quite useful:"},{"location":"8-Labs/Newly Formatted/Lab1/#raw-cells","text":"","title":"Raw Cells:"},{"location":"8-Labs/Newly Formatted/Lab1/#raw-cells-provide-a-place-in-which-you-can-write-output-directly-raw-cells-are-not-evaluated-by-the-notebook","text":"","title":"Raw cells provide a place in which you can write output directly. Raw cells are not evaluated by the notebook."},{"location":"8-Labs/Newly Formatted/Lab1/#lets-meet-worlds-most-popular-python","text":"","title":"Let's meet world's most popular python!"},{"location":"8-Labs/Newly Formatted/Lab1/#what-is-python","text":"\"Python is an interpreted, high-level and general-purpose programming language. Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\" - Wikipedia @ https://en.wikipedia.org/wiki/Python_(programming_language)","title":"What is python?"},{"location":"8-Labs/Newly Formatted/Lab1/#how-to-have-access-to-it","text":"","title":"How to have access to it?"},{"location":"8-Labs/Newly Formatted/Lab1/#there-are-plenty-of-ways-from-online-compilers-to-our-beloved-jupyter-notebook-on-your-local-machines-here-are-a-few-examples-of-online-compilers","text":"a. https://www.programiz.com/python-programming/online-compiler/ b. https://www.onlinegdb.com/online_python_compiler c. https://www.w3schools.com/python/python_compiler.asp d. https://repl.it/languages/python3","title":"There are plenty of ways, from online compilers to our beloved Jupyter Notebook on your local machines. Here are a few examples of online compilers:"},{"location":"8-Labs/Newly Formatted/Lab1/#we-can-do-the-exact-same-thing-in-this-notebook-but-we-need-a-code-cell","text":"print(\"Hello World\") Hello World","title":"We can do the exact same thing in this notebook. But we need a CODE cell."},{"location":"8-Labs/Newly Formatted/Lab1/#this-is-the-classic-first-program-of-many-languages-the-script-input-is-quite-simple-we-instruct-the-computer-to-print-the-literal-string-hello-world-to-standard-inputoutput-device-which-is-the-console-lets-change-it-and-see-what-happens","text":"print(\"This is my first notebook!\") This is my first notebook!","title":"This is the classic \"first program\" of many languages! The script input is quite simple, we instruct the computer to print the literal string \"hello world\" to standard input/output device which is the console. Let's change it and see what happens:"},{"location":"8-Labs/Newly Formatted/Lab1/#how-to-save-a-notebook","text":"As a notebook file (.ipynb): Go to File > Download As > Notebook (.ipynb) As an HTML file (.html): Go to File > Download As > HTML (.html) As a Pdf (.pdf): Go to File > Download As > PDF via LaTex (.pdf) or Save it as an HTML file and then convert that to a pdf via a website such as https://html2pdf.com/ Unless stated otherwise, we want you to submit your weekly lab assignments in PDF and your exam and project deliverables in both PDF and .ipynb formats. This notebook was inspired by several blogposts including: \"Markdown for Jupyter notebooks cheatsheet\" by Inge Halilovic available at *https://medium.com/@ingeh/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed \"Jupyter Notebook: An Introduction\" by Mike Driscoll available at *https://realpython.com/jupyter-notebook-introduction/ Here are some great reads on this topic: - \"Jupyter Notebook Tutorial: The Definitive Guide\" by Karlijn Willems available at https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook - \"Introduction to Jupyter Notebooks\" by Quinn Dombrowski, Tassie Gniady, and David Kloster available at https://programminghistorian.org/en/lessons/jupyter-notebooks - \"12 Things to know about Jupyter Notebook Markdown\" by Dayal Chand Aichara available at *https://medium.com/game-of-data/12-things-to-know-about-jupyter-notebook-markdown-3f6cef811707 Here are some great videos on these topics: - \"Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\" by Corey Schafer available at https://www.youtube.com/watch?v=HW29067qVWk - \"Quick introduction to Jupyter Notebook\" by Michael Fudge available at https://www.youtube.com/watch?v=jZ952vChhuI - \"What is Jupyter Notebook?\" by codebasics available at *https://www.youtube.com/watch?v=q_BzsPxwLOE","title":"How to save a notebook?"},{"location":"8-Labs/Newly Formatted/Lab1/#exercise-lets-see-who-you-are","text":"","title":"Exercise: Let's see who you are! "},{"location":"8-Labs/Newly Formatted/Lab1/#similar-to-the-example-use-a-code-cell-and-print-a-paragraph-about-you-you-can-introduce-yourselves-and-write-about-interesting-things-to-and-about-you","text":"","title":"Similar to the example, use a code cell and print a paragraph about you. You can introduce yourselves and write about interesting things to and about you!"},{"location":"8-Labs/Newly Formatted/Lab14/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: (LINK NEEDS FIXING!) Lab14 Laboratory 14: \"A Bullet or A Goat?\" or \"Things you should know before playing with strangers!\" # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Python for Simulation What is Russian roulette? Russian roulette (Russian: \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u0440\u0443\u043b\u0435\u0442\u043a\u0430, russkaya ruletka) is a lethal game of chance in which a player places a single round in a revolver, spins the cylinder, places the muzzle against their head, and pulls the trigger in hopes that the loaded chamber does not align with the primer percussion mechanism and the barrel, causing the weapon to discharge. Russian refers to the supposed country of origin, and roulette to the element of risk-taking and the spinning of the revolver's cylinder, which is reminiscent of a spinning roulette wheel. - Wikipedia @ https://en.wikipedia.org/wiki/Russian_roulette A game of dafts, a game of chance One where revolver's the one to dance Rounds and rounds, it goes and spins Makes you regret all those sins \\ A game of fools, one of lethality With a one to six probability There were two guys and a gun With six chambers but only one... \\ CLICK, one pushed the gun CLICK, one missed the fun CLICK, \"that awful sound\" ... BANG!, one had his brains all around! Example: Simulate a game of Russian Roulette: For 2 rounds For 5 rounds For 10 rounds import numpy as np #import numpy revolver = np.array([1,0,0,0,0,0]) #create a numpy array with 1 bullet and 5 empty chambers print(np.random.choice(revolver,2)) #randomly select a value from revolver - simulation [0 0] print(np.random.choice(revolver,5)) [0 0 1 0 0] print(np.random.choice(revolver,10)) [0 0 0 1 0 0 0 0 0 0] Example: Simulate the results of throwing a D6 (regular dice) for 10 times. import numpy as np #import numpy dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 np.random.choice(dice,10) #randomly selecting a value from dice for 10 times- simulation array([5, 6, 5, 5, 1, 4, 6, 6, 3, 4]) Example: Assume the following rules: If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. Define a function to simulate a game with the above rules, assuming a D6, and compute the net gain of the player over any given number of rolls. Compute the net gain for 5, 50, and 500 rolls def D6game(nrolls): import numpy as np #import numpy dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 rolls = np.random.choice(dice,nrolls) #randomly selecting a value from dice for nrolls times- simulation gainlist =[] #create an empty list for gains|losses for i in np.arange(len(rolls)): #Apply the rules if rolls[i]<=2: gainlist.append(-1) elif rolls[i]<=4: gainlist.append(0) elif rolls[i]<=6: gainlist.append(+1) return (np.sum(gainlist)) #sum up all gains|losses # return (gainlist,\"The net gain is equal to:\",np.sum(gainlist)) D6game(5) 2 D6game(50) -4 D6game(500) 0 Let's Make A Deal Game Show and Monty Hall Problem The Monty Hall problem is a brain teaser, in the form of a probability puzzle, loosely based on the American television game show Let's Make a Deal and named after its original host, Monty Hall. The problem was originally posed (and solved) in a letter by Steve Selvin to the American Statistician in 1975 (Selvin 1975a), (Selvin 1975b). \"Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\" Is it to your advantage to switch your choice?\" From Wikipedia: https://en.wikipedia.org/wiki/Monty_Hall_problem Example: Simulate Monty Hall Game for 1000 times. Use a barplot and discuss whether players are better off sticking to their initial choice, or switching doors? def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 2', 'Goat 1', 'Car'] Goat 2 Goat 1 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining for i in np.arange(1000): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list import pandas as pd #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Car Goat 2 Goat 1 1 Car Goat 2 Goat 1 2 Goat 1 Goat 2 Car 3 Car Goat 2 Goat 1 4 Goat 2 Goat 1 Car ... ... ... ... 995 Goat 1 Goat 2 Car 996 Goat 1 Goat 2 Car 997 Car Goat 1 Goat 2 998 Car Goat 1 Goat 2 999 Goat 1 Goat 2 Car 1000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot import matplotlib.pyplot as plt # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show() According to the plot, it is statitically beneficial for the players to switch doors because the initial chance for being correct is only 1/3 Python for Probability Important Terminology: Experiment: An occurrence with an uncertain outcome that we can observe. For example, rolling a die. Outcome: The result of an experiment; one particular state of the world. What Laplace calls a \"case.\" For example: 4. Sample Space: The set of all possible outcomes for the experiment. For example, {1, 2, 3, 4, 5, 6}. Event: A subset of possible outcomes that together have some property we are interested in. For example, the event \"even die roll\" is the set of outcomes {2, 4, 6}. Probability: As Laplace said, the probability of an event with respect to a sample space is the number of favorable cases (outcomes from the sample space that are in the event) divided by the total number of cases in the sample space. (This assumes that all outcomes in the sample space are equally likely.) Since it is a ratio, probability will always be a number between 0 (representing an impossible event) and 1 (representing a certain event). For example, the probability of an even die roll is 3/6 = 1/2. From https://people.math.ethz.ch/~jteichma/probability.html import numpy as np import pandas as pd import matplotlib.pyplot as plt Example: In a game of Russian Roulette, the chance of surviving each round is 5/6 which is almost 83%. Using a for loop, compute probability of surviving For 2 rounds For 5 rounds For 10 rounds nrounds =[] probs =[] for i in range(3): nrounds.append(i) probs.append((5/6)**i) #probability of surviving- not getting the bullet! RRDF = pd.DataFrame({\"# of Rounds\": nrounds, \"Probability of Surviving\": probs}) RRDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rounds Probability of Surviving 0 0 1.000000 1 1 0.833333 2 2 0.694444 nrounds =[] probs =[] for i in range(6): nrounds.append(i) probs.append((5/6)**i) #probability of surviving- not getting the bullet! RRDF = pd.DataFrame({\"# of Rounds\": nrounds, \"Probability of Surviving\": probs}) RRDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rounds Probability of Surviving 0 0 1.000000 1 1 0.833333 2 2 0.694444 3 3 0.578704 4 4 0.482253 5 5 0.401878 nrounds =[] probs =[] for i in range(11): nrounds.append(i) probs.append((5/6)**i) #probability of surviving- not getting the bullet! RRDF = pd.DataFrame({\"# of Rounds\": nrounds, \"Probability of Surviving\": probs}) RRDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rounds Probability of Surviving 0 0 1.000000 1 1 0.833333 2 2 0.694444 3 3 0.578704 4 4 0.482253 5 5 0.401878 6 6 0.334898 7 7 0.279082 8 8 0.232568 9 9 0.193807 10 10 0.161506 RRDF.plot.scatter(x=\"# of Rounds\", y=\"Probability of Surviving\",color=\"red\") <matplotlib.axes._subplots.AxesSubplot at 0x2c7f96de308> Example: What will be the probability of constantly throwing an even number with a D20 in For 2 rolls For 5 rolls For 10 rolls For 15 rolls nrolls =[] probs =[] for i in range(1,16,1): nrolls.append(i) probs.append((1/2)**i) #probability of throwing an even number-10/20 or 1/2 DRDF = pd.DataFrame({\"# of Rolls\": nrolls, \"Probability of constantly throwing an even number\": probs}) DRDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rolls Probability of constantly throwing an even number 0 1 0.500000 1 2 0.250000 2 3 0.125000 3 4 0.062500 4 5 0.031250 5 6 0.015625 6 7 0.007812 7 8 0.003906 8 9 0.001953 9 10 0.000977 10 11 0.000488 11 12 0.000244 12 13 0.000122 13 14 0.000061 14 15 0.000031 DRDF.plot.scatter(x=\"# of Rolls\", y=\"Probability of constantly throwing an even number\",color=\"crimson\") <matplotlib.axes._subplots.AxesSubplot at 0x2c7f978f488> Example: What will be the probability of throwing at least one 6 with a D6: For 2 rolls For 5 rolls For 10 rolls For 50 rolls - Make a scatter plot for this one! nRolls =[] probs =[] for i in range(1,3,1): nRolls.append(i) probs.append(1-(5/6)**i) #probability of at least one 6: 1-(5/6) rollsDF = pd.DataFrame({\"# of Rolls\": nRolls, \"Probability of rolling at least one 6\": probs}) rollsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rolls Probability of rolling at least one 6 0 1 0.166667 1 2 0.305556 nRolls =[] probs =[] for i in range(1,6,1): nRolls.append(i) probs.append(1-(5/6)**i) #probability of at least one 6: 1-(5/6) rollsDF = pd.DataFrame({\"# of Rolls\": nRolls, \"Probability of rolling at least one 6\": probs}) rollsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rolls Probability of rolling at least one 6 0 1 0.166667 1 2 0.305556 2 3 0.421296 3 4 0.517747 4 5 0.598122 nRolls =[] probs =[] for i in range(1,11,1): nRolls.append(i) probs.append(1-(5/6)**i) #probability of at least one 6: 1-(5/6) rollsDF = pd.DataFrame({\"# of Rolls\": nRolls, \"Probability of rolling at least one 6\": probs}) rollsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rolls Probability of rolling at least one 6 0 1 0.166667 1 2 0.305556 2 3 0.421296 3 4 0.517747 4 5 0.598122 5 6 0.665102 6 7 0.720918 7 8 0.767432 8 9 0.806193 9 10 0.838494 nRolls =[] probs =[] for i in range(1,51,1): nRolls.append(i) probs.append(1-(5/6)**i) #probability of at least one 6: 1-(5/6) rollsDF = pd.DataFrame({\"# of Rolls\": nRolls, \"Probability of rolling at least one 6\": probs}) rollsDF.plot.scatter(x=\"# of Rolls\", y=\"Probability of rolling at least one 6\") <matplotlib.axes._subplots.AxesSubplot at 0x2c7f97a3d88> Example: What is the probability of drawing an ace at least once (with replacement): in 2 tries in 5 tries in 10 tries in 20 tries - make a scatter plot. nDraws =[] probs =[] for i in range(1,3,1): nDraws.append(i) probs.append(1-(48/52)**i) #probability of drawing an ace least once : 1-(48/52) DrawsDF = pd.DataFrame({\"# of Draws\": nDraws, \"Probability of drawing an ace at least once\": probs}) DrawsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Draws Probability of drawing an ace at least once 0 1 0.076923 1 2 0.147929 nDraws =[] probs =[] for i in range(1,6,1): nDraws.append(i) probs.append(1-(48/52)**i) #probability of drawing an ace least once : 1-(48/52) DrawsDF = pd.DataFrame({\"# of Draws\": nDraws, \"Probability of drawing an ace at least once\": probs}) DrawsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Draws Probability of drawing an ace at least once 0 1 0.076923 1 2 0.147929 2 3 0.213473 3 4 0.273975 4 5 0.329823 nDraws =[] probs =[] for i in range(1,11,1): nDraws.append(i) probs.append(1-(48/52)**i) #probability of drawing an ace least once : 1-(48/52) DrawsDF = pd.DataFrame({\"# of Draws\": nDraws, \"Probability of drawing an ace at least once\": probs}) DrawsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Draws Probability of drawing an ace at least once 0 1 0.076923 1 2 0.147929 2 3 0.213473 3 4 0.273975 4 5 0.329823 5 6 0.381375 6 7 0.428962 7 8 0.472888 8 9 0.513435 9 10 0.550863 nDraws =[] probs =[] for i in range(1,21,1): nDraws.append(i) probs.append(1-(48/52)**i) #probability of drawing an ace at least once : 1-(48/52) DrawsDF = pd.DataFrame({\"# of Draws\": nDraws, \"Probability of drawing an ace at least once\": probs}) DrawsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Draws Probability of drawing an ace at least once 0 1 0.076923 1 2 0.147929 2 3 0.213473 3 4 0.273975 4 5 0.329823 5 6 0.381375 6 7 0.428962 7 8 0.472888 8 9 0.513435 9 10 0.550863 10 11 0.585412 11 12 0.617303 12 13 0.646742 13 14 0.673915 14 15 0.698999 15 16 0.722153 16 17 0.743525 17 18 0.763254 18 19 0.781466 19 20 0.798276 DrawsDF.plot.scatter(x=\"# of Draws\", y=\"Probability of drawing an ace at least once\") <matplotlib.axes._subplots.AxesSubplot at 0x2c7f980aac8> Example: A) Write a function to find the probability of an event in percentage form based on given outcomes and sample space B) Use the function and compute the probability of rolling a 4 with a D6 C) Use the function and compute the probability of drawing a King from a standard deck of cards D) Use the function and compute the probability of drawing the King of Hearts from a standard deck of cards E) Use the function and compute the probability of drawing an ace after drawing a king F) Use the function and compute the probability of drawing an ace after drawing an ace G) Use the function and compute the probability of drawing a heart OR a club F) Use the function and compute the probability of drawing a Royal Flush *hint: (in poker) a straight flush including ace, king, queen, jack, and ten all in the same suit, which is the hand of the highest possible value This problem is designed based on an example by Daniel Poston from DataCamp, accessible @ https://www.datacamp.com/community/tutorials/statistics-python-tutorial-probability-1 # A # Create function that returns probability percent rounded to one decimal place def Prob(outcome, sampspace): probability = (outcome / sampspace) * 100 return round(probability, 1) # B outcome = 1 #Rolling a 4 is only one of the possible outcomes space = 6 #Rolling a D6 can have 6 different outcomes Prob(outcome, space) 16.7 # C outcome = 4 #Drawing a king is four of the possible outcomes space = 52 #Drawing from a standard deck of cards can have 52 different outcomes Prob(outcome, space) 7.7 # D outcome = 1 #Drawing the king of hearts is only 1 of the possible outcomes space = 52 #Drawing from a standard deck of cards can have 52 different outcomes Prob(outcome, space) 1.9 # E outcome = 4 #Drawing an ace is 4 of the possible outcomes space = 51 #One card has been drawn Prob(outcome, space) 7.8 # F outcome = 3 #Once Ace is already drawn space = 51 #One card has been drawn Prob(outcome, space) 5.9 # G hearts = 13 #13 cards of hearts in a deck space = 52 #total number of cards in a deck clubs = 13 #13 cards of clubs in a deck Prob_heartsORclubs= Prob(hearts, space) + Prob(clubs, space) print(\"Probability of drawing a heart or a club is\",Prob_heartsORclubs,\"%\") Probability of drawing a heart or a club is 50.0 % # F draw1 = 5 #5 cards are needed space1 = 52 #out of the possible 52 cards draw2 = 4 #4 cards are needed space2 = 51 #out of the possible 51 cards draw3 = 3 #3 cards are needed space3 = 50 #out of the possible 50 cards draw4 = 2 #2 cards are needed space4 = 49 #out of the possible 49 cards draw5 = 1 #1 cards is needed space5 = 48 #out of the possible 48 cards #Probability of a getting a Royal Flush Prob_RF= 4*(Prob(draw1, space1)/100) * (Prob(draw2, space2)/100) * (Prob(draw3, space3)/100) * (Prob(draw4, space4)/100) * (Prob(draw5, space5)/100) print(\"Probability of drawing a royal flush is\",Prob_RF,\"%\") Probability of drawing a royal flush is 1.5473203199999998e-06 % Example: Two unbiased dice are thrown once and the total score is observed. Define an appropriate function and use a simulation to find the estimated probability that : the total score is greater than 10? the total score is even and greater than 7? This problem is designed based on an example by Elliott Saslow from Medium.com, accessible @ https://medium.com/future-vision/simulating-probability-events-in-python-5dd29e34e381 import numpy as np def DiceRoll1(nSimulation): count =0 dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 for i in range(nSimulation): die1 = np.random.choice(dice,1) #randomly selecting a value from dice - throw the D6 once die2 = np.random.choice(dice,1) #randomly selecting a value from dice - throw the D6 once again! score = die1 + die2 #summing them up if score > 10: #if it meets our desired condition: count +=1 #add one to the \"count\" return count/nSimulation #compute the probability of the desired event by dividing count by the total number of trials nSimulation = 10000 print(\"The probability of rolling a number greater than 10 after\",nSimulation,\"rolld is:\",DiceRoll1(nSimulation)*100,\"%\") The probability of rolling a number greater than 10 after 10000 rolld is: 8.35 % import numpy as np def DiceRoll2(nSimulation): count =0 dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 for i in range(nSimulation): die1 = np.random.choice(dice,1) #randomly selecting a value from dice - throw the D6 once die2 = np.random.choice(dice,1) #randomly selecting a value from dice - throw the D6 once again! score = die1 + die2 if score %2 ==0 and score > 7: #the total score is even and greater than 7 count +=1 return count/nSimulation nSimulation = 10000 print(\"The probability of rolling an even number and greater than 7 after\",nSimulation,\" rolls is:\",DiceRoll2(nSimulation)*100,\"%\") The probability of rolling an even number and greater than 7 after 10000 rolls is: 24.77 % Example: An urn contains 10 white balls, 20 reds and 30 greens. We want to draw 5 balls with replacement. Use a simulation (10000 trials) to find the estimated probability that: we draw 3 white and 2 red balls we draw 5 balls of the same color This problem is designed based on an example by Elliott Saslow from Medium.com, accessible @ https://medium.com/future-vision/simulating-probability-events-in-python-5dd29e34e381 # A import numpy as np import random d = {} #Create an empty dictionary to associate numbers and colors for i in range(0,60,1): #total of 60 balls if i <10: #10 white balls d[i]=\"White\" elif i>9 and i<30: #20 red balls d[i]=\"Red\" else: #60-30=30 green balls d[i]=\"Green\" # nSimulation= 10000 #How many trials? outcome1= 0 #initial value on the desired outcome counter for i in range(nSimulation): draw=[] #an empty list for the draws for i in range(5): #how many balls we want to draw? draw.append(d[random.randint(0,59)]) #randomly choose a number from 0 to 59- simulation of drawing balls drawarray = np.array(draw) #convert the list into a numpy array white = sum(drawarray== \"White\") #count the white balls red = sum(drawarray== \"Red\") #count the red balls green = sum(drawarray== \"Green\") #count the green balls if white ==3 and red==2: #If the desired condition is met, add one to the counter outcome1 +=1 print(\"The probability of drawing 3 white and 2 red balls is\",(outcome1/nSimulation)*100,\"%\") The probability of drawing 3 white and 2 red balls is 0.54 % # B import numpy as np import random d = {} for i in range(0,60,1): if i <10: d[i]=\"White\" elif i>9 and i<30: d[i]=\"Red\" else: d[i]=\"Green\" # nSimulation= 10000 outcome1= 0 outcome2= 0 #we can consider multiple desired outcomes for i in range(nSimulation): draw=[] for i in range(5): draw.append(d[random.randint(0,59)]) drawarray = np.array(draw) white = sum(drawarray== \"White\") red = sum(drawarray== \"Red\") green = sum(drawarray== \"Green\") if white ==3 and red==2: outcome1 +=1 if white ==5 or red==5 or green==5: outcome2 +=1 print(\"The probability of drawing 3 white and 2 red balls is\",(outcome1/nSimulation)*100,\"%\") print(\"The probability of drawing 5 balls of the same color is\",(outcome2/nSimulation)*100,\"%\") The probability of drawing 3 white and 2 red balls is 0.53 % The probability of drawing 5 balls of the same color is 3.8 % Here are some of the resources used for creating this notebook: \"Poker Probability and Statistics with Python\" by Daniel Poston available at https://www.datacamp.com/community/tutorials/statistics-python-tutorial-probability-1 \"Simulating probability events in Python\" by Elliott Saslow available at https://medium.com/future-vision/simulating-probability-events-in-python-5dd29e34e381 Here are some great reads on this topic: - \"Simulate the Monty Hall Problem Using Python\" by randerson112358 available at https://medium.com/swlh/simulate-the-monty-hall-problem-using-python-7b76b943640e - \"The Monty Hall problem\" available at https://scipython.com/book/chapter-4-the-core-python-language-ii/examples/the-monty-hall-problem/ - \"Introduction to Probability Using Python\" by Lisandra Melo available at https://medium.com/future-vision/simulating-probability-events-in-python-5dd29e34e381 - \"Introduction to probability and statistics for Data Scientists and machine learning using python : Part-1\" by Arun Singh available at https://medium.com/@anayan/introduction-to-probability-and-statistics-for-data-scientists-and-machine-learning-using-python-377a9b082487 Here are some great videos on these topics: - \"Monty Hall Problem - Numberphile\" by Numberphile available at https://www.youtube.com/watch?v=4Lb-6rxZxx0 - \"The Monty Hall Problem\" by D!NG available at https://www.youtube.com/watch?v=TVq2ivVpZgQ - \"21 - Monty Hall - PROPENSITY BASED THEORETICAL MODEL PROBABILITY - MATHEMATICS in the MOVIES\" by Motivating Mathematical Education and STEM available at https://www.youtube.com/watch?v=iBdjqtR2iK4 - \"The Monty Hall Problem\" by niansenx available at https://www.youtube.com/watch?v=mhlc7peGlGg - \"The Monty Hall Problem - Explained\" by AsapSCIENCE available at https://www.youtube.com/watch?v=9vRUxbzJZ9Y - \"Introduction to Probability | 365 Data Science Online Course\" by 365 Data Science available at https://www.youtube.com/watch?v=soZRfdnkUQg - \"Probability explained | Independent and dependent events | Probability and Statistics | Khan Academy\" by Khan Academy available at https://www.youtube.com/watch?v=uzkc-qNVoOk - \"Math Antics - Basic Probability\" by mathantics available at https://www.youtube.com/watch?v=KzfWUEJjG18 Exercise: Risk or Probability Are they the same? Are they different? Discuss your opinion. Make sure to cite any resources that you may use.","title":"Lab14"},{"location":"8-Labs/Newly Formatted/Lab14/#laboratory-14-a-bullet-or-a-goat-or-things-you-should-know-before-playing-with-strangers","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 14: \"A Bullet or A Goat?\" or \"Things you should know before playing with strangers!\""},{"location":"8-Labs/Newly Formatted/Lab14/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab14/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab14/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Newly Formatted/Lab14/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab14/#python-for-simulation","text":"","title":"Python for Simulation"},{"location":"8-Labs/Newly Formatted/Lab14/#what-is-russian-roulette","text":"Russian roulette (Russian: \u0440\u0443\u0441\u0441\u043a\u0430\u044f \u0440\u0443\u043b\u0435\u0442\u043a\u0430, russkaya ruletka) is a lethal game of chance in which a player places a single round in a revolver, spins the cylinder, places the muzzle against their head, and pulls the trigger in hopes that the loaded chamber does not align with the primer percussion mechanism and the barrel, causing the weapon to discharge. Russian refers to the supposed country of origin, and roulette to the element of risk-taking and the spinning of the revolver's cylinder, which is reminiscent of a spinning roulette wheel. - Wikipedia @ https://en.wikipedia.org/wiki/Russian_roulette A game of dafts, a game of chance One where revolver's the one to dance Rounds and rounds, it goes and spins Makes you regret all those sins \\ A game of fools, one of lethality With a one to six probability There were two guys and a gun With six chambers but only one... \\ CLICK, one pushed the gun CLICK, one missed the fun CLICK, \"that awful sound\" ... BANG!, one had his brains all around!","title":"What is Russian roulette?"},{"location":"8-Labs/Newly Formatted/Lab14/#example-simulate-a-game-of-russian-roulette","text":"For 2 rounds For 5 rounds For 10 rounds import numpy as np #import numpy revolver = np.array([1,0,0,0,0,0]) #create a numpy array with 1 bullet and 5 empty chambers print(np.random.choice(revolver,2)) #randomly select a value from revolver - simulation [0 0] print(np.random.choice(revolver,5)) [0 0 1 0 0] print(np.random.choice(revolver,10)) [0 0 0 1 0 0 0 0 0 0]","title":"Example: Simulate a game of Russian Roulette:"},{"location":"8-Labs/Newly Formatted/Lab14/#example-simulate-the-results-of-throwing-a-d6-regular-dice-for-10-times","text":"import numpy as np #import numpy dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 np.random.choice(dice,10) #randomly selecting a value from dice for 10 times- simulation array([5, 6, 5, 5, 1, 4, 6, 6, 3, 4])","title":"Example: Simulate the results of throwing a D6 (regular dice) for 10 times."},{"location":"8-Labs/Newly Formatted/Lab14/#example-assume-the-following-rules","text":"If the dice shows 1 or 2 spots, my net gain is -1 dollar. If the dice shows 3 or 4 spots, my net gain is 0 dollars. If the dice shows 5 or 6 spots, my net gain is 1 dollar. Define a function to simulate a game with the above rules, assuming a D6, and compute the net gain of the player over any given number of rolls. Compute the net gain for 5, 50, and 500 rolls def D6game(nrolls): import numpy as np #import numpy dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 rolls = np.random.choice(dice,nrolls) #randomly selecting a value from dice for nrolls times- simulation gainlist =[] #create an empty list for gains|losses for i in np.arange(len(rolls)): #Apply the rules if rolls[i]<=2: gainlist.append(-1) elif rolls[i]<=4: gainlist.append(0) elif rolls[i]<=6: gainlist.append(+1) return (np.sum(gainlist)) #sum up all gains|losses # return (gainlist,\"The net gain is equal to:\",np.sum(gainlist)) D6game(5) 2 D6game(50) -4 D6game(500) 0","title":"Example: Assume the following rules:"},{"location":"8-Labs/Newly Formatted/Lab14/#lets-make-a-deal-game-show-and-monty-hall-problem","text":"The Monty Hall problem is a brain teaser, in the form of a probability puzzle, loosely based on the American television game show Let's Make a Deal and named after its original host, Monty Hall. The problem was originally posed (and solved) in a letter by Steve Selvin to the American Statistician in 1975 (Selvin 1975a), (Selvin 1975b). \"Suppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\" Is it to your advantage to switch your choice?\" From Wikipedia: https://en.wikipedia.org/wiki/Monty_Hall_problem","title":"Let's Make A Deal Game Show and Monty Hall Problem"},{"location":"8-Labs/Newly Formatted/Lab14/#example-simulate-monty-hall-game-for-1000-times-use-a-barplot-and-discuss-whether-players-are-better-off-sticking-to-their-initial-choice-or-switching-doors","text":"def othergoat(x): #Define a function to return \"the other goat\"! if x == \"Goat 1\": return \"Goat 2\" elif x == \"Goat 2\": return \"Goat 1\" Doors = np.array([\"Car\",\"Goat 1\",\"Goat 2\"]) #Define a list for objects behind the doors goats = np.array([\"Goat 1\" , \"Goat 2\"]) #Define a list for goats! def MHgame(): #Function to simulate the Monty Hall Game #For each guess, return [\"the guess\",\"the revealed\", \"the remaining\"] userguess=np.random.choice(Doors) #randomly selects a door as userguess if userguess == \"Goat 1\": return [userguess, \"Goat 2\",\"Car\"] if userguess == \"Goat 2\": return [userguess, \"Goat 1\",\"Car\"] if userguess == \"Car\": revealed = np.random.choice(goats) return [userguess, revealed,othergoat(revealed)] # Check and see if the MHgame function is doing what it is supposed to do: for i in np.arange(1): a =MHgame() print(a) print(a[0]) print(a[1]) print(a[2]) ['Goat 2', 'Goat 1', 'Car'] Goat 2 Goat 1 Car c1 = [] #Create an empty list for the userguess c2 = [] #Create an empty list for the revealed c3 = [] #Create an empty list for the remaining for i in np.arange(1000): #Simulate the game for 1000 rounds - or any other number of rounds you desire game = MHgame() c1.append(game[0]) #In each round, add the first element to the userguess list c2.append(game[1]) #In each round, add the second element to the revealed list c3.append(game[2]) #In each round, add the third element to the remaining list import pandas as pd #Create a data frame (gamedf) with 3 columns (\"Guess\",\"Revealed\", \"Remaining\") and 1000 (or how many number of rounds) rows gamedf = pd.DataFrame({'Guess':c1, 'Revealed':c2, 'Remaining':c3}) gamedf .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Guess Revealed Remaining 0 Car Goat 2 Goat 1 1 Car Goat 2 Goat 1 2 Goat 1 Goat 2 Car 3 Car Goat 2 Goat 1 4 Goat 2 Goat 1 Car ... ... ... ... 995 Goat 1 Goat 2 Car 996 Goat 1 Goat 2 Car 997 Car Goat 1 Goat 2 998 Car Goat 1 Goat 2 999 Goat 1 Goat 2 Car 1000 rows \u00d7 3 columns # Get the count of each item in the first and 3rd column original_car =gamedf[gamedf.Guess == 'Car'].shape[0] remaining_car =gamedf[gamedf.Remaining == 'Car'].shape[0] original_g1 =gamedf[gamedf.Guess == 'Goat 1'].shape[0] remaining_g1 =gamedf[gamedf.Remaining == 'Goat 1'].shape[0] original_g2 =gamedf[gamedf.Guess == 'Goat 2'].shape[0] remaining_g2 =gamedf[gamedf.Remaining == 'Goat 2'].shape[0] # Let's plot a grouped barplot import matplotlib.pyplot as plt # set width of bar barWidth = 0.25 # set height of bar bars1 = [original_car,original_g1,original_g2] bars2 = [remaining_car,remaining_g1,remaining_g2] # Set position of bar on X axis r1 = np.arange(len(bars1)) r2 = [x + barWidth for x in r1] # Make the plot plt.bar(r1, bars1, color='darkorange', width=barWidth, edgecolor='white', label='Original Guess') plt.bar(r2, bars2, color='midnightblue', width=barWidth, edgecolor='white', label='Remaining Door') # Add xticks on the middle of the group bars plt.xlabel('Item', fontweight='bold') plt.xticks([r + barWidth/2 for r in range(len(bars1))], ['Car', 'Goat 1', 'Goat 2']) # Create legend & Show graphic plt.legend() plt.show() According to the plot, it is statitically beneficial for the players to switch doors because the initial chance for being correct is only 1/3","title":"Example: Simulate Monty Hall Game for 1000 times. Use a barplot and discuss whether players are better off sticking to their initial choice, or switching doors?"},{"location":"8-Labs/Newly Formatted/Lab14/#python-for-probability","text":"","title":"Python for Probability"},{"location":"8-Labs/Newly Formatted/Lab14/#important-terminology","text":"Experiment: An occurrence with an uncertain outcome that we can observe. For example, rolling a die. Outcome: The result of an experiment; one particular state of the world. What Laplace calls a \"case.\" For example: 4. Sample Space: The set of all possible outcomes for the experiment. For example, {1, 2, 3, 4, 5, 6}. Event: A subset of possible outcomes that together have some property we are interested in. For example, the event \"even die roll\" is the set of outcomes {2, 4, 6}. Probability: As Laplace said, the probability of an event with respect to a sample space is the number of favorable cases (outcomes from the sample space that are in the event) divided by the total number of cases in the sample space. (This assumes that all outcomes in the sample space are equally likely.) Since it is a ratio, probability will always be a number between 0 (representing an impossible event) and 1 (representing a certain event). For example, the probability of an even die roll is 3/6 = 1/2. From https://people.math.ethz.ch/~jteichma/probability.html import numpy as np import pandas as pd import matplotlib.pyplot as plt","title":"Important Terminology:"},{"location":"8-Labs/Newly Formatted/Lab14/#example-in-a-game-of-russian-roulette-the-chance-of-surviving-each-round-is-56-which-is-almost-83-using-a-for-loop-compute-probability-of-surviving","text":"For 2 rounds For 5 rounds For 10 rounds nrounds =[] probs =[] for i in range(3): nrounds.append(i) probs.append((5/6)**i) #probability of surviving- not getting the bullet! RRDF = pd.DataFrame({\"# of Rounds\": nrounds, \"Probability of Surviving\": probs}) RRDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rounds Probability of Surviving 0 0 1.000000 1 1 0.833333 2 2 0.694444 nrounds =[] probs =[] for i in range(6): nrounds.append(i) probs.append((5/6)**i) #probability of surviving- not getting the bullet! RRDF = pd.DataFrame({\"# of Rounds\": nrounds, \"Probability of Surviving\": probs}) RRDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rounds Probability of Surviving 0 0 1.000000 1 1 0.833333 2 2 0.694444 3 3 0.578704 4 4 0.482253 5 5 0.401878 nrounds =[] probs =[] for i in range(11): nrounds.append(i) probs.append((5/6)**i) #probability of surviving- not getting the bullet! RRDF = pd.DataFrame({\"# of Rounds\": nrounds, \"Probability of Surviving\": probs}) RRDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rounds Probability of Surviving 0 0 1.000000 1 1 0.833333 2 2 0.694444 3 3 0.578704 4 4 0.482253 5 5 0.401878 6 6 0.334898 7 7 0.279082 8 8 0.232568 9 9 0.193807 10 10 0.161506 RRDF.plot.scatter(x=\"# of Rounds\", y=\"Probability of Surviving\",color=\"red\") <matplotlib.axes._subplots.AxesSubplot at 0x2c7f96de308>","title":"Example: In a game of Russian Roulette, the chance of surviving each round is 5/6 which is almost 83%. Using a for loop, compute probability of surviving"},{"location":"8-Labs/Newly Formatted/Lab14/#example-what-will-be-the-probability-of-constantly-throwing-an-even-number-with-a-d20-in","text":"For 2 rolls For 5 rolls For 10 rolls For 15 rolls nrolls =[] probs =[] for i in range(1,16,1): nrolls.append(i) probs.append((1/2)**i) #probability of throwing an even number-10/20 or 1/2 DRDF = pd.DataFrame({\"# of Rolls\": nrolls, \"Probability of constantly throwing an even number\": probs}) DRDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rolls Probability of constantly throwing an even number 0 1 0.500000 1 2 0.250000 2 3 0.125000 3 4 0.062500 4 5 0.031250 5 6 0.015625 6 7 0.007812 7 8 0.003906 8 9 0.001953 9 10 0.000977 10 11 0.000488 11 12 0.000244 12 13 0.000122 13 14 0.000061 14 15 0.000031 DRDF.plot.scatter(x=\"# of Rolls\", y=\"Probability of constantly throwing an even number\",color=\"crimson\") <matplotlib.axes._subplots.AxesSubplot at 0x2c7f978f488>","title":"Example: What will be the probability of constantly throwing an even number with a D20 in"},{"location":"8-Labs/Newly Formatted/Lab14/#example-what-will-be-the-probability-of-throwing-at-least-one-6-with-a-d6","text":"For 2 rolls For 5 rolls For 10 rolls For 50 rolls - Make a scatter plot for this one! nRolls =[] probs =[] for i in range(1,3,1): nRolls.append(i) probs.append(1-(5/6)**i) #probability of at least one 6: 1-(5/6) rollsDF = pd.DataFrame({\"# of Rolls\": nRolls, \"Probability of rolling at least one 6\": probs}) rollsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rolls Probability of rolling at least one 6 0 1 0.166667 1 2 0.305556 nRolls =[] probs =[] for i in range(1,6,1): nRolls.append(i) probs.append(1-(5/6)**i) #probability of at least one 6: 1-(5/6) rollsDF = pd.DataFrame({\"# of Rolls\": nRolls, \"Probability of rolling at least one 6\": probs}) rollsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rolls Probability of rolling at least one 6 0 1 0.166667 1 2 0.305556 2 3 0.421296 3 4 0.517747 4 5 0.598122 nRolls =[] probs =[] for i in range(1,11,1): nRolls.append(i) probs.append(1-(5/6)**i) #probability of at least one 6: 1-(5/6) rollsDF = pd.DataFrame({\"# of Rolls\": nRolls, \"Probability of rolling at least one 6\": probs}) rollsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Rolls Probability of rolling at least one 6 0 1 0.166667 1 2 0.305556 2 3 0.421296 3 4 0.517747 4 5 0.598122 5 6 0.665102 6 7 0.720918 7 8 0.767432 8 9 0.806193 9 10 0.838494 nRolls =[] probs =[] for i in range(1,51,1): nRolls.append(i) probs.append(1-(5/6)**i) #probability of at least one 6: 1-(5/6) rollsDF = pd.DataFrame({\"# of Rolls\": nRolls, \"Probability of rolling at least one 6\": probs}) rollsDF.plot.scatter(x=\"# of Rolls\", y=\"Probability of rolling at least one 6\") <matplotlib.axes._subplots.AxesSubplot at 0x2c7f97a3d88>","title":"Example: What will be the probability of throwing at least one 6 with a D6:"},{"location":"8-Labs/Newly Formatted/Lab14/#example-what-is-the-probability-of-drawing-an-ace-at-least-once-with-replacement","text":"in 2 tries in 5 tries in 10 tries in 20 tries - make a scatter plot. nDraws =[] probs =[] for i in range(1,3,1): nDraws.append(i) probs.append(1-(48/52)**i) #probability of drawing an ace least once : 1-(48/52) DrawsDF = pd.DataFrame({\"# of Draws\": nDraws, \"Probability of drawing an ace at least once\": probs}) DrawsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Draws Probability of drawing an ace at least once 0 1 0.076923 1 2 0.147929 nDraws =[] probs =[] for i in range(1,6,1): nDraws.append(i) probs.append(1-(48/52)**i) #probability of drawing an ace least once : 1-(48/52) DrawsDF = pd.DataFrame({\"# of Draws\": nDraws, \"Probability of drawing an ace at least once\": probs}) DrawsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Draws Probability of drawing an ace at least once 0 1 0.076923 1 2 0.147929 2 3 0.213473 3 4 0.273975 4 5 0.329823 nDraws =[] probs =[] for i in range(1,11,1): nDraws.append(i) probs.append(1-(48/52)**i) #probability of drawing an ace least once : 1-(48/52) DrawsDF = pd.DataFrame({\"# of Draws\": nDraws, \"Probability of drawing an ace at least once\": probs}) DrawsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Draws Probability of drawing an ace at least once 0 1 0.076923 1 2 0.147929 2 3 0.213473 3 4 0.273975 4 5 0.329823 5 6 0.381375 6 7 0.428962 7 8 0.472888 8 9 0.513435 9 10 0.550863 nDraws =[] probs =[] for i in range(1,21,1): nDraws.append(i) probs.append(1-(48/52)**i) #probability of drawing an ace at least once : 1-(48/52) DrawsDF = pd.DataFrame({\"# of Draws\": nDraws, \"Probability of drawing an ace at least once\": probs}) DrawsDF .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } # of Draws Probability of drawing an ace at least once 0 1 0.076923 1 2 0.147929 2 3 0.213473 3 4 0.273975 4 5 0.329823 5 6 0.381375 6 7 0.428962 7 8 0.472888 8 9 0.513435 9 10 0.550863 10 11 0.585412 11 12 0.617303 12 13 0.646742 13 14 0.673915 14 15 0.698999 15 16 0.722153 16 17 0.743525 17 18 0.763254 18 19 0.781466 19 20 0.798276 DrawsDF.plot.scatter(x=\"# of Draws\", y=\"Probability of drawing an ace at least once\") <matplotlib.axes._subplots.AxesSubplot at 0x2c7f980aac8>","title":"Example: What is the probability of drawing an ace at least once (with replacement):"},{"location":"8-Labs/Newly Formatted/Lab14/#example","text":"A) Write a function to find the probability of an event in percentage form based on given outcomes and sample space B) Use the function and compute the probability of rolling a 4 with a D6 C) Use the function and compute the probability of drawing a King from a standard deck of cards D) Use the function and compute the probability of drawing the King of Hearts from a standard deck of cards E) Use the function and compute the probability of drawing an ace after drawing a king F) Use the function and compute the probability of drawing an ace after drawing an ace G) Use the function and compute the probability of drawing a heart OR a club F) Use the function and compute the probability of drawing a Royal Flush *hint: (in poker) a straight flush including ace, king, queen, jack, and ten all in the same suit, which is the hand of the highest possible value This problem is designed based on an example by Daniel Poston from DataCamp, accessible @ https://www.datacamp.com/community/tutorials/statistics-python-tutorial-probability-1 # A # Create function that returns probability percent rounded to one decimal place def Prob(outcome, sampspace): probability = (outcome / sampspace) * 100 return round(probability, 1) # B outcome = 1 #Rolling a 4 is only one of the possible outcomes space = 6 #Rolling a D6 can have 6 different outcomes Prob(outcome, space) 16.7 # C outcome = 4 #Drawing a king is four of the possible outcomes space = 52 #Drawing from a standard deck of cards can have 52 different outcomes Prob(outcome, space) 7.7 # D outcome = 1 #Drawing the king of hearts is only 1 of the possible outcomes space = 52 #Drawing from a standard deck of cards can have 52 different outcomes Prob(outcome, space) 1.9 # E outcome = 4 #Drawing an ace is 4 of the possible outcomes space = 51 #One card has been drawn Prob(outcome, space) 7.8 # F outcome = 3 #Once Ace is already drawn space = 51 #One card has been drawn Prob(outcome, space) 5.9 # G hearts = 13 #13 cards of hearts in a deck space = 52 #total number of cards in a deck clubs = 13 #13 cards of clubs in a deck Prob_heartsORclubs= Prob(hearts, space) + Prob(clubs, space) print(\"Probability of drawing a heart or a club is\",Prob_heartsORclubs,\"%\") Probability of drawing a heart or a club is 50.0 % # F draw1 = 5 #5 cards are needed space1 = 52 #out of the possible 52 cards draw2 = 4 #4 cards are needed space2 = 51 #out of the possible 51 cards draw3 = 3 #3 cards are needed space3 = 50 #out of the possible 50 cards draw4 = 2 #2 cards are needed space4 = 49 #out of the possible 49 cards draw5 = 1 #1 cards is needed space5 = 48 #out of the possible 48 cards #Probability of a getting a Royal Flush Prob_RF= 4*(Prob(draw1, space1)/100) * (Prob(draw2, space2)/100) * (Prob(draw3, space3)/100) * (Prob(draw4, space4)/100) * (Prob(draw5, space5)/100) print(\"Probability of drawing a royal flush is\",Prob_RF,\"%\") Probability of drawing a royal flush is 1.5473203199999998e-06 %","title":"Example:"},{"location":"8-Labs/Newly Formatted/Lab14/#example-two-unbiased-dice-are-thrown-once-and-the-total-score-is-observed-define-an-appropriate-function-and-use-a-simulation-to-find-the-estimated-probability-that","text":"the total score is greater than 10? the total score is even and greater than 7? This problem is designed based on an example by Elliott Saslow from Medium.com, accessible @ https://medium.com/future-vision/simulating-probability-events-in-python-5dd29e34e381 import numpy as np def DiceRoll1(nSimulation): count =0 dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 for i in range(nSimulation): die1 = np.random.choice(dice,1) #randomly selecting a value from dice - throw the D6 once die2 = np.random.choice(dice,1) #randomly selecting a value from dice - throw the D6 once again! score = die1 + die2 #summing them up if score > 10: #if it meets our desired condition: count +=1 #add one to the \"count\" return count/nSimulation #compute the probability of the desired event by dividing count by the total number of trials nSimulation = 10000 print(\"The probability of rolling a number greater than 10 after\",nSimulation,\"rolld is:\",DiceRoll1(nSimulation)*100,\"%\") The probability of rolling a number greater than 10 after 10000 rolld is: 8.35 % import numpy as np def DiceRoll2(nSimulation): count =0 dice = np.array([1,2,3,4,5,6]) #create a numpy array with values of a D6 for i in range(nSimulation): die1 = np.random.choice(dice,1) #randomly selecting a value from dice - throw the D6 once die2 = np.random.choice(dice,1) #randomly selecting a value from dice - throw the D6 once again! score = die1 + die2 if score %2 ==0 and score > 7: #the total score is even and greater than 7 count +=1 return count/nSimulation nSimulation = 10000 print(\"The probability of rolling an even number and greater than 7 after\",nSimulation,\" rolls is:\",DiceRoll2(nSimulation)*100,\"%\") The probability of rolling an even number and greater than 7 after 10000 rolls is: 24.77 %","title":"Example: Two unbiased dice are thrown once and the total score is observed. Define an appropriate function and use a simulation to find the estimated probability that :"},{"location":"8-Labs/Newly Formatted/Lab14/#example-an-urn-contains-10-white-balls-20-reds-and-30-greens-we-want-to-draw-5-balls-with-replacement-use-a-simulation-10000-trials-to-find-the-estimated-probability-that","text":"we draw 3 white and 2 red balls we draw 5 balls of the same color This problem is designed based on an example by Elliott Saslow from Medium.com, accessible @ https://medium.com/future-vision/simulating-probability-events-in-python-5dd29e34e381 # A import numpy as np import random d = {} #Create an empty dictionary to associate numbers and colors for i in range(0,60,1): #total of 60 balls if i <10: #10 white balls d[i]=\"White\" elif i>9 and i<30: #20 red balls d[i]=\"Red\" else: #60-30=30 green balls d[i]=\"Green\" # nSimulation= 10000 #How many trials? outcome1= 0 #initial value on the desired outcome counter for i in range(nSimulation): draw=[] #an empty list for the draws for i in range(5): #how many balls we want to draw? draw.append(d[random.randint(0,59)]) #randomly choose a number from 0 to 59- simulation of drawing balls drawarray = np.array(draw) #convert the list into a numpy array white = sum(drawarray== \"White\") #count the white balls red = sum(drawarray== \"Red\") #count the red balls green = sum(drawarray== \"Green\") #count the green balls if white ==3 and red==2: #If the desired condition is met, add one to the counter outcome1 +=1 print(\"The probability of drawing 3 white and 2 red balls is\",(outcome1/nSimulation)*100,\"%\") The probability of drawing 3 white and 2 red balls is 0.54 % # B import numpy as np import random d = {} for i in range(0,60,1): if i <10: d[i]=\"White\" elif i>9 and i<30: d[i]=\"Red\" else: d[i]=\"Green\" # nSimulation= 10000 outcome1= 0 outcome2= 0 #we can consider multiple desired outcomes for i in range(nSimulation): draw=[] for i in range(5): draw.append(d[random.randint(0,59)]) drawarray = np.array(draw) white = sum(drawarray== \"White\") red = sum(drawarray== \"Red\") green = sum(drawarray== \"Green\") if white ==3 and red==2: outcome1 +=1 if white ==5 or red==5 or green==5: outcome2 +=1 print(\"The probability of drawing 3 white and 2 red balls is\",(outcome1/nSimulation)*100,\"%\") print(\"The probability of drawing 5 balls of the same color is\",(outcome2/nSimulation)*100,\"%\") The probability of drawing 3 white and 2 red balls is 0.53 % The probability of drawing 5 balls of the same color is 3.8 % Here are some of the resources used for creating this notebook: \"Poker Probability and Statistics with Python\" by Daniel Poston available at https://www.datacamp.com/community/tutorials/statistics-python-tutorial-probability-1 \"Simulating probability events in Python\" by Elliott Saslow available at https://medium.com/future-vision/simulating-probability-events-in-python-5dd29e34e381 Here are some great reads on this topic: - \"Simulate the Monty Hall Problem Using Python\" by randerson112358 available at https://medium.com/swlh/simulate-the-monty-hall-problem-using-python-7b76b943640e - \"The Monty Hall problem\" available at https://scipython.com/book/chapter-4-the-core-python-language-ii/examples/the-monty-hall-problem/ - \"Introduction to Probability Using Python\" by Lisandra Melo available at https://medium.com/future-vision/simulating-probability-events-in-python-5dd29e34e381 - \"Introduction to probability and statistics for Data Scientists and machine learning using python : Part-1\" by Arun Singh available at https://medium.com/@anayan/introduction-to-probability-and-statistics-for-data-scientists-and-machine-learning-using-python-377a9b082487 Here are some great videos on these topics: - \"Monty Hall Problem - Numberphile\" by Numberphile available at https://www.youtube.com/watch?v=4Lb-6rxZxx0 - \"The Monty Hall Problem\" by D!NG available at https://www.youtube.com/watch?v=TVq2ivVpZgQ - \"21 - Monty Hall - PROPENSITY BASED THEORETICAL MODEL PROBABILITY - MATHEMATICS in the MOVIES\" by Motivating Mathematical Education and STEM available at https://www.youtube.com/watch?v=iBdjqtR2iK4 - \"The Monty Hall Problem\" by niansenx available at https://www.youtube.com/watch?v=mhlc7peGlGg - \"The Monty Hall Problem - Explained\" by AsapSCIENCE available at https://www.youtube.com/watch?v=9vRUxbzJZ9Y - \"Introduction to Probability | 365 Data Science Online Course\" by 365 Data Science available at https://www.youtube.com/watch?v=soZRfdnkUQg - \"Probability explained | Independent and dependent events | Probability and Statistics | Khan Academy\" by Khan Academy available at https://www.youtube.com/watch?v=uzkc-qNVoOk - \"Math Antics - Basic Probability\" by mathantics available at https://www.youtube.com/watch?v=KzfWUEJjG18","title":"Example: An urn contains 10 white balls, 20 reds and 30 greens. We want to draw 5 balls with replacement. Use a simulation (10000 trials) to find the estimated probability that:"},{"location":"8-Labs/Newly Formatted/Lab14/#exercise-risk-or-probability","text":"","title":"Exercise: Risk or Probability  "},{"location":"8-Labs/Newly Formatted/Lab14/#are-they-the-same-are-they-different-discuss-your-opinion","text":"","title":"Are they the same? Are they different? Discuss your opinion."},{"location":"8-Labs/Newly Formatted/Lab14/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab18/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: (LINK NEEDS FIXING!) Lab18 Laboratory 18: \"Reject it or Fail!\" or a Lab on \"Hypothesis Testing\" # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) A = 123456789 #lET'S SAY THAT THIS IS MY R NUMBER print(hex(A)) #print(bin(A)) 0x75bcd15 Full name: R#: HEX: Title of the notebook Date: Remember where we left our last laboratory session? Accept my gratitude if you do! But in case you saw Agent K and Agent J sometime after Thursday or for any other reason, do not recall it, here is where were we left things: We had a dataset with two sets of numbers (Set 1 and Set2). We did a bunch of stuff and decided that the Normal Distribution Data Model provides a good fit for both of sample sets. We, then used the right parameters for Normal Data Model (mean and standard deviation) to generate one new sample set based on each set. We then looked at the four sets next to each other and asked a rather simple question: Are these sets different or similar? While we reached some assertions based on visual assessment, we did not manage to solidify our assertation in any numerical way. Well, now is the time! #Load the necessary packages import numpy as np import pandas as pd import matplotlib.pyplot as plt #Previously ... data = pd.read_csv(\"lab13_data.csv\") set1 = np.array(data['Set1']) set2 = np.array(data['Set2']) mu1 = set1.mean() sd1 = set1.std() mu2 = set2.mean() sd2 = set2.std() set1_s = np.random.normal(mu1, sd1, 100) set2_s = np.random.normal(mu2, sd2, 100) data2 = pd.DataFrame({'Set1s':set1_s,'Set2s':set2_s}) #Previously ... fig, ax = plt.subplots() data2.plot.hist(density=False, ax=ax, title='Histogram: Set1 and Set1 samples vs. Set2 and Set2 samples', bins=40) data.plot.hist(density=False, ax=ax, bins=40) ax.set_ylabel('Count') ax.grid(axis='y') #Previously ... fig = plt.figure(figsize =(10, 7)) plt.boxplot ([set1, set1_s, set2, set2_s],1, '') plt.show() We can use statistical hypothesis tests to confirm that our sets are from Normal Distribution Data Models. We can use the Shapiro-Wilk Normality Test: # the Shapiro-Wilk Normality Test for set1 from scipy.stats import shapiro stat, p = shapiro(data['Set1']) print('stat=%.3f, p=%.3f' % (stat, p)) if p > 0.05: print('Probably Gaussian') else: print('Probably not Gaussian') stat=0.992, p=0.793 Probably Gaussian # the Shapiro-Wilk Normality Test for set2 from scipy.stats import shapiro stat, p = shapiro(data['Set2']) print('stat=%.3f, p=%.3f' % (stat, p)) if p > 0.05: print('Probably Gaussian') else: print('Probably not Gaussian') stat=0.981, p=0.151 Probably Gaussian Now let's confirm that set1 and set1_s are from the same distribution. We can use the Mann-Whitney U Test for this: from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(data['Set1'],data2['Set1s']) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=4902.000, p-value at rejection =0.406 Probably the same distribution Let's also confirm that set2 and set2_s are from the same distribution: from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(data['Set2'],data2['Set2s']) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=4811.000, p-value at rejection =0.323 Probably the same distribution Based on the results we can say set1 and set1_s probably belong to the same distrubtion. The same can be stated about set2 and set2_s. Now let's check and see if set1 and set2 are SIGNIFICANTLY different or not? from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(data['Set1'],data['Set2']) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=0.000, p-value at rejection =0.000 Probably different distributions The test's result indicate that the set1 and set2 belong to distirbutions with different measures of central tendency (means). We can check the same for set1_s and set2_s as well: from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(data2['Set1s'],data2['Set2s']) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=0.000, p-value at rejection =0.000 Probably different distributions Now we can state at a 95% confidence level that set1 and set2 are different. The same for set1s and set2s. Example: A dataset containing marks obtained by students on basic skills like basic math and language skills (reading and writing) is collected from an educational institution and we have been tasked to give them some important inferences. Hypothesis: There is no difference in means of student performance in any of basic literacy skills i.e. reading, writing, math. This is based on an example by Joju John Varghese on \"Hypothesis Testing for Inference using a Dataset\" available @ https://medium.com/swlh/hypothesis-testing-for-inference-using-a-data-set-aaa799e94cdf. The dataset is available @ https://www.kaggle.com/spscientist/students-performance-in-exams. df = pd.read_csv(\"StudentsPerformance.csv\") df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 0 female group B bachelor's degree standard none 72 72 74 1 female group C some college standard completed 69 90 88 2 female group B master's degree standard none 90 95 93 3 male group A associate's degree free/reduced none 47 57 44 4 male group C some college standard none 76 78 75 df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } math score reading score writing score count 1000.00000 1000.000000 1000.000000 mean 66.08900 69.169000 68.054000 std 15.16308 14.600192 15.195657 min 0.00000 17.000000 10.000000 25% 57.00000 59.000000 57.750000 50% 66.00000 70.000000 69.000000 75% 77.00000 79.000000 79.000000 max 100.00000 100.000000 100.000000 set1 = df['math score'] set2 = df['reading score'] set3 = df['writing score'] import seaborn as sns sns.distplot(set1,color='navy', rug=True) sns.distplot(set2,color='darkorange', rug=True) sns.distplot(set3,color='green', rug=True) plt.xlim(0,100) plt.xlabel('Test Results') Text(0.5, 0, 'Test Results') It seems that all three samples have the same population means and it seems there is no significant difference between them at all. Let's set the null and alternative hypothesis: Ho: There is no difference in performance of students between math, reading and writing skills. Ha: There is a difference in performance of students between math, reading and writing skills. from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(set1,set2) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=441452.500, p-value at rejection =0.000 Probably different distributions from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(set1,set3) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=461212.500, p-value at rejection =0.001 Probably different distributions from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(set2,set3) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=480672.000, p-value at rejection =0.067 Probably the same distribution from scipy.stats import kruskal stat, p = kruskal(set1, set2, set3) print('Statistics=%.3f, p=%.3f' % (stat, p)) # interpret alpha = 0.05 if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') Statistics=21.225, p=0.000 Probably different distributions Example: Website Design: A practical example of A/B Testing inspired by an example in \"A/B Test Significance in Python\" by Samuel Hinton available at https://cosmiccoding.com.au/tutorials/ab_tests Imagine you\u2019re in charge of a website (e.g., an online videogame shop). You have the current version of the website (aka. \"A\"), but aren\u2019t happy with it. For instance, you are not selling as much as you like. You want to change the design of the \"Add to Cart\" button (aka. \"B\") and maybe that will increase your sells. you set up your website so that half the people are directed to the old website, and half to one where you\u2019ve made your change. You have data from both, and want to know, with confidence, \u201cDoes the change I made increase the sells?\u201d. This is an A/B test. Often this is used interchangably with the term \u201csplit testing\u201d, though in general A/B tests test small changes, and split testing might be when you present two entirely different websites to the user. Why not just change the website and monitor it for a week? Good question - by having two sites active at once and randomly directing users to one or the other, you control for all other variables. If one week later puts you the week before Christmas, this will impact sales, and you might draw the wrong conclusion because of these confounding effects. Why is it not an A/B/C test? you can have as many perturbations running as you want, but got to keep the name simple. The more perturbations you try though, the smaller a number of samples you\u2019ll have for each case, and the harder it will be to draw statistically significant conclusions. Let us assume you have 1000 users, 550 were directed to site A, 450 to site B. In site A, 48 users made a purchase. In site B, 56 users made a purchase. Is this a statistically significant result? num_a= 550 num_b = 450 click_a= 48 click_b = 56 rate_a= click_a / num_a rate_b = click_b / num_b print(rate_a) print(rate_b) 0.08727272727272728 0.12444444444444444 You can click a button, or not. Two discrete options are available, so this is a textbook binomial distribution, with some unknown rate for site A and site B. We don\u2019t know the true click rate, but we can estimate it using our small sample. import matplotlib.pyplot as plt import numpy as np from scipy.stats import binom # Determine the probability of having x number of clicks - Binomial Dist. clicks = np.arange(10, 100) prob_a = binom(num_a, rate_a).pmf(clicks) prob_b = binom(num_b, rate_b).pmf(clicks) # Make the bar plots. plt.bar(clicks, prob_a, label=\"A\", alpha=0.7) plt.bar(clicks, prob_b, label=\"B\", alpha=0.7) plt.legend() plt.xlabel(\"# of Sells\"); plt.ylabel(\"Probability\"); So we can see here that b has an edge, but its certaintly possible if we pick two random points according to the histograms for A and B, that A might actually be higher than B! As we\u2019re interested in the average # of sells, this averaging of an underlying distribution means our final estimate will be well approximated by a normal distribution. So let\u2019s reformulate, using the normal approximation here: from scipy.stats import norm # Where does this come from? See this link: https://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation std_a = np.sqrt(rate_a * (1 - rate_a) / num_a) std_b = np.sqrt(rate_b * (1 - rate_b) / num_b) click_rate = np.linspace(0, 0.2, 200) prob_a = norm(rate_a, std_a).pdf(click_rate) prob_b = norm(rate_b, std_b).pdf(click_rate) # Make the bar plots. plt.plot(click_rate, prob_a, label=\"A\") plt.plot(click_rate, prob_b, label=\"B\") plt.legend(frameon=False) plt.xlabel(\"Purchase rate\"); plt.ylabel(\"Probability\") Text(0, 0.5, 'Probability') This is also a better plot than the first one, because we\u2019ve removed the confusing effect of site A and site B having a slightly different number of visitors had. So our question is still the same: What is the chance that a draw from B is higher than a draw from A. Is it significant? To answer this, let us utilise the handy fact that the sum (or difference) of normally distributed random numbers is also a normal. This is simple - take the difference in the means and sum the variance. We\u2019ll do two things below: First, get the z-score, and second, plot the proper distribution. z_score = (rate_b - rate_a) / np.sqrt(std_a**2 + std_b**2) p = norm(rate_b - rate_a, np.sqrt(std_a**2 + std_b**2)) x = np.linspace(-0.05, 0.15, 1000) y = p.pdf(x) area_under_curve = p.sf(0) plt.plot(x, y, label=\"PDF\") plt.fill_between(x, 0, y, where=x>0, label=\"Prob(b>a)\", alpha=0.3) plt.annotate(f\"Area={area_under_curve:0.3f}\", (0.02, 5)) plt.legend() plt.xlabel(\"Difference in purchase rate\"); plt.ylabel(\"Prob\"); print(f\"zscore is {z_score:0.3f}, with p-value {norm().sf(z_score):0.3f}\") zscore is 1.890, with p-value 0.029 we can say that given the null hypothesis (\"B is less than or equal to A\") is true , we would expect to get this result or a result more extreme only 2.9% of the time. As that is a significant result (typically p < 5%), we reject the null hypothesis, and state that we have evidence that B > A. we\u2019ve made a lot of plots for this to try and explain the concept. You can easily write a tiny function to simplify all of this. Whether you want the confidence or the p-value just means changing the final \"norm.cdf\" to \"norm.sf\". def get_confidence_ab_test(click_a, num_a, click_b, num_b): rate_a = click_a / num_a rate_b = click_b / num_b std_a = np.sqrt(rate_a * (1 - rate_a) / num_a) std_b = np.sqrt(rate_b * (1 - rate_b) / num_b) z_score = (rate_b - rate_a) / np.sqrt(std_a**2 + std_b**2) return norm.sf(z_score) print(get_confidence_ab_test(click_a, num_a, click_b, num_b)) 0.029402650172421833 Remember Non-parametric Statistical Hypothesis Tests? We can use them here as well! Imagine we have the raw results of clicks (purchases), as 0s or 1s, as our distribution. from scipy.stats import mannwhitneyu a_dist = np.zeros(num_a) a_dist[:click_a] = 1 b_dist = np.zeros(num_b) b_dist[:click_b] = 1 stat, p_value = mannwhitneyu(a_dist, b_dist, alternative=\"less\") print(f\"Mann-Whitney U test for null hypothesis B <= A is {p_value:0.3f}\") Mann-Whitney U test for null hypothesis B <= A is 0.028 Example: Times Roman or Times New Roman- That is the question! The Daily Planet newspaper is considering swiching from the Times Roman Font to the Times New Roman as an attempt to to make it easier for their audience to read the newspaper and hence, increase the online purchases. *hint: The Daily Planet is a fictional broadsheet newspaper appearing in American comic books published by DC Comics, commonly in association with Superman. Read more @ https://en.wikipedia.org/wiki/Daily_Planet They have prepared two versions of their newspapers, one of each font. On the first day, they had a total of 1398 visitors. Randonmly, half of them (699 visitors) saw the original version (Time Roman) and the other half saw the alternative version (Times New Roman). The first group purchased a total of 175 copies of the newspaper. This number was 200 copies for the second group. Based on the observations of the first day, first, calculate the purchase rate for each group and then, decide whether the Daily Planet should change their font in order to increase the online engagement time? #Day 1 group1= 699 group2= 699 sold1= 175 sold2 = 200 rate1= sold1/group1 rate2 = sold2/group2 print(f\"The ratio for the first group is {rate1:0.4f} copies sold per person\") print(f\"The ratio for the second group is {rate2:0.4f} copies sold per person\") from scipy.stats import mannwhitneyu import numpy as np a_dist = np.zeros(group1) a_dist[:sold1] = 1 b_dist = np.zeros(group2) b_dist[:sold2] = 1 stat, p_value = mannwhitneyu(a_dist, b_dist, alternative=\"less\") print(f\"Mann-Whitney U test for null hypothesis B <= A is {p_value:0.3f}\") The ratio for the first group is 0.2504 copies sold per person The ratio for the second group is 0.2861 copies sold per person Mann-Whitney U test for null hypothesis B <= A is 0.066 After a week, they had a total of 10086 visitors. Randonmly, half of them (5043 visitors) saw the original version (Time Roman) and the other half saw the alternative version (Times New Roman). The first group purchased a total of 1072 copies of the newspaper. This number was 1190 copies for the second group. Based on the observations of the first week, first, calculate the purchase rate for each group and then, decide whether the Daily Planet should change their font in order to increase the online engagement time? #Week 1 group1= 5043 group2= 5043 sold1= 1072 sold2 = 1190 rate1= sold1/group1 rate2 = sold2/group2 print(f\"The ratio for the first group is {rate1:0.3f} copies sold per person\") print(f\"The ratio for the second group is {rate2:0.3f} copies sold per person\") from scipy.stats import mannwhitneyu import numpy as np a_dist = np.zeros(group1) a_dist[:sold1] = 1 b_dist = np.zeros(group2) b_dist[:sold2] = 1 stat, p_value = mannwhitneyu(a_dist, b_dist, alternative=\"less\") print(f\"Mann-Whitney U test for null hypothesis B <= A is {p_value:0.3f}\") The ratio for the first group is 0.213 copies sold per person The ratio for the second group is 0.236 copies sold per person Mann-Whitney U test for null hypothesis B <= A is 0.002 After a month, they had a total of 42000 visitors. Randonmly, half of them (21000 visitors) saw the original version (Time Roman) and the other half saw the alternative version (Times New Roman). The first group purchased a total of 4300 copies of the newspaper. This number was 5700 copies for the second group. Based on the observations of the first month, first, calculate the purchase rate for each group and then, decide whether the Daily Planet should change their font in order to increase the online engagement time? #Month 1 group1= 21000 group2= 21000 sold1= 4300 sold2 = 5700 rate1= sold1/group1 rate2 = sold2/group2 print(f\"The ratio for the first group is {rate1:0.3f} copies sold per person\") print(f\"The ratio for the second group is {rate2:0.3f} copies sold per person\") from scipy.stats import mannwhitneyu import numpy as np a_dist = np.zeros(group1) a_dist[:sold1] = 1 b_dist = np.zeros(group2) b_dist[:sold2] = 1 stat, p_value = mannwhitneyu(a_dist, b_dist, alternative=\"less\") print(f\"Mann-Whitney U test for null hypothesis B <= A is {p_value:0.15f}\") The ratio for the first group is 0.205 copies sold per person The ratio for the second group is 0.271 copies sold per person Mann-Whitney U test for null hypothesis B <= A is 0.000000000000000 Here are some great reads on this topic: - \"Hypothesis testing in Machine learning using Python\" by Yogesh Agrawal available at https://towardsdatascience.com/hypothesis-testing-in-machine-learning-using-python-a0dc89e169ce - \"Quick Guide To Perform Hypothesis Testing\" available at https://www.analyticsvidhya.com/blog/2020/12/quick-guide-to-perform-hypothesis-testing/ - \"A Gentle Introduction to Statistical Hypothesis Testing\" by Jason Brownlee available at https://machinelearningmastery.com/statistical-hypothesis-tests/ - \"17 Statistical Hypothesis Tests in Python (Cheat Sheet)\" by Jason Brownlee available at https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/ Some great reads on A/B Testing: - \"Implementing A/B Tests in Python\" by Robbie Geoghegan available at https://medium.com/@robbiegeoghegan/implementing-a-b-tests-in-python-514e9eb5b3a1 - \"The Math Behind A/B Testing with Example Python Code\" by Nguyen Ngo available at https://towardsdatascience.com/the-math-behind-a-b-testing-with-example-code-part-1-of-2-7be752e1d06f - \"A/B Testing\" available at https://www.optimizely.com/optimization-glossary/ab-testing/ - \"A/B Testing Guide\" available at https://vwo.com/ab-testing/ Some great videos: - \"What is A/B Testing? | Data Science in Minutes\" by Data Science Dojo available at https://www.youtube.com/watch?v=zFMgpxG-chM - \"A/B Testing Intro: Why, What, Where, & How to A/B Test\" by Testing Theory available at https://www.youtube.com/watch?v=CH89jd4haRE - \"A/B Testing\" by Udacity available at https://www.youtube.com/watch?v=8H6QmMQWPEI - \"Statistical Hypothesis Testing- Data Science with Python\" by Technology for Noobs available at https://www.youtube.com/watch?v=kd6zKBa9Rfk - \"Hypothesis Testing, p-value & Confidence Intervals, Exploratory Data Analysis In Python Statistics\" by TheEngineeringWorld available at https://www.youtube.com/watch?v=kz1IXqcFVCo - \"Python Tutorial : Hypothesis tests\" by DataCamp available at https://www.youtube.com/watch?v=6wbldEMpRXc* Exercise: Wait a minute ... Isn't The Kruskal-Wallis test missing something? We used the Kruskal-Wallis to check whether the three sets belong to the same distribution or at least one of them is different. The question is, how can we find the sets that are different? What is the missing piece in tests such as The Kruskal-Wallis Test? Make sure to cite any resources that you may use.","title":"Lab18"},{"location":"8-Labs/Newly Formatted/Lab18/#laboratory-18-reject-it-or-fail-or-a-lab-on-hypothesis-testing","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) A = 123456789 #lET'S SAY THAT THIS IS MY R NUMBER print(hex(A)) #print(bin(A)) 0x75bcd15","title":"Laboratory 18: \"Reject it or Fail!\" or a Lab on \"Hypothesis Testing\" "},{"location":"8-Labs/Newly Formatted/Lab18/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab18/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab18/#hex","text":"","title":"HEX:"},{"location":"8-Labs/Newly Formatted/Lab18/#title-of-the-notebook","text":"","title":"Title of the notebook"},{"location":"8-Labs/Newly Formatted/Lab18/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab18/#remember-where-we-left-our-last-laboratory-session","text":"","title":"Remember where we left our last laboratory session?"},{"location":"8-Labs/Newly Formatted/Lab18/#accept-my-gratitude-if-you-do-but-in-case-you-saw-agent-k-and-agent-j-sometime-after-thursday-or-for-any-other-reason-do-not-recall-it-here-is-where-were-we-left-things","text":"","title":"Accept my gratitude if you do! But in case you saw Agent K and Agent J sometime after Thursday or for any other reason, do not recall it, here is where were we left things:"},{"location":"8-Labs/Newly Formatted/Lab18/#we-had-a-dataset-with-two-sets-of-numbers-set-1-and-set2-we-did-a-bunch-of-stuff-and-decided-that-the-normal-distribution-data-model-provides-a-good-fit-for-both-of-sample-sets-we-then-used-the-right-parameters-for-normal-data-model-mean-and-standard-deviation-to-generate-one-new-sample-set-based-on-each-set-we-then-looked-at-the-four-sets-next-to-each-other-and-asked-a-rather-simple-question-are-these-sets-different-or-similar","text":"","title":"We had a dataset with two sets of numbers (Set 1 and Set2). We did a bunch of stuff and decided that the Normal Distribution Data Model provides a good fit for both of sample sets. We, then used the right parameters for Normal Data Model (mean and standard deviation) to generate one new sample set based on each set. We then looked at the four sets next to each other and asked a rather simple question: Are these sets different or similar?"},{"location":"8-Labs/Newly Formatted/Lab18/#while-we-reached-some-assertions-based-on-visual-assessment-we-did-not-manage-to-solidify-our-assertation-in-any-numerical-way-well-now-is-the-time","text":"#Load the necessary packages import numpy as np import pandas as pd import matplotlib.pyplot as plt #Previously ... data = pd.read_csv(\"lab13_data.csv\") set1 = np.array(data['Set1']) set2 = np.array(data['Set2']) mu1 = set1.mean() sd1 = set1.std() mu2 = set2.mean() sd2 = set2.std() set1_s = np.random.normal(mu1, sd1, 100) set2_s = np.random.normal(mu2, sd2, 100) data2 = pd.DataFrame({'Set1s':set1_s,'Set2s':set2_s}) #Previously ... fig, ax = plt.subplots() data2.plot.hist(density=False, ax=ax, title='Histogram: Set1 and Set1 samples vs. Set2 and Set2 samples', bins=40) data.plot.hist(density=False, ax=ax, bins=40) ax.set_ylabel('Count') ax.grid(axis='y') #Previously ... fig = plt.figure(figsize =(10, 7)) plt.boxplot ([set1, set1_s, set2, set2_s],1, '') plt.show() We can use statistical hypothesis tests to confirm that our sets are from Normal Distribution Data Models. We can use the Shapiro-Wilk Normality Test: # the Shapiro-Wilk Normality Test for set1 from scipy.stats import shapiro stat, p = shapiro(data['Set1']) print('stat=%.3f, p=%.3f' % (stat, p)) if p > 0.05: print('Probably Gaussian') else: print('Probably not Gaussian') stat=0.992, p=0.793 Probably Gaussian # the Shapiro-Wilk Normality Test for set2 from scipy.stats import shapiro stat, p = shapiro(data['Set2']) print('stat=%.3f, p=%.3f' % (stat, p)) if p > 0.05: print('Probably Gaussian') else: print('Probably not Gaussian') stat=0.981, p=0.151 Probably Gaussian Now let's confirm that set1 and set1_s are from the same distribution. We can use the Mann-Whitney U Test for this: from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(data['Set1'],data2['Set1s']) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=4902.000, p-value at rejection =0.406 Probably the same distribution Let's also confirm that set2 and set2_s are from the same distribution: from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(data['Set2'],data2['Set2s']) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=4811.000, p-value at rejection =0.323 Probably the same distribution Based on the results we can say set1 and set1_s probably belong to the same distrubtion. The same can be stated about set2 and set2_s. Now let's check and see if set1 and set2 are SIGNIFICANTLY different or not? from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(data['Set1'],data['Set2']) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=0.000, p-value at rejection =0.000 Probably different distributions The test's result indicate that the set1 and set2 belong to distirbutions with different measures of central tendency (means). We can check the same for set1_s and set2_s as well: from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(data2['Set1s'],data2['Set2s']) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=0.000, p-value at rejection =0.000 Probably different distributions Now we can state at a 95% confidence level that set1 and set2 are different. The same for set1s and set2s.","title":"While we reached some assertions based on visual assessment, we did not manage to solidify our assertation in any numerical way. Well, now is the time!"},{"location":"8-Labs/Newly Formatted/Lab18/#example-a-dataset-containing-marks-obtained-by-students-on-basic-skills-like-basic-math-and-language-skills-reading-and-writing-is-collected-from-an-educational-institution-and-we-have-been-tasked-to-give-them-some-important-inferences","text":"Hypothesis: There is no difference in means of student performance in any of basic literacy skills i.e. reading, writing, math. This is based on an example by Joju John Varghese on \"Hypothesis Testing for Inference using a Dataset\" available @ https://medium.com/swlh/hypothesis-testing-for-inference-using-a-data-set-aaa799e94cdf. The dataset is available @ https://www.kaggle.com/spscientist/students-performance-in-exams. df = pd.read_csv(\"StudentsPerformance.csv\") df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gender race/ethnicity parental level of education lunch test preparation course math score reading score writing score 0 female group B bachelor's degree standard none 72 72 74 1 female group C some college standard completed 69 90 88 2 female group B master's degree standard none 90 95 93 3 male group A associate's degree free/reduced none 47 57 44 4 male group C some college standard none 76 78 75 df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } math score reading score writing score count 1000.00000 1000.000000 1000.000000 mean 66.08900 69.169000 68.054000 std 15.16308 14.600192 15.195657 min 0.00000 17.000000 10.000000 25% 57.00000 59.000000 57.750000 50% 66.00000 70.000000 69.000000 75% 77.00000 79.000000 79.000000 max 100.00000 100.000000 100.000000 set1 = df['math score'] set2 = df['reading score'] set3 = df['writing score'] import seaborn as sns sns.distplot(set1,color='navy', rug=True) sns.distplot(set2,color='darkorange', rug=True) sns.distplot(set3,color='green', rug=True) plt.xlim(0,100) plt.xlabel('Test Results') Text(0.5, 0, 'Test Results')","title":"Example: A dataset containing marks obtained by students on basic skills like basic math and language skills (reading and writing) is collected from an educational institution and we have been tasked to give them some important inferences."},{"location":"8-Labs/Newly Formatted/Lab18/#it-seems-that-all-three-samples-have-the-same-population-means-and-it-seems-there-is-no-significant-difference-between-them-at-all-lets-set-the-null-and-alternative-hypothesis","text":"Ho: There is no difference in performance of students between math, reading and writing skills. Ha: There is a difference in performance of students between math, reading and writing skills. from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(set1,set2) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=441452.500, p-value at rejection =0.000 Probably different distributions from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(set1,set3) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=461212.500, p-value at rejection =0.001 Probably different distributions from scipy.stats import mannwhitneyu # import a useful non-parametric test stat, p = mannwhitneyu(set2,set3) print('statistic=%.3f, p-value at rejection =%.3f' % (stat, p)) if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') statistic=480672.000, p-value at rejection =0.067 Probably the same distribution from scipy.stats import kruskal stat, p = kruskal(set1, set2, set3) print('Statistics=%.3f, p=%.3f' % (stat, p)) # interpret alpha = 0.05 if p > 0.05: print('Probably the same distribution') else: print('Probably different distributions') Statistics=21.225, p=0.000 Probably different distributions","title":"It seems that all three samples have the same population means and it seems there is no significant difference between them at all. Let's set the null and alternative hypothesis:"},{"location":"8-Labs/Newly Formatted/Lab18/#example-website-design-a-practical-example-of-ab-testing","text":"inspired by an example in \"A/B Test Significance in Python\" by Samuel Hinton available at https://cosmiccoding.com.au/tutorials/ab_tests","title":"Example: Website Design: A practical example of A/B Testing"},{"location":"8-Labs/Newly Formatted/Lab18/#imagine-youre-in-charge-of-a-website-eg-an-online-videogame-shop-you-have-the-current-version-of-the-website-aka-a-but-arent-happy-with-it-for-instance-you-are-not-selling-as-much-as-you-like-you-want-to-change-the-design-of-the-add-to-cart-button-aka-b-and-maybe-that-will-increase-your-sells","text":"","title":"Imagine you\u2019re in charge of a website (e.g., an online videogame shop). You have the current version of the website (aka. \"A\"), but aren\u2019t happy with it. For instance, you are not selling as much as you like. You want to change the design of the \"Add to Cart\" button (aka. \"B\") and maybe that will increase your sells. "},{"location":"8-Labs/Newly Formatted/Lab18/#you-set-up-your-website-so-that-half-the-people-are-directed-to-the-old-website-and-half-to-one-where-youve-made-your-change-you-have-data-from-both-and-want-to-know-with-confidence-does-the-change-i-made-increase-the-sells","text":"This is an A/B test. Often this is used interchangably with the term \u201csplit testing\u201d, though in general A/B tests test small changes, and split testing might be when you present two entirely different websites to the user.","title":"you set up your website so that half the people are directed to the old website, and half to one where you\u2019ve made your change. You have data from both, and want to know, with confidence, \u201cDoes the change I made increase the sells?\u201d."},{"location":"8-Labs/Newly Formatted/Lab18/#why-not-just-change-the-website-and-monitor-it-for-a-week","text":"Good question - by having two sites active at once and randomly directing users to one or the other, you control for all other variables. If one week later puts you the week before Christmas, this will impact sales, and you might draw the wrong conclusion because of these confounding effects.","title":"Why not just change the website and monitor it for a week?"},{"location":"8-Labs/Newly Formatted/Lab18/#why-is-it-not-an-abc-test","text":"you can have as many perturbations running as you want, but got to keep the name simple. The more perturbations you try though, the smaller a number of samples you\u2019ll have for each case, and the harder it will be to draw statistically significant conclusions.","title":"Why is it not an A/B/C test?"},{"location":"8-Labs/Newly Formatted/Lab18/#let-us-assume-you-have-1000-users-550-were-directed-to-site-a-450-to-site-b-in-site-a-48-users-made-a-purchase-in-site-b-56-users-made-a-purchase-is-this-a-statistically-significant-result","text":"num_a= 550 num_b = 450 click_a= 48 click_b = 56 rate_a= click_a / num_a rate_b = click_b / num_b print(rate_a) print(rate_b) 0.08727272727272728 0.12444444444444444","title":"Let us assume you have 1000 users, 550 were directed to site A, 450 to site B. In site A, 48 users made a purchase. In site B, 56 users made a purchase. Is this a statistically significant result?"},{"location":"8-Labs/Newly Formatted/Lab18/#you-can-click-a-button-or-not-two-discrete-options-are-available-so-this-is-a-textbook-binomial-distribution-with-some-unknown-rate-for-site-a-and-site-b-we-dont-know-the-true-click-rate-but-we-can-estimate-it-using-our-small-sample","text":"import matplotlib.pyplot as plt import numpy as np from scipy.stats import binom # Determine the probability of having x number of clicks - Binomial Dist. clicks = np.arange(10, 100) prob_a = binom(num_a, rate_a).pmf(clicks) prob_b = binom(num_b, rate_b).pmf(clicks) # Make the bar plots. plt.bar(clicks, prob_a, label=\"A\", alpha=0.7) plt.bar(clicks, prob_b, label=\"B\", alpha=0.7) plt.legend() plt.xlabel(\"# of Sells\"); plt.ylabel(\"Probability\");","title":"You can click a button, or not. Two discrete options are available, so this is a textbook binomial distribution, with some unknown rate for site A and site B. We don\u2019t know the true click rate, but we can estimate it using our small sample."},{"location":"8-Labs/Newly Formatted/Lab18/#so-we-can-see-here-that-b-has-an-edge-but-its-certaintly-possible-if-we-pick-two-random-points-according-to-the-histograms-for-a-and-b-that-a-might-actually-be-higher-than-b","text":"","title":"So we can see here that b has an edge, but its certaintly possible if we pick two random points according to the histograms for A and B, that A might actually be higher than B! "},{"location":"8-Labs/Newly Formatted/Lab18/#as-were-interested-in-the-average-of-sells-this-averaging-of-an-underlying-distribution-means-our-final-estimate-will-be-well-approximated-by-a-normal-distribution-so-lets-reformulate-using-the-normal-approximation-here","text":"from scipy.stats import norm # Where does this come from? See this link: https://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation std_a = np.sqrt(rate_a * (1 - rate_a) / num_a) std_b = np.sqrt(rate_b * (1 - rate_b) / num_b) click_rate = np.linspace(0, 0.2, 200) prob_a = norm(rate_a, std_a).pdf(click_rate) prob_b = norm(rate_b, std_b).pdf(click_rate) # Make the bar plots. plt.plot(click_rate, prob_a, label=\"A\") plt.plot(click_rate, prob_b, label=\"B\") plt.legend(frameon=False) plt.xlabel(\"Purchase rate\"); plt.ylabel(\"Probability\") Text(0, 0.5, 'Probability')","title":"As we\u2019re interested in the average # of sells, this averaging of an underlying distribution means our final estimate will be well approximated by a normal distribution. So let\u2019s reformulate, using the normal approximation here:"},{"location":"8-Labs/Newly Formatted/Lab18/#this-is-also-a-better-plot-than-the-first-one-because-weve-removed-the-confusing-effect-of-site-a-and-site-b-having-a-slightly-different-number-of-visitors-had-so-our-question-is-still-the-same-what-is-the-chance-that-a-draw-from-b-is-higher-than-a-draw-from-a-is-it-significant","text":"","title":"This is also a better plot than the first one, because we\u2019ve removed the confusing effect of site A and site B having a slightly different number of visitors had. So our question is still the same: What is the chance that a draw from B is higher than a draw from A. Is it significant? "},{"location":"8-Labs/Newly Formatted/Lab18/#to-answer-this-let-us-utilise-the-handy-fact-that-the-sum-or-difference-of-normally-distributed-random-numbers-is-also-a-normal-this-is-simple-take-the-difference-in-the-means-and-sum-the-variance-well-do-two-things-below-first-get-the-z-score-and-second-plot-the-proper-distribution","text":"z_score = (rate_b - rate_a) / np.sqrt(std_a**2 + std_b**2) p = norm(rate_b - rate_a, np.sqrt(std_a**2 + std_b**2)) x = np.linspace(-0.05, 0.15, 1000) y = p.pdf(x) area_under_curve = p.sf(0) plt.plot(x, y, label=\"PDF\") plt.fill_between(x, 0, y, where=x>0, label=\"Prob(b>a)\", alpha=0.3) plt.annotate(f\"Area={area_under_curve:0.3f}\", (0.02, 5)) plt.legend() plt.xlabel(\"Difference in purchase rate\"); plt.ylabel(\"Prob\"); print(f\"zscore is {z_score:0.3f}, with p-value {norm().sf(z_score):0.3f}\") zscore is 1.890, with p-value 0.029","title":"To answer this, let us utilise the handy fact that the sum (or difference) of normally distributed random numbers is also a normal. This is simple - take the difference in the means and sum the variance. We\u2019ll do two things below: First, get the z-score, and second, plot the proper distribution."},{"location":"8-Labs/Newly Formatted/Lab18/#we-can-say-that-given-the-null-hypothesis-b-is-less-than-or-equal-to-a-is-true-we-would-expect-to-get-this-result-or-a-result-more-extreme-only-29-of-the-time-as-that-is-a-significant-result-typically-p-5-we-reject-the-null-hypothesis-and-state-that-we-have-evidence-that-b-a","text":"","title":"we can say that given the null hypothesis (\"B is less than or equal to A\") is true , we would expect to get this result or a result more extreme only 2.9% of the time. As that is a significant result (typically p &lt; 5%), we reject the null hypothesis, and state that we have evidence that B &gt; A. "},{"location":"8-Labs/Newly Formatted/Lab18/#weve-made-a-lot-of-plots-for-this-to-try-and-explain-the-concept-you-can-easily-write-a-tiny-function-to-simplify-all-of-this-whether-you-want-the-confidence-or-the-p-value-just-means-changing-the-final-normcdf-to-normsf","text":"def get_confidence_ab_test(click_a, num_a, click_b, num_b): rate_a = click_a / num_a rate_b = click_b / num_b std_a = np.sqrt(rate_a * (1 - rate_a) / num_a) std_b = np.sqrt(rate_b * (1 - rate_b) / num_b) z_score = (rate_b - rate_a) / np.sqrt(std_a**2 + std_b**2) return norm.sf(z_score) print(get_confidence_ab_test(click_a, num_a, click_b, num_b)) 0.029402650172421833","title":"we\u2019ve made a lot of plots for this to try and explain the concept. You can easily write a tiny function to simplify all of this. Whether you want the confidence or the p-value just means changing the final \"norm.cdf\" to \"norm.sf\"."},{"location":"8-Labs/Newly Formatted/Lab18/#remember-non-parametric-statistical-hypothesis-tests-we-can-use-them-here-as-well","text":"","title":"Remember Non-parametric Statistical Hypothesis Tests? We can use them here as well! "},{"location":"8-Labs/Newly Formatted/Lab18/#imagine-we-have-the-raw-results-of-clicks-purchases-as-0s-or-1s-as-our-distribution","text":"from scipy.stats import mannwhitneyu a_dist = np.zeros(num_a) a_dist[:click_a] = 1 b_dist = np.zeros(num_b) b_dist[:click_b] = 1 stat, p_value = mannwhitneyu(a_dist, b_dist, alternative=\"less\") print(f\"Mann-Whitney U test for null hypothesis B <= A is {p_value:0.3f}\") Mann-Whitney U test for null hypothesis B <= A is 0.028","title":"Imagine we have the raw results of clicks (purchases), as 0s or 1s, as our distribution."},{"location":"8-Labs/Newly Formatted/Lab18/#example-times-roman-or-times-new-roman-that-is-the-question","text":"","title":"Example: Times Roman or Times New Roman- That is the question! "},{"location":"8-Labs/Newly Formatted/Lab18/#the-daily-planet-newspaper-is-considering-swiching-from-the-times-roman-font-to-the-times-new-roman-as-an-attempt-to-to-make-it-easier-for-their-audience-to-read-the-newspaper-and-hence-increase-the-online-purchases","text":"*hint: The Daily Planet is a fictional broadsheet newspaper appearing in American comic books published by DC Comics, commonly in association with Superman. Read more @ https://en.wikipedia.org/wiki/Daily_Planet","title":"The Daily Planet newspaper is considering swiching from the Times Roman Font to the Times New Roman as an attempt to to make it easier for their audience to read the newspaper and hence, increase the online purchases."},{"location":"8-Labs/Newly Formatted/Lab18/#they-have-prepared-two-versions-of-their-newspapers-one-of-each-font-on-the-first-day-they-had-a-total-of-1398-visitors-randonmly-half-of-them-699-visitors-saw-the-original-version-time-roman-and-the-other-half-saw-the-alternative-version-times-new-roman","text":"","title":"They have prepared two versions of their newspapers, one of each font. On the first day, they had a total of 1398 visitors. Randonmly, half of them (699 visitors) saw the original version (Time Roman) and the other half saw the alternative version (Times New Roman).    "},{"location":"8-Labs/Newly Formatted/Lab18/#the-first-group-purchased-a-total-of-175-copies-of-the-newspaper-this-number-was-200-copies-for-the-second-group-based-on-the-observations-of-the-first-day-first-calculate-the-purchase-rate-for-each-group-and-then-decide-whether-the-daily-planet-should-change-their-font-in-order-to-increase-the-online-engagement-time","text":"#Day 1 group1= 699 group2= 699 sold1= 175 sold2 = 200 rate1= sold1/group1 rate2 = sold2/group2 print(f\"The ratio for the first group is {rate1:0.4f} copies sold per person\") print(f\"The ratio for the second group is {rate2:0.4f} copies sold per person\") from scipy.stats import mannwhitneyu import numpy as np a_dist = np.zeros(group1) a_dist[:sold1] = 1 b_dist = np.zeros(group2) b_dist[:sold2] = 1 stat, p_value = mannwhitneyu(a_dist, b_dist, alternative=\"less\") print(f\"Mann-Whitney U test for null hypothesis B <= A is {p_value:0.3f}\") The ratio for the first group is 0.2504 copies sold per person The ratio for the second group is 0.2861 copies sold per person Mann-Whitney U test for null hypothesis B <= A is 0.066","title":"The first group purchased a total of 175 copies of the newspaper. This number was 200 copies for the second group. Based on the observations of the first day, first, calculate the purchase rate for each group and then, decide whether the Daily Planet should change their font in order to increase the online engagement time?    "},{"location":"8-Labs/Newly Formatted/Lab18/#after-a-week-they-had-a-total-of-10086-visitors-randonmly-half-of-them-5043-visitors-saw-the-original-version-time-roman-and-the-other-half-saw-the-alternative-version-times-new-roman","text":"","title":"After a week, they had a total of 10086 visitors. Randonmly, half of them (5043 visitors) saw the original version (Time Roman) and the other half saw the alternative version (Times New Roman).    "},{"location":"8-Labs/Newly Formatted/Lab18/#the-first-group-purchased-a-total-of-1072-copies-of-the-newspaper-this-number-was-1190-copies-for-the-second-group-based-on-the-observations-of-the-first-week-first-calculate-the-purchase-rate-for-each-group-and-then-decide-whether-the-daily-planet-should-change-their-font-in-order-to-increase-the-online-engagement-time","text":"#Week 1 group1= 5043 group2= 5043 sold1= 1072 sold2 = 1190 rate1= sold1/group1 rate2 = sold2/group2 print(f\"The ratio for the first group is {rate1:0.3f} copies sold per person\") print(f\"The ratio for the second group is {rate2:0.3f} copies sold per person\") from scipy.stats import mannwhitneyu import numpy as np a_dist = np.zeros(group1) a_dist[:sold1] = 1 b_dist = np.zeros(group2) b_dist[:sold2] = 1 stat, p_value = mannwhitneyu(a_dist, b_dist, alternative=\"less\") print(f\"Mann-Whitney U test for null hypothesis B <= A is {p_value:0.3f}\") The ratio for the first group is 0.213 copies sold per person The ratio for the second group is 0.236 copies sold per person Mann-Whitney U test for null hypothesis B <= A is 0.002","title":"The first group purchased a total of 1072 copies of the newspaper. This number was 1190 copies for the second group. Based on the observations of the first week, first, calculate the purchase rate for each group and then, decide whether the Daily Planet should change their font in order to increase the online engagement time?    "},{"location":"8-Labs/Newly Formatted/Lab18/#after-a-month-they-had-a-total-of-42000-visitors-randonmly-half-of-them-21000-visitors-saw-the-original-version-time-roman-and-the-other-half-saw-the-alternative-version-times-new-roman","text":"","title":"After a month, they had a total of 42000 visitors. Randonmly, half of them (21000 visitors) saw the original version (Time Roman) and the other half saw the alternative version (Times New Roman).    "},{"location":"8-Labs/Newly Formatted/Lab18/#the-first-group-purchased-a-total-of-4300-copies-of-the-newspaper-this-number-was-5700-copies-for-the-second-group-based-on-the-observations-of-the-first-month-first-calculate-the-purchase-rate-for-each-group-and-then-decide-whether-the-daily-planet-should-change-their-font-in-order-to-increase-the-online-engagement-time","text":"#Month 1 group1= 21000 group2= 21000 sold1= 4300 sold2 = 5700 rate1= sold1/group1 rate2 = sold2/group2 print(f\"The ratio for the first group is {rate1:0.3f} copies sold per person\") print(f\"The ratio for the second group is {rate2:0.3f} copies sold per person\") from scipy.stats import mannwhitneyu import numpy as np a_dist = np.zeros(group1) a_dist[:sold1] = 1 b_dist = np.zeros(group2) b_dist[:sold2] = 1 stat, p_value = mannwhitneyu(a_dist, b_dist, alternative=\"less\") print(f\"Mann-Whitney U test for null hypothesis B <= A is {p_value:0.15f}\") The ratio for the first group is 0.205 copies sold per person The ratio for the second group is 0.271 copies sold per person Mann-Whitney U test for null hypothesis B <= A is 0.000000000000000 Here are some great reads on this topic: - \"Hypothesis testing in Machine learning using Python\" by Yogesh Agrawal available at https://towardsdatascience.com/hypothesis-testing-in-machine-learning-using-python-a0dc89e169ce - \"Quick Guide To Perform Hypothesis Testing\" available at https://www.analyticsvidhya.com/blog/2020/12/quick-guide-to-perform-hypothesis-testing/ - \"A Gentle Introduction to Statistical Hypothesis Testing\" by Jason Brownlee available at https://machinelearningmastery.com/statistical-hypothesis-tests/ - \"17 Statistical Hypothesis Tests in Python (Cheat Sheet)\" by Jason Brownlee available at https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/ Some great reads on A/B Testing: - \"Implementing A/B Tests in Python\" by Robbie Geoghegan available at https://medium.com/@robbiegeoghegan/implementing-a-b-tests-in-python-514e9eb5b3a1 - \"The Math Behind A/B Testing with Example Python Code\" by Nguyen Ngo available at https://towardsdatascience.com/the-math-behind-a-b-testing-with-example-code-part-1-of-2-7be752e1d06f - \"A/B Testing\" available at https://www.optimizely.com/optimization-glossary/ab-testing/ - \"A/B Testing Guide\" available at https://vwo.com/ab-testing/ Some great videos: - \"What is A/B Testing? | Data Science in Minutes\" by Data Science Dojo available at https://www.youtube.com/watch?v=zFMgpxG-chM - \"A/B Testing Intro: Why, What, Where, & How to A/B Test\" by Testing Theory available at https://www.youtube.com/watch?v=CH89jd4haRE - \"A/B Testing\" by Udacity available at https://www.youtube.com/watch?v=8H6QmMQWPEI - \"Statistical Hypothesis Testing- Data Science with Python\" by Technology for Noobs available at https://www.youtube.com/watch?v=kd6zKBa9Rfk - \"Hypothesis Testing, p-value & Confidence Intervals, Exploratory Data Analysis In Python Statistics\" by TheEngineeringWorld available at https://www.youtube.com/watch?v=kz1IXqcFVCo - \"Python Tutorial : Hypothesis tests\" by DataCamp available at https://www.youtube.com/watch?v=6wbldEMpRXc*","title":"The first group purchased a total of 4300 copies of the newspaper. This number was 5700 copies for the second group. Based on the observations of the first month, first, calculate the purchase rate for each group and then, decide whether the Daily Planet should change their font in order to increase the online engagement time?    "},{"location":"8-Labs/Newly Formatted/Lab18/#exercise-wait-a-minute-isnt-the-kruskal-wallis-test-missing-something","text":"","title":"Exercise: Wait a minute ... Isn't The Kruskal-Wallis test missing something?  "},{"location":"8-Labs/Newly Formatted/Lab18/#we-used-the-kruskal-wallis-to-check-whether-the-three-sets-belong-to-the-same-distribution-or-at-least-one-of-them-is-different-the-question-is-how-can-we-find-the-sets-that-are-different-what-is-the-missing-piece-in-tests-such-as-the-kruskal-wallis-test","text":"","title":"We used the Kruskal-Wallis to check whether the three sets belong to the same distribution or at least one of them is different. The question is, how can we find the sets that are different? What is the missing piece in tests such as The Kruskal-Wallis Test?"},{"location":"8-Labs/Newly Formatted/Lab18/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab19/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: (LINK NEEDS FIXING!) Lab19 Laboratory 19: \"Game of Gods and Men\" | On Confidence Intervals with a Pinch of Theology # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: HEX: Title of the notebook Date: Italy & Soccer: How many people love soccer? inspired by an example in \"A (very) friendly introduction to Confidence Intervals\" by Dima Shulga available at https://towardsdatascience.com/a-very-friendly-introduction-to-confidence-intervals-9add126e714 and \"Introduction of Confidence Interval\" by Irfan Rahman available at https://medium.com/steps-towards-data-science/confidence-interval-a7fb3484d7b4 *hint: According to UN estimate data, almost 60 million (60,449,841) people live in Italy For the first example in this lab, we are going to look at a problem from two perspectives, or two \"modes\" if you may: The GOD mode and The MAN mode. The GOD MODE: In GOD mode, we are assuming that we know EVERYTHING about our population (in this case, the population of Italy). Suppose we know (theoretically) the exact percentage of people in Italy that love soccer and it\u2019s 75%. Let's say we want to know the chance of randomly selecting a group of 1000 people that only 73% of them love soccer! totalpop = 60*10**6 # Total adult population of Italy (60M) fbl_p = 0.75 #percentage of those loving soccer|football ! fblpop = int(totalpop * fbl_p) #Population of those who love football nfblpop = int(totalpop * (1-fbl_p)) #Population of those who doesn't love football Let's create a numpy array with 60 million elements, with a 1 for each one person who loves soccer, and zero otherwise. import numpy as np fblpop_1 = np.ones(fblpop) #An array of \"1\"s | its length is equal to the population of those who love football | DO NOT ATTEMPT TO PRINT!!! nfblpop_0 = np.zeros(nfblpop) #An array of \"0\"s | its length is equal to the population of those who doesn't love football | DO NOT ATTEMPT TO PRINT!!! totpop_01 = np.hstack([fblpop_1,nfblpop_0]) #An array of \"0 & 1\"s | its length is equal to the total population of Italy | DO NOT ATTEMPT TO PRINT!!! As a check, we can get the percentage of \"1\"s in the array by calculating the mean of it, and indeed it is 75%. print(np.mean(totpop_01)) 0.75 Now, lets take few samples and see what percentage do we get: np.mean(np.random.choice(totpop_01, size=1000)) # Run multiple times 0.767 # Let's do it in a more sophisticated/engineery/data sciency way! for i in range(10): #Let's take 10 samples sample = np.random.choice(totpop_01, size=1000) print('Sample', i, ':', np.mean(sample)) Sample 0 : 0.727 Sample 1 : 0.74 Sample 2 : 0.739 Sample 3 : 0.753 Sample 4 : 0.776 Sample 5 : 0.766 Sample 6 : 0.769 Sample 7 : 0.741 Sample 8 : 0.769 Sample 9 : 0.741 You can see that we\u2019re getting different values for each sample, but the intuition (and statistics theory) says that the average of large amount of samples should be very close to the real percentage. Let\u2019s do that! lets take many samples and see what happens: values = [] #Create an empty list for i in range(10000): #Let's take 10000 samples sample = np.random.choice(totpop_01, size=1000) #Notice that the sample size is not changing mean = np.mean(sample) values.append(mean) #Store the mean of each sample set print(np.mean(values)) #Printing the mean of means! values = np.array(values) print(values.std()) #Printing the standard deviation of means! 0.7499531 0.013818230725747793 We created 10000 samples, checked the percentage of people who love soccer in each sample, and then just averaged them. we got 74.99% which is very close to the real value 75% that we as the GOD knew! Let\u2019s plot a histogram of all the values we got in all the samples. Interestingly, this histogram is very similar to the normal distribution! import seaborn as sns sns.distplot(values,color='purple', rug=True,kde=True) <matplotlib.axes._subplots.AxesSubplot at 0x21c485c1688> if we do this process a very large number of times (infinite number of times) we will get an histogram that is very close to the normal distribution and we can know the parameters of this distribution. values = [] #Create an empty list for i in range(1000000): #Let's take 1000000 samples sample = np.random.choice(totpop_01, size=1000) #Notice that the sample size is not changing mean = np.mean(sample) values.append(mean) #Store the mean of each sample set print(np.mean(values)) #Printing the mean of means! 0.7500088180000001 import seaborn as sns sns.distplot(values,color='purple', rug=True,kde=True) <matplotlib.axes._subplots.AxesSubplot at 0x21c4562bcc8> First of all, we can see that the center (the mean) of the histogram is near 75%, exactly as we expected, but we are able to say much more just by looking at the histogram, for example, we can say, that half of the samples are larger than 75%, or, we can say that roughly 25% are larger than 76%. We can also say that almost 95% of the samples are between 72% and 78%. Let's also have a look at the boxplot: import matplotlib.pyplot as plt fig = plt.figure(figsize =(10, 7)) plt.boxplot (values,1, '') plt.show() At this point, many people might ask two important questions, \u201cHow can I take infinite number of samples?\u201d and \u201cHow does it helps me?\u201d. The answer to the first one is that if you are GOD, there is no stopping you! If you are not GOD, you can't! To asnwer the second question, Let\u2019s go back to our example, we initially took one sample of 1000 people and got a value close to 75% but not exactly 75%. We wanted to know, what is the chance that a random sample of 1000 people will have 73% soccer lovers. Using the information above, we can say that there\u2019s a chance of (roughly) 20% that we\u2019ll get a value that is smaller or equal to 73%. We don\u2019t actually need to do the infinite samples. In other words, you don't have to be GOD! You will know why and how in a few moments... The MAN MODE: Back in our horrid and miserable Man mode, we don\u2019t know the actual percentage of soccer lovers in Italy. In fact, we know nothing about the population. We do know one thing though: We just took a sample and got 73%. But how does it help us? What we also DO know, is that if we took infinite number of samples, the distibution of their means will look like this: Here \u03bc is the population mean (real percentage of soccer lovers in our example), and \u03c3 is the standard deviation of the population. If we know this (and we know the standard deviation) we are able to say that ~68% of the samples will fall in the red area or, more than 95% of the samples will fall outside the green area (in the middle) in this plot: This is where the concept of margin of error becomes of great importance: Let's mix the GOD mode and the MAN mode. LET's DO MAD MODE! Of course the distance is symmetric, So if the sample percentage will fall 95% of the time between real percentage-3 and real percentage +3, then the real percentage will be 95% of the times between sample percentage -3 and sample percentage +3. import seaborn as sns sns.distplot(values,color='purple', rug=True,kde=True) <matplotlib.axes._subplots.AxesSubplot at 0x1ccf33b1208> If we took a sample and got 73%, we can say that we are 95% confident that the real percentage is between 70% (73 -3) and 76% (73+3). This is the Confidence Interval, the interval is 73 +- 3 and the confidence is 95%. Example: From a normally distributed population, we randolmy took a sample of 500 students with a mean score of 461 on the math section of SAT. Suppose the standard deviation of the population is 100, what is the estimated true population mean for the 95% confidence interval. # Step 1- Organize the data n = 500 #Sample size Xbar = 461 #Sample mean C = 0.95 #Confidence level std = 100 #Standard deviation (\u03c3) z = 1.96 #The z value associated with 95% Confidence Interval # Assuming a normally distributed population # Assuming randomly selected samples # Step2- Calculate the margin of error import math margin = z*(std/math.sqrt(n)) print('The margin of error is equal to : ',margin) The margin of error is equal to : 8.765386471799175 # Step3- Find the estimated true population mean for the 95% confidence interval # To find the range of values you just have to add and subtract 8.765 from 461 low = Xbar-margin high = Xbar+margin print('the true population mean will be captured within the confidence interval of (',low,' , ',high,') and the confidence is 95%') the true population mean will be captured within the confidence interval of ( 452.23461352820084 , 469.76538647179916 ) and the confidence is 95% Some great reads on Confidence Intervals: - \"Confidence Intervals for Machine Learning\" by Jason Brownlee available at https://machinelearningmastery.com/confidence-intervals-for-machine-learning/ - \"Comprehensive Confidence Intervals for Python Developers\" available at https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers - \"Confidence Interval\" available at http://napitupulu-jon.appspot.com/posts/confidence-interval-coursera-statistics.html - \"Introduction to Confidence Intervals\" available at https://courses.lumenlearning.com/introstats1/chapter/introduction-confidence-intervals/ Some great videos on Confidence Intervals: - \"Understanding Confidence Intervals: Statistics Help\" by Dr Nic's Maths and Stats available at https://www.youtube.com/watch?v=tFWsuO9f74o - \"Confidence intervals and margin of error | AP Statistics | Khan Academy\" by Khan Academy available at https://www.youtube.com/watch?v=hlM7zdf7zwU - \"StatQuest: Confidence Intervals\" by StatQuest with Josh Starmer available at* https://www.youtube.com/watch?v=TqOeMYtOc1w Exercise: Samples, Populations | Men and Gods Why are confidence intervals useful? Make sure to cite any resources that you may use.","title":"Lab19"},{"location":"8-Labs/Newly Formatted/Lab19/#laboratory-19-game-of-gods-and-men-on-confidence-intervals-with-a-pinch-of-theology","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 19: \"Game of Gods and Men\" | On Confidence Intervals with a Pinch of Theology "},{"location":"8-Labs/Newly Formatted/Lab19/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab19/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab19/#hex","text":"","title":"HEX:"},{"location":"8-Labs/Newly Formatted/Lab19/#title-of-the-notebook","text":"","title":"Title of the notebook"},{"location":"8-Labs/Newly Formatted/Lab19/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab19/#italy-soccer-how-many-people-love-soccer","text":"inspired by an example in \"A (very) friendly introduction to Confidence Intervals\" by Dima Shulga available at https://towardsdatascience.com/a-very-friendly-introduction-to-confidence-intervals-9add126e714 and \"Introduction of Confidence Interval\" by Irfan Rahman available at https://medium.com/steps-towards-data-science/confidence-interval-a7fb3484d7b4 *hint: According to UN estimate data, almost 60 million (60,449,841) people live in Italy","title":"Italy &amp; Soccer: How many people love soccer? "},{"location":"8-Labs/Newly Formatted/Lab19/#for-the-first-example-in-this-lab-we-are-going-to-look-at-a-problem-from-two-perspectives-or-two-modes-if-you-may","text":"","title":"For the first example in this lab, we are going to look at a problem from two perspectives, or two \"modes\" if you may: "},{"location":"8-Labs/Newly Formatted/Lab19/#the-god-mode-and-the-man-mode","text":"","title":"The GOD mode and The MAN mode."},{"location":"8-Labs/Newly Formatted/Lab19/#the-god-mode","text":"","title":"The GOD MODE:"},{"location":"8-Labs/Newly Formatted/Lab19/#in-god-mode-we-are-assuming-that-we-know-everything-about-our-population-in-this-case-the-population-of-italy-suppose-we-know-theoretically-the-exact-percentage-of-people-in-italy-that-love-soccer-and-its-75","text":"","title":"In GOD mode, we are assuming that we know EVERYTHING about our population (in this case, the population of Italy). Suppose we know (theoretically) the exact percentage of people in Italy that love soccer and it\u2019s 75%. "},{"location":"8-Labs/Newly Formatted/Lab19/#lets-say-we-want-to-know-the-chance-of-randomly-selecting-a-group-of-1000-people-that-only-73-of-them-love-soccer","text":"totalpop = 60*10**6 # Total adult population of Italy (60M) fbl_p = 0.75 #percentage of those loving soccer|football ! fblpop = int(totalpop * fbl_p) #Population of those who love football nfblpop = int(totalpop * (1-fbl_p)) #Population of those who doesn't love football","title":"Let's say we want to know the chance of randomly selecting a group of 1000 people that only 73% of them love soccer!"},{"location":"8-Labs/Newly Formatted/Lab19/#lets-create-a-numpy-array-with-60-million-elements-with-a-1-for-each-one-person-who-loves-soccer-and-zero-otherwise","text":"import numpy as np fblpop_1 = np.ones(fblpop) #An array of \"1\"s | its length is equal to the population of those who love football | DO NOT ATTEMPT TO PRINT!!! nfblpop_0 = np.zeros(nfblpop) #An array of \"0\"s | its length is equal to the population of those who doesn't love football | DO NOT ATTEMPT TO PRINT!!! totpop_01 = np.hstack([fblpop_1,nfblpop_0]) #An array of \"0 & 1\"s | its length is equal to the total population of Italy | DO NOT ATTEMPT TO PRINT!!!","title":"Let's create a numpy array with 60 million elements, with a 1 for each one person who loves soccer, and zero otherwise."},{"location":"8-Labs/Newly Formatted/Lab19/#as-a-check-we-can-get-the-percentage-of-1s-in-the-array-by-calculating-the-mean-of-it-and-indeed-it-is-75","text":"print(np.mean(totpop_01)) 0.75","title":"As a check, we can get the percentage of \"1\"s in the array by calculating the mean of it, and indeed it is 75%."},{"location":"8-Labs/Newly Formatted/Lab19/#now-lets-take-few-samples-and-see-what-percentage-do-we-get","text":"np.mean(np.random.choice(totpop_01, size=1000)) # Run multiple times 0.767 # Let's do it in a more sophisticated/engineery/data sciency way! for i in range(10): #Let's take 10 samples sample = np.random.choice(totpop_01, size=1000) print('Sample', i, ':', np.mean(sample)) Sample 0 : 0.727 Sample 1 : 0.74 Sample 2 : 0.739 Sample 3 : 0.753 Sample 4 : 0.776 Sample 5 : 0.766 Sample 6 : 0.769 Sample 7 : 0.741 Sample 8 : 0.769 Sample 9 : 0.741","title":"Now, lets take few samples and see what percentage do we get:"},{"location":"8-Labs/Newly Formatted/Lab19/#you-can-see-that-were-getting-different-values-for-each-sample-but-the-intuition-and-statistics-theory-says-that-the-average-of-large-amount-of-samples-should-be-very-close-to-the-real-percentage-lets-do-that-lets-take-many-samples-and-see-what-happens","text":"values = [] #Create an empty list for i in range(10000): #Let's take 10000 samples sample = np.random.choice(totpop_01, size=1000) #Notice that the sample size is not changing mean = np.mean(sample) values.append(mean) #Store the mean of each sample set print(np.mean(values)) #Printing the mean of means! values = np.array(values) print(values.std()) #Printing the standard deviation of means! 0.7499531 0.013818230725747793","title":"You can see that we\u2019re getting different values for each sample, but the intuition (and statistics theory) says that the average of large amount of samples should be very close to the real percentage. Let\u2019s do that! lets take many samples and see what happens:"},{"location":"8-Labs/Newly Formatted/Lab19/#we-created-10000-samples-checked-the-percentage-of-people-who-love-soccer-in-each-sample-and-then-just-averaged-them-we-got-7499-which-is-very-close-to-the-real-value-75-that-we-as-the-god-knew","text":"","title":"We created 10000 samples, checked the percentage of people who love soccer in each sample, and then just averaged them. we got 74.99% which is very close to the real value 75% that we as the GOD knew! "},{"location":"8-Labs/Newly Formatted/Lab19/#lets-plot-a-histogram-of-all-the-values-we-got-in-all-the-samples-interestingly-this-histogram-is-very-similar-to-the-normal-distribution","text":"import seaborn as sns sns.distplot(values,color='purple', rug=True,kde=True) <matplotlib.axes._subplots.AxesSubplot at 0x21c485c1688>","title":"Let\u2019s plot a histogram of all the values we got in all the samples. Interestingly, this histogram is  very similar to the normal distribution!"},{"location":"8-Labs/Newly Formatted/Lab19/#if-we-do-this-process-a-very-large-number-of-times-infinite-number-of-times-we-will-get-an-histogram-that-is-very-close-to-the-normal-distribution-and-we-can-know-the-parameters-of-this-distribution","text":"values = [] #Create an empty list for i in range(1000000): #Let's take 1000000 samples sample = np.random.choice(totpop_01, size=1000) #Notice that the sample size is not changing mean = np.mean(sample) values.append(mean) #Store the mean of each sample set print(np.mean(values)) #Printing the mean of means! 0.7500088180000001 import seaborn as sns sns.distplot(values,color='purple', rug=True,kde=True) <matplotlib.axes._subplots.AxesSubplot at 0x21c4562bcc8>","title":"if we do this process a very large number of times (infinite number of times) we will get an histogram that is very close to the normal distribution and we can know the parameters of this distribution."},{"location":"8-Labs/Newly Formatted/Lab19/#first-of-all-we-can-see-that-the-center-the-mean-of-the-histogram-is-near-75-exactly-as-we-expected-but-we-are-able-to-say-much-more-just-by-looking-at-the-histogram-for-example-we-can-say-that-half-of-the-samples-are-larger-than-75-or-we-can-say-that-roughly-25-are-larger-than-76-we-can-also-say-that-almost-95-of-the-samples-are-between-72-and-78-lets-also-have-a-look-at-the-boxplot","text":"import matplotlib.pyplot as plt fig = plt.figure(figsize =(10, 7)) plt.boxplot (values,1, '') plt.show()","title":"First of all, we can see that the center (the mean) of the histogram is near 75%, exactly as we expected, but we are able to say much more just by looking at the histogram, for example, we can say, that half of the samples are larger than 75%, or, we can say that roughly 25% are larger than 76%. We can also say that almost 95% of the samples are between 72% and 78%. Let's also have a look at the boxplot:"},{"location":"8-Labs/Newly Formatted/Lab19/#at-this-point-many-people-might-ask-two-important-questions-how-can-i-take-infinite-number-of-samples-and-how-does-it-helps-me","text":"","title":"At this point, many people might ask two important questions, \u201cHow can I take infinite number of samples?\u201d and \u201cHow does it helps me?\u201d. "},{"location":"8-Labs/Newly Formatted/Lab19/#the-answer-to-the-first-one-is-that-if-you-are-god-there-is-no-stopping-you-if-you-are-not-god-you-cant","text":"","title":"The answer to the first one is that if you are GOD, there is no stopping you! If you are not GOD, you can't! "},{"location":"8-Labs/Newly Formatted/Lab19/#to-asnwer-the-second-question-lets-go-back-to-our-example-we-initially-took-one-sample-of-1000-people-and-got-a-value-close-to-75-but-not-exactly-75-we-wanted-to-know-what-is-the-chance-that-a-random-sample-of-1000-people-will-have-73-soccer-lovers-using-the-information-above-we-can-say-that-theres-a-chance-of-roughly-20-that-well-get-a-value-that-is-smaller-or-equal-to-73","text":"","title":"To asnwer the second question, Let\u2019s go back to our example, we initially took one sample of 1000 people and got a value close to 75% but not exactly 75%. We wanted to know, what is the chance that a random sample of 1000 people will have 73% soccer lovers. Using the information above, we can say that there\u2019s a chance of (roughly) 20% that we\u2019ll get a value that is smaller or equal to 73%. "},{"location":"8-Labs/Newly Formatted/Lab19/#we-dont-actually-need-to-do-the-infinite-samples-in-other-words-you-dont-have-to-be-god-you-will-know-why-and-how-in-a-few-moments","text":"","title":"We don\u2019t actually need to do the infinite samples. In other words, you don't have to be GOD! You will know why and how in a few moments..."},{"location":"8-Labs/Newly Formatted/Lab19/#the-man-mode","text":"","title":"The MAN MODE:"},{"location":"8-Labs/Newly Formatted/Lab19/#back-in-our-horrid-and-miserable-man-mode-we-dont-know-the-actual-percentage-of-soccer-lovers-in-italy-in-fact-we-know-nothing-about-the-population","text":"","title":"Back in our horrid and miserable Man mode, we don\u2019t know the actual percentage of soccer lovers in Italy. In fact, we know nothing about the population."},{"location":"8-Labs/Newly Formatted/Lab19/#we-do-know-one-thing-though-we-just-took-a-sample-and-got-73-but-how-does-it-help-us","text":"","title":"We do know one thing though: We just took a sample and got 73%. But how does it help us? "},{"location":"8-Labs/Newly Formatted/Lab19/#what-we-also-do-know-is-that-if-we-took-infinite-number-of-samples-the-distibution-of-their-means-will-look-like-this","text":"","title":"What we also DO know, is that if we took infinite number of samples, the distibution of their means will look like this: "},{"location":"8-Labs/Newly Formatted/Lab19/#here-is-the-population-mean-real-percentage-of-soccer-lovers-in-our-example-and-is-the-standard-deviation-of-the-population-if-we-know-this-and-we-know-the-standard-deviation-we-are-able-to-say-that-68-of-the-samples-will-fall-in-the-red-area-or-more-than-95-of-the-samples-will-fall-outside-the-green-area-in-the-middle-in-this-plot","text":"","title":"Here \u03bc is the population mean (real percentage of soccer lovers in our example), and \u03c3 is the standard deviation of the population. If we know this (and we know the standard deviation) we are able to say that ~68% of the samples will fall in the red area or, more than 95% of the samples will fall outside the green area (in the middle) in this plot: "},{"location":"8-Labs/Newly Formatted/Lab19/#this-is-where-the-concept-of-margin-of-error-becomes-of-great-importance","text":"","title":"This is where the concept of margin of error becomes of great importance:"},{"location":"8-Labs/Newly Formatted/Lab19/#lets-mix-the-god-mode-and-the-man-mode-lets-do-mad-mode","text":"","title":"Let's mix the GOD mode and the MAN mode. LET's DO MAD MODE!"},{"location":"8-Labs/Newly Formatted/Lab19/#of-course-the-distance-is-symmetric-so-if-the-sample-percentage-will-fall-95-of-the-time-between-real-percentage-3-and-real-percentage-3-then-the-real-percentage-will-be-95-of-the-times-between-sample-percentage-3-and-sample-percentage-3","text":"import seaborn as sns sns.distplot(values,color='purple', rug=True,kde=True) <matplotlib.axes._subplots.AxesSubplot at 0x1ccf33b1208>","title":"Of course the distance is symmetric, So if the sample percentage will fall 95% of the time between real percentage-3 and real percentage +3, then the real percentage will be 95% of the times between sample percentage -3 and sample percentage +3."},{"location":"8-Labs/Newly Formatted/Lab19/#if-we-took-a-sample-and-got-73-we-can-say-that-we-are-95-confident-that-the-real-percentage-is-between-70-73-3-and-76-733","text":"","title":"If we took a sample and got 73%, we can say that we are 95% confident that the real percentage is between 70% (73 -3) and 76% (73+3)."},{"location":"8-Labs/Newly Formatted/Lab19/#this-is-the-confidence-interval-the-interval-is-73-3-and-the-confidence-is-95","text":"","title":"This is the Confidence Interval, the interval is 73 +- 3 and the confidence is 95%."},{"location":"8-Labs/Newly Formatted/Lab19/#example-from-a-normally-distributed-population-we-randolmy-took-a-sample-of-500-students-with-a-mean-score-of-461-on-the-math-section-of-sat-suppose-the-standard-deviation-of-the-population-is-100-what-is-the-estimated-true-population-mean-for-the-95-confidence-interval","text":"# Step 1- Organize the data n = 500 #Sample size Xbar = 461 #Sample mean C = 0.95 #Confidence level std = 100 #Standard deviation (\u03c3) z = 1.96 #The z value associated with 95% Confidence Interval # Assuming a normally distributed population # Assuming randomly selected samples # Step2- Calculate the margin of error import math margin = z*(std/math.sqrt(n)) print('The margin of error is equal to : ',margin) The margin of error is equal to : 8.765386471799175 # Step3- Find the estimated true population mean for the 95% confidence interval # To find the range of values you just have to add and subtract 8.765 from 461 low = Xbar-margin high = Xbar+margin print('the true population mean will be captured within the confidence interval of (',low,' , ',high,') and the confidence is 95%') the true population mean will be captured within the confidence interval of ( 452.23461352820084 , 469.76538647179916 ) and the confidence is 95% Some great reads on Confidence Intervals: - \"Confidence Intervals for Machine Learning\" by Jason Brownlee available at https://machinelearningmastery.com/confidence-intervals-for-machine-learning/ - \"Comprehensive Confidence Intervals for Python Developers\" available at https://aegis4048.github.io/comprehensive_confidence_intervals_for_python_developers - \"Confidence Interval\" available at http://napitupulu-jon.appspot.com/posts/confidence-interval-coursera-statistics.html - \"Introduction to Confidence Intervals\" available at https://courses.lumenlearning.com/introstats1/chapter/introduction-confidence-intervals/ Some great videos on Confidence Intervals: - \"Understanding Confidence Intervals: Statistics Help\" by Dr Nic's Maths and Stats available at https://www.youtube.com/watch?v=tFWsuO9f74o - \"Confidence intervals and margin of error | AP Statistics | Khan Academy\" by Khan Academy available at https://www.youtube.com/watch?v=hlM7zdf7zwU - \"StatQuest: Confidence Intervals\" by StatQuest with Josh Starmer available at* https://www.youtube.com/watch?v=TqOeMYtOc1w","title":"Example: From a normally distributed population, we randolmy took a sample of 500 students with a mean score of 461 on the math section of SAT. Suppose the standard deviation of the population is 100, what is the estimated true population mean for the 95% confidence interval."},{"location":"8-Labs/Newly Formatted/Lab19/#exercise-samples-populations-men-and-gods","text":"","title":"Exercise: Samples, Populations | Men and Gods "},{"location":"8-Labs/Newly Formatted/Lab19/#why-are-confidence-intervals-useful","text":"","title":"Why are confidence intervals useful?"},{"location":"8-Labs/Newly Formatted/Lab19/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab2/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab2 Laboratory 2: First Steps... Notice the code cell below! From this notebook forward please include and run the script in the cell, it will help in debugging a notebook. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Also, from now on, please make sure that you have the following markdown cell, filled with your own information, on top of your notebooks: Full name: R#: Title of the notebook: Date: Now, let's get to work! Variables Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float). TimeOfConcentration + 5 5.0 Naming Rules Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print, input, if, while, and for. There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables. Operators The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below: # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10. What's it with # ? Comments are added by writing a hashtag symbol (#) followed by any text of your choice. Any text that follows the hashtag symbol on the same line is ignored by the Python interpreter. Arithmetic Operators In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x ** y Raises value in x by value in y. ( e.g. xy) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0 Data Type In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary Integer Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309 Real (Float) A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427 String(Alphanumeric) A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) print(MyName[0:4]) # Notice how the string is sliced- This is Python: ALWAYS start counting from zero! All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Theo Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting. Changing Types A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens! Expressions Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15 Example: Simple Input/Output Get two floating point numbers via the input() function and store them under the variable names float1 and float2 . Then, compare them, and try a few operations on them! float1 = input(\"Please enter float1: \") float1 = float(float1) ... Print float1 and float2 to the output screen. print(\"float1:\", float1) ... Then check whether float1 is greater than or equal to float2 . float1 = input(\"Please enter float1: \") float2 = input(\"Please enter float2: \") Please enter float1: 2.5 Please enter float2: 5 print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5 float1 = float(float1) float2 = float(float2) print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5.0 float1>float2 False float1+float2 7.5 float1/float2 0.5 Here are some great reads on this topic: - \"Variables in Python\" by John Sturtz available at https://realpython.com/python-variables/ - \"A Beginner\u2019s Guide To Python Variables\" by Avijeet Biswal available at https://www.simplilearn.com/tutorials/python-tutorial/python-variables - \"A Very Basic Introduction to Variables in Python\" by Dr. Python available at *https://medium.com/@doctorsmonsters/a-very-basic-introduction-to-variables-in-python-4231e36dac52 Here are some great videos on these topics: - \"Python Tutorial for Absolute Beginners #1 - What Are Variables?\" by CS Dojo available at https://www.youtube.com/watch?v=Z1Yd7upQsXY - \"#4 Python Tutorial for Beginners | Variables in Python\" by Telusko available at https://www.youtube.com/watch?v=TqPzwenhMj0 - \"Variables and Types in Python\" by DataCamp available at *https://www.youtube.com/watch?v=OH86oLzVzzw Exercise: Integer or Float? Think of a few cases where one might need to convert a float into an integer. * Make sure to cite any resources that you may use.","title":"Lab2"},{"location":"8-Labs/Newly Formatted/Lab2/#laboratory-2-first-steps","text":"","title":"Laboratory 2: First Steps... "},{"location":"8-Labs/Newly Formatted/Lab2/#notice-the-code-cell-below","text":"","title":"Notice the code cell below!"},{"location":"8-Labs/Newly Formatted/Lab2/#from-this-notebook-forward-please-include-and-run-the-script-in-the-cell-it-will-help-in-debugging-a-notebook","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"From this notebook forward please include and run the script in the cell, it will help in debugging a notebook."},{"location":"8-Labs/Newly Formatted/Lab2/#also-from-now-on-please-make-sure-that-you-have-the-following-markdown-cell-filled-with-your-own-information-on-top-of-your-notebooks","text":"","title":"Also, from now on, please make sure that you have the following markdown cell, filled with your own information, on top of your notebooks:"},{"location":"8-Labs/Newly Formatted/Lab2/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab2/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab2/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Newly Formatted/Lab2/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab2/#now-lets-get-to-work","text":"","title":"Now, let's get to work!"},{"location":"8-Labs/Newly Formatted/Lab2/#variables","text":"Variables are names given to data that we want to store and manipulate in programs. A variable has a name and a value. The value representation depends on what type of object the variable represents. The utility of variables comes in when we have a structure that is universal, but values of variables within the structure will change - otherwise it would be simple enough to just hardwire the arithmetic. Suppose we want to store the time of concentration for some hydrologic calculation. To do so, we can name a variable TimeOfConcentration , and then assign a value to the variable, for instance: TimeOfConcentration = 0.0 After this assignment statement the variable is created in the program and has a value of 0.0. The use of a decimal point in the initial assignment establishes the variable as a float (a real variable is called a floating point representation -- or just a float). TimeOfConcentration + 5 5.0","title":"Variables"},{"location":"8-Labs/Newly Formatted/Lab2/#naming-rules","text":"Variable names in Python can only contain letters (a - z, A - Z), numerals (0 - 9), or underscores. The first character cannot be a number, otherwise there is considerable freedom in naming. The names can be reasonably long. runTime , run_Time , _run_Time2 , _2runTime are all valid names, but 2runTime is not valid, and will create an error when you try to use it. # Script to illustrate variable names runTime = 1 _2runTime = 2 # change to 2runTime = 2 and rerun script runTime2 = 2 print(runTime,_2runTime,runTime2) 1 2 2 There are some reserved words that cannot be used as variable names because they have preassigned meaning in Parseltongue. These words include print, input, if, while, and for. There are several more; the interpreter won't allow you to use these names as variables and will issue an error message when you attempt to run a program with such words used as variables.","title":"Naming Rules"},{"location":"8-Labs/Newly Formatted/Lab2/#operators","text":"The = sign used in the variable definition is called an assignment operator (or assignment sign). The symbol means that the expression to the right of the symbol is to be evaluated and the result placed into the variable on the left side of the symbol. The \"operation\" is assignment, the \"=\" symbol is the operator name. Consider the script below: # Assignment Operator x = 5 y = 10 print (x,y) x=y # reverse order y=x and re-run, what happens? print (x,y) 5 10 10 10 So look at what happened. When we assigned values to the variables named x and y , they started life as 5 and 10. We then wrote those values to the console, and the program returned 5 and 10. Then we assigned y to x which took the value in y and replaced the value that was in x with this value. We then wrote the contents again, and both variables have the value 10.","title":"Operators"},{"location":"8-Labs/Newly Formatted/Lab2/#whats-it-with","text":"Comments are added by writing a hashtag symbol (#) followed by any text of your choice. Any text that follows the hashtag symbol on the same line is ignored by the Python interpreter.","title":"What's it with # ?"},{"location":"8-Labs/Newly Formatted/Lab2/#arithmetic-operators","text":"In addition to assignment we can also perform arithmetic operations on variables. The fundamental arithmetic operators are: Symbol Meaning Example = Assignment x=3 Assigns value of 3 to x. + Addition x+y Adds values in x and y. - Subtraction x-y Subtracts values in y from x. * Multiplication x*y Multiplies values in x and y. / Division x/y Divides value in x by value in y. // Floor division x//y Divide x by y, truncate result to whole number. % Modulus x%y Returns remainder when x is divided by y. ** Exponentation x ** y Raises value in x by value in y. ( e.g. xy) += Additive assignment x+=2 Equivalent to x = x+2. -= Subtractive assignment x-=2 Equivalent to x = x-2. *= Multiplicative assignment x*=3 Equivalent to x = x*3. /= Divide assignment x/3 Equivalent to x = x/3. Run the script in the next cell for some illustrative results # Uniary Arithmetic Operators x = 10 y = 5 print(x, y) print(x+y) print(x-y) print(x*y) print(x/y) print((x+1)//y) print((x+1)%y) print(x**y) 10 5 15 5 50 2.0 2 1 100000 # Arithmetic assignment operators x = 1 x += 2 print(type(x),x) x = 1 x -= 2 print(type(x),x) x = 1 x *=3 print(type(x),x) x = 10 x /= 2 print(type(x),x) # Interesting what division does to variable type <class 'int'> 3 <class 'int'> -1 <class 'int'> 3 <class 'float'> 5.0","title":"Arithmetic Operators"},{"location":"8-Labs/Newly Formatted/Lab2/#data-type","text":"In the computer data are all binary digits (actually 0 and +5 volts). At a higher level of abstraction data are typed into integers, real, or alphanumeric representation. The type affects the kind of arithmetic operations that are allowed (as well as the kind of arithmetic - integer versus real arithmetic; lexicographical ordering of alphanumeric , etc.) In scientific programming, a common (and really difficult to detect) source of slight inaccuracies (that tend to snowball as the program runs) is mixed mode arithmetic required because two numeric values are of different types (integer and real). Learn more from the textbook https://www.inferentialthinking.com/chapters/04/Data_Types.html Here we present a quick summary","title":"Data Type"},{"location":"8-Labs/Newly Formatted/Lab2/#integer","text":"Integers are numbers without any fractional portion (nothing after the decimal point { which is not used in integers). Numbers like -3, -2, -1, 0, 1, 2, 200 are integers. A number like 1.1 is not an integer, and 1.0 is also not an integer (the presence of the decimal point makes the number a real). To declare an integer in Python, just assign the variable name to an integer for example MyPhoneNumber = 14158576309","title":"Integer"},{"location":"8-Labs/Newly Formatted/Lab2/#real-float","text":"A real or float is a number that has (or can have) a fractional portion - the number has decimal parts. The numbers 3.14159, -0.001, 11.11, 1., are all floats. The last one is especially tricky, if you don't notice the decimal point you might think it is an integer but the inclusion of the decimal point in Python tells the program that the value is to be treated as a float. To declare a float in Python, just assign the variable name to a float for example MyMassInKilos = 74.8427","title":"Real (Float)"},{"location":"8-Labs/Newly Formatted/Lab2/#stringalphanumeric","text":"A string is a data type that is treated as text elements. The usual letters are strings, but numbers can be included. The numbers in a string are simply characters and cannot be directly used in arithmetic. There are some kinds of arithmetic that can be performed on strings but generally we process string variables to capture the text nature of their contents. To declare a string in Python, just assign the variable name to a string value - the trick is the value is enclosed in quotes. The quotes are delimiters that tell the program that the characters between the quotes are characters and are to be treated as literal representation. For example MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" are all string variables. The last assignment is made a string on purpose. String variables can be combined using an operation called concatenation. The symbol for concatenation is the plus symbol + . Strings can also be converted to all upper case using the upper() function. The syntax for the upper() function is 'string to be upper case'.upper() . Notice the \"dot\" in the syntax. The operation passes everything to the left of the dot to the function which then operates on that content and returns the result all upper case (or an error if the input stream is not a string). # Variable Types Example MyPhoneNumber = 14158576309 MyMassInKilos = 74.8427 MyName = 'Theodore' MyCatName = \"Dusty\" DustyMassInKilos = \"7.48427\" print(\"All about me\") print(\"Name: \",MyName, \" Mass :\",MyMassInKilos,\"Kg\" ) print('Phone : ',MyPhoneNumber) print('My cat\\'s name :', MyCatName) # the \\ escape character is used to get the ' into the literal print(\"All about concatenation!\") print(\"A Silly String : \",MyCatName+MyName+DustyMassInKilos) print(\"A SILLY STRING : \", (MyCatName+MyName+DustyMassInKilos).upper()) print(MyName[0:4]) # Notice how the string is sliced- This is Python: ALWAYS start counting from zero! All about me Name: Theodore Mass : 74.8427 Kg Phone : 14158576309 My cat's name : Dusty All about concatenation! A Silly String : DustyTheodore7.48427 A SILLY STRING : DUSTYTHEODORE7.48427 Theo Strings can be formatted using the % operator or the format() function. The concepts will be introduced later on as needed in the workbook, you can Google search for examples of how to do such formatting.","title":"String(Alphanumeric)"},{"location":"8-Labs/Newly Formatted/Lab2/#changing-types","text":"A variable type can be changed. This activity is called type casting. Three functions allow type casting: int() , float() , and str() . The function names indicate the result of using the function, hence int() returns an integer, float() returns a oat, and str() returns a string. There is also the useful function type() which returns the type of variable. The easiest way to understand is to see an example. # Type Casting Examples MyInteger = 234 MyFloat = 876.543 MyString = 'What is your name?' print(MyInteger,MyFloat,MyString) print('Integer as float',float(MyInteger)) print('Float as integer',int(MyFloat)) print('Integer as string',str(MyInteger)) print('Integer as hexadecimal',hex(MyInteger)) print('Integer Type',type((MyInteger))) # insert the hex conversion and see what happens!","title":"Changing Types"},{"location":"8-Labs/Newly Formatted/Lab2/#expressions","text":"Expressions are the \"algebraic\" constructions that are evaluated and then placed into a variable. Consider x1 = 7 + 3 * 6 / 2 - 1 The expression is evaluated from the left to right and in words is Into the object named x1 place the result of: integer 7 + (integer 6 divide by integer 2 = float 3 * integer 3 = float 9 - integer 1 = float 8) = float 15 The division operation by default produces a float result unless forced otherwise. The result is the variable x1 is a float with a value of 15.0 # Expressions Example x1 = 7 + 3 * 6 // 2 - 1 # Change / into // and see what happens! print(type(x1),x1) ## Simple I/O (Input/Output) <class 'int'> 15","title":"Expressions"},{"location":"8-Labs/Newly Formatted/Lab2/#example-simple-inputoutput","text":"Get two floating point numbers via the input() function and store them under the variable names float1 and float2 . Then, compare them, and try a few operations on them! float1 = input(\"Please enter float1: \") float1 = float(float1) ... Print float1 and float2 to the output screen. print(\"float1:\", float1) ... Then check whether float1 is greater than or equal to float2 . float1 = input(\"Please enter float1: \") float2 = input(\"Please enter float2: \") Please enter float1: 2.5 Please enter float2: 5 print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5 float1 = float(float1) float2 = float(float2) print(\"float1:\", float1) print(\"float2:\", float2) float1: 2.5 float2: 5.0 float1>float2 False float1+float2 7.5 float1/float2 0.5 Here are some great reads on this topic: - \"Variables in Python\" by John Sturtz available at https://realpython.com/python-variables/ - \"A Beginner\u2019s Guide To Python Variables\" by Avijeet Biswal available at https://www.simplilearn.com/tutorials/python-tutorial/python-variables - \"A Very Basic Introduction to Variables in Python\" by Dr. Python available at *https://medium.com/@doctorsmonsters/a-very-basic-introduction-to-variables-in-python-4231e36dac52 Here are some great videos on these topics: - \"Python Tutorial for Absolute Beginners #1 - What Are Variables?\" by CS Dojo available at https://www.youtube.com/watch?v=Z1Yd7upQsXY - \"#4 Python Tutorial for Beginners | Variables in Python\" by Telusko available at https://www.youtube.com/watch?v=TqPzwenhMj0 - \"Variables and Types in Python\" by DataCamp available at *https://www.youtube.com/watch?v=OH86oLzVzzw","title":"Example: Simple Input/Output"},{"location":"8-Labs/Newly Formatted/Lab2/#exercise-integer-or-float","text":"","title":"Exercise: Integer or Float? "},{"location":"8-Labs/Newly Formatted/Lab2/#think-of-a-few-cases-where-one-might-need-to-convert-a-float-into-an-integer","text":"","title":"Think of a few cases where one might need to convert a float into an integer."},{"location":"8-Labs/Newly Formatted/Lab2/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab20/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: (LINK NEEDS FIXING!) Lab20 Laboratory 20: On Precognition and Other Sins of the Human Brain # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook Date: The human brain is amazing and mysterious in many ways. Have a look at these sequences. You, with the assistance of your brain, can guess the next item in each sequence, right? A,B,C,D,E, ____ ? 5,10,15,20,25, ____ ? 2,4,8,16,32 ____ ? 0,1,1,2,3, ____ ? 1, 11, 21, 1211,111221, ____ ? But how does our brain do this? How do we 'guess | predict' the next step? Is it that there is only one possible option? is it that we have the previous items? or is it the relationship between the items? What if we have more than a single sequence? Maybe two sets of numbers? How can we predict the next \"item\" in a situation like that? Blue Points? Red Line? Fit? Does it ring any bells? --- 3 Problem 2 (8 pts) The table below contains some experimental observations. Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 7.0 60.3 8.0 77.7 9.0 97.3 10.0 121.1 Plot the speed vs time (speed on y-axis, time on x-axis) using a scatter plot. Use blue markers. Plot a red line on the scatterplot based on the linear model f(x) = mx + b By trial-and-error find values of m and b that provide a good visual fit (i.e. makes the red line explain the blue markers). Using this data model estimate the speed at t = 15~\\texttt{sec.} --- Let's go over some important terminology: Linear Regression: a basic predictive analytics technique that uses historical data to predict an output variable. The Predictor variable (input): the variable(s) that help predict the value of the output variable. It is commonly referred to as X. The Output variable: the variable that we want to predict. It is commonly referred to as Y. To estimate Y using linear regression, we assume the equation: Ye = \u03b2X + \u03b1 where Y\u2091 is the estimated or predicted value of Y based on our linear equation. Our goal is to find statistically significant values of the parameters \u03b1 and \u03b2 that minimise the difference between Y and Y\u2091. If we are able to determine the optimum values of these two parameters, then we will have the line of best fit that we can use to predict the values of Y, given the value of X. So, how do we estimate \u03b1 and \u03b2? We can use a method called Ordinary Least Squares (OLS). The objective of the least squares method is to find values of \u03b1 and \u03b2 that minimise the sum of the squared difference between Y and Y\u2091 (distance between the linear fit and the observed points). We will not go through the derivation here, but using calculus we can show that the values of the unknown parameters are as follows: where X\u0304 is the mean of X values and \u0232 is the mean of Y values. \u03b2 is simply the covariance of X and Y (Cov(X, Y) devided by the variance of X (Var(X)). Covariance: In probability theory and statistics, covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, (i.e., the variables tend to show similar behavior), the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (i.e., the variables tend to show opposite behavior), the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret because it is not normalized and hence depends on the magnitudes of the variables. The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation. The Correlation Coefficient: Correlation coefficients are used in statistics to measure how strong a relationship is between two variables. There are several types of correlation coefficient, but the most popular is Pearson\u2019s. Pearson\u2019s correlation (also called Pearson\u2019s R) is a correlation coefficient commonly used in linear regression.Correlation coefficient formulas are used to find how strong a relationship is between data. The formulat for Pearson\u2019s R: The formulas return a value between -1 and 1, where: 1 : A correlation coefficient of 1 means that for every positive increase in one variable, there is a positive increase of a fixed proportion in the other. For example, shoe sizes go up in (almost) perfect correlation with foot length. -1: A correlation coefficient of -1 means that for every positive increase in one variable, there is a negative decrease of a fixed proportion in the other. For example, the amount of gas in a tank decreases in (almost) perfect correlation with speed. 0 : Zero means that for every increase, there isn\u2019t a positive or negative increase. The two just aren\u2019t related. Example: Let's have a look at the Problem 1 from Exam II We had a table of recoded times and speeds from some experimental observations: Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 7.0 60.3 8.0 77.7 9.0 97.3 10.0 121.1 First let's create a dataframe: # Load the necessary packages import numpy as np import pandas as pd import statistics from matplotlib import pyplot as plt # Create a dataframe: time = [0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0] speed = [0, 3, 7, 12, 20, 30, 45.6, 60.3, 77.7, 97.3, 121.2] data = pd.DataFrame({'Time':time, 'Speed':speed}) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time Speed 0 0.0 0.0 1 1.0 3.0 2 2.0 7.0 3 3.0 12.0 4 4.0 20.0 5 5.0 30.0 6 6.0 45.6 7 7.0 60.3 8 8.0 77.7 9 9.0 97.3 10 10.0 121.2 Now, let's explore the data: data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time Speed count 11.000000 11.000000 mean 5.000000 43.100000 std 3.316625 41.204077 min 0.000000 0.000000 25% 2.500000 9.500000 50% 5.000000 30.000000 75% 7.500000 69.000000 max 10.000000 121.200000 time_var = statistics.variance(time) speed_var = statistics.variance(speed) print(\"Variance of recorded times is \",time_var) print(\"Variance of recorded speed is \",speed_var) Variance of recorded times is 11.0 Variance of recorded speed is 1697.7759999999998 Is there a relationship ( based on covariance, correlation) between time and speed? # To find the covariance data.cov() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time Speed Time 11.00 131.750 Speed 131.75 1697.776 # To find the correlation among the columns # using pearson method data.corr(method ='pearson') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time Speed Time 1.000000 0.964082 Speed 0.964082 1.000000 Let's do linear regression with primitive Python: To estimate \"y\" using the OLS method, we need to calculate \"xmean\" and \"ymean\", the covariance of X and y (\"xycov\"), and the variance of X (\"xvar\") before we can determine the values for alpha and beta. In our case, X is time and y is Speed. # Calculate the mean of X and y xmean = np.mean(time) ymean = np.mean(speed) # Calculate the terms needed for the numator and denominator of beta data['xycov'] = (data['Time'] - xmean) * (data['Speed'] - ymean) data['xvar'] = (data['Time'] - xmean)**2 # Calculate beta and alpha beta = data['xycov'].sum() / data['xvar'].sum() alpha = ymean - (beta * xmean) print(f'alpha = {alpha}') print(f'beta = {beta}') alpha = -16.78636363636363 beta = 11.977272727272727 We now have an estimate for alpha and beta! Our model can be written as Y\u2091 = 11.977 X -16.786, and we can make predictions: X = np.array(time) ypred = alpha + beta * X print(ypred) [-16.78636364 -4.80909091 7.16818182 19.14545455 31.12272727 43.1 55.07727273 67.05454545 79.03181818 91.00909091 102.98636364] Let\u2019s plot our prediction ypred against the actual values of y, to get a better visual understanding of our model: # Plot regression against actual data plt.figure(figsize=(12, 6)) plt.plot(X, ypred, color=\"red\") # regression line plt.plot(time, speed, 'ro', color=\"blue\") # scatter plot showing actual data plt.title('Actual vs Predicted') plt.xlabel('Time (s)') plt.ylabel('Speed (m/s)') plt.show() The red line is our line of best fit, Y\u2091 = 11.977 X -16.786. We can see from this graph that there is a positive linear relationship between X and y. Using our model, we can predict y from any values of X! For example, if we had a value X = 20, we can predict that: ypred_20 = alpha + beta * 20 print(ypred_20) 222.7590909090909 Linear Regression with statsmodels: First, we use statsmodels\u2019 ols function to initialise our simple linear regression model. This takes the formula y ~ X, where X is the predictor variable (Time) and y is the output variable (Speed). Then, we fit the model by calling the OLS object\u2019s fit() method. import statsmodels.formula.api as smf # Initialise and fit linear regression model using `statsmodels` model = smf.ols('Speed ~ Time', data=data) model = model.fit() We no longer have to calculate alpha and beta ourselves as this method does it automatically for us! Calling model.params will show us the model\u2019s parameters: model.params Intercept -16.786364 Time 11.977273 dtype: float64 In the notation that we have been using, \u03b1 is the intercept and \u03b2 is the slope i.e. \u03b1 =-16.786364 and \u03b2 = 11.977273. # Predict values speed_pred = model.predict() # Plot regression against actual data plt.figure(figsize=(12, 6)) plt.plot(data['Time'], data['Speed'], 'o') # scatter plot showing actual data plt.plot(data['Time'], speed_pred, 'r', linewidth=2) # regression line plt.xlabel('Time (s)') plt.ylabel('Speed (m/s)') plt.title('model vs observed') plt.show() How good do you feel about this predictive model? Will you trust it? Example: Advertising and Sells! This is a classic regression problem. we have a dataset of the spendings on TV, Radio, and Newspaper advertisements and number of sales for a specific product. We are interested in exploring the relationship between these parameters and answering the following questions: Can TV advertising spending predict the number of sales for the product? Can Radio advertising spending predict the number of sales for the product? Can Newspaper advertising spending predict the number of sales for the product? Can we use the three of them to predict the number of sales for the product? | Multiple Linear Regression Model Which parameter is a better predictor of the number of sales for the product? # Import and display first rows of the advertising dataset df = pd.read_csv('advertising.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 # Describe the df df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TV Radio Newspaper Sales count 200.000000 200.000000 200.000000 200.000000 mean 147.042500 23.264000 30.554000 14.022500 std 85.854236 14.846809 21.778621 5.217457 min 0.700000 0.000000 0.300000 1.600000 25% 74.375000 9.975000 12.750000 10.375000 50% 149.750000 22.900000 25.750000 12.900000 75% 218.825000 36.525000 45.100000 17.400000 max 296.400000 49.600000 114.000000 27.000000 tv = np.array(df['TV']) radio = np.array(df['Radio']) newspaper = np.array(df['Newspaper']) sales = np.array(df['Sales']) # Get Variance and Covariance - What can we infer? df.cov() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TV Radio Newspaper Sales TV 7370.949893 69.862492 105.919452 350.390195 Radio 69.862492 220.427743 114.496979 44.635688 Newspaper 105.919452 114.496979 474.308326 25.941392 Sales 350.390195 44.635688 25.941392 27.221853 # Get Correlation Coefficient - What can we infer? df.corr(method ='pearson') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TV Radio Newspaper Sales TV 1.000000 0.054809 0.056648 0.782224 Radio 0.054809 1.000000 0.354104 0.576223 Newspaper 0.056648 0.354104 1.000000 0.228299 Sales 0.782224 0.576223 0.228299 1.000000 # Answer the first question: Can TV advertising spending predict the number of sales for the product? import statsmodels.formula.api as smf # Initialise and fit linear regression model using `statsmodels` model = smf.ols('Sales ~ TV', data=df) model = model.fit() print(model.params) Intercept 7.032594 TV 0.047537 dtype: float64 # Predict values TV_pred = model.predict() # Plot regression against actual data - What do we see? plt.figure(figsize=(12, 6)) plt.plot(df['TV'], df['Sales'], 'o') # scatter plot showing actual data plt.plot(df['TV'], TV_pred, 'r', linewidth=2) # regression line plt.xlabel('TV advertising spending') plt.ylabel('Sales') plt.title('Predicting with TV spendings only') plt.show() # Answer the second question: Can Radio advertising spending predict the number of sales for the product? import statsmodels.formula.api as smf # Initialise and fit linear regression model using `statsmodels` model = smf.ols('Sales ~ Radio', data=df) model = model.fit() print(model.params) Intercept 9.311638 Radio 0.202496 dtype: float64 # Predict values RADIO_pred = model.predict() # Plot regression against actual data - What do we see? plt.figure(figsize=(12, 6)) plt.plot(df['Radio'], df['Sales'], 'o') # scatter plot showing actual data plt.plot(df['Radio'], RADIO_pred, 'r', linewidth=2) # regression line plt.xlabel('Radio advertising spending') plt.ylabel('Sales') plt.title('Predicting with Radio spendings only') plt.show() # Answer the third question: Can Newspaper advertising spending predict the number of sales for the product? import statsmodels.formula.api as smf # Initialise and fit linear regression model using `statsmodels` model = smf.ols('Sales ~ Newspaper', data=df) model = model.fit() print(model.params) Intercept 12.351407 Newspaper 0.054693 dtype: float64 # Predict values NP_pred = model.predict() # Plot regression against actual data - What do we see? plt.figure(figsize=(12, 6)) plt.plot(df['Newspaper'], df['Sales'], 'o') # scatter plot showing actual data plt.plot(df['Newspaper'], NP_pred, 'r', linewidth=2) # regression line plt.xlabel('Newspaper advertising spending') plt.ylabel('Sales') plt.title('Predicting with Newspaper spendings only') plt.show() # Answer the fourth question: Can we use the three of them to predict the number of sales for the product? # This is a case of multiple linear regression model. This is simply a linear regression model with more than one predictor: # and is modelled by: Y\u2091 = \u03b1 + \u03b2\u2081X\u2081 + \u03b2\u2082X\u2082 + \u2026 + \u03b2\u209aX\u209a , where p is the number of predictors. # In this case: Sales = \u03b1 + \u03b21*TV + \u03b22*Radio + \u03b23*Newspaper # Multiple Linear Regression with scikit-learn: from sklearn.linear_model import LinearRegression # Build linear regression model using TV,Radio and Newspaper as predictors # Split data into predictors X and output Y predictors = ['TV', 'Radio', 'Newspaper'] X = df[predictors] y = df['Sales'] # Initialise and fit model lm = LinearRegression() model = lm.fit(X, y) print(f'alpha = {model.intercept_}') print(f'betas = {model.coef_}') alpha = 2.9388893694594085 betas = [ 0.04576465 0.18853002 -0.00103749] # Therefore, our model can be written as: #Sales = 2.938 + 0.046*TV + 0.1885*Radio -0.001*Newspaper # we can predict sales from any combination of TV and Radio and Newspaper advertising costs! #For example, if we wanted to know how many sales we would make if we invested # $300 in TV advertising and $200 in Radio advertising and $50 in Newspaper advertising #all we have to do is plug in the values: new_X = [[300, 200,50]] print(model.predict(new_X)) [54.32241174] # Answer the final question : Which parameter is a better predictor of the number of sales for the product? # How can we answer that? # WHAT CAN WE INFER FROM THE BETAs ? So far on linear regression ... What is linear regression? a basic predictive analytics technique that uses historical data to predict an output variable. Why do we need linear regression? To explore the relationship between predictor and output variables and predict the output variable based on known values of predictors. How does linear regression work? To estimate Y using linear regression, we assume the equation: \ud835\udc4c\ud835\udc52=\u03b2\ud835\udc4b+\u03b1 Our goal is to find statistically significant values of the parameters \u03b1 and \u03b2 that minimise the difference between Y and Y\u2091. If we are able to determine the optimum values of these two parameters, then we will have the line of best fit that we can use to predict the values of Y, given the value of X. How to estimate the coefficients? We used a method called \"Ordinary Least Squares (OLS)\". But that is not the only way. Let's put a pin on that! Remember when we discussed Probability Density Function (PDF) for the normal distribution? - Probably not! This equation is telling us the probability our sample x from our random variable X, when the true parameters of the distribution are \u03bc and \u03c3. Example1 :Let\u2019s say our sample is 3, what is the probability it comes from a distribution of \u03bc = 3 and \u03c3 = 1? What if it came from a distribution with \u03bc = 7 and \u03c3 = 2? Which one is more probable? import numpy as np import pandas as pd import statistics import scipy.stats from matplotlib import pyplot as plt scipy.stats.norm.pdf(3, 3, 1) 0.3989422804014327 scipy.stats.norm.pdf(3, 7, 2) 0.02699548325659403 So it is much more likely it came from the first distribution. The PDF equation has shown us how likely those values are to appear in a distribution with certain parameters. Keep that in mind for later. But what if we had a bunch of points we wanted to estimate? Let\u2019s assume we get a bunch of samples fromX which we know to come from some normal distribution, and all are mutually independent from each other. If this is the case, the total probability of observing all of the data is the product of obtaining each data point individually. This should kinda remind you of our class on probability, where we talked about the probability of multiple events happening back to back (e.g., the royal flush set). Example2 : What is the probability of 2 and 6 being drawn from a distribution with \u03bc = 4 and \u03c3 = 1 scipy.stats.norm.pdf(2, 4, 1) * scipy.stats.norm.pdf(6, 4, 1) 0.0029150244650281948 Maximum Likelihood Estimation (MLE) is used to specify a distribution of unknown parameters, then using your data to pull out the actual parameter values.To go back to the pin!, let's look at our linear model: The noise parameter (error) is basically why the points (samples) do not fall exactly on the line. The error for each point would be the distance from the point to our line. We\u2019d like to explicitly include those errors in our model. One method of doing this, is to assume the errors are distributed from a Gaussian distribution with a mean of 0 and some unknown variance \u03c3\u00b2. The Gaussian seems like a good choice, because our errors look like they\u2019re symmetric about were the line would be, and that small errors are more likely than large errors. This model has three parameters: the slope and intercept of our line and the variance of the noise distribution. Our main goal is to find the best parameters for the slope and intercept of our line. let\u2019s rewrite our model from above as a single conditional distribution given x: This is equivalent to pushing our x through the equation of the line and then adding noise from the 0 mean Gaussian. Now, we can write the conditional distribution of y given x in terms of this Gaussian. This is just the equation of a Gaussian distribution\u2019s probability density function, with our linear equation in place of the mean: The semicolon in the conditional distribution acts just like a comma, but it\u2019s a useful notation for separating our observed data from the parameters. Each point is independent and identically distributed (iid), so we can write the likelihood function with respect to all of our observed points as the product of each individual probability density. Since \u03c3\u00b2 is the same for each data point, we can factor out the term of the Gaussian which doesn\u2019t include x or y from the product: The next step in MLE, is to find the parameters which maximize this function. To make our equation simpler, let\u2019s take the log of our likelihood. Recall, that maximizing the log-likelihood is the same as maximizing the likelihood since the log is monotonic. The natural log cancels out with the exponential, turns products into sums of logs, and division into subtraction of logs; so our log-likelihood looks much simpler: To clean things up a bit more, let\u2019s write the output of our line as a single value: Now our log-likelihood can be written as:: To remove the negative signs, let\u2019s recall that maximizing a number is the same thing as minimizing the negative of the number. So instead of maximizing the likelihood, let\u2019s minimize the negative log-likelihood: Our ultimate goal is to find the parameters of our line. To minimize the negative log-likelihood with respect to the linear parameters (the \u03b8s), we can imagine that our variance term is a fixed constant. Removing any constant\u2019s which don\u2019t include our \u03b8s won\u2019t alter the solution. Therefore, we can throw out any constant terms and elegantly write what we\u2019re trying to minimize as: The maximum likelihood estimate for our linear model is the line which minimizes the sum of squared errors! Now, let's solve for parameters. We\u2019ve concluded that the maximum likelihood estimates for our slope and intercept can be found by minimizing the sum of squared errors. Let\u2019s expand out our minimization objective and use i as our index over our n data points: The square in the SSE formula makes it quadratic with a single minimum. The minimum can be found by taking the derivative with respect to each of the parameters, setting it equal to 0, and solving for the parameters in turn. Taking the partial derivative with respect to the intercept, Setting the derivative equal to 0 and solving for the intercept gives us: Taking the partial derivative with respect to the slope, Setting the derivative equal to 0 and solving for the slope gives us: And now it's time to put it all together: def find_line(xs, ys): \"\"\"Calculates the slope and intercept\"\"\" # number of points n = len(xs) # calculate means x_bar = sum(xs)/n y_bar = sum(ys)/n # calculate slope num = 0 denom = 0 for i in range(n): num += (xs[i]-x_bar)*(ys[i]-y_bar) denom += (xs[i]-x_bar)**2 slope = num/denom # calculate intercept intercept = y_bar - slope*x_bar return slope, intercept Example: Let's have a look at the familiar problem from Exam II which was also an Example in the previous lab! We had a table of recorded times and speeds from some experimental observations. Use MLE to find the intercept and the slope: Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 7.0 60.3 8.0 77.7 9.0 97.3 10.0 121.1 time = [0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0] speed = [0, 3, 7, 12, 20, 30, 45.6, 60.3, 77.7, 97.3, 121.2] find_line(time, speed) #Is this similar to our past results?! (11.977272727272727, -16.78636363636364) # Predict values X = np.array(time) alpha = -16.78636363636364 beta = 11.977272727272727 ypred = alpha + beta * X # Plot regression against actual data plt.figure(figsize=(12, 6)) plt.plot(X, speed, 'o') # scatter plot showing actual data plt.plot(X, ypred, 'r', linewidth=2) # regression line plt.xlabel('Time (s)') plt.ylabel('Speed (m/s)') plt.title('model vs observed') plt.show() Goodness-of-Fit So far, we have assessed the quality of fits visually. We can make numerical assessments as well via Goodness-of-Fit (GOF) measures. Let's discuss three of the most common metrics for evaluating predictions on regression machine learning problems: Mean Absolute Error (MAE): The Mean Absolute Error (or MAE) is the average of the absolute differences between predictions and actual values. It gives an idea of how wrong the predictions were. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting). Here is the formula: It is thus an arithmetic average of the absolute errors |ei|=|yi-xi|, where yi is the prediction and xi the true value. This is known as a scale-dependent accuracy measure and therefore cannot be used to make comparisons between series using different scales. # calculate manually d = speed - ypred mae_m = np.mean(abs(d)) print(\"Results by manual calculation:\") print(\"MAE:\",mae_m) import sklearn.metrics as metrics mae = metrics.mean_absolute_error(speed, ypred) print(mae) Results by manual calculation: MAE: 8.927272727272728 8.927272727272728 Mean Squared Error (MSE) and Root Mean Squared Error (RMSE): The Mean Squared Error (or MSE) is much like the mean absolute error in that it provides a gross idea of the magnitude of error. It measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value. The MSE is a measure of the quality of an estimator\u2014it is always non-negative, and values closer to zero are better. An MSE of zero, meaning that the estimator predicts observations of the parameter with perfect accuracy, is ideal (but typically not possible).Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the Root Mean Squared Error (or RMSE). RMSE is the most widely used metric for regression tasksHere is the formula: mse_m = np.mean(d**2) rmse_m = np.sqrt(mse_m) print(\"MSE:\", mse_m) print(\"RMSE:\", rmse_m) mse = metrics.mean_squared_error(speed, ypred) rmse = np.sqrt(mse) # or mse**(0.5) print(mse) print(rmse) MSE: 108.88210743801659 RMSE: 10.434658951686758 108.88210743801659 10.434658951686758 R^2 Metric: The R^2 (or R Squared) metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature, this measure is called the coefficient of determination. This is a value between 0 and 1 for no-fit and perfect fit respectively. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model..Here is the formula: r2_m = 1-(sum(d**2)/sum((speed-np.mean(speed))**2)) print(\"R-Squared:\", r2_m) r2 = metrics.r2_score(speed, ypred) print(r2) R-Squared: 0.9294545816516323 0.9294545816516323 This notebook was inspired by several blogposts including: - \"Introduction to Linear Regression in Python\" by Lorraine Li available at https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0 - \"In Depth: Linear Regression\" available at https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html - \"A friendly introduction to linear regression (using Python)\" available at https://www.dataschool.io/linear-regression-in-python/ - \"What is Maximum Likelihood Estimation \u2014 Examples in Python\" by Robert R.F. DeFilippi available at https://medium.com/@rrfd/what-is-maximum-likelihood-estimation-examples-in-python-791153818030 - \"Linear Regression\" by William Fleshman available at https://towardsdatascience.com/linear-regression-91eeae7d6a2e - \"Regression Accuracy Check in Python (MAE, MSE, RMSE, R-Squared)\" available at https://www.datatechnotes.com/2019/10/accuracy-check-in-python-mae-mse-rmse-r.html Here are some great reads on these topics: - \"Linear Regression in Python\" by Sadrach Pierre available at https://towardsdatascience.com/linear-regression-in-python-a1d8c13f3242 - \"Introduction to Linear Regression in Python\" available at https://cmdlinetips.com/2019/09/introduction-to-linear-regression-in-python/ - \"Linear Regression in Python\" by Mirko Stojiljkovi\u0107 available at https://realpython.com/linear-regression-in-python/ - \"A Gentle Introduction to Linear Regression With Maximum Likelihood Estimation\" by Jason Brownlee available at https://machinelearningmastery.com/linear-regression-with-maximum-likelihood-estimation/ - \"Metrics To Evaluate Machine Learning Algorithms in Python\" by Jason Brownlee available at https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/ - \"A Gentle Introduction to Maximum Likelihood Estimation\" by Jonathan Balaban available at https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f - \"Regression: An Explanation of Regression Metrics And What Can Go Wrong\" by Divyanshu Mishra available at https://towardsdatascience.com/regression-an-explanation-of-regression-metrics-and-what-can-go-wrong-a39a9793d914 - \"Tutorial: Understanding Regression Error Metrics in Python\" available at https://www.dataquest.io/blog/understanding-regression-error-metrics/ Here are some great videos on these topics: - \"StatQuest: Fitting a line to data, aka least squares, aka linear regression.\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=PaFPbb66DxQ&list=PLblh5JKOoLUIzaEkCLIUxQFjPIlapw8nU - \"Statistics 101: Linear Regression, The Very Basics\" by Brandon Foltz available at https://www.youtube.com/watch?v=ZkjP5RJLQF4 - \"How to Build a Linear Regression Model in Python | Part 1\" and 2,3,4! by Sigma Coding available at https://www.youtube.com/watch?v=MRm5sBfdBBQ - \"StatQuest: Maximum Likelihood, clearly explained!!!\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=XepXtl9YKwc - \"Maximum Likelihood for Regression Coefficients (part 1 of 3)\" and part 2 and 3 by Professor Knudson available at https://www.youtube.com/watch?v=avs4V7wBRw0 - \"StatQuest: R-squared explained\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=2AQKmw14mHM Exercise: Linear Regression - Yea or Nay List some of the pros and cons of linear regression. Make sure to cite any resources that you may use.","title":"Lab20"},{"location":"8-Labs/Newly Formatted/Lab20/#laboratory-20-on-precognition-and-other-sins-of-the-human-brain","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 20: On Precognition and Other Sins of the Human Brain "},{"location":"8-Labs/Newly Formatted/Lab20/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab20/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab20/#title-of-the-notebook","text":"","title":"Title of the notebook"},{"location":"8-Labs/Newly Formatted/Lab20/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab20/#the-human-brain-is-amazing-and-mysterious-in-many-ways-have-a-look-at-these-sequences-you-with-the-assistance-of-your-brain-can-guess-the-next-item-in-each-sequence-right","text":"A,B,C,D,E, ____ ? 5,10,15,20,25, ____ ? 2,4,8,16,32 ____ ? 0,1,1,2,3, ____ ? 1, 11, 21, 1211,111221, ____ ?","title":"The human brain is amazing and mysterious in many ways. Have a look at these sequences. You, with the assistance of your brain, can guess the next item in each sequence, right? "},{"location":"8-Labs/Newly Formatted/Lab20/#but-how-does-our-brain-do-this-how-do-we-guess-predict-the-next-step-is-it-that-there-is-only-one-possible-option-is-it-that-we-have-the-previous-items-or-is-it-the-relationship-between-the-items","text":"","title":"But how does our brain do this? How do we 'guess | predict' the next step? Is it that there is only one possible option? is it that we have the previous items? or is it the relationship between the items?"},{"location":"8-Labs/Newly Formatted/Lab20/#what-if-we-have-more-than-a-single-sequence-maybe-two-sets-of-numbers-how-can-we-predict-the-next-item-in-a-situation-like-that","text":"","title":"What if we have more than a single sequence? Maybe two sets of numbers? How can we predict the next \"item\" in a situation like that? "},{"location":"8-Labs/Newly Formatted/Lab20/#blue-points-red-line-fit-does-it-ring-any-bells","text":"","title":"Blue Points? Red Line? Fit? Does it ring any bells? "},{"location":"8-Labs/Newly Formatted/Lab20/#-","text":"","title":"---"},{"location":"8-Labs/Newly Formatted/Lab20/#3-problem-2-8-pts","text":"The table below contains some experimental observations. Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 7.0 60.3 8.0 77.7 9.0 97.3 10.0 121.1 Plot the speed vs time (speed on y-axis, time on x-axis) using a scatter plot. Use blue markers. Plot a red line on the scatterplot based on the linear model f(x) = mx + b By trial-and-error find values of m and b that provide a good visual fit (i.e. makes the red line explain the blue markers). Using this data model estimate the speed at t = 15~\\texttt{sec.}","title":"3 Problem 2 (8 pts)"},{"location":"8-Labs/Newly Formatted/Lab20/#-_1","text":"","title":"---"},{"location":"8-Labs/Newly Formatted/Lab20/#lets-go-over-some-important-terminology","text":"Linear Regression: a basic predictive analytics technique that uses historical data to predict an output variable. The Predictor variable (input): the variable(s) that help predict the value of the output variable. It is commonly referred to as X. The Output variable: the variable that we want to predict. It is commonly referred to as Y.","title":"Let's go over some important terminology:"},{"location":"8-Labs/Newly Formatted/Lab20/#to-estimate-y-using-linear-regression-we-assume-the-equation-ye-x","text":"where Y\u2091 is the estimated or predicted value of Y based on our linear equation.","title":"To estimate Y using linear regression, we assume the equation: Ye = \u03b2X + \u03b1"},{"location":"8-Labs/Newly Formatted/Lab20/#our-goal-is-to-find-statistically-significant-values-of-the-parameters-and-that-minimise-the-difference-between-y-and-ye-if-we-are-able-to-determine-the-optimum-values-of-these-two-parameters-then-we-will-have-the-line-of-best-fit-that-we-can-use-to-predict-the-values-of-y-given-the-value-of-x","text":"","title":"Our goal is to find statistically significant values of the parameters \u03b1 and \u03b2 that minimise the difference between Y and Y\u2091. If we are able to determine the optimum values of these two parameters, then we will have the line of best fit that we can use to predict the values of Y, given the value of X. "},{"location":"8-Labs/Newly Formatted/Lab20/#so-how-do-we-estimate-and","text":"","title":"So, how do we estimate \u03b1 and \u03b2? "},{"location":"8-Labs/Newly Formatted/Lab20/#we-can-use-a-method-called-ordinary-least-squares-ols","text":"","title":"We can use a method called Ordinary Least Squares (OLS). "},{"location":"8-Labs/Newly Formatted/Lab20/#the-objective-of-the-least-squares-method-is-to-find-values-of-and-that-minimise-the-sum-of-the-squared-difference-between-y-and-ye-distance-between-the-linear-fit-and-the-observed-points-we-will-not-go-through-the-derivation-here-but-using-calculus-we-can-show-that-the-values-of-the-unknown-parameters-are-as-follows","text":"","title":"The objective of the least squares method is to find values of \u03b1 and \u03b2 that minimise the sum of the squared difference between Y and Y\u2091 (distance between the linear fit and the observed points). We will not go through the derivation here, but using calculus we can show that the values of the unknown parameters are as follows: "},{"location":"8-Labs/Newly Formatted/Lab20/#where-x-is-the-mean-of-x-values-and-y-is-the-mean-of-y-values-is-simply-the-covariance-of-x-and-y-covx-y-devided-by-the-variance-of-x-varx","text":"Covariance: In probability theory and statistics, covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, (i.e., the variables tend to show similar behavior), the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (i.e., the variables tend to show opposite behavior), the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret because it is not normalized and hence depends on the magnitudes of the variables. The normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation. The Correlation Coefficient: Correlation coefficients are used in statistics to measure how strong a relationship is between two variables. There are several types of correlation coefficient, but the most popular is Pearson\u2019s. Pearson\u2019s correlation (also called Pearson\u2019s R) is a correlation coefficient commonly used in linear regression.Correlation coefficient formulas are used to find how strong a relationship is between data. The formulat for Pearson\u2019s R: The formulas return a value between -1 and 1, where: 1 : A correlation coefficient of 1 means that for every positive increase in one variable, there is a positive increase of a fixed proportion in the other. For example, shoe sizes go up in (almost) perfect correlation with foot length. -1: A correlation coefficient of -1 means that for every positive increase in one variable, there is a negative decrease of a fixed proportion in the other. For example, the amount of gas in a tank decreases in (almost) perfect correlation with speed. 0 : Zero means that for every increase, there isn\u2019t a positive or negative increase. The two just aren\u2019t related.","title":"where X\u0304 is the mean of X values and \u0232 is the mean of Y values. \u03b2 is simply the covariance of X and Y (Cov(X, Y)  devided by the variance of X (Var(X)). "},{"location":"8-Labs/Newly Formatted/Lab20/#example-lets-have-a-look-at-the-problem-1-from-exam-ii","text":"","title":"Example: Let's have a look at the Problem  1 from Exam II"},{"location":"8-Labs/Newly Formatted/Lab20/#we-had-a-table-of-recoded-times-and-speeds-from-some-experimental-observations","text":"Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 7.0 60.3 8.0 77.7 9.0 97.3 10.0 121.1","title":"We had a table of recoded times and speeds from some experimental observations:"},{"location":"8-Labs/Newly Formatted/Lab20/#first-lets-create-a-dataframe","text":"# Load the necessary packages import numpy as np import pandas as pd import statistics from matplotlib import pyplot as plt # Create a dataframe: time = [0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0] speed = [0, 3, 7, 12, 20, 30, 45.6, 60.3, 77.7, 97.3, 121.2] data = pd.DataFrame({'Time':time, 'Speed':speed}) data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time Speed 0 0.0 0.0 1 1.0 3.0 2 2.0 7.0 3 3.0 12.0 4 4.0 20.0 5 5.0 30.0 6 6.0 45.6 7 7.0 60.3 8 8.0 77.7 9 9.0 97.3 10 10.0 121.2","title":"First let's create a dataframe:"},{"location":"8-Labs/Newly Formatted/Lab20/#now-lets-explore-the-data","text":"data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time Speed count 11.000000 11.000000 mean 5.000000 43.100000 std 3.316625 41.204077 min 0.000000 0.000000 25% 2.500000 9.500000 50% 5.000000 30.000000 75% 7.500000 69.000000 max 10.000000 121.200000 time_var = statistics.variance(time) speed_var = statistics.variance(speed) print(\"Variance of recorded times is \",time_var) print(\"Variance of recorded speed is \",speed_var) Variance of recorded times is 11.0 Variance of recorded speed is 1697.7759999999998","title":"Now, let's explore the data:"},{"location":"8-Labs/Newly Formatted/Lab20/#is-there-a-relationship-based-on-covariance-correlation-between-time-and-speed","text":"# To find the covariance data.cov() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time Speed Time 11.00 131.750 Speed 131.75 1697.776 # To find the correlation among the columns # using pearson method data.corr(method ='pearson') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time Speed Time 1.000000 0.964082 Speed 0.964082 1.000000","title":"Is there a relationship ( based on covariance, correlation) between time and speed?"},{"location":"8-Labs/Newly Formatted/Lab20/#lets-do-linear-regression-with-primitive-python","text":"","title":"Let's do linear regression with primitive Python:"},{"location":"8-Labs/Newly Formatted/Lab20/#to-estimate-y-using-the-ols-method-we-need-to-calculate-xmean-and-ymean-the-covariance-of-x-and-y-xycov-and-the-variance-of-x-xvar-before-we-can-determine-the-values-for-alpha-and-beta-in-our-case-x-is-time-and-y-is-speed","text":"# Calculate the mean of X and y xmean = np.mean(time) ymean = np.mean(speed) # Calculate the terms needed for the numator and denominator of beta data['xycov'] = (data['Time'] - xmean) * (data['Speed'] - ymean) data['xvar'] = (data['Time'] - xmean)**2 # Calculate beta and alpha beta = data['xycov'].sum() / data['xvar'].sum() alpha = ymean - (beta * xmean) print(f'alpha = {alpha}') print(f'beta = {beta}') alpha = -16.78636363636363 beta = 11.977272727272727","title":"To estimate \"y\" using the OLS method, we need to calculate \"xmean\" and \"ymean\", the covariance of X and y (\"xycov\"), and the variance of X (\"xvar\") before we can determine the values for alpha and beta. In our case, X is time and y is Speed."},{"location":"8-Labs/Newly Formatted/Lab20/#we-now-have-an-estimate-for-alpha-and-beta-our-model-can-be-written-as-ye-11977-x-16786-and-we-can-make-predictions","text":"X = np.array(time) ypred = alpha + beta * X print(ypred) [-16.78636364 -4.80909091 7.16818182 19.14545455 31.12272727 43.1 55.07727273 67.05454545 79.03181818 91.00909091 102.98636364]","title":"We now have an estimate for alpha and beta! Our model can be written as Y\u2091 = 11.977 X -16.786, and we can make predictions:"},{"location":"8-Labs/Newly Formatted/Lab20/#lets-plot-our-prediction-ypred-against-the-actual-values-of-y-to-get-a-better-visual-understanding-of-our-model","text":"# Plot regression against actual data plt.figure(figsize=(12, 6)) plt.plot(X, ypred, color=\"red\") # regression line plt.plot(time, speed, 'ro', color=\"blue\") # scatter plot showing actual data plt.title('Actual vs Predicted') plt.xlabel('Time (s)') plt.ylabel('Speed (m/s)') plt.show()","title":"Let\u2019s plot our prediction ypred against the actual values of y, to get a better visual understanding of our model:"},{"location":"8-Labs/Newly Formatted/Lab20/#the-red-line-is-our-line-of-best-fit-ye-11977-x-16786-we-can-see-from-this-graph-that-there-is-a-positive-linear-relationship-between-x-and-y-using-our-model-we-can-predict-y-from-any-values-of-x","text":"","title":"The red line is our line of best fit, Y\u2091 = 11.977 X -16.786. We can see from this graph that there is a positive linear relationship between X and y. Using our model, we can predict y from any values of X! "},{"location":"8-Labs/Newly Formatted/Lab20/#for-example-if-we-had-a-value-x-20-we-can-predict-that","text":"ypred_20 = alpha + beta * 20 print(ypred_20) 222.7590909090909","title":"For example, if we had a value X = 20, we can predict that:"},{"location":"8-Labs/Newly Formatted/Lab20/#linear-regression-with-statsmodels","text":"","title":"Linear Regression with statsmodels:"},{"location":"8-Labs/Newly Formatted/Lab20/#first-we-use-statsmodels-ols-function-to-initialise-our-simple-linear-regression-model-this-takes-the-formula-y-x-where-x-is-the-predictor-variable-time-and-y-is-the-output-variable-speed-then-we-fit-the-model-by-calling-the-ols-objects-fit-method","text":"import statsmodels.formula.api as smf # Initialise and fit linear regression model using `statsmodels` model = smf.ols('Speed ~ Time', data=data) model = model.fit()","title":"First, we use statsmodels\u2019 ols function to initialise our simple linear regression model. This takes the formula y ~ X, where X is the predictor variable (Time) and y is the output variable (Speed). Then, we fit the model by calling the OLS object\u2019s fit() method."},{"location":"8-Labs/Newly Formatted/Lab20/#we-no-longer-have-to-calculate-alpha-and-beta-ourselves-as-this-method-does-it-automatically-for-us-calling-modelparams-will-show-us-the-models-parameters","text":"model.params Intercept -16.786364 Time 11.977273 dtype: float64","title":"We no longer have to calculate alpha and beta ourselves as this method does it automatically for us! Calling model.params will show us the model\u2019s parameters:"},{"location":"8-Labs/Newly Formatted/Lab20/#in-the-notation-that-we-have-been-using-is-the-intercept-and-is-the-slope-ie-16786364-and-11977273","text":"# Predict values speed_pred = model.predict() # Plot regression against actual data plt.figure(figsize=(12, 6)) plt.plot(data['Time'], data['Speed'], 'o') # scatter plot showing actual data plt.plot(data['Time'], speed_pred, 'r', linewidth=2) # regression line plt.xlabel('Time (s)') plt.ylabel('Speed (m/s)') plt.title('model vs observed') plt.show()","title":"In the notation that we have been using, \u03b1 is the intercept and \u03b2 is the slope i.e. \u03b1 =-16.786364 and \u03b2 = 11.977273."},{"location":"8-Labs/Newly Formatted/Lab20/#how-good-do-you-feel-about-this-predictive-model-will-you-trust-it","text":"","title":"How good do you feel about this predictive model? Will you trust it?"},{"location":"8-Labs/Newly Formatted/Lab20/#example-advertising-and-sells","text":"","title":"Example: Advertising and Sells! "},{"location":"8-Labs/Newly Formatted/Lab20/#this-is-a-classic-regression-problem-we-have-a-dataset-of-the-spendings-on-tv-radio-and-newspaper-advertisements-and-number-of-sales-for-a-specific-product-we-are-interested-in-exploring-the-relationship-between-these-parameters-and-answering-the-following-questions","text":"Can TV advertising spending predict the number of sales for the product? Can Radio advertising spending predict the number of sales for the product? Can Newspaper advertising spending predict the number of sales for the product? Can we use the three of them to predict the number of sales for the product? | Multiple Linear Regression Model Which parameter is a better predictor of the number of sales for the product? # Import and display first rows of the advertising dataset df = pd.read_csv('advertising.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 # Describe the df df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TV Radio Newspaper Sales count 200.000000 200.000000 200.000000 200.000000 mean 147.042500 23.264000 30.554000 14.022500 std 85.854236 14.846809 21.778621 5.217457 min 0.700000 0.000000 0.300000 1.600000 25% 74.375000 9.975000 12.750000 10.375000 50% 149.750000 22.900000 25.750000 12.900000 75% 218.825000 36.525000 45.100000 17.400000 max 296.400000 49.600000 114.000000 27.000000 tv = np.array(df['TV']) radio = np.array(df['Radio']) newspaper = np.array(df['Newspaper']) sales = np.array(df['Sales']) # Get Variance and Covariance - What can we infer? df.cov() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TV Radio Newspaper Sales TV 7370.949893 69.862492 105.919452 350.390195 Radio 69.862492 220.427743 114.496979 44.635688 Newspaper 105.919452 114.496979 474.308326 25.941392 Sales 350.390195 44.635688 25.941392 27.221853 # Get Correlation Coefficient - What can we infer? df.corr(method ='pearson') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TV Radio Newspaper Sales TV 1.000000 0.054809 0.056648 0.782224 Radio 0.054809 1.000000 0.354104 0.576223 Newspaper 0.056648 0.354104 1.000000 0.228299 Sales 0.782224 0.576223 0.228299 1.000000 # Answer the first question: Can TV advertising spending predict the number of sales for the product? import statsmodels.formula.api as smf # Initialise and fit linear regression model using `statsmodels` model = smf.ols('Sales ~ TV', data=df) model = model.fit() print(model.params) Intercept 7.032594 TV 0.047537 dtype: float64 # Predict values TV_pred = model.predict() # Plot regression against actual data - What do we see? plt.figure(figsize=(12, 6)) plt.plot(df['TV'], df['Sales'], 'o') # scatter plot showing actual data plt.plot(df['TV'], TV_pred, 'r', linewidth=2) # regression line plt.xlabel('TV advertising spending') plt.ylabel('Sales') plt.title('Predicting with TV spendings only') plt.show() # Answer the second question: Can Radio advertising spending predict the number of sales for the product? import statsmodels.formula.api as smf # Initialise and fit linear regression model using `statsmodels` model = smf.ols('Sales ~ Radio', data=df) model = model.fit() print(model.params) Intercept 9.311638 Radio 0.202496 dtype: float64 # Predict values RADIO_pred = model.predict() # Plot regression against actual data - What do we see? plt.figure(figsize=(12, 6)) plt.plot(df['Radio'], df['Sales'], 'o') # scatter plot showing actual data plt.plot(df['Radio'], RADIO_pred, 'r', linewidth=2) # regression line plt.xlabel('Radio advertising spending') plt.ylabel('Sales') plt.title('Predicting with Radio spendings only') plt.show() # Answer the third question: Can Newspaper advertising spending predict the number of sales for the product? import statsmodels.formula.api as smf # Initialise and fit linear regression model using `statsmodels` model = smf.ols('Sales ~ Newspaper', data=df) model = model.fit() print(model.params) Intercept 12.351407 Newspaper 0.054693 dtype: float64 # Predict values NP_pred = model.predict() # Plot regression against actual data - What do we see? plt.figure(figsize=(12, 6)) plt.plot(df['Newspaper'], df['Sales'], 'o') # scatter plot showing actual data plt.plot(df['Newspaper'], NP_pred, 'r', linewidth=2) # regression line plt.xlabel('Newspaper advertising spending') plt.ylabel('Sales') plt.title('Predicting with Newspaper spendings only') plt.show() # Answer the fourth question: Can we use the three of them to predict the number of sales for the product? # This is a case of multiple linear regression model. This is simply a linear regression model with more than one predictor: # and is modelled by: Y\u2091 = \u03b1 + \u03b2\u2081X\u2081 + \u03b2\u2082X\u2082 + \u2026 + \u03b2\u209aX\u209a , where p is the number of predictors. # In this case: Sales = \u03b1 + \u03b21*TV + \u03b22*Radio + \u03b23*Newspaper # Multiple Linear Regression with scikit-learn: from sklearn.linear_model import LinearRegression # Build linear regression model using TV,Radio and Newspaper as predictors # Split data into predictors X and output Y predictors = ['TV', 'Radio', 'Newspaper'] X = df[predictors] y = df['Sales'] # Initialise and fit model lm = LinearRegression() model = lm.fit(X, y) print(f'alpha = {model.intercept_}') print(f'betas = {model.coef_}') alpha = 2.9388893694594085 betas = [ 0.04576465 0.18853002 -0.00103749] # Therefore, our model can be written as: #Sales = 2.938 + 0.046*TV + 0.1885*Radio -0.001*Newspaper # we can predict sales from any combination of TV and Radio and Newspaper advertising costs! #For example, if we wanted to know how many sales we would make if we invested # $300 in TV advertising and $200 in Radio advertising and $50 in Newspaper advertising #all we have to do is plug in the values: new_X = [[300, 200,50]] print(model.predict(new_X)) [54.32241174] # Answer the final question : Which parameter is a better predictor of the number of sales for the product? # How can we answer that? # WHAT CAN WE INFER FROM THE BETAs ?","title":"This is a classic regression problem. we have a dataset of the spendings on TV, Radio, and Newspaper advertisements and number of sales for a specific product. We are interested in exploring the relationship between these parameters and answering the following questions:"},{"location":"8-Labs/Newly Formatted/Lab20/#so-far-on-linear-regression","text":"What is linear regression? a basic predictive analytics technique that uses historical data to predict an output variable. Why do we need linear regression? To explore the relationship between predictor and output variables and predict the output variable based on known values of predictors. How does linear regression work? To estimate Y using linear regression, we assume the equation: \ud835\udc4c\ud835\udc52=\u03b2\ud835\udc4b+\u03b1 Our goal is to find statistically significant values of the parameters \u03b1 and \u03b2 that minimise the difference between Y and Y\u2091. If we are able to determine the optimum values of these two parameters, then we will have the line of best fit that we can use to predict the values of Y, given the value of X. How to estimate the coefficients? We used a method called \"Ordinary Least Squares (OLS)\". But that is not the only way. Let's put a pin on that!","title":"So far on linear regression ... "},{"location":"8-Labs/Newly Formatted/Lab20/#remember-when-we-discussed-probability-density-function-pdf-for-the-normal-distribution-probably-not","text":"","title":"Remember when we discussed Probability Density Function (PDF) for the normal distribution? - Probably not!"},{"location":"8-Labs/Newly Formatted/Lab20/#this-equation-is-telling-us-the-probability-our-sample-x-from-our-random-variable-x-when-the-true-parameters-of-the-distribution-are-and","text":"","title":"This equation is telling us the probability our sample x from our random variable X, when the true parameters of the distribution are \u03bc and \u03c3. "},{"location":"8-Labs/Newly Formatted/Lab20/#example1-lets-say-our-sample-is-3-what-is-the-probability-it-comes-from-a-distribution-of-3-and-1-what-if-it-came-from-a-distribution-with-7-and-2-which-one-is-more-probable","text":"import numpy as np import pandas as pd import statistics import scipy.stats from matplotlib import pyplot as plt scipy.stats.norm.pdf(3, 3, 1) 0.3989422804014327 scipy.stats.norm.pdf(3, 7, 2) 0.02699548325659403","title":"Example1 :Let\u2019s say our sample is 3, what is the probability it comes from a distribution of \u03bc = 3 and \u03c3 = 1? What if it came from a distribution with \u03bc = 7 and \u03c3 = 2? Which one is more probable?"},{"location":"8-Labs/Newly Formatted/Lab20/#so-it-is-much-more-likely-it-came-from-the-first-distribution-the-pdf-equation-has-shown-us-how-likely-those-values-are-to-appear-in-a-distribution-with-certain-parameters-keep-that-in-mind-for-later-but-what-if-we-had-a-bunch-of-points-we-wanted-to-estimate","text":"","title":"So it is much more likely it came from the first distribution. The PDF equation has shown us how likely those values are to appear in a distribution with certain parameters. Keep that in mind for later. But what if we had a bunch of points we wanted to estimate?"},{"location":"8-Labs/Newly Formatted/Lab20/#lets-assume-we-get-a-bunch-of-samples-fromx-which-we-know-to-come-from-some-normal-distribution-and-all-are-mutually-independent-from-each-other-if-this-is-the-case-the-total-probability-of-observing-all-of-the-data-is-the-product-of-obtaining-each-data-point-individually","text":"","title":"Let\u2019s assume we get a bunch of samples fromX which we know to come from some normal distribution, and all are mutually independent from each other. If this is the case, the total probability of observing all of the data is the product of obtaining each data point individually."},{"location":"8-Labs/Newly Formatted/Lab20/#this-should-kinda-remind-you-of-our-class-on-probability-where-we-talked-about-the-probability-of-multiple-events-happening-back-to-back-eg-the-royal-flush-set","text":"","title":"This should kinda remind you of our class on probability, where we talked about the probability of multiple events happening back to back (e.g., the royal flush set)."},{"location":"8-Labs/Newly Formatted/Lab20/#example2-what-is-the-probability-of-2-and-6-being-drawn-from-a-distribution-with-4-and-1","text":"scipy.stats.norm.pdf(2, 4, 1) * scipy.stats.norm.pdf(6, 4, 1) 0.0029150244650281948","title":"Example2 : What is the probability of 2 and 6 being drawn from a distribution with \u03bc = 4 and \u03c3 = 1"},{"location":"8-Labs/Newly Formatted/Lab20/#maximum-likelihood-estimation-mle-is-used-to-specify-a-distribution-of-unknown-parameters-then-using-your-data-to-pull-out-the-actual-parameter-valuesto-go-back-to-the-pin-lets-look-at-our-linear-model","text":"","title":"Maximum Likelihood Estimation (MLE) is used to specify a distribution of unknown parameters, then using your data to pull out the actual parameter values.To go back to the pin!, let's look at our linear model:"},{"location":"8-Labs/Newly Formatted/Lab20/#the-noise-parameter-error-is-basically-why-the-points-samples-do-not-fall-exactly-on-the-line-the-error-for-each-point-would-be-the-distance-from-the-point-to-our-line-wed-like-to-explicitly-include-those-errors-in-our-model-one-method-of-doing-this-is-to-assume-the-errors-are-distributed-from-a-gaussian-distribution-with-a-mean-of-0-and-some-unknown-variance-2-the-gaussian-seems-like-a-good-choice-because-our-errors-look-like-theyre-symmetric-about-were-the-line-would-be-and-that-small-errors-are-more-likely-than-large-errors","text":"","title":"The noise parameter (error) is basically why the points (samples) do not fall exactly on the line. The error for each point would be the distance from the point to our line. We\u2019d like to explicitly include those errors in our model. One method of doing this, is to assume the errors are distributed from a Gaussian distribution with a mean of 0 and some unknown variance \u03c3\u00b2. The Gaussian seems like a good choice, because our errors look like they\u2019re symmetric about were the line would be, and that small errors are more likely than large errors. "},{"location":"8-Labs/Newly Formatted/Lab20/#this-model-has-three-parameters-the-slope-and-intercept-of-our-line-and-the-variance-of-the-noise-distribution-our-main-goal-is-to-find-the-best-parameters-for-the-slope-and-intercept-of-our-line","text":"","title":"This model has three parameters: the slope and intercept of our line and the variance of the noise distribution. Our main goal is to find the best parameters for the slope and intercept of our line."},{"location":"8-Labs/Newly Formatted/Lab20/#lets-rewrite-our-model-from-above-as-a-single-conditional-distribution-given-x","text":"","title":"let\u2019s rewrite our model from above as a single conditional distribution given x:"},{"location":"8-Labs/Newly Formatted/Lab20/#this-is-equivalent-to-pushing-our-x-through-the-equation-of-the-line-and-then-adding-noise-from-the-0-mean-gaussian-now-we-can-write-the-conditional-distribution-of-y-given-x-in-terms-of-this-gaussian-this-is-just-the-equation-of-a-gaussian-distributions-probability-density-function-with-our-linear-equation-in-place-of-the-mean","text":"","title":"This is equivalent to pushing our x through the equation of the line and then adding noise from the 0 mean Gaussian. Now, we can write the conditional distribution of y given x in terms of this Gaussian. This is just the equation of a Gaussian distribution\u2019s probability density function, with our linear equation in place of the mean:"},{"location":"8-Labs/Newly Formatted/Lab20/#the-semicolon-in-the-conditional-distribution-acts-just-like-a-comma-but-its-a-useful-notation-for-separating-our-observed-data-from-the-parameters","text":"","title":"The semicolon in the conditional distribution acts just like a comma, but it\u2019s a useful notation for separating our observed data from the parameters. "},{"location":"8-Labs/Newly Formatted/Lab20/#each-point-is-independent-and-identically-distributed-iid-so-we-can-write-the-likelihood-function-with-respect-to-all-of-our-observed-points-as-the-product-of-each-individual-probability-density-since-2-is-the-same-for-each-data-point-we-can-factor-out-the-term-of-the-gaussian-which-doesnt-include-x-or-y-from-the-product","text":"","title":"Each point is independent and identically distributed (iid), so we can write the likelihood function with respect to all of our observed points as the product of each individual probability density. Since \u03c3\u00b2 is the same for each data point, we can factor out the term of the Gaussian which doesn\u2019t include x or y from the product:"},{"location":"8-Labs/Newly Formatted/Lab20/#the-next-step-in-mle-is-to-find-the-parameters-which-maximize-this-function-to-make-our-equation-simpler-lets-take-the-log-of-our-likelihood-recall-that-maximizing-the-log-likelihood-is-the-same-as-maximizing-the-likelihood-since-the-log-is-monotonic-the-natural-log-cancels-out-with-the-exponential-turns-products-into-sums-of-logs-and-division-into-subtraction-of-logs-so-our-log-likelihood-looks-much-simpler","text":"","title":"The next step in MLE, is to find the parameters which maximize this function. To make our equation simpler, let\u2019s take the log of our likelihood. Recall, that maximizing the log-likelihood is the same as maximizing the likelihood since the log is monotonic. The natural log cancels out with the exponential, turns products into sums of logs, and division into subtraction of logs; so our log-likelihood looks much simpler:"},{"location":"8-Labs/Newly Formatted/Lab20/#to-clean-things-up-a-bit-more-lets-write-the-output-of-our-line-as-a-single-value","text":"","title":"To clean things up a bit more, let\u2019s write the output of our line as a single value:"},{"location":"8-Labs/Newly Formatted/Lab20/#now-our-log-likelihood-can-be-written-as","text":"","title":"Now our log-likelihood can be written as::"},{"location":"8-Labs/Newly Formatted/Lab20/#to-remove-the-negative-signs-lets-recall-that-maximizing-a-number-is-the-same-thing-as-minimizing-the-negative-of-the-number-so-instead-of-maximizing-the-likelihood-lets-minimize-the-negative-log-likelihood","text":"","title":"To remove the negative signs, let\u2019s recall that maximizing a number is the same thing as minimizing the negative of the number. So instead of maximizing the likelihood, let\u2019s minimize the negative log-likelihood:"},{"location":"8-Labs/Newly Formatted/Lab20/#our-ultimate-goal-is-to-find-the-parameters-of-our-line-to-minimize-the-negative-log-likelihood-with-respect-to-the-linear-parameters-the-s-we-can-imagine-that-our-variance-term-is-a-fixed-constant-removing-any-constants-which-dont-include-our-s-wont-alter-the-solution-therefore-we-can-throw-out-any-constant-terms-and-elegantly-write-what-were-trying-to-minimize-as","text":"","title":"Our ultimate goal is to find the parameters of our line. To minimize the negative log-likelihood with respect to the linear parameters (the \u03b8s), we can imagine that our variance term is a fixed constant. Removing any constant\u2019s which don\u2019t include our \u03b8s won\u2019t alter the solution. Therefore, we can throw out any constant terms and elegantly write what we\u2019re trying to minimize as:"},{"location":"8-Labs/Newly Formatted/Lab20/#the-maximum-likelihood-estimate-for-our-linear-model-is-the-line-which-minimizes-the-sum-of-squared-errors","text":"","title":"The maximum likelihood estimate for our linear model is the line which minimizes the sum of squared errors!"},{"location":"8-Labs/Newly Formatted/Lab20/#now-lets-solve-for-parameters-weve-concluded-that-the-maximum-likelihood-estimates-for-our-slope-and-intercept-can-be-found-by-minimizing-the-sum-of-squared-errors-lets-expand-out-our-minimization-objective-and-use-i-as-our-index-over-our-n-data-points","text":"","title":"Now, let's solve for parameters. We\u2019ve concluded that the maximum likelihood estimates for our slope and intercept can be found by minimizing the sum of squared errors. Let\u2019s expand out our minimization objective and use i as our index over our n data points:"},{"location":"8-Labs/Newly Formatted/Lab20/#the-square-in-the-sse-formula-makes-it-quadratic-with-a-single-minimum-the-minimum-can-be-found-by-taking-the-derivative-with-respect-to-each-of-the-parameters-setting-it-equal-to-0-and-solving-for-the-parameters-in-turn","text":"","title":"The square in the SSE formula makes it quadratic with a single minimum. The minimum can be found by taking the derivative with respect to each of the parameters, setting it equal to 0, and solving for the parameters in turn. "},{"location":"8-Labs/Newly Formatted/Lab20/#taking-the-partial-derivative-with-respect-to-the-intercept-setting-the-derivative-equal-to-0-and-solving-for-the-intercept-gives-us","text":"","title":"Taking the partial derivative with respect to the intercept, Setting the derivative equal to 0 and solving for the intercept gives us:"},{"location":"8-Labs/Newly Formatted/Lab20/#taking-the-partial-derivative-with-respect-to-the-slope-setting-the-derivative-equal-to-0-and-solving-for-the-slope-gives-us","text":"","title":"Taking the partial derivative with respect to the slope, Setting the derivative equal to 0 and solving for the slope gives us:"},{"location":"8-Labs/Newly Formatted/Lab20/#and-now-its-time-to-put-it-all-together","text":"def find_line(xs, ys): \"\"\"Calculates the slope and intercept\"\"\" # number of points n = len(xs) # calculate means x_bar = sum(xs)/n y_bar = sum(ys)/n # calculate slope num = 0 denom = 0 for i in range(n): num += (xs[i]-x_bar)*(ys[i]-y_bar) denom += (xs[i]-x_bar)**2 slope = num/denom # calculate intercept intercept = y_bar - slope*x_bar return slope, intercept","title":"And now it's time to put it all together:"},{"location":"8-Labs/Newly Formatted/Lab20/#example-lets-have-a-look-at-the-familiar-problem-from-exam-ii-which-was-also-an-example-in-the-previous-lab","text":"","title":"Example: Let's have a look at the familiar problem  from Exam II which was also an Example in the previous lab!"},{"location":"8-Labs/Newly Formatted/Lab20/#we-had-a-table-of-recorded-times-and-speeds-from-some-experimental-observations-use-mle-to-find-the-intercept-and-the-slope","text":"Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 7.0 60.3 8.0 77.7 9.0 97.3 10.0 121.1 time = [0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0] speed = [0, 3, 7, 12, 20, 30, 45.6, 60.3, 77.7, 97.3, 121.2] find_line(time, speed) #Is this similar to our past results?! (11.977272727272727, -16.78636363636364) # Predict values X = np.array(time) alpha = -16.78636363636364 beta = 11.977272727272727 ypred = alpha + beta * X # Plot regression against actual data plt.figure(figsize=(12, 6)) plt.plot(X, speed, 'o') # scatter plot showing actual data plt.plot(X, ypred, 'r', linewidth=2) # regression line plt.xlabel('Time (s)') plt.ylabel('Speed (m/s)') plt.title('model vs observed') plt.show()","title":"We had a table of recorded times and speeds from some experimental observations. Use MLE to find the intercept and the slope:"},{"location":"8-Labs/Newly Formatted/Lab20/#goodness-of-fit","text":"","title":"Goodness-of-Fit "},{"location":"8-Labs/Newly Formatted/Lab20/#so-far-we-have-assessed-the-quality-of-fits-visually-we-can-make-numerical-assessments-as-well-via-goodness-of-fit-gof-measures-lets-discuss-three-of-the-most-common-metrics-for-evaluating-predictions-on-regression-machine-learning-problems","text":"","title":"So far, we have assessed the quality of fits visually. We can make numerical assessments as well via Goodness-of-Fit (GOF) measures. Let's discuss three of the most common metrics for evaluating predictions on regression machine learning problems: "},{"location":"8-Labs/Newly Formatted/Lab20/#mean-absolute-error-mae","text":"The Mean Absolute Error (or MAE) is the average of the absolute differences between predictions and actual values. It gives an idea of how wrong the predictions were. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting). Here is the formula: It is thus an arithmetic average of the absolute errors |ei|=|yi-xi|, where yi is the prediction and xi the true value. This is known as a scale-dependent accuracy measure and therefore cannot be used to make comparisons between series using different scales. # calculate manually d = speed - ypred mae_m = np.mean(abs(d)) print(\"Results by manual calculation:\") print(\"MAE:\",mae_m) import sklearn.metrics as metrics mae = metrics.mean_absolute_error(speed, ypred) print(mae) Results by manual calculation: MAE: 8.927272727272728 8.927272727272728","title":"Mean Absolute Error (MAE): "},{"location":"8-Labs/Newly Formatted/Lab20/#mean-squared-error-mse-and-root-mean-squared-error-rmse","text":"The Mean Squared Error (or MSE) is much like the mean absolute error in that it provides a gross idea of the magnitude of error. It measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value. The MSE is a measure of the quality of an estimator\u2014it is always non-negative, and values closer to zero are better. An MSE of zero, meaning that the estimator predicts observations of the parameter with perfect accuracy, is ideal (but typically not possible).Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the Root Mean Squared Error (or RMSE). RMSE is the most widely used metric for regression tasksHere is the formula: mse_m = np.mean(d**2) rmse_m = np.sqrt(mse_m) print(\"MSE:\", mse_m) print(\"RMSE:\", rmse_m) mse = metrics.mean_squared_error(speed, ypred) rmse = np.sqrt(mse) # or mse**(0.5) print(mse) print(rmse) MSE: 108.88210743801659 RMSE: 10.434658951686758 108.88210743801659 10.434658951686758","title":"Mean Squared Error (MSE) and Root Mean Squared Error (RMSE): "},{"location":"8-Labs/Newly Formatted/Lab20/#r2-metric","text":"The R^2 (or R Squared) metric provides an indication of the goodness of fit of a set of predictions to the actual values. In statistical literature, this measure is called the coefficient of determination. This is a value between 0 and 1 for no-fit and perfect fit respectively. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model..Here is the formula: r2_m = 1-(sum(d**2)/sum((speed-np.mean(speed))**2)) print(\"R-Squared:\", r2_m) r2 = metrics.r2_score(speed, ypred) print(r2) R-Squared: 0.9294545816516323 0.9294545816516323 This notebook was inspired by several blogposts including: - \"Introduction to Linear Regression in Python\" by Lorraine Li available at https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0 - \"In Depth: Linear Regression\" available at https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html - \"A friendly introduction to linear regression (using Python)\" available at https://www.dataschool.io/linear-regression-in-python/ - \"What is Maximum Likelihood Estimation \u2014 Examples in Python\" by Robert R.F. DeFilippi available at https://medium.com/@rrfd/what-is-maximum-likelihood-estimation-examples-in-python-791153818030 - \"Linear Regression\" by William Fleshman available at https://towardsdatascience.com/linear-regression-91eeae7d6a2e - \"Regression Accuracy Check in Python (MAE, MSE, RMSE, R-Squared)\" available at https://www.datatechnotes.com/2019/10/accuracy-check-in-python-mae-mse-rmse-r.html Here are some great reads on these topics: - \"Linear Regression in Python\" by Sadrach Pierre available at https://towardsdatascience.com/linear-regression-in-python-a1d8c13f3242 - \"Introduction to Linear Regression in Python\" available at https://cmdlinetips.com/2019/09/introduction-to-linear-regression-in-python/ - \"Linear Regression in Python\" by Mirko Stojiljkovi\u0107 available at https://realpython.com/linear-regression-in-python/ - \"A Gentle Introduction to Linear Regression With Maximum Likelihood Estimation\" by Jason Brownlee available at https://machinelearningmastery.com/linear-regression-with-maximum-likelihood-estimation/ - \"Metrics To Evaluate Machine Learning Algorithms in Python\" by Jason Brownlee available at https://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/ - \"A Gentle Introduction to Maximum Likelihood Estimation\" by Jonathan Balaban available at https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f - \"Regression: An Explanation of Regression Metrics And What Can Go Wrong\" by Divyanshu Mishra available at https://towardsdatascience.com/regression-an-explanation-of-regression-metrics-and-what-can-go-wrong-a39a9793d914 - \"Tutorial: Understanding Regression Error Metrics in Python\" available at https://www.dataquest.io/blog/understanding-regression-error-metrics/ Here are some great videos on these topics: - \"StatQuest: Fitting a line to data, aka least squares, aka linear regression.\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=PaFPbb66DxQ&list=PLblh5JKOoLUIzaEkCLIUxQFjPIlapw8nU - \"Statistics 101: Linear Regression, The Very Basics\" by Brandon Foltz available at https://www.youtube.com/watch?v=ZkjP5RJLQF4 - \"How to Build a Linear Regression Model in Python | Part 1\" and 2,3,4! by Sigma Coding available at https://www.youtube.com/watch?v=MRm5sBfdBBQ - \"StatQuest: Maximum Likelihood, clearly explained!!!\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=XepXtl9YKwc - \"Maximum Likelihood for Regression Coefficients (part 1 of 3)\" and part 2 and 3 by Professor Knudson available at https://www.youtube.com/watch?v=avs4V7wBRw0 - \"StatQuest: R-squared explained\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=2AQKmw14mHM","title":"R^2 Metric: "},{"location":"8-Labs/Newly Formatted/Lab20/#exercise-linear-regression-yea-or-nay","text":"","title":"Exercise: Linear Regression - Yea or Nay "},{"location":"8-Labs/Newly Formatted/Lab20/#list-some-of-the-pros-and-cons-of-linear-regression","text":"","title":"List some of the pros and cons of linear regression."},{"location":"8-Labs/Newly Formatted/Lab20/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab21/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: (LINK NEEDS FIXING!) Lab21 Laboratory 21: \"Confidence in Linear Regression\" or \"The Exclusive Guide to Trial by Combat in Westeros\" # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook Date: Last week, we talked about linear regression ... What is linear regression? a basic predictive analytics technique that uses historical data to predict an output variable. Why do we need linear regression? To explore the relationship between predictor and output variables and predict the output variable based on known values of predictors. How does linear regression work? To estimate Y using linear regression, we assume the equation: \ud835\udc4c\ud835\udc52=\u03b2\ud835\udc4b+\u03b1 Our goal is to find statistically significant values of the parameters \u03b1 and \u03b2 that minimise the difference between Y and Y\u2091. If we are able to determine the optimum values of these two parameters, then we will have the line of best fit that we can use to predict the values of Y, given the value of X. How to estimate the coefficients? We have used \"Ordinary Least Squares (OLS)\" and \"Maximum Likelihood Estimation (MLE)\" methods. We can get formulas for the slope and intercept of the line of best fit from each method. Once we have the equation of the line of best fit, we can use it to fit a line and assess the quality of fit as well as predicting. How to assess the fit? We have used graphs and visual assessments to describe the fits, identify regions with more and less errors, and decide whether the fit is trustworthy or not. We also use \"Goodness-of-Fit (GOF)\" metrics to describe the errors and performance of the linear models. - How confident are we with a prediction? By definition, the prediction of a linear regression model is an estimate or an approximation and contains some uncertainty. The uncertainty comes from the errors in the model itself and noise in the input data. The model is an approximation of the relationship between the input variables and the output variables. The model error can be decomposed into three sources of error: the variance of the model, the bias of the model, and the variance of the irreducible error (the noise) in the data. Error(Model) = Variance(Model) + Bias(Model) + Variance(Irreducible Error) That time in Westeros... Before going any further, let's assume that you were arrested by the king's guard as you were minding your business in the streets of King's Landing for the crime of planning for the murder of King Joffrey Baratheon. As much as you hate King Joffrey you had no plans for killing him but no one believes you. In the absence of witnesses or a confession, you demand trial by combat. But they inform you that the Germanic law to settle accusations is no longer used and it has been replaced with a new method. You get to choose a bowman. That bowman will make 3 shots for you. And if he hits the bullseye you will walk a free man. Otherwise, you will be hanged. You have two options. The first bowman is Horace. He is known as one of the greatest target archers of all time. He is old though and due to lack of an efficient social security system in Westeros, he has to work as a hired bowman for the high court to earn a living. You ask around and you hear that he still can shoot a bullseye but as his hands shake, he sometimes misses by a lot. The second archer is Daryl. He is also a wellknown archer but unfortunately he has a drinking problem. You have understood that there has been cases that he has shot the bullseye in all of his three shots and there has been cases that he has completely missed the bullseye. The thing about him is that his three shots are always very close together. Now, you get to pick. Between Horace and Daryl, who would you choose to shoot for your freedom? Bias, Variance, and the bowman dilemma! We used the example above to give you an initial understanding of bias and variance and their impact on a model's performance. Given this is a complicated and yet important aspect of data modeling and machine learning, without getting into too much detail, we will discuss these concepts. Bias reflects how close the functional form of the model can get to the true relationship between the predictors and the outcome. Variance refers to the amount by which [the model] would change if we estimated it using a different training data set. Looking at the picture above, Horace was an archer with high variance and low bias, while Daryl had high bias and low variability. In an ideal world, we want low bias and low variance which we cannot have. When there is a high bias error, it results in a very simplistic model that does not consider the variations very well. Since it does not learn the training data very well, it is called Underfitting. When the model has a high variance, it will still consider the noise as something to learn from. That is, the model learns the noise from the training data, hence when confronted with new (testing) data, it is unable to predict accurately based on it. Since in the case of high variance, the model learns too much from the training data, it is called overfitting. To summarise: - A model with a high bias error underfits data and makes very simplistic assumptions on it - A model with a high variance error overfits the data and learns too much from it - A good model is where both Bias and Variance errors are balanced. The balance between the Bias error and the Variance error is the Bias-Variance Tradeoff. The irreducible error is the error that we can not remove with our model, or with any model. The error is caused by elements outside our control, such as statistical noise in the observations. A model with low bias and high variance predicts points that are around the center generally, but pretty far away from each other (Horace). A model with high bias and low variance is pretty far away from the bull\u2019s eye, but since the variance is low, the predicted points are closer to each other (Daryl). Bias and Variance play an important role in deciding which predictive model to use: Something that you will definitly learn more about if you go further in the field of machine learning and predicitve models. How can we measure bias and variance? There are GOF metrics that can measure the bias and variance of a model: For example the Nash\u2013Sutcliffe model efficiency coefficient and the Kling-Gupta Efficiency (KGE). The Nash\u2013Sutcliffe efficiency is calculated as one minus the ratio of the error variance of the modeled time-series divided by the variance of the observed time-series. In the situation of a perfect model with an estimation error variance equal to zero, the resulting Nash-Sutcliffe Efficiency equals 1 (NSE = 1). KGE provides a diagnostically interesting decomposition of the Nash-Sutcliffe efficiency (and hence MSE), which facilitates the analysis of the relative importance of its different components (correlation, bias and variability). Example: Let's have a look at our old good example of TV, Radio, and Newspaper advertisements and number of sales for a specific product.! Let's say that we are interested to compare the performance of the linear models that use TV spendings and Radio spendings as their predictor variables in terms of accuracy, bias, and variability. import numpy as np import pandas as pd import statistics import scipy.stats from matplotlib import pyplot as plt import statsmodels.formula.api as smf import sklearn.metrics as metrics # Import and display first rows of the advertising dataset df = pd.read_csv('advertising.csv') tv = np.array(df['TV']) radio = np.array(df['Radio']) newspaper = np.array(df['Newspaper']) sales = np.array(df['Sales']) # Initialise and fit linear regression model using `statsmodels` # TV Spending as predictor model_tv = smf.ols('Sales ~ TV', data=df) model_tv = model_tv.fit() TV_pred = model_tv.predict() # Radio Spending as predictor model_rd = smf.ols('Sales ~ Radio', data=df) model_rd = model_rd.fit() RD_pred = model_rd.predict() print(\"RMSE for TV ad spendings as predictor is \",np.sqrt(metrics.mean_squared_error(sales, TV_pred))) print(\"RMSE for Radio ad spendings as predictor is \",np.sqrt(metrics.mean_squared_error(sales, RD_pred))) RMSE for TV ad spendings as predictor is 3.2423221486546887 RMSE for Radio ad spendings as predictor is 4.2535159274564185 print(\"R2 for TV ad spendings as predictor is \",metrics.r2_score(sales, TV_pred)) print(\"R2 for Radio ad spendings as predictor is \",metrics.r2_score(sales, RD_pred)) R2 for TV ad spendings as predictor is 0.611875050850071 R2 for Radio ad spendings as predictor is 0.33203245544529525 from scipy.stats import pearsonr tv_r = pearsonr(TV_pred, sales) rd_r = pearsonr(RD_pred, sales) print(\"Pearson's r for TV ad spendings as predictor is \",tv_r[0]) print(\"Pearson's for Radio ad spendings as predictor is \",rd_r[0]) Pearson's r for TV ad spendings as predictor is 0.7822244248616065 Pearson's for Radio ad spendings as predictor is 0.5762225745710552 from hydroeval import * #Notice this importing method tv_nse = evaluator(nse, TV_pred, sales) rd_nse = evaluator(nse, RD_pred, sales) print(\"NSE for TV ad spendings as predictor is \",tv_nse) print(\"NSE for Radio ad spendings as predictor is \",rd_nse) NSE for TV ad spendings as predictor is [0.61187505] NSE for Radio ad spendings as predictor is [0.33203246] tv_kge = evaluator(kgeprime, TV_pred, sales) rd_kge = evaluator(kgeprime, RD_pred, sales) print(\"KGE for TV ad spendings as predictor is \",tv_kge) print(\"KGE for Radio ad spendings as predictor is \",rd_kge) #KGE: Kling-Gupta efficiencies range from -Inf to 1. Essentially, the closer to 1, the more accurate the model is. #r: the Pearson product-moment correlation coefficient. Ideal value is r=1 #Gamma: the ratio between the coefficient of variation (CV) of the simulated values to #the coefficient of variation of the observed ones. Ideal value is Gamma=1 #Beta: the ratio between the mean of the simulated values and the mean of the observed ones. Ideal value is Beta=1 KGE for TV ad spendings as predictor is [[0.69201883] [0.78222442] [0.78222442] [1. ]] KGE for Radio ad spendings as predictor is [[0.40068822] [0.57622257] [0.57622257] [1. ]] tv_kge = evaluator(kgeprime,TV_pred,TV_pred) print(tv_kge) [[1.] [1.] [1.] [1.]] How confident are we with our linear regression model? The 95% confidence interval for the forecasted values \u0177 of x is where This means that there is a 95% probability that the true linear regression line of the population will lie within the confidence interval of the regression line calculated from the sample data. In the graph on the left of Figure 1, a linear regression line is calculated to fit the sample data points. The confidence interval consists of the space between the two curves (dotted lines). Thus there is a 95% probability that the true best-fit line for the population lies within the confidence interval (e.g. any of the lines in the figure on the right above). There is also a concept called a prediction interval. Here we look at any specific value of x, x0, and find an interval around the predicted value \u01770 for x0 such that there is a 95% probability that the real value of y (in the population) corresponding to x0 is within this interval (see the graph on the right side). The 95% prediction interval of the forecasted value \u01770 for x0 is where the standard error of the prediction is For any specific value x0 the prediction interval is more meaningful than the confidence interval. Example: Let's work on another familier example. We had a table of recoded times and speeds from some experimental observations: Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 7.0 60.3 8.0 77.7 9.0 97.3 10.0 121.1 This time we want to explore the confidence and prediciton intervals for our linear regression model: time = [0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0] speed = [0, 3, 7, 12, 20, 30, 45.6, 60.3, 77.7, 97.3, 121.2] x = np.array(time) Y = np.array(speed) #We already know these parameters from last week but let's assume that we don't! # alpha = -16.78636363636364 # beta = 11.977272727272727 #Our linear model: ypred = alpha + beta * x import statsmodels.api as sm #needed for linear regression from statsmodels.sandbox.regression.predstd import wls_prediction_std #needed to get prediction interval X = sm.add_constant(x) re = sm.OLS(Y, X).fit() print(re.summary()) print(re.params) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.929 Model: OLS Adj. R-squared: 0.922 Method: Least Squares F-statistic: 118.6 Date: Thu, 08 Apr 2021 Prob (F-statistic): 1.75e-06 Time: 12:36:47 Log-Likelihood: -41.405 No. Observations: 11 AIC: 86.81 Df Residuals: 9 BIC: 87.61 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -16.7864 6.507 -2.580 0.030 -31.507 -2.066 x1 11.9773 1.100 10.889 0.000 9.489 14.465 ============================================================================== Omnibus: 1.397 Durbin-Watson: 0.386 Prob(Omnibus): 0.497 Jarque-Bera (JB): 0.993 Skew: 0.508 Prob(JB): 0.609 Kurtosis: 1.934 Cond. No. 11.3 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [-16.78636364 11.97727273] C:\\Users\\Farha\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1450: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=11 \"anyway, n=%i\" % int(n)) prstd, iv_l, iv_u = wls_prediction_std(re) #iv_l and iv_u give you the limits of the prediction interval for each point. print(iv_l) print(iv_u) [-46.74787932 -33.82587196 -21.09197931 -8.56161668 3.75287435 15.84348029 27.70741981 39.34747423 50.77165706 61.99230986 73.02484795] [ 13.17515205 24.20769014 35.42834294 46.85252577 58.49258019 70.35651971 82.44712565 94.76161668 107.29197931 120.02587196 132.94787932] from statsmodels.stats.outliers_influence import summary_table st, data, ss2 = summary_table(re, alpha=0.05) fittedvalues = data[:, 2] predict_mean_se = data[:, 3] predict_mean_ci_low, predict_mean_ci_upp = data[:, 4:6].T predict_ci_low, predict_ci_upp = data[:, 6:8].T plt.plot(x, Y, 'o') plt.plot(x, fittedvalues, '-',color='red', lw=2) plt.plot(x, predict_ci_low, '--', color='green',lw=2) #Lower prediction band plt.plot(x, predict_ci_upp, '--', color='green',lw=2) #Upper prediction band plt.plot(x, predict_mean_ci_low,'--', color='orange', lw=2) #Lower confidence band plt.plot(x, predict_mean_ci_upp,'--', color='orange', lw=2) #Upper confidence band plt.show() This notebook was inspired by several blogposts including: \"How to Calculate the Bias-Variance Trade-off with Python\" by Jason Brownlee available at* https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/ \"Bias and Variance in Machine Learning \u2013 A Fantastic Guide for Beginners!\" by PURVA HUILGOL available at* https://www.analyticsvidhya.com/blog/2020/08/bias-and-variance-tradeoff-machine-learning/ \"Prediction Intervals for Machine Learning\" by Jason Brownlee available at* https://machinelearningmastery.com/prediction-intervals-for-machine-learning/ \"Confidence and prediction intervals for forecasted values\" by Charles Zaiontz available at* https://www.real-statistics.com/regression/confidence-and-prediction-intervals/ \"3.7 OLS Prediction and Prediction Intervals\" available at* http://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE_Book/3-7-UnivarPredict.html \"Using python statsmodels for OLS linear regression\" available at* https://markthegraph.blogspot.com/2015/05/using-python-statsmodels-for-ols-linear.html Here are some great reads on these topics: - \"How to Calculate the Bias-Variance Trade-off with Python\" available at https://aidevelopmenthub.com/how-to-calculate-the-bias-variance-trade-off-with-python/ - \"Understanding the Bias-Variance Tradeoff\" available at http://scott.fortmann-roe.com/docs/BiasVariance.html - \"SCIKIT-LEARN : BIAS-VARIANCE TRADEOFF\" available at https://www.bogotobogo.com/python/scikit-learn/scikit_machine_learning_Bias-variance-Tradeoff.php - \"Linear Regression Confidence Intervals\" available at https://rstudio-pubs-static.s3.amazonaws.com/195401_20b3272a8bb04615ae7ee4c81d18ffb5.html - \"Prediction Interval: Simple Definition, Examples\" available at* https://www.statisticshowto.com/prediction-interval/ Here are some great videos on these topics: - \"Machine Learning Fundamentals: Bias and Variance\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=EuBBz3bI-aA - \"Bias Variance Trade off\" by The Semicolon available at https://www.youtube.com/watch?v=lpkSGTT8uMg - \"Intervals (for the Mean Response and a Single Response) in Simple Linear Regression\" by jbstatistics available at https://www.youtube.com/watch?v=V-sReSM887I - \"Calculate Confidence and prediction intervals for a response in SLR by hand\" by Katie Ann Jager available at https://www.youtube.com/watch?v=JqObYVX1UP0 Exercise: An Ideal Model; Why not? Why do you think we cannot achieve low bias and low variability at the same time? Make sure to cite any resources that you may use.","title":"Lab21"},{"location":"8-Labs/Newly Formatted/Lab21/#laboratory-21-confidence-in-linear-regression-or-the-exclusive-guide-to-trial-by-combat-in-westeros","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 21: \"Confidence in Linear Regression\" or \"The Exclusive Guide to Trial by Combat in Westeros\" "},{"location":"8-Labs/Newly Formatted/Lab21/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab21/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab21/#title-of-the-notebook","text":"","title":"Title of the notebook"},{"location":"8-Labs/Newly Formatted/Lab21/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab21/#last-week-we-talked-about-linear-regression","text":"What is linear regression? a basic predictive analytics technique that uses historical data to predict an output variable. Why do we need linear regression? To explore the relationship between predictor and output variables and predict the output variable based on known values of predictors. How does linear regression work? To estimate Y using linear regression, we assume the equation: \ud835\udc4c\ud835\udc52=\u03b2\ud835\udc4b+\u03b1 Our goal is to find statistically significant values of the parameters \u03b1 and \u03b2 that minimise the difference between Y and Y\u2091. If we are able to determine the optimum values of these two parameters, then we will have the line of best fit that we can use to predict the values of Y, given the value of X. How to estimate the coefficients? We have used \"Ordinary Least Squares (OLS)\" and \"Maximum Likelihood Estimation (MLE)\" methods. We can get formulas for the slope and intercept of the line of best fit from each method. Once we have the equation of the line of best fit, we can use it to fit a line and assess the quality of fit as well as predicting. How to assess the fit? We have used graphs and visual assessments to describe the fits, identify regions with more and less errors, and decide whether the fit is trustworthy or not. We also use \"Goodness-of-Fit (GOF)\" metrics to describe the errors and performance of the linear models. - How confident are we with a prediction? By definition, the prediction of a linear regression model is an estimate or an approximation and contains some uncertainty. The uncertainty comes from the errors in the model itself and noise in the input data. The model is an approximation of the relationship between the input variables and the output variables. The model error can be decomposed into three sources of error: the variance of the model, the bias of the model, and the variance of the irreducible error (the noise) in the data. Error(Model) = Variance(Model) + Bias(Model) + Variance(Irreducible Error)","title":"Last week, we talked about linear regression ... "},{"location":"8-Labs/Newly Formatted/Lab21/#that-time-in-westeros","text":"Before going any further, let's assume that you were arrested by the king's guard as you were minding your business in the streets of King's Landing for the crime of planning for the murder of King Joffrey Baratheon. As much as you hate King Joffrey you had no plans for killing him but no one believes you. In the absence of witnesses or a confession, you demand trial by combat. But they inform you that the Germanic law to settle accusations is no longer used and it has been replaced with a new method. You get to choose a bowman. That bowman will make 3 shots for you. And if he hits the bullseye you will walk a free man. Otherwise, you will be hanged. You have two options. The first bowman is Horace. He is known as one of the greatest target archers of all time. He is old though and due to lack of an efficient social security system in Westeros, he has to work as a hired bowman for the high court to earn a living. You ask around and you hear that he still can shoot a bullseye but as his hands shake, he sometimes misses by a lot. The second archer is Daryl. He is also a wellknown archer but unfortunately he has a drinking problem. You have understood that there has been cases that he has shot the bullseye in all of his three shots and there has been cases that he has completely missed the bullseye. The thing about him is that his three shots are always very close together. Now, you get to pick. Between Horace and Daryl, who would you choose to shoot for your freedom? Bias, Variance, and the bowman dilemma! We used the example above to give you an initial understanding of bias and variance and their impact on a model's performance. Given this is a complicated and yet important aspect of data modeling and machine learning, without getting into too much detail, we will discuss these concepts. Bias reflects how close the functional form of the model can get to the true relationship between the predictors and the outcome. Variance refers to the amount by which [the model] would change if we estimated it using a different training data set. Looking at the picture above, Horace was an archer with high variance and low bias, while Daryl had high bias and low variability. In an ideal world, we want low bias and low variance which we cannot have. When there is a high bias error, it results in a very simplistic model that does not consider the variations very well. Since it does not learn the training data very well, it is called Underfitting. When the model has a high variance, it will still consider the noise as something to learn from. That is, the model learns the noise from the training data, hence when confronted with new (testing) data, it is unable to predict accurately based on it. Since in the case of high variance, the model learns too much from the training data, it is called overfitting. To summarise: - A model with a high bias error underfits data and makes very simplistic assumptions on it - A model with a high variance error overfits the data and learns too much from it - A good model is where both Bias and Variance errors are balanced. The balance between the Bias error and the Variance error is the Bias-Variance Tradeoff. The irreducible error is the error that we can not remove with our model, or with any model. The error is caused by elements outside our control, such as statistical noise in the observations. A model with low bias and high variance predicts points that are around the center generally, but pretty far away from each other (Horace). A model with high bias and low variance is pretty far away from the bull\u2019s eye, but since the variance is low, the predicted points are closer to each other (Daryl). Bias and Variance play an important role in deciding which predictive model to use: Something that you will definitly learn more about if you go further in the field of machine learning and predicitve models. How can we measure bias and variance? There are GOF metrics that can measure the bias and variance of a model: For example the Nash\u2013Sutcliffe model efficiency coefficient and the Kling-Gupta Efficiency (KGE). The Nash\u2013Sutcliffe efficiency is calculated as one minus the ratio of the error variance of the modeled time-series divided by the variance of the observed time-series. In the situation of a perfect model with an estimation error variance equal to zero, the resulting Nash-Sutcliffe Efficiency equals 1 (NSE = 1). KGE provides a diagnostically interesting decomposition of the Nash-Sutcliffe efficiency (and hence MSE), which facilitates the analysis of the relative importance of its different components (correlation, bias and variability).","title":"That time in Westeros..."},{"location":"8-Labs/Newly Formatted/Lab21/#example-lets-have-a-look-at-our-old-good-example-of-tv-radio-and-newspaper-advertisements-and-number-of-sales-for-a-specific-product","text":"","title":"Example: Let's have a look at our old good example of TV, Radio, and Newspaper advertisements and number of sales for a specific product.! "},{"location":"8-Labs/Newly Formatted/Lab21/#lets-say-that-we-are-interested-to-compare-the-performance-of-the-linear-models-that-use-tv-spendings-and-radio-spendings-as-their-predictor-variables-in-terms-of-accuracy-bias-and-variability","text":"import numpy as np import pandas as pd import statistics import scipy.stats from matplotlib import pyplot as plt import statsmodels.formula.api as smf import sklearn.metrics as metrics # Import and display first rows of the advertising dataset df = pd.read_csv('advertising.csv') tv = np.array(df['TV']) radio = np.array(df['Radio']) newspaper = np.array(df['Newspaper']) sales = np.array(df['Sales']) # Initialise and fit linear regression model using `statsmodels` # TV Spending as predictor model_tv = smf.ols('Sales ~ TV', data=df) model_tv = model_tv.fit() TV_pred = model_tv.predict() # Radio Spending as predictor model_rd = smf.ols('Sales ~ Radio', data=df) model_rd = model_rd.fit() RD_pred = model_rd.predict() print(\"RMSE for TV ad spendings as predictor is \",np.sqrt(metrics.mean_squared_error(sales, TV_pred))) print(\"RMSE for Radio ad spendings as predictor is \",np.sqrt(metrics.mean_squared_error(sales, RD_pred))) RMSE for TV ad spendings as predictor is 3.2423221486546887 RMSE for Radio ad spendings as predictor is 4.2535159274564185 print(\"R2 for TV ad spendings as predictor is \",metrics.r2_score(sales, TV_pred)) print(\"R2 for Radio ad spendings as predictor is \",metrics.r2_score(sales, RD_pred)) R2 for TV ad spendings as predictor is 0.611875050850071 R2 for Radio ad spendings as predictor is 0.33203245544529525 from scipy.stats import pearsonr tv_r = pearsonr(TV_pred, sales) rd_r = pearsonr(RD_pred, sales) print(\"Pearson's r for TV ad spendings as predictor is \",tv_r[0]) print(\"Pearson's for Radio ad spendings as predictor is \",rd_r[0]) Pearson's r for TV ad spendings as predictor is 0.7822244248616065 Pearson's for Radio ad spendings as predictor is 0.5762225745710552 from hydroeval import * #Notice this importing method tv_nse = evaluator(nse, TV_pred, sales) rd_nse = evaluator(nse, RD_pred, sales) print(\"NSE for TV ad spendings as predictor is \",tv_nse) print(\"NSE for Radio ad spendings as predictor is \",rd_nse) NSE for TV ad spendings as predictor is [0.61187505] NSE for Radio ad spendings as predictor is [0.33203246] tv_kge = evaluator(kgeprime, TV_pred, sales) rd_kge = evaluator(kgeprime, RD_pred, sales) print(\"KGE for TV ad spendings as predictor is \",tv_kge) print(\"KGE for Radio ad spendings as predictor is \",rd_kge) #KGE: Kling-Gupta efficiencies range from -Inf to 1. Essentially, the closer to 1, the more accurate the model is. #r: the Pearson product-moment correlation coefficient. Ideal value is r=1 #Gamma: the ratio between the coefficient of variation (CV) of the simulated values to #the coefficient of variation of the observed ones. Ideal value is Gamma=1 #Beta: the ratio between the mean of the simulated values and the mean of the observed ones. Ideal value is Beta=1 KGE for TV ad spendings as predictor is [[0.69201883] [0.78222442] [0.78222442] [1. ]] KGE for Radio ad spendings as predictor is [[0.40068822] [0.57622257] [0.57622257] [1. ]] tv_kge = evaluator(kgeprime,TV_pred,TV_pred) print(tv_kge) [[1.] [1.] [1.] [1.]] How confident are we with our linear regression model? The 95% confidence interval for the forecasted values \u0177 of x is where This means that there is a 95% probability that the true linear regression line of the population will lie within the confidence interval of the regression line calculated from the sample data. In the graph on the left of Figure 1, a linear regression line is calculated to fit the sample data points. The confidence interval consists of the space between the two curves (dotted lines). Thus there is a 95% probability that the true best-fit line for the population lies within the confidence interval (e.g. any of the lines in the figure on the right above). There is also a concept called a prediction interval. Here we look at any specific value of x, x0, and find an interval around the predicted value \u01770 for x0 such that there is a 95% probability that the real value of y (in the population) corresponding to x0 is within this interval (see the graph on the right side). The 95% prediction interval of the forecasted value \u01770 for x0 is where the standard error of the prediction is For any specific value x0 the prediction interval is more meaningful than the confidence interval.","title":"Let's say that we are interested to compare the performance of the linear models that use TV spendings and Radio spendings as their predictor variables in terms of accuracy, bias, and variability."},{"location":"8-Labs/Newly Formatted/Lab21/#example-lets-work-on-another-familier-example","text":"","title":"Example: Let's work on another familier example.  "},{"location":"8-Labs/Newly Formatted/Lab21/#we-had-a-table-of-recoded-times-and-speeds-from-some-experimental-observations","text":"Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 7.0 60.3 8.0 77.7 9.0 97.3 10.0 121.1","title":"We had a table of recoded times and speeds from some experimental observations:"},{"location":"8-Labs/Newly Formatted/Lab21/#this-time-we-want-to-explore-the-confidence-and-prediciton-intervals-for-our-linear-regression-model","text":"time = [0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0] speed = [0, 3, 7, 12, 20, 30, 45.6, 60.3, 77.7, 97.3, 121.2] x = np.array(time) Y = np.array(speed) #We already know these parameters from last week but let's assume that we don't! # alpha = -16.78636363636364 # beta = 11.977272727272727 #Our linear model: ypred = alpha + beta * x import statsmodels.api as sm #needed for linear regression from statsmodels.sandbox.regression.predstd import wls_prediction_std #needed to get prediction interval X = sm.add_constant(x) re = sm.OLS(Y, X).fit() print(re.summary()) print(re.params) OLS Regression Results ============================================================================== Dep. Variable: y R-squared: 0.929 Model: OLS Adj. R-squared: 0.922 Method: Least Squares F-statistic: 118.6 Date: Thu, 08 Apr 2021 Prob (F-statistic): 1.75e-06 Time: 12:36:47 Log-Likelihood: -41.405 No. Observations: 11 AIC: 86.81 Df Residuals: 9 BIC: 87.61 Df Model: 1 Covariance Type: nonrobust ============================================================================== coef std err t P>|t| [0.025 0.975] ------------------------------------------------------------------------------ const -16.7864 6.507 -2.580 0.030 -31.507 -2.066 x1 11.9773 1.100 10.889 0.000 9.489 14.465 ============================================================================== Omnibus: 1.397 Durbin-Watson: 0.386 Prob(Omnibus): 0.497 Jarque-Bera (JB): 0.993 Skew: 0.508 Prob(JB): 0.609 Kurtosis: 1.934 Cond. No. 11.3 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [-16.78636364 11.97727273] C:\\Users\\Farha\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1450: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=11 \"anyway, n=%i\" % int(n)) prstd, iv_l, iv_u = wls_prediction_std(re) #iv_l and iv_u give you the limits of the prediction interval for each point. print(iv_l) print(iv_u) [-46.74787932 -33.82587196 -21.09197931 -8.56161668 3.75287435 15.84348029 27.70741981 39.34747423 50.77165706 61.99230986 73.02484795] [ 13.17515205 24.20769014 35.42834294 46.85252577 58.49258019 70.35651971 82.44712565 94.76161668 107.29197931 120.02587196 132.94787932] from statsmodels.stats.outliers_influence import summary_table st, data, ss2 = summary_table(re, alpha=0.05) fittedvalues = data[:, 2] predict_mean_se = data[:, 3] predict_mean_ci_low, predict_mean_ci_upp = data[:, 4:6].T predict_ci_low, predict_ci_upp = data[:, 6:8].T plt.plot(x, Y, 'o') plt.plot(x, fittedvalues, '-',color='red', lw=2) plt.plot(x, predict_ci_low, '--', color='green',lw=2) #Lower prediction band plt.plot(x, predict_ci_upp, '--', color='green',lw=2) #Upper prediction band plt.plot(x, predict_mean_ci_low,'--', color='orange', lw=2) #Lower confidence band plt.plot(x, predict_mean_ci_upp,'--', color='orange', lw=2) #Upper confidence band plt.show() This notebook was inspired by several blogposts including: \"How to Calculate the Bias-Variance Trade-off with Python\" by Jason Brownlee available at* https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/ \"Bias and Variance in Machine Learning \u2013 A Fantastic Guide for Beginners!\" by PURVA HUILGOL available at* https://www.analyticsvidhya.com/blog/2020/08/bias-and-variance-tradeoff-machine-learning/ \"Prediction Intervals for Machine Learning\" by Jason Brownlee available at* https://machinelearningmastery.com/prediction-intervals-for-machine-learning/ \"Confidence and prediction intervals for forecasted values\" by Charles Zaiontz available at* https://www.real-statistics.com/regression/confidence-and-prediction-intervals/ \"3.7 OLS Prediction and Prediction Intervals\" available at* http://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE_Book/3-7-UnivarPredict.html \"Using python statsmodels for OLS linear regression\" available at* https://markthegraph.blogspot.com/2015/05/using-python-statsmodels-for-ols-linear.html Here are some great reads on these topics: - \"How to Calculate the Bias-Variance Trade-off with Python\" available at https://aidevelopmenthub.com/how-to-calculate-the-bias-variance-trade-off-with-python/ - \"Understanding the Bias-Variance Tradeoff\" available at http://scott.fortmann-roe.com/docs/BiasVariance.html - \"SCIKIT-LEARN : BIAS-VARIANCE TRADEOFF\" available at https://www.bogotobogo.com/python/scikit-learn/scikit_machine_learning_Bias-variance-Tradeoff.php - \"Linear Regression Confidence Intervals\" available at https://rstudio-pubs-static.s3.amazonaws.com/195401_20b3272a8bb04615ae7ee4c81d18ffb5.html - \"Prediction Interval: Simple Definition, Examples\" available at* https://www.statisticshowto.com/prediction-interval/ Here are some great videos on these topics: - \"Machine Learning Fundamentals: Bias and Variance\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=EuBBz3bI-aA - \"Bias Variance Trade off\" by The Semicolon available at https://www.youtube.com/watch?v=lpkSGTT8uMg - \"Intervals (for the Mean Response and a Single Response) in Simple Linear Regression\" by jbstatistics available at https://www.youtube.com/watch?v=V-sReSM887I - \"Calculate Confidence and prediction intervals for a response in SLR by hand\" by Katie Ann Jager available at https://www.youtube.com/watch?v=JqObYVX1UP0","title":"This time we want to explore the confidence and prediciton intervals for our linear regression model:"},{"location":"8-Labs/Newly Formatted/Lab21/#exercise-an-ideal-model-why-not","text":"","title":"Exercise: An Ideal Model; Why not? "},{"location":"8-Labs/Newly Formatted/Lab21/#why-do-you-think-we-cannot-achieve-low-bias-and-low-variability-at-the-same-time","text":"","title":"Why do you think we cannot achieve low bias and low variability at the same time?"},{"location":"8-Labs/Newly Formatted/Lab21/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab22/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: (LINK NEEDS FIXING!) Lab22 Laboratory 22: \"On The Virtue and Value of Classification\" or \"Who Ordered a Classy Fire?\" # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook Date: For the last few sessions we have talked about simple linear regression ... We discussed ... The theory and implementation of simple linear regression in Python OLS and MLE methods for estimation of slope and intercept coefficients Errors (Noise, Variance, Bias) and their impacts on model's performance Confidence and prediction intervals And Multiple Linear Regressions What if we want to predict a discrete variable? The general idea behind our efforts was to use a set of observed events (samples) to capture the relationship between one or more predictor (AKA input, indipendent) variables and an output (AKA response, dependent) variable. The nature of the dependent variables differentiates regression and classification problems. Regression problems have continuous and usually unbounded outputs. An example is when you\u2019re estimating the salary as a function of experience and education level. Or all the examples we have covered so far! On the other hand, classification problems have discrete and finite outputs called classes or categories. For example, predicting if an employee is going to be promoted or not (true or false) is a classification problem. There are two main types of classification problems: Binary or binomial classification: exactly two classes to choose between (usually 0 and 1, true and false, or positive and negative) Multiclass or multinomial classification: three or more classes of the outputs to choose from When Do We Need Classification? We can apply classification in many fields of science and technology. For example, text classification algorithms are used to separate legitimate and spam emails, as well as positive and negative comments. Other examples involve medical applications, biological classification, credit scoring, and more. Logistic Regression What is logistic regression? Logistic regression is a fundamental classification technique. It belongs to the group of linear classifiers and is somewhat similar to polynomial and linear regression. Logistic regression is fast and relatively uncomplicated, and it\u2019s convenient for users to interpret the results. Although it\u2019s essentially a method for binary classification, it can also be applied to multiclass problems. Logistic regression is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature. Dichotomous means there are only two possible classes. For example, it can be used for cancer detection problems. It computes the probability of an event occurrence. Logistic regression can be considered a special case of linear regression where the target variable is categorical in nature. It uses a log of odds as the dependent variable. Logistic Regression predicts the probability of occurrence of a binary event utilizing a logit function. HOW? Remember the general format of the multiple linear regression model: Where, y is dependent variable and x1, x2 ... and Xn are explanatory variables. This was, as you know by now, a linear function. There is another famous function known as the Sigmoid Function , also called logistic function . Here is the equation for the Sigmoid function: This image shows the sigmoid function (or S-shaped curve) of some variable \ud835\udc65: As you see, The sigmoid function has values very close to either 0 or 1 across most of its domain. It can take any real-valued number and map it into a value between 0 and 1. If the curve goes to positive infinity, y predicted will become 1, and if the curve goes to negative infinity, y predicted will become 0. This fact makes it suitable for application in classification methods since we are dealing with two discrete classes (labels, categories, ...). If the output of the sigmoid function is more than 0.5, we can classify the outcome as 1 or YES, and if it is less than 0.5, we can classify it as 0 or NO. This cutoff value (threshold) is not always fixed at 0.5. If we apply the Sigmoid function on linear regression: Notice the difference between linear regression and logistic regression: logistic regression is estimated using Maximum Likelihood Estimation (MLE) approach. Maximizing the likelihood function determines the parameters that are most likely to produce the observed data. Let's work on an example in Python! Example 1: Diagnosing Diabetes The \"diabetes.csv\" dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on. Columns Info. Pregnancies Number of times pregnant Glucose Plasma glucose concentration a 2 hours in an oral glucose tolerance test BloodPressure Diastolic blood pressure (mm Hg) SkinThickness Triceps skin fold thickness (mm) Insulin 2-Hour serum insulin (mu U/ml) BMI Body mass index (weight in kg/(height in m)^2) Diabetes pedigree Diabetes pedigree function Age Age (years) Outcome Class variable (0 or 1) 268 of 768 are 1, the others are 0 Let's see if we can build a logistic regression model to accurately predict whether or not the patients in the dataset have diabetes or not? Acknowledgements: Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press. import numpy as np import pandas as pd from matplotlib import pyplot as plt import sklearn.metrics as metrics import seaborn as sns %matplotlib inline # Import the dataset: data = pd.read_csv(\"diabetes.csv\") data.rename(columns = {'Pregnancies':'pregnant', 'Glucose':'glucose','BloodPressure':'bp','SkinThickness':'skin', 'Insulin ':'Insulin','BMI':'bmi','DiabetesPedigreeFunction':'pedigree','Age':'age', 'Outcome':'label'}, inplace = True) data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pregnant glucose bp skin Insulin bmi pedigree age label 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pregnant glucose bp skin Insulin bmi pedigree age label count 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 mean 3.845052 120.894531 69.105469 20.536458 79.799479 31.992578 0.471876 33.240885 0.348958 std 3.369578 31.972618 19.355807 15.952218 115.244002 7.884160 0.331329 11.760232 0.476951 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.078000 21.000000 0.000000 25% 1.000000 99.000000 62.000000 0.000000 0.000000 27.300000 0.243750 24.000000 0.000000 50% 3.000000 117.000000 72.000000 23.000000 30.500000 32.000000 0.372500 29.000000 0.000000 75% 6.000000 140.250000 80.000000 32.000000 127.250000 36.600000 0.626250 41.000000 1.000000 max 17.000000 199.000000 122.000000 99.000000 846.000000 67.100000 2.420000 81.000000 1.000000 #Check some histograms sns.distplot(data['pregnant'], kde = True, rug= True, color ='orange') <matplotlib.axes._subplots.AxesSubplot at 0x1dcca695b48> sns.distplot(data['glucose'], kde = True, rug= True, color ='darkblue') <matplotlib.axes._subplots.AxesSubplot at 0x1dccaa4f288> sns.distplot(data['label'], kde = False, rug= True, color ='purple', bins=2) <matplotlib.axes._subplots.AxesSubplot at 0x285dfb3a0c8> sns.jointplot(x ='glucose', y ='label', data = data, kind ='kde') <seaborn.axisgrid.JointGrid at 0x1dccac73f88> Selecting Feature: Here, we need to divide the given columns into two types of variables dependent(or target variable) and independent variable(or feature variables or predictors). #split dataset in features and target variable feature_cols = ['pregnant', 'glucose', 'bp', 'skin', 'Insulin', 'bmi', 'pedigree', 'age'] X = data[feature_cols] # Features y = data.label # Target variable Splitting Data: To understand model performance, dividing the dataset into a training set and a test set is a good strategy. Let's split dataset by using function train_test_split(). You need to pass 3 parameters: features, target, and test_set size. Additionally, you can use random_state to select records randomly. Here, the Dataset is broken into two parts in a ratio of 75:25. It means 75% data will be used for model training and 25% for model testing: # split X and y into training and testing sets from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0) Model Development and Prediction: First, import the Logistic Regression module and create a Logistic Regression classifier object using LogisticRegression() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). # import the class from sklearn.linear_model import LogisticRegression # instantiate the model (using the default parameters) #logreg = LogisticRegression() logreg = LogisticRegression() # fit the model with data logreg.fit(X_train,y_train) # y_pred=logreg.predict(X_test) C:\\Users\\Farha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. FutureWarning) - How to assess the performance of logistic regression? Binary classification has four possible types of results: - True negatives: correctly predicted negatives (zeros) - True positives: correctly predicted positives (ones) - False negatives: incorrectly predicted negatives (zeros) - False positives: incorrectly predicted positives (ones) We usually evaluate the performance of a classifier by comparing the actual and predicted outputsand counting the correct and incorrect predictions. A confusion matrix is a table that is used to evaluate the performance of a classification model. <br> ![](https://image.jimcdn.com/app/cms/image/transf/dimension=699x10000:format=png/path/s8ff3310143614e07/image/iab2d53abc26a2bc7/version/1549760945/image.png) <br> Some indicators of binary classifiers include the following: - The most straightforward indicator of classification accuracy is the ratio of the number of correct predictions to the total number of predictions (or observations). - The positive predictive value is the ratio of the number of true positives to the sum of the numbers of true and false positives. - The negative predictive value is the ratio of the number of true negatives to the sum of the numbers of true and false negatives. - The sensitivity (also known as recall or true positive rate) is the ratio of the number of true positives to the number of actual positives. - The precision score quantifies the ability of a classifier to not label a negative example as positive. The precision score can be interpreted as the probability that a positive prediction made by the classifier is positive. - The specificity (or true negative rate) is the ratio of the number of true negatives to the number of actual negatives. The extent of importance of recall and precision depends on the problem. Achieving a high recall is more important than getting a high precision in cases like when we would like to detect as many heart patients as possible. For some other models, like classifying whether a bank customer is a loan defaulter or not, it is desirable to have a high precision since the bank wouldn\u2019t want to lose customers who were denied a loan based on the model\u2019s prediction that they would be defaulters. There are also a lot of situations where both precision and recall are equally important. Then we would aim for not only a high recall but a high precision as well. In such cases, we use something called F1-score. F1-score is the Harmonic mean of the Precision and Recall: This is easier to work with since now, instead of balancing precision and recall, we can just aim for a good F1-score and that would be indicative of a good Precision and a good Recall value as well. Model Evaluation using Confusion Matrix: A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise. # import the metrics class from sklearn import metrics cnf_matrix = metrics.confusion_matrix(y_pred, y_test) cnf_matrix array([[119, 26], [ 11, 36]], dtype=int64) Here, you can see the confusion matrix in the form of the array object. The dimension of this matrix is 2*2 because this model is binary classification. You have two classes 0 and 1. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. In the output, 119 and 36 are actual predictions, and 26 and 11 are incorrect predictions. Visualizing Confusion Matrix using Heatmap: Let's visualize the results of the model in the form of a confusion matrix using matplotlib and seaborn. class_names=[0,1] # name of classes fig, ax = plt.subplots() tick_marks = np.arange(len(class_names)) plt.xticks(tick_marks, class_names) plt.yticks(tick_marks, class_names) # create heatmap sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g') ax.xaxis.set_label_position(\"top\") plt.tight_layout() plt.title('Confusion matrix', y=1.1) plt.ylabel('Predicted label') plt.xlabel('Actual label') Text(0.5, 257.44, 'Actual label') Confusion Matrix Evaluation Metrics: Let's evaluate the model using model evaluation metrics such as accuracy, precision, and recall. print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) print(\"Precision:\",metrics.precision_score(y_test, y_pred)) print(\"Recall:\",metrics.recall_score(y_test, y_pred)) print(\"F1-score:\",metrics.f1_score(y_test, y_pred)) Accuracy: 0.8072916666666666 Precision: 0.7659574468085106 Recall: 0.5806451612903226 F1-score: 0.6605504587155964 from sklearn.metrics import classification_report print(classification_report(y_test, y_pred)) precision recall f1-score support 0 0.82 0.92 0.87 130 1 0.77 0.58 0.66 62 accuracy 0.81 192 macro avg 0.79 0.75 0.76 192 weighted avg 0.80 0.81 0.80 192 Example: Credit Card Fraud Detection For many companies, losses involving transaction fraud amount to more than 10% of their total expenses. The concern with these massive losses leads companies to constantly seek new solutions to prevent, detect and eliminate fraud. Machine Learning is one of the most promising technological weapons to combat financial fraud. The objective of this project is to create a simple Logistic Regression model capable of detecting fraud in credit card operations, thus seeking to minimize the risk and loss of the business. The dataset used contains transactions carried out by European credit card holders that took place over two days in September 2013, and is a shorter version of a dataset that is available on kaggle at https://www.kaggle.com/mlg-ulb/creditcardfraud/version/3. \"It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\" Columns Info. Time Number of seconds elapsed between this transaction and the first transaction in the dataset V1-V28 Result of a PCA Dimensionality reduction to protect user identities and sensitive features(v1-v28) Amount Transaction amount Class 1 for fraudulent transactions, 0 otherwise NOTE: Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. Acknowledgements The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the DefeatFraud project Please cite the following works: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015 Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi) Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-A\u00ebl; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier Carcillo, Fabrizio; Le Borgne, Yann-A\u00ebl; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing Bertrand Lebichot, Yann-A\u00ebl Le Borgne, Liyun He, Frederic Obl\u00e9, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019 Fabrizio Carcillo, Yann-A\u00ebl Le Borgne, Olivier Caelen, Frederic Obl\u00e9, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019 As you know by now, the first step is to load some necessary libraries: import numpy as np import pandas as pd from matplotlib import pyplot as plt import sklearn.metrics as metrics import seaborn as sns %matplotlib inline Then, we should read the dataset and explore it using tools such as descriptive statistics: # Import the dataset: data = pd.read_csv(\"creditcard_m.csv\") data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class 0 0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 ... -0.018307 0.277838 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 149.62 0 1 0 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 0.085102 -0.255425 ... -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 2.69 0 2 1 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 ... 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 378.66 0 3 1 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 0.377436 -1.387024 ... -0.108300 0.005274 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 123.50 0 4 2 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 -0.270533 0.817739 ... -0.009431 0.798278 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 69.99 0 5 rows \u00d7 31 columns As expected, the dataset has 31 columns and the target variable is located in the last one. Let's check and see whether we have any missing values in the dataset: data.isnull().sum() Time 0 V1 0 V2 0 V3 0 V4 0 V5 0 V6 0 V7 0 V8 0 V9 0 V10 0 V11 0 V12 0 V13 0 V14 0 V15 0 V16 0 V17 0 V18 0 V19 0 V20 0 V21 0 V22 0 V23 0 V24 0 V25 0 V26 0 V27 0 V28 0 Amount 0 Class 0 dtype: int64 Great! No missing values! data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class count 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 ... 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 mean 51858.089636 -0.249409 0.017429 0.672713 0.139812 -0.282655 0.078898 -0.117062 0.065205 -0.092188 ... -0.039503 -0.118547 -0.033419 0.012095 0.130218 0.023580 0.000651 0.002244 91.210270 0.001886 std 20867.521978 1.816521 1.614340 1.268657 1.322410 1.307926 1.284004 1.166853 1.230046 1.088755 ... 0.721638 0.635371 0.591946 0.595760 0.437298 0.492026 0.389003 0.307370 247.334466 0.043384 min 0.000000 -56.407510 -72.715728 -33.680984 -5.519697 -42.147898 -26.160506 -31.764946 -73.216718 -9.283925 ... -34.830382 -10.933144 -44.807735 -2.836627 -10.295397 -2.534330 -22.565679 -11.710896 0.000000 0.000000 25% 37912.750000 -1.020760 -0.564561 0.170073 -0.714009 -0.903653 -0.662022 -0.603820 -0.131071 -0.714753 ... -0.226206 -0.548060 -0.171763 -0.324841 -0.136182 -0.326158 -0.060305 -0.004172 6.000000 0.000000 50% 53665.500000 -0.269833 0.104206 0.750038 0.167473 -0.314849 -0.176600 -0.064160 0.080302 -0.154499 ... -0.059815 -0.095518 -0.044999 0.068815 0.166593 -0.064948 0.011781 0.023609 23.920000 0.000000 75% 69322.000000 1.157985 0.776185 1.363041 0.993562 0.237514 0.465404 0.409714 0.374985 0.482352 ... 0.113587 0.301082 0.083271 0.408740 0.418787 0.287195 0.087053 0.077127 81.000000 0.000000 max 83479.000000 1.960497 18.902453 9.382558 16.715537 34.801666 22.529298 36.677268 20.007208 15.594995 ... 27.202839 10.503090 19.002942 4.022866 5.541598 3.517346 12.152401 33.847808 19656.530000 1.000000 8 rows \u00d7 31 columns print ('Not Fraud % ',round(data['Class'].value_counts()[0]/len(data)*100,2)) print () print (round(data.Amount[data.Class == 0].describe(),2)) print () print () print ('Fraud % ',round(data['Class'].value_counts()[1]/len(data)*100,2)) print () print (round(data.Amount[data.Class == 1].describe(),2)) Not Fraud % 99.81 count 139736.00 mean 91.16 std 247.34 min 0.00 25% 6.02 50% 23.94 75% 80.92 max 19656.53 Name: Amount, dtype: float64 Fraud % 0.19 count 264.00 mean 115.39 std 245.19 min 0.00 25% 1.00 50% 9.56 75% 99.99 max 1809.68 Name: Amount, dtype: float64 We have a total of 140000 samples in this dataset. The PCA components (V1-V28) look as if they have similar spreads and rather small mean values in comparison to another predictors such as 'Time'. The majority (75%) of transactions are below 81 euros with some considerably high outliers (the max is 19656.53 euros). Around 0.19% of all the observed transactions were found to be fraudulent which means that we are dealing with an extremely unbalanced dataset. An important characteristic of such problems. Although the share may seem small, each fraud transaction can represent a very significant expense, which together can represent billions of dollars of lost revenue each year. The next step is to defind our predictors and target: #split dataset in features and target variable y = data.Class # Target variable X = data.loc[:, data.columns != \"Class\"] # Features The next step would be to split our dataset and define the training and testing sets. The random seed (np.random.seed) is used to ensure that the same data is used for all runs. Let's do a 70/30 split: # split X and y into training and testing sets np.random.seed(123) from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=1) Now it is time for model development and prediction! import the Logistic Regression module and create a Logistic Regression classifier object using LogisticRegression() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict(). # import the class from sklearn.linear_model import LogisticRegression # instantiate the model (using the default parameters) #logreg = LogisticRegression() logreg = LogisticRegression(solver='lbfgs',max_iter=10000) # fit the model with data -TRAIN the model logreg.fit(X_train,y_train) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=10000, multi_class='warn', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) # TEST the model y_pred=logreg.predict(X_test) Once the model and the predictions are ready, we can assess the performance of our classifier. First, we need to get our confusion matrix: A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise. from sklearn import metrics cnf_matrix = metrics.confusion_matrix(y_pred, y_test) print(cnf_matrix) tpos = cnf_matrix[0][0] fneg = cnf_matrix[1][1] fpos = cnf_matrix[0][1] tneg = cnf_matrix[1][0] print(\"True Positive Cases are\",tpos) #How many non-fraud cases were identified as non-fraud cases - GOOD print(\"True Negative Cases are\",tneg) #How many Fraud cases were identified as Fraud cases - GOOD print(\"False Positive Cases are\",fpos) #How many Fraud cases were identified as non-fraud cases - BAD | (type 1 error) print(\"False Negative Cases are\",fneg) #How many non-fraud cases were identified as Fraud cases - BAD | (type 2 error) [[34913 33] [ 16 38]] True Positive Cases are 34913 True Negative Cases are 16 False Positive Cases are 33 False Negative Cases are 38 class_names=[0,1] # name of classes fig, ax = plt.subplots() tick_marks = np.arange(len(class_names)) plt.xticks(tick_marks, class_names) plt.yticks(tick_marks, class_names) # create heatmap sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g') ax.xaxis.set_label_position(\"top\") plt.tight_layout() plt.title('Confusion matrix', y=1.1) plt.ylabel('Predicted label') plt.xlabel('Actual label') Text(0.5, 257.44, 'Actual label') We should go further and evaluate the model using model evaluation metrics such as accuracy, precision, and recall. These are calculated based on the confustion matrix: print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) Accuracy: 0.9986 That is a fantastic accuracy score, isn't it? print(\"Precision:\",metrics.precision_score(y_test, y_pred)) print(\"Recall:\",metrics.recall_score(y_test, y_pred)) print(\"F1-score:\",metrics.f1_score(y_test, y_pred)) Precision: 0.7037037037037037 Recall: 0.5352112676056338 F1-score: 0.608 from sklearn.metrics import classification_report print(classification_report(y_test, y_pred)) precision recall f1-score support 0 1.00 1.00 1.00 34929 1 0.70 0.54 0.61 71 accuracy 1.00 35000 macro avg 0.85 0.77 0.80 35000 weighted avg 1.00 1.00 1.00 35000 Although the accuracy is excellent, the model struggles with fraud detection and has not captured about 30 out of 71 fraudulent transactions. Accuracy in a highly unbalanced data set does not represent a correct value for the efficiency of a model. That's where precision, recall and more specifically F1-score as their combinations becomes important: Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case. In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on. This notebook was inspired by several blogposts including: \"Logistic Regression in Python\" by Mirko Stojiljkovi\u0107 available at* https://realpython.com/logistic-regression-python/ \"Understanding Logistic Regression in Python\" by Avinash Navlani available at* https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python \"Understanding Logistic Regression with Python: Practical Guide 1\" by Mayank Tripathi available at* https://datascience.foundation/sciencewhitepaper/understanding-logistic-regression-with-python-practical-guide-1 \"Understanding Data Science Classification Metrics in Scikit-Learn in Python\" by Andrew Long available at* https://towardsdatascience.com/understanding-data-science-classification-metrics-in-scikit-learn-in-python-3bc336865019 Here are some great reads on these topics: - \"Example of Logistic Regression in Python\" available at https://datatofish.com/logistic-regression-python/ - \"Building A Logistic Regression in Python, Step by Step\" by Susan Li available at https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8 - \"How To Perform Logistic Regression In Python?\" by Mohammad Waseem available at https://www.edureka.co/blog/logistic-regression-in-python/ - \"Logistic Regression in Python Using Scikit-learn\" by Dhiraj K available at https://heartbeat.fritz.ai/logistic-regression-in-python-using-scikit-learn-d34e882eebb1 - \"ML | Logistic Regression using Python\" available at* https://www.geeksforgeeks.org/ml-logistic-regression-using-python/ Here are some great videos on these topics: - \"StatQuest: Logistic Regression\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=yIYKR4sgzI8&list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe - \"Linear Regression vs Logistic Regression | Data Science Training | Edureka\" by edureka! available at https://www.youtube.com/watch?v=OCwZyYH14uw - \"Logistic Regression in Python | Logistic Regression Example | Machine Learning Algorithms | Edureka\" by edureka! available at https://www.youtube.com/watch?v=VCJdg7YBbAQ - \"How to evaluate a classifier in scikit-learn\" by Data School available at https://www.youtube.com/watch?v=85dtiMz9tSo - \"How to evaluate a classifier in scikit-learn\" by Data School available at* https://www.youtube.com/watch?v=85dtiMz9tSo Exercise: Logistic Regression in Engineering Think of a few applications of Logistic Regression in Engineering? Make sure to cite any resources that you may use.","title":"Lab22"},{"location":"8-Labs/Newly Formatted/Lab22/#laboratory-22-on-the-virtue-and-value-of-classification-or-who-ordered-a-classy-fire","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 22: \"On The Virtue and Value of Classification\" or \"Who Ordered a Classy Fire?\" "},{"location":"8-Labs/Newly Formatted/Lab22/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab22/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab22/#title-of-the-notebook","text":"","title":"Title of the notebook"},{"location":"8-Labs/Newly Formatted/Lab22/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab22/#for-the-last-few-sessions-we-have-talked-about-simple-linear-regression","text":"","title":"For the last few sessions we have talked about simple linear regression ... "},{"location":"8-Labs/Newly Formatted/Lab22/#we-discussed","text":"The theory and implementation of simple linear regression in Python OLS and MLE methods for estimation of slope and intercept coefficients Errors (Noise, Variance, Bias) and their impacts on model's performance Confidence and prediction intervals And Multiple Linear Regressions What if we want to predict a discrete variable? The general idea behind our efforts was to use a set of observed events (samples) to capture the relationship between one or more predictor (AKA input, indipendent) variables and an output (AKA response, dependent) variable. The nature of the dependent variables differentiates regression and classification problems. Regression problems have continuous and usually unbounded outputs. An example is when you\u2019re estimating the salary as a function of experience and education level. Or all the examples we have covered so far! On the other hand, classification problems have discrete and finite outputs called classes or categories. For example, predicting if an employee is going to be promoted or not (true or false) is a classification problem. There are two main types of classification problems: Binary or binomial classification: exactly two classes to choose between (usually 0 and 1, true and false, or positive and negative) Multiclass or multinomial classification: three or more classes of the outputs to choose from When Do We Need Classification? We can apply classification in many fields of science and technology. For example, text classification algorithms are used to separate legitimate and spam emails, as well as positive and negative comments. Other examples involve medical applications, biological classification, credit scoring, and more.","title":"We discussed ..."},{"location":"8-Labs/Newly Formatted/Lab22/#logistic-regression","text":"What is logistic regression? Logistic regression is a fundamental classification technique. It belongs to the group of linear classifiers and is somewhat similar to polynomial and linear regression. Logistic regression is fast and relatively uncomplicated, and it\u2019s convenient for users to interpret the results. Although it\u2019s essentially a method for binary classification, it can also be applied to multiclass problems. Logistic regression is a statistical method for predicting binary classes. The outcome or target variable is dichotomous in nature. Dichotomous means there are only two possible classes. For example, it can be used for cancer detection problems. It computes the probability of an event occurrence. Logistic regression can be considered a special case of linear regression where the target variable is categorical in nature. It uses a log of odds as the dependent variable. Logistic Regression predicts the probability of occurrence of a binary event utilizing a logit function. HOW? Remember the general format of the multiple linear regression model: Where, y is dependent variable and x1, x2 ... and Xn are explanatory variables. This was, as you know by now, a linear function. There is another famous function known as the Sigmoid Function , also called logistic function . Here is the equation for the Sigmoid function: This image shows the sigmoid function (or S-shaped curve) of some variable \ud835\udc65: As you see, The sigmoid function has values very close to either 0 or 1 across most of its domain. It can take any real-valued number and map it into a value between 0 and 1. If the curve goes to positive infinity, y predicted will become 1, and if the curve goes to negative infinity, y predicted will become 0. This fact makes it suitable for application in classification methods since we are dealing with two discrete classes (labels, categories, ...). If the output of the sigmoid function is more than 0.5, we can classify the outcome as 1 or YES, and if it is less than 0.5, we can classify it as 0 or NO. This cutoff value (threshold) is not always fixed at 0.5. If we apply the Sigmoid function on linear regression: Notice the difference between linear regression and logistic regression: logistic regression is estimated using Maximum Likelihood Estimation (MLE) approach. Maximizing the likelihood function determines the parameters that are most likely to produce the observed data. Let's work on an example in Python!","title":"Logistic Regression"},{"location":"8-Labs/Newly Formatted/Lab22/#example-1-diagnosing-diabetes","text":"","title":"Example 1: Diagnosing Diabetes "},{"location":"8-Labs/Newly Formatted/Lab22/#the-diabetescsv-dataset-is-originally-from-the-national-institute-of-diabetes-and-digestive-and-kidney-diseases-the-objective-of-the-dataset-is-to-diagnostically-predict-whether-or-not-a-patient-has-diabetes-based-on-certain-diagnostic-measurements-included-in-the-dataset","text":"Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.","title":"The \"diabetes.csv\" dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset."},{"location":"8-Labs/Newly Formatted/Lab22/#the-datasets-consists-of-several-medical-predictor-variables-and-one-target-variable-outcome-predictor-variables-includes-the-number-of-pregnancies-the-patient-has-had-their-bmi-insulin-level-age-and-so-on","text":"Columns Info. Pregnancies Number of times pregnant Glucose Plasma glucose concentration a 2 hours in an oral glucose tolerance test BloodPressure Diastolic blood pressure (mm Hg) SkinThickness Triceps skin fold thickness (mm) Insulin 2-Hour serum insulin (mu U/ml) BMI Body mass index (weight in kg/(height in m)^2) Diabetes pedigree Diabetes pedigree function Age Age (years) Outcome Class variable (0 or 1) 268 of 768 are 1, the others are 0","title":"The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on."},{"location":"8-Labs/Newly Formatted/Lab22/#lets-see-if-we-can-build-a-logistic-regression-model-to-accurately-predict-whether-or-not-the-patients-in-the-dataset-have-diabetes-or-not","text":"Acknowledgements: Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press. import numpy as np import pandas as pd from matplotlib import pyplot as plt import sklearn.metrics as metrics import seaborn as sns %matplotlib inline # Import the dataset: data = pd.read_csv(\"diabetes.csv\") data.rename(columns = {'Pregnancies':'pregnant', 'Glucose':'glucose','BloodPressure':'bp','SkinThickness':'skin', 'Insulin ':'Insulin','BMI':'bmi','DiabetesPedigreeFunction':'pedigree','Age':'age', 'Outcome':'label'}, inplace = True) data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pregnant glucose bp skin Insulin bmi pedigree age label 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pregnant glucose bp skin Insulin bmi pedigree age label count 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 mean 3.845052 120.894531 69.105469 20.536458 79.799479 31.992578 0.471876 33.240885 0.348958 std 3.369578 31.972618 19.355807 15.952218 115.244002 7.884160 0.331329 11.760232 0.476951 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.078000 21.000000 0.000000 25% 1.000000 99.000000 62.000000 0.000000 0.000000 27.300000 0.243750 24.000000 0.000000 50% 3.000000 117.000000 72.000000 23.000000 30.500000 32.000000 0.372500 29.000000 0.000000 75% 6.000000 140.250000 80.000000 32.000000 127.250000 36.600000 0.626250 41.000000 1.000000 max 17.000000 199.000000 122.000000 99.000000 846.000000 67.100000 2.420000 81.000000 1.000000 #Check some histograms sns.distplot(data['pregnant'], kde = True, rug= True, color ='orange') <matplotlib.axes._subplots.AxesSubplot at 0x1dcca695b48> sns.distplot(data['glucose'], kde = True, rug= True, color ='darkblue') <matplotlib.axes._subplots.AxesSubplot at 0x1dccaa4f288> sns.distplot(data['label'], kde = False, rug= True, color ='purple', bins=2) <matplotlib.axes._subplots.AxesSubplot at 0x285dfb3a0c8> sns.jointplot(x ='glucose', y ='label', data = data, kind ='kde') <seaborn.axisgrid.JointGrid at 0x1dccac73f88>","title":"Let's see if we can build a logistic regression model to accurately predict whether or not the patients in the dataset have diabetes or not?"},{"location":"8-Labs/Newly Formatted/Lab22/#selecting-feature-here-we-need-to-divide-the-given-columns-into-two-types-of-variables-dependentor-target-variable-and-independent-variableor-feature-variables-or-predictors","text":"#split dataset in features and target variable feature_cols = ['pregnant', 'glucose', 'bp', 'skin', 'Insulin', 'bmi', 'pedigree', 'age'] X = data[feature_cols] # Features y = data.label # Target variable","title":"Selecting Feature: Here, we need to divide the given columns into two types of variables dependent(or target variable) and independent variable(or feature variables or predictors)."},{"location":"8-Labs/Newly Formatted/Lab22/#splitting-data-to-understand-model-performance-dividing-the-dataset-into-a-training-set-and-a-test-set-is-a-good-strategy-lets-split-dataset-by-using-function-train_test_split-you-need-to-pass-3-parameters-features-target-and-test_set-size-additionally-you-can-use-random_state-to-select-records-randomly-here-the-dataset-is-broken-into-two-parts-in-a-ratio-of-7525-it-means-75-data-will-be-used-for-model-training-and-25-for-model-testing","text":"# split X and y into training and testing sets from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)","title":"Splitting Data: To understand model performance, dividing the dataset into a training set and a test set is a good strategy. Let's split dataset by using function train_test_split(). You need to pass 3 parameters: features, target, and test_set size. Additionally, you can use random_state to select records randomly. Here, the Dataset is broken into two parts in a ratio of 75:25. It means 75% data will be used for model training and 25% for model testing:"},{"location":"8-Labs/Newly Formatted/Lab22/#model-development-and-prediction-first-import-the-logistic-regression-module-and-create-a-logistic-regression-classifier-object-using-logisticregression-function-then-fit-your-model-on-the-train-set-using-fit-and-perform-prediction-on-the-test-set-using-predict","text":"# import the class from sklearn.linear_model import LogisticRegression # instantiate the model (using the default parameters) #logreg = LogisticRegression() logreg = LogisticRegression() # fit the model with data logreg.fit(X_train,y_train) # y_pred=logreg.predict(X_test) C:\\Users\\Farha\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning. FutureWarning) - How to assess the performance of logistic regression? Binary classification has four possible types of results: - True negatives: correctly predicted negatives (zeros) - True positives: correctly predicted positives (ones) - False negatives: incorrectly predicted negatives (zeros) - False positives: incorrectly predicted positives (ones) We usually evaluate the performance of a classifier by comparing the actual and predicted outputsand counting the correct and incorrect predictions. A confusion matrix is a table that is used to evaluate the performance of a classification model. <br> ![](https://image.jimcdn.com/app/cms/image/transf/dimension=699x10000:format=png/path/s8ff3310143614e07/image/iab2d53abc26a2bc7/version/1549760945/image.png) <br> Some indicators of binary classifiers include the following: - The most straightforward indicator of classification accuracy is the ratio of the number of correct predictions to the total number of predictions (or observations). - The positive predictive value is the ratio of the number of true positives to the sum of the numbers of true and false positives. - The negative predictive value is the ratio of the number of true negatives to the sum of the numbers of true and false negatives. - The sensitivity (also known as recall or true positive rate) is the ratio of the number of true positives to the number of actual positives. - The precision score quantifies the ability of a classifier to not label a negative example as positive. The precision score can be interpreted as the probability that a positive prediction made by the classifier is positive. - The specificity (or true negative rate) is the ratio of the number of true negatives to the number of actual negatives. The extent of importance of recall and precision depends on the problem. Achieving a high recall is more important than getting a high precision in cases like when we would like to detect as many heart patients as possible. For some other models, like classifying whether a bank customer is a loan defaulter or not, it is desirable to have a high precision since the bank wouldn\u2019t want to lose customers who were denied a loan based on the model\u2019s prediction that they would be defaulters. There are also a lot of situations where both precision and recall are equally important. Then we would aim for not only a high recall but a high precision as well. In such cases, we use something called F1-score. F1-score is the Harmonic mean of the Precision and Recall: This is easier to work with since now, instead of balancing precision and recall, we can just aim for a good F1-score and that would be indicative of a good Precision and a good Recall value as well.","title":"Model Development and Prediction: First, import the Logistic Regression module and create a Logistic Regression classifier object using LogisticRegression() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict()."},{"location":"8-Labs/Newly Formatted/Lab22/#model-evaluation-using-confusion-matrix-a-confusion-matrix-is-a-table-that-is-used-to-evaluate-the-performance-of-a-classification-model-you-can-also-visualize-the-performance-of-an-algorithm-the-fundamental-of-a-confusion-matrix-is-the-number-of-correct-and-incorrect-predictions-are-summed-up-class-wise","text":"# import the metrics class from sklearn import metrics cnf_matrix = metrics.confusion_matrix(y_pred, y_test) cnf_matrix array([[119, 26], [ 11, 36]], dtype=int64)","title":"Model Evaluation using Confusion Matrix: A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise."},{"location":"8-Labs/Newly Formatted/Lab22/#here-you-can-see-the-confusion-matrix-in-the-form-of-the-array-object-the-dimension-of-this-matrix-is-22-because-this-model-is-binary-classification-you-have-two-classes-0-and-1-diagonal-values-represent-accurate-predictions-while-non-diagonal-elements-are-inaccurate-predictions-in-the-output-119-and-36-are-actual-predictions-and-26-and-11-are-incorrect-predictions","text":"","title":"Here, you can see the confusion matrix in the form of the array object. The dimension of this matrix is 2*2 because this model is binary classification. You have two classes 0 and 1. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. In the output, 119 and 36 are actual predictions, and 26 and 11 are incorrect predictions."},{"location":"8-Labs/Newly Formatted/Lab22/#visualizing-confusion-matrix-using-heatmap-lets-visualize-the-results-of-the-model-in-the-form-of-a-confusion-matrix-using-matplotlib-and-seaborn","text":"class_names=[0,1] # name of classes fig, ax = plt.subplots() tick_marks = np.arange(len(class_names)) plt.xticks(tick_marks, class_names) plt.yticks(tick_marks, class_names) # create heatmap sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g') ax.xaxis.set_label_position(\"top\") plt.tight_layout() plt.title('Confusion matrix', y=1.1) plt.ylabel('Predicted label') plt.xlabel('Actual label') Text(0.5, 257.44, 'Actual label')","title":"Visualizing Confusion Matrix using Heatmap: Let's visualize the results of the model in the form of a confusion matrix using matplotlib and seaborn."},{"location":"8-Labs/Newly Formatted/Lab22/#confusion-matrix-evaluation-metrics-lets-evaluate-the-model-using-model-evaluation-metrics-such-as-accuracy-precision-and-recall","text":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) print(\"Precision:\",metrics.precision_score(y_test, y_pred)) print(\"Recall:\",metrics.recall_score(y_test, y_pred)) print(\"F1-score:\",metrics.f1_score(y_test, y_pred)) Accuracy: 0.8072916666666666 Precision: 0.7659574468085106 Recall: 0.5806451612903226 F1-score: 0.6605504587155964 from sklearn.metrics import classification_report print(classification_report(y_test, y_pred)) precision recall f1-score support 0 0.82 0.92 0.87 130 1 0.77 0.58 0.66 62 accuracy 0.81 192 macro avg 0.79 0.75 0.76 192 weighted avg 0.80 0.81 0.80 192","title":"Confusion Matrix Evaluation Metrics: Let's evaluate the model using model evaluation metrics such as accuracy, precision, and recall."},{"location":"8-Labs/Newly Formatted/Lab22/#example-credit-card-fraud-detection","text":"","title":"Example: Credit Card Fraud Detection "},{"location":"8-Labs/Newly Formatted/Lab22/#for-many-companies-losses-involving-transaction-fraud-amount-to-more-than-10-of-their-total-expenses-the-concern-with-these-massive-losses-leads-companies-to-constantly-seek-new-solutions-to-prevent-detect-and-eliminate-fraud-machine-learning-is-one-of-the-most-promising-technological-weapons-to-combat-financial-fraud-the-objective-of-this-project-is-to-create-a-simple-logistic-regression-model-capable-of-detecting-fraud-in-credit-card-operations-thus-seeking-to-minimize-the-risk-and-loss-of-the-business","text":"","title":"For many companies, losses involving transaction fraud amount to more than 10% of their total expenses. The concern with these massive losses leads companies to constantly seek new solutions to prevent, detect and eliminate fraud. Machine Learning is one of the most promising technological weapons to combat financial fraud. The objective of this project is to create a simple Logistic Regression model capable of detecting fraud in credit card operations, thus seeking to minimize the risk and loss of the business."},{"location":"8-Labs/Newly Formatted/Lab22/#the-dataset-used-contains-transactions-carried-out-by-european-credit-card-holders-that-took-place-over-two-days-in-september-2013-and-is-a-shorter-version-of-a-dataset-that-is-available-on-kaggle-at-httpswwwkagglecommlg-ulbcreditcardfraudversion3","text":"","title":"The dataset used contains transactions carried out by European credit card holders that took place over two days in September 2013, and is a shorter version of a dataset that is available on kaggle at https://www.kaggle.com/mlg-ulb/creditcardfraud/version/3."},{"location":"8-Labs/Newly Formatted/Lab22/#it-contains-only-numerical-input-variables-which-are-the-result-of-a-pca-transformation-unfortunately-due-to-confidentiality-issues-we-cannot-provide-the-original-features-and-more-background-information-about-the-data-features-v1-v2-v28-are-the-principal-components-obtained-with-pca-the-only-features-which-have-not-been-transformed-with-pca-are-time-and-amount-feature-time-contains-the-seconds-elapsed-between-each-transaction-and-the-first-transaction-in-the-dataset-the-feature-amount-is-the-transaction-amount-this-feature-can-be-used-for-example-dependant-cost-senstive-learning-feature-class-is-the-response-variable-and-it-takes-value-1-in-case-of-fraud-and-0-otherwise","text":"Columns Info. Time Number of seconds elapsed between this transaction and the first transaction in the dataset V1-V28 Result of a PCA Dimensionality reduction to protect user identities and sensitive features(v1-v28) Amount Transaction amount Class 1 for fraudulent transactions, 0 otherwise NOTE: Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. Acknowledgements The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the DefeatFraud project Please cite the following works: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015 Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi) Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-A\u00ebl; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier Carcillo, Fabrizio; Le Borgne, Yann-A\u00ebl; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing Bertrand Lebichot, Yann-A\u00ebl Le Borgne, Liyun He, Frederic Obl\u00e9, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019 Fabrizio Carcillo, Yann-A\u00ebl Le Borgne, Olivier Caelen, Frederic Obl\u00e9, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019","title":"\"It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\""},{"location":"8-Labs/Newly Formatted/Lab22/#as-you-know-by-now-the-first-step-is-to-load-some-necessary-libraries","text":"import numpy as np import pandas as pd from matplotlib import pyplot as plt import sklearn.metrics as metrics import seaborn as sns %matplotlib inline","title":"As you know by now, the first step is to load some necessary libraries:"},{"location":"8-Labs/Newly Formatted/Lab22/#then-we-should-read-the-dataset-and-explore-it-using-tools-such-as-descriptive-statistics","text":"# Import the dataset: data = pd.read_csv(\"creditcard_m.csv\") data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class 0 0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 0.098698 0.363787 ... -0.018307 0.277838 -0.110474 0.066928 0.128539 -0.189115 0.133558 -0.021053 149.62 0 1 0 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 0.085102 -0.255425 ... -0.225775 -0.638672 0.101288 -0.339846 0.167170 0.125895 -0.008983 0.014724 2.69 0 2 1 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 0.247676 -1.514654 ... 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752 378.66 0 3 1 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 0.377436 -1.387024 ... -0.108300 0.005274 -0.190321 -1.175575 0.647376 -0.221929 0.062723 0.061458 123.50 0 4 2 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 -0.270533 0.817739 ... -0.009431 0.798278 -0.137458 0.141267 -0.206010 0.502292 0.219422 0.215153 69.99 0 5 rows \u00d7 31 columns","title":"Then, we should read the dataset and explore it using tools such as descriptive statistics:"},{"location":"8-Labs/Newly Formatted/Lab22/#as-expected-the-dataset-has-31-columns-and-the-target-variable-is-located-in-the-last-one-lets-check-and-see-whether-we-have-any-missing-values-in-the-dataset","text":"data.isnull().sum() Time 0 V1 0 V2 0 V3 0 V4 0 V5 0 V6 0 V7 0 V8 0 V9 0 V10 0 V11 0 V12 0 V13 0 V14 0 V15 0 V16 0 V17 0 V18 0 V19 0 V20 0 V21 0 V22 0 V23 0 V24 0 V25 0 V26 0 V27 0 V28 0 Amount 0 Class 0 dtype: int64","title":"As expected, the dataset has 31 columns and the target variable is located in the last one. Let's check and see whether we have any missing values in the dataset:"},{"location":"8-Labs/Newly Formatted/Lab22/#great-no-missing-values","text":"data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class count 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 ... 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 140000.000000 mean 51858.089636 -0.249409 0.017429 0.672713 0.139812 -0.282655 0.078898 -0.117062 0.065205 -0.092188 ... -0.039503 -0.118547 -0.033419 0.012095 0.130218 0.023580 0.000651 0.002244 91.210270 0.001886 std 20867.521978 1.816521 1.614340 1.268657 1.322410 1.307926 1.284004 1.166853 1.230046 1.088755 ... 0.721638 0.635371 0.591946 0.595760 0.437298 0.492026 0.389003 0.307370 247.334466 0.043384 min 0.000000 -56.407510 -72.715728 -33.680984 -5.519697 -42.147898 -26.160506 -31.764946 -73.216718 -9.283925 ... -34.830382 -10.933144 -44.807735 -2.836627 -10.295397 -2.534330 -22.565679 -11.710896 0.000000 0.000000 25% 37912.750000 -1.020760 -0.564561 0.170073 -0.714009 -0.903653 -0.662022 -0.603820 -0.131071 -0.714753 ... -0.226206 -0.548060 -0.171763 -0.324841 -0.136182 -0.326158 -0.060305 -0.004172 6.000000 0.000000 50% 53665.500000 -0.269833 0.104206 0.750038 0.167473 -0.314849 -0.176600 -0.064160 0.080302 -0.154499 ... -0.059815 -0.095518 -0.044999 0.068815 0.166593 -0.064948 0.011781 0.023609 23.920000 0.000000 75% 69322.000000 1.157985 0.776185 1.363041 0.993562 0.237514 0.465404 0.409714 0.374985 0.482352 ... 0.113587 0.301082 0.083271 0.408740 0.418787 0.287195 0.087053 0.077127 81.000000 0.000000 max 83479.000000 1.960497 18.902453 9.382558 16.715537 34.801666 22.529298 36.677268 20.007208 15.594995 ... 27.202839 10.503090 19.002942 4.022866 5.541598 3.517346 12.152401 33.847808 19656.530000 1.000000 8 rows \u00d7 31 columns print ('Not Fraud % ',round(data['Class'].value_counts()[0]/len(data)*100,2)) print () print (round(data.Amount[data.Class == 0].describe(),2)) print () print () print ('Fraud % ',round(data['Class'].value_counts()[1]/len(data)*100,2)) print () print (round(data.Amount[data.Class == 1].describe(),2)) Not Fraud % 99.81 count 139736.00 mean 91.16 std 247.34 min 0.00 25% 6.02 50% 23.94 75% 80.92 max 19656.53 Name: Amount, dtype: float64 Fraud % 0.19 count 264.00 mean 115.39 std 245.19 min 0.00 25% 1.00 50% 9.56 75% 99.99 max 1809.68 Name: Amount, dtype: float64","title":"Great! No missing values!"},{"location":"8-Labs/Newly Formatted/Lab22/#we-have-a-total-of-140000-samples-in-this-dataset-the-pca-components-v1-v28-look-as-if-they-have-similar-spreads-and-rather-small-mean-values-in-comparison-to-another-predictors-such-as-time-the-majority-75-of-transactions-are-below-81-euros-with-some-considerably-high-outliers-the-max-is-1965653-euros-around-019-of-all-the-observed-transactions-were-found-to-be-fraudulent-which-means-that-we-are-dealing-with-an-extremely-unbalanced-dataset-an-important-characteristic-of-such-problems-although-the-share-may-seem-small-each-fraud-transaction-can-represent-a-very-significant-expense-which-together-can-represent-billions-of-dollars-of-lost-revenue-each-year","text":"","title":"We have a total of 140000 samples in this dataset. The PCA components (V1-V28) look as if they have similar spreads and rather small mean values in comparison to another predictors such as 'Time'. The majority (75%) of transactions are below 81 euros with some considerably high outliers (the max is 19656.53 euros). Around 0.19% of all the observed transactions were found to be fraudulent which means that we are dealing with an extremely unbalanced dataset. An important characteristic of such problems. Although the share may seem small, each fraud transaction can represent a very significant expense, which together can represent billions of dollars of lost revenue each year."},{"location":"8-Labs/Newly Formatted/Lab22/#the-next-step-is-to-defind-our-predictors-and-target","text":"#split dataset in features and target variable y = data.Class # Target variable X = data.loc[:, data.columns != \"Class\"] # Features","title":"The next step is to defind our predictors and target:"},{"location":"8-Labs/Newly Formatted/Lab22/#the-next-step-would-be-to-split-our-dataset-and-define-the-training-and-testing-sets-the-random-seed-nprandomseed-is-used-to-ensure-that-the-same-data-is-used-for-all-runs-lets-do-a-7030-split","text":"# split X and y into training and testing sets np.random.seed(123) from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=1)","title":"The next step would be to split our dataset and define the training and testing sets. The random seed (np.random.seed) is used to ensure that the same data is used for all runs. Let's do a 70/30 split:"},{"location":"8-Labs/Newly Formatted/Lab22/#now-it-is-time-for-model-development-and-prediction","text":"","title":"Now it is time for model development and prediction!"},{"location":"8-Labs/Newly Formatted/Lab22/#import-the-logistic-regression-module-and-create-a-logistic-regression-classifier-object-using-logisticregression-function-then-fit-your-model-on-the-train-set-using-fit-and-perform-prediction-on-the-test-set-using-predict","text":"# import the class from sklearn.linear_model import LogisticRegression # instantiate the model (using the default parameters) #logreg = LogisticRegression() logreg = LogisticRegression(solver='lbfgs',max_iter=10000) # fit the model with data -TRAIN the model logreg.fit(X_train,y_train) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=10000, multi_class='warn', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) # TEST the model y_pred=logreg.predict(X_test)","title":"import the Logistic Regression module and create a Logistic Regression classifier object using LogisticRegression() function. Then, fit your model on the train set using fit() and perform prediction on the test set using predict()."},{"location":"8-Labs/Newly Formatted/Lab22/#once-the-model-and-the-predictions-are-ready-we-can-assess-the-performance-of-our-classifier-first-we-need-to-get-our-confusion-matrix","text":"A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a confusion matrix is the number of correct and incorrect predictions are summed up class-wise. from sklearn import metrics cnf_matrix = metrics.confusion_matrix(y_pred, y_test) print(cnf_matrix) tpos = cnf_matrix[0][0] fneg = cnf_matrix[1][1] fpos = cnf_matrix[0][1] tneg = cnf_matrix[1][0] print(\"True Positive Cases are\",tpos) #How many non-fraud cases were identified as non-fraud cases - GOOD print(\"True Negative Cases are\",tneg) #How many Fraud cases were identified as Fraud cases - GOOD print(\"False Positive Cases are\",fpos) #How many Fraud cases were identified as non-fraud cases - BAD | (type 1 error) print(\"False Negative Cases are\",fneg) #How many non-fraud cases were identified as Fraud cases - BAD | (type 2 error) [[34913 33] [ 16 38]] True Positive Cases are 34913 True Negative Cases are 16 False Positive Cases are 33 False Negative Cases are 38 class_names=[0,1] # name of classes fig, ax = plt.subplots() tick_marks = np.arange(len(class_names)) plt.xticks(tick_marks, class_names) plt.yticks(tick_marks, class_names) # create heatmap sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g') ax.xaxis.set_label_position(\"top\") plt.tight_layout() plt.title('Confusion matrix', y=1.1) plt.ylabel('Predicted label') plt.xlabel('Actual label') Text(0.5, 257.44, 'Actual label')","title":"Once the model and the predictions are ready, we can assess the performance of our classifier. First, we need to get our confusion matrix:"},{"location":"8-Labs/Newly Formatted/Lab22/#we-should-go-further-and-evaluate-the-model-using-model-evaluation-metrics-such-as-accuracy-precision-and-recall-these-are-calculated-based-on-the-confustion-matrix","text":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) Accuracy: 0.9986","title":"We should go further and evaluate the model using model evaluation metrics such as accuracy, precision, and recall. These are calculated based on the confustion matrix:"},{"location":"8-Labs/Newly Formatted/Lab22/#that-is-a-fantastic-accuracy-score-isnt-it","text":"print(\"Precision:\",metrics.precision_score(y_test, y_pred)) print(\"Recall:\",metrics.recall_score(y_test, y_pred)) print(\"F1-score:\",metrics.f1_score(y_test, y_pred)) Precision: 0.7037037037037037 Recall: 0.5352112676056338 F1-score: 0.608 from sklearn.metrics import classification_report print(classification_report(y_test, y_pred)) precision recall f1-score support 0 1.00 1.00 1.00 34929 1 0.70 0.54 0.61 71 accuracy 1.00 35000 macro avg 0.85 0.77 0.80 35000 weighted avg 1.00 1.00 1.00 35000","title":"That is a fantastic accuracy score, isn't it?"},{"location":"8-Labs/Newly Formatted/Lab22/#although-the-accuracy-is-excellent-the-model-struggles-with-fraud-detection-and-has-not-captured-about-30-out-of-71-fraudulent-transactions","text":"","title":"Although the accuracy is excellent, the model struggles with fraud detection and has not captured about 30 out of 71 fraudulent transactions."},{"location":"8-Labs/Newly Formatted/Lab22/#accuracy-in-a-highly-unbalanced-data-set-does-not-represent-a-correct-value-for-the-efficiency-of-a-model-thats-where-precision-recall-and-more-specifically-f1-score-as-their-combinations-becomes-important","text":"Accuracy is used when the True Positives and True negatives are more important while F1-score is used when the False Negatives and False Positives are crucial Accuracy can be used when the class distribution is similar while F1-score is a better metric when there are imbalanced classes as in the above case. In most real-life classification problems, imbalanced class distribution exists and thus F1-score is a better metric to evaluate our model on. This notebook was inspired by several blogposts including: \"Logistic Regression in Python\" by Mirko Stojiljkovi\u0107 available at* https://realpython.com/logistic-regression-python/ \"Understanding Logistic Regression in Python\" by Avinash Navlani available at* https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python \"Understanding Logistic Regression with Python: Practical Guide 1\" by Mayank Tripathi available at* https://datascience.foundation/sciencewhitepaper/understanding-logistic-regression-with-python-practical-guide-1 \"Understanding Data Science Classification Metrics in Scikit-Learn in Python\" by Andrew Long available at* https://towardsdatascience.com/understanding-data-science-classification-metrics-in-scikit-learn-in-python-3bc336865019 Here are some great reads on these topics: - \"Example of Logistic Regression in Python\" available at https://datatofish.com/logistic-regression-python/ - \"Building A Logistic Regression in Python, Step by Step\" by Susan Li available at https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8 - \"How To Perform Logistic Regression In Python?\" by Mohammad Waseem available at https://www.edureka.co/blog/logistic-regression-in-python/ - \"Logistic Regression in Python Using Scikit-learn\" by Dhiraj K available at https://heartbeat.fritz.ai/logistic-regression-in-python-using-scikit-learn-d34e882eebb1 - \"ML | Logistic Regression using Python\" available at* https://www.geeksforgeeks.org/ml-logistic-regression-using-python/ Here are some great videos on these topics: - \"StatQuest: Logistic Regression\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=yIYKR4sgzI8&list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe - \"Linear Regression vs Logistic Regression | Data Science Training | Edureka\" by edureka! available at https://www.youtube.com/watch?v=OCwZyYH14uw - \"Logistic Regression in Python | Logistic Regression Example | Machine Learning Algorithms | Edureka\" by edureka! available at https://www.youtube.com/watch?v=VCJdg7YBbAQ - \"How to evaluate a classifier in scikit-learn\" by Data School available at https://www.youtube.com/watch?v=85dtiMz9tSo - \"How to evaluate a classifier in scikit-learn\" by Data School available at* https://www.youtube.com/watch?v=85dtiMz9tSo","title":"Accuracy in a highly unbalanced data set does not represent a correct value for the efficiency of a model. That's where precision, recall and more specifically F1-score as their combinations becomes important:"},{"location":"8-Labs/Newly Formatted/Lab22/#exercise-logistic-regression-in-engineering","text":"","title":"Exercise: Logistic Regression in Engineering "},{"location":"8-Labs/Newly Formatted/Lab22/#think-of-a-few-applications-of-logistic-regression-in-engineering","text":"","title":"Think of a few applications of Logistic Regression in Engineering?"},{"location":"8-Labs/Newly Formatted/Lab22/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab23/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: (LINK NEEDS FIXING!) Lab23 Laboratory 23: \"It's a beautiful day in the neighborhood\" or in the forest! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook Date: The K-nearest neighbors (KNN) algorithm is a type of supervised machine learning algorithms. KNN is extremely easy to implement in its most basic form, and yet performs quite complex classification tasks. It is a lazy learning algorithm since it doesn't have a specialized training phase. Rather, it uses all of the data for training while classifying a new data point or instance. KNN is a non-parametric learning algorithm, which means that it doesn't assume anything about the underlying data. This is an extremely useful feature since most of the real world data doesn't really follow any theoretical assumption. Theory The First Law of Geography, according to Waldo Tobler, is \"everything is related to everything else, but near things are more related than distant things.\" The intuition behind the KNN algorithm is one of the simplest of all the supervised machine learning algorithms. It simply calculates the distance of a new data point to all other training data points. The distance can be of any type e.g Euclidean or Manhattan etc. It then selects the K-nearest data points, where K can be any integer. Finally it assigns the data point to the class to which the majority of the K data points belong. distance measures play an important role in machine learning. Perhaps four of the most commonly used distance measures in machine learning are as follows: - Euclidean Distance: Calculates the distance between two real-valued vectors. Although there are other possible choices, most instance-based learners use Euclidean distance. - Manhattan Distance: Also called the Taxicab distance or the City Block distance, calculates the distance between two real-valued vectors. It is perhaps more useful to vectors that describe objects on a uniform grid, like a chessboard or city blocks. The taxicab name for the measure refers to the intuition for what the measure calculates: the shortest path that a taxicab would take between city blocks (coordinates on the grid). - Minkowski Distance: Calculates the distance between two real-valued vectors. It is a generalization of the Euclidean and Manhattan distance measures and adds a parameter, called the \u201corder\u201d or \u201cp\u201c, that allows different distance measures to be calculated. When p is set to 1, the calculation is the same as the Manhattan distance. When p is set to 2, it is the same as the Euclidean distance. Let's see this algorithm in action with the help of a simple example. Suppose you have a dataset with two variables, which when plotted, looks like the one in the following figure. Your task is to classify a new data point with 'X' into \"Blue\" class or \"Red\" class. The coordinate values of the data point are x=45 and y=50. Suppose the value of K is 3. The KNN algorithm starts by calculating the distance of point X from all the points. It then finds the 3 nearest points with least distance to point X. This is shown in the figure below. The three nearest points have been encircled. The final step of the KNN algorithm is to assign new point to the class to which majority of the three nearest points belong. From the figure above we can see that the two of the three nearest points belong to the class \"Red\" while one belongs to the class \"Blue\". Therefore the new data point will be classified as \"Red\". Why KNN? It is extremely easy to implement It is lazy learning algorithm and therefore requires no training prior to making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g SVM, linear regression, etc. Since the algorithm requires no training before making predictions, new data can be added seamlessly. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.) Example: Iris Plants Classification This is perhaps the best known problem and database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers. The Iris Data Set contains information on sepal length, sepal width, petal length, petal width all in cm, and class of iris plants. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. Hence, it is a multiclass classification problem and the number of observations for each class is balanced. Let's use a KNN model in Python and see if we can classifity iris plants based on the four given predictors. Acknowledgements 1. Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). 2. Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. 3. Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. 4. Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. 5. See also: 1988 MLC Proceedings, 54-64. Cheeseman et al's AUTOCLASS II conceptual clustering system finds 3 classes in the data. As you know by now, the first step is to load some necessary libraries: import numpy as np import pandas as pd from matplotlib import pyplot as plt import sklearn.metrics as metrics import seaborn as sns %matplotlib inline Then, we should read the dataset and explore it using tools such as descriptive statistics: # This is something cool that you can do on your local machine (Jupyter): url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" # Assign colum names to the dataset names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class'] # Read dataset to pandas dataframe dataset = pd.read_csv(url, names=names) dataset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal-length sepal-width petal-length petal-width Class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa dataset = pd.read_csv('iris.csv') dataset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal-length sepal-width petal-length petal-width Class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa dataset.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal-length sepal-width petal-length petal-width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.054000 3.758667 1.198667 std 0.828066 0.433594 1.764420 0.763161 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 We should seperate the predictors and target - similar to what we did for logisitc regression: X = dataset.iloc[:, :-1].values y = dataset.iloc[:, 4].values Then, the dataset should be split into training and testing. This way our algorithm is tested on un-seen data, as it would be in a real-world application. Let's go with a 80/20 split: from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) #This means that out of total 150 records: #the training set will contain 120 records & #the test set contains 30 of those records. It is extremely straight forward to train the KNN algorithm and make predictions with it, especially when using Scikit-Learn. The first step is to import the \"KNeighborsClassifier\" class from the \"sklearn.neighbors\" library. In the second line, this class is initialized with one parameter, i.e. \"n_neigbours\". This is basically the value for the K. There is no ideal value for K and it is selected after testing and evaluation, however to start out, 5 seems to be the most commonly used value for KNN algorithm. from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors=5) classifier.fit(X_train, y_train) KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform') The final step is to make predictions on our test data. To do so, execute the following script: y_pred = classifier.predict(X_test) As it's time to evaluate our model, we will go to our rather new friends, confusion matrix, precision, recall and f1 score as the most commonly used discrete GOF metrics. from sklearn.metrics import classification_report, confusion_matrix print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) [[11 0 0] [ 0 9 2] [ 0 0 8]] precision recall f1-score support Iris-setosa 1.00 1.00 1.00 11 Iris-versicolor 1.00 0.82 0.90 11 Iris-virginica 0.80 1.00 0.89 8 accuracy 0.93 30 macro avg 0.93 0.94 0.93 30 weighted avg 0.95 0.93 0.93 30 What if we had used a different value for K? What is the best value for K? One way to help you find the best value of K is to plot the graph of K value and the corresponding error rate for the dataset. In this section, we will plot the mean error for the predicted values of test set for all the K values between 1 and 50. To do so, let's first calculate the mean of error for all the predicted values where K ranges from 1 and 50: error = [] # Calculating error for K values between 1 and 50 # In each iteration the mean error for predicted values of test set is calculated and # the result is appended to the error list. for i in range(1, 50): knn = KNeighborsClassifier(n_neighbors=i) knn.fit(X_train, y_train) pred_i = knn.predict(X_test) error.append(np.mean(pred_i != y_test)) The next step is to plot the error values against K values: plt.figure(figsize=(12, 6)) plt.plot(range(1, 50), error, color='red', linestyle='dashed', marker='o', markerfacecolor='blue', markersize=10) plt.title('Error Rate K Value') plt.xlabel('K Value') plt.ylabel('Mean Error') Text(0, 0.5, 'Mean Error') Final remarks ... KNN is a simple yet powerful classification algorithm. It requires no training for making predictions, which is typically one of the most difficult parts of a machine learning algorithm. The KNN algorithm have been widely used to find document similarity and pattern recognition. Decision Trees and Random Forests Decision trees, AKA Classification And Regression Tree (CART) models, are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification. For example, if you wanted to build a decision tree to classify an animal you come across while on a hike, you might construct the one shown here: The binary splitting makes this extremely efficient: in a well-constructed tree, each question will cut the number of options by approximately half, very quickly narrowing the options even among a large number of classes. The trick, of course, comes in deciding which questions to ask at each step. In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data: that is, each node in the tree splits the data into two groups using a cutoff value within one of the features. Let's now look at an example of this. A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it. This figure presents a visualization of the first four levels of a decision tree classifier for this data: Notice that after the first split, every point in the upper branch remains unchanged, so there is no need to further subdivide this branch. Except for nodes that contain all of one color, at each level every region is again split along one of the two features. Notice that as the depth increases, we tend to get very strangely shaped classification regions; for example, at a depth of five, there is a tall and skinny purple region between the yellow and blue regions. It's clear that this is less a result of the true, intrinsic data distribution, and more a result of the particular sampling or noise properties of the data. That is, this decision tree, even at only five levels deep, is clearly over-fitting our data. Such over-fitting turns out to be a general property of decision trees: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from. Another way to see this over-fitting is to look at models trained on different subsets of the data\u2014for example, in this figure we train two different trees, each on half of the original data: It is clear that in some places, the two trees produce consistent results (e.g., in the four corners), while in other places, the two trees give very different classifications (e.g., in the regions between any two clusters). The key observation is that the inconsistencies tend to happen where the classification is less certain, and thus by using information from both of these trees, we might come up with a better result! Just as using information from two trees improves our results, we might expect that using information from many trees would improve our results even further. AND WHAT WOULD WE HAVE IF WE HAD MANY TREES? YES! A FOREST! (If we had many Ents (smart trees ;) ), we could have Fangorn Forest!) This notion\u2014that multiple overfitting estimators can be combined to reduce the effect of this overfitting\u2014is what underlies an ensemble method called bagging. Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which over-fits the data, and averages the results to find a better classification. An ensemble of randomized decision trees is known as a random forest. What is Random Forest? Random Forest is a versatile machine learning method capable of performing both regression and classification tasks. It also undertakes dimensional reduction methods, treats missing values, outlier values and other essential steps of data exploration, and does a fairly good job. It is a type of ensemble learning method, where a group of weak models combine to form a powerful model. In Random Forest, we grow multiple trees. To classify a new object based on attributes, each tree gives a classification and we say the tree \u201cvotes\u201d for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees. How Random Forest algorithm works? Random forest is like bootstrapping algorithm with Decision tree (CART) model. Say, we have 1000 observation in the complete population with 10 variables. Random forest tries to build multiple CART models with different samples and different initial variables. For instance, it will take a random sample of 100 observation and 5 randomly chosen initial variables to build a CART model. It will repeat the process (say) 10 times and then make a final prediction on each observation. Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction. Let's consider an imaginary example: Out of a large population, Say, the algorithm Random forest picks up 10k observation with only one variable (for simplicity) to build each CART model. In total, we are looking at 5 CART model being built with different variables. In a real life problem, you will have more number of population sample and different combinations of input variables. The target variable is the salary bands: Band1 : Below 40000 Band2 : 40000 - 150000 Band3 : Above 150000 Following are the outputs of the 5 different CART model: CART1 : Based on \"Age\" as predictor: CART2 : Based on \"Gender\" as predictor: CART3 : Based on \"Education\" as predictor: CART4 : Based on \"Residence\" as predictor: CART5 : Based on \"Industry\" as predictor: Using these 5 CART models, we need to come up with single set of probability to belong to each of the salary classes. For simplicity, we will just take a mean of probabilities in this case study. Other than simple mean, we also consider vote method to come up with the final prediction. To come up with the final prediction let\u2019s locate the following profile in each CART model: Age : 35 years Gender : Male Highest Educational Qualification : Diploma holder Industry : Manufacturing Residence : Metro For each of these CART model, following is the distribution across salary bands : The final probability is simply the average of the probability in the same salary bands in different CART models. As you can see from this analysis, that there is 70% chance of this individual falling in class 1 (less than 40,000) and around 24% chance of the individual falling in class 2. Example: Re-using the Iris Plants Classification The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers. The Iris Data Set contains information on sepal length, sepal width, petal length, petal width all in cm, and class of iris plants. The data set contains 3 classes of 50 instances each. Let's use Random Forest in Python and see if we can classifity iris plants based on the four given predictors. Acknowledgements 1. Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). 2. Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. 3. Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. 4. Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. 5. See also: 1988 MLC Proceedings, 54-64. Cheeseman et al's AUTOCLASS II conceptual clustering system finds 3 classes in the data. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns dataset = pd.read_csv('iris.csv') #dataset.head() sns.pairplot(dataset, hue='Class') #A very cool plot to explore a dataset # Notice that iris-setosa is easily identifiable by petal length and petal width, # while the other two species are much more difficult to distinguish. <seaborn.axisgrid.PairGrid at 0x1998c254dc8> X = dataset.iloc[:, :-1].values y = dataset.iloc[:, 4].values from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75) from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456) rf.fit(X_train, y_train) predicted = rf.predict(X_test) from sklearn.metrics import classification_report, confusion_matrix print(confusion_matrix(y_test, predicted)) print(metrics.classification_report(y_test, y_pred=predicted)) [[40 0 0] [ 0 31 5] [ 0 1 36]] precision recall f1-score support Iris-setosa 1.00 1.00 1.00 40 Iris-versicolor 0.97 0.86 0.91 36 Iris-virginica 0.88 0.97 0.92 37 accuracy 0.95 113 macro avg 0.95 0.94 0.94 113 weighted avg 0.95 0.95 0.95 113 cm = pd.DataFrame(confusion_matrix(y_test, predicted), columns=iris.target_names, index=iris.target_names) sns.heatmap(cm, annot=True) #This lets us know that our model correctly separates the setosa examples, #but exhibits a small amount of confusion when attempting to distinguish between versicolor and virginica. <matplotlib.axes._subplots.AxesSubplot at 0x1998cd37708> Pros and Cons associated with Random Forest ... Pros: this algorithm can solve both type of problems i.e. classification and regression and does a decent estimation at both fronts. It is effective in high dimensional spaces. One of the most essential benefits of Random forest is, the power to handle large data sets with higher dimensionality. It can handle thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods. Further, the model outputs Importance of variable, which can be a very handy feature. It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing. It has methods for balancing errors in datasets where classes are imbalanced. The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection. Both training and prediction are very fast, because of the simplicity of the underlying decision trees. In addition, both tasks can be straightforwardly parallelized, because the individual trees are entirely independent entities. The nonparametric model is extremely flexible, and can thus perform well on tasks that are under-fit by other estimators. Cons: It surely does a good job at classification but not as good as for regression problem as it does not give continuous output. In case of regression, it doesn\u2019t predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy. Random Forest can feel like a black box approach for statistical modelers \u2013 you have very little control on what the model does. You can at best \u2013 try different parameters and random seeds! This notebook was inspired by several blogposts including: \"K-Nearest Neighbors Algorithm in Python and Scikit-Learn\" by Scott Robinson available at* https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/ \"Develop k-Nearest Neighbors in Python From Scratch\" by Jason Brownlee available at* https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/ \"4 Distance Measures for Machine Learning\" by Jason Brownlee available at* https://machinelearningmastery.com/distance-measures-for-machine-learning/ \"KNN Classification using Scikit-learn\" by Avinash Navlani available at* https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn \"In-Depth: Decision Trees and Random Forests\" by Jake VanderPlas available at *https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html \"Powerful Guide to learn Random Forest (with codes in R & Python)\" by SUNIL RAY available at *https://www.analyticsvidhya.com/blog/2015/09/random-forest-algorithm-multiple-challenges/?utm_source=blog \"Introduction to Random forest \u2013 Simplified\" by TAVISH SRIVASTAVA available at *https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/ Here are some great reads on these topics: - \"KNN in Python\" by Czako Zoltan available at https://towardsdatascience.com/knn-in-python-835643e2fb53 - \"K Nearest Neighbor Algorithm In Python\" by Cory Maklin available at https://towardsdatascience.com/k-nearest-neighbor-python-2fccc47d2a55 - \"k-nearest neighbor algorithm in Python\" available at https://www.geeksforgeeks.org/k-nearest-neighbor-algorithm-in-python/ - \"Using Random Forests in Python with Scikit-Learn\" available at https://www.blopig.com/blog/2017/07/using-random-forests-in-python-with-scikit-learn/ - \"Random Forest Regression in Python\" available at https://www.geeksforgeeks.org/random-forest-regression-in-python/ - \"Random Forest Algorithm with Python and Scikit-Learn\" by Usman Malik available at https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/ Here are some great videos on these topics: - \"StatQuest: K-nearest neighbors, Clearly Explained\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=HVXime0nQeI - \"How kNN algorithm works\" by Thales Sehn K\u00f6rting available at https://www.youtube.com/watch?v=UqYde-LULfs - \"KNN Algorithm Using Python | How KNN Algorithm Works | Data Science For Beginners | Simplilearn\" by Simplilearn available at https://www.youtube.com/watch?v=4HKqjENq9OU - \"Decision Tree (CART) - Machine Learning Fun and Easy\" by Augmented Startups available at https://www.youtube.com/watch?v=DCZ3tsQIoGU - \"StatQuest: Random Forests Part 1 - Building, Using and Evaluating\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=J4Wdy0Wc_xQ - \"StatQuest: Random Forests Part 2: Missing data and clustering\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=sQ870aTKqiM - \"Random Forest - Fun and Easy Machine Learning\" by Augmented Startups available at *https://www.youtube.com/watch?v=D_2LkhMJcfY Exercise: So Close No Matter How Far What are some issues with too many neighbors (high K-value)? How about too few neighbors (low K-value)? Make sure to cite any resources that you may use.","title":"Lab23"},{"location":"8-Labs/Newly Formatted/Lab23/#laboratory-23-its-a-beautiful-day-in-the-neighborhood-or-in-the-forest","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 23: \"It's a beautiful day in the neighborhood\" or in the forest! "},{"location":"8-Labs/Newly Formatted/Lab23/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab23/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab23/#title-of-the-notebook","text":"","title":"Title of the notebook"},{"location":"8-Labs/Newly Formatted/Lab23/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab23/#the-k-nearest-neighbors-knn-algorithm-is-a-type-of-supervised-machine-learning-algorithms-knn-is-extremely-easy-to-implement-in-its-most-basic-form-and-yet-performs-quite-complex-classification-tasks-it-is-a-lazy-learning-algorithm-since-it-doesnt-have-a-specialized-training-phase-rather-it-uses-all-of-the-data-for-training-while-classifying-a-new-data-point-or-instance-knn-is-a-non-parametric-learning-algorithm-which-means-that-it-doesnt-assume-anything-about-the-underlying-data-this-is-an-extremely-useful-feature-since-most-of-the-real-world-data-doesnt-really-follow-any-theoretical-assumption","text":"","title":"The K-nearest neighbors (KNN) algorithm is a type of supervised machine learning algorithms. KNN is extremely easy to implement in its most basic form, and yet performs quite complex classification tasks. It is a lazy learning algorithm since it doesn't have a specialized training phase. Rather, it uses all of the data for training while classifying a new data point or instance. KNN is a non-parametric learning algorithm, which means that it doesn't assume anything about the underlying data. This is an extremely useful feature since most of the real world data doesn't really follow any theoretical assumption. "},{"location":"8-Labs/Newly Formatted/Lab23/#theory","text":"","title":"Theory"},{"location":"8-Labs/Newly Formatted/Lab23/#the-first-law-of-geography-according-to-waldo-tobler-is-everything-is-related-to-everything-else-but-near-things-are-more-related-than-distant-things-the-intuition-behind-the-knn-algorithm-is-one-of-the-simplest-of-all-the-supervised-machine-learning-algorithms-it-simply-calculates-the-distance-of-a-new-data-point-to-all-other-training-data-points-the-distance-can-be-of-any-type-eg-euclidean-or-manhattan-etc-it-then-selects-the-k-nearest-data-points-where-k-can-be-any-integer-finally-it-assigns-the-data-point-to-the-class-to-which-the-majority-of-the-k-data-points-belong","text":"distance measures play an important role in machine learning. Perhaps four of the most commonly used distance measures in machine learning are as follows: - Euclidean Distance: Calculates the distance between two real-valued vectors. Although there are other possible choices, most instance-based learners use Euclidean distance. - Manhattan Distance: Also called the Taxicab distance or the City Block distance, calculates the distance between two real-valued vectors. It is perhaps more useful to vectors that describe objects on a uniform grid, like a chessboard or city blocks. The taxicab name for the measure refers to the intuition for what the measure calculates: the shortest path that a taxicab would take between city blocks (coordinates on the grid). - Minkowski Distance: Calculates the distance between two real-valued vectors. It is a generalization of the Euclidean and Manhattan distance measures and adds a parameter, called the \u201corder\u201d or \u201cp\u201c, that allows different distance measures to be calculated. When p is set to 1, the calculation is the same as the Manhattan distance. When p is set to 2, it is the same as the Euclidean distance.","title":"The First Law of Geography, according to Waldo Tobler, is \"everything is related to everything else, but near things are more related than distant things.\" The intuition behind the KNN algorithm is one of the simplest of all the supervised machine learning algorithms. It simply calculates the distance of a new data point to all other training data points. The distance can be of any type e.g Euclidean or Manhattan etc. It then selects the K-nearest data points, where K can be any integer. Finally it assigns the data point to the class to which the majority of the K data points belong."},{"location":"8-Labs/Newly Formatted/Lab23/#lets-see-this-algorithm-in-action-with-the-help-of-a-simple-example-suppose-you-have-a-dataset-with-two-variables-which-when-plotted-looks-like-the-one-in-the-following-figure","text":"","title":"Let's see this algorithm in action with the help of a simple example. Suppose you have a dataset with two variables, which when plotted, looks like the one in the following figure."},{"location":"8-Labs/Newly Formatted/Lab23/#your-task-is-to-classify-a-new-data-point-with-x-into-blue-class-or-red-class-the-coordinate-values-of-the-data-point-are-x45-and-y50-suppose-the-value-of-k-is-3-the-knn-algorithm-starts-by-calculating-the-distance-of-point-x-from-all-the-points-it-then-finds-the-3-nearest-points-with-least-distance-to-point-x-this-is-shown-in-the-figure-below-the-three-nearest-points-have-been-encircled","text":"","title":"Your task is to classify a new data point with 'X' into \"Blue\" class or \"Red\" class. The coordinate values of the data point are x=45 and y=50. Suppose the value of K is 3. The KNN algorithm starts by calculating the distance of point X from all the points. It then finds the 3 nearest points with least distance to point X. This is shown in the figure below. The three nearest points have been encircled."},{"location":"8-Labs/Newly Formatted/Lab23/#the-final-step-of-the-knn-algorithm-is-to-assign-new-point-to-the-class-to-which-majority-of-the-three-nearest-points-belong-from-the-figure-above-we-can-see-that-the-two-of-the-three-nearest-points-belong-to-the-class-red-while-one-belongs-to-the-class-blue-therefore-the-new-data-point-will-be-classified-as-red","text":"","title":"The final step of the KNN algorithm is to assign new point to the class to which majority of the three nearest points belong. From the figure above we can see that the two of the three nearest points belong to the class \"Red\" while one belongs to the class \"Blue\". Therefore the new data point will be classified as \"Red\"."},{"location":"8-Labs/Newly Formatted/Lab23/#why-knn","text":"It is extremely easy to implement It is lazy learning algorithm and therefore requires no training prior to making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g SVM, linear regression, etc. Since the algorithm requires no training before making predictions, new data can be added seamlessly. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)","title":"Why KNN?"},{"location":"8-Labs/Newly Formatted/Lab23/#example-iris-plants-classification","text":"","title":"Example: Iris Plants Classification "},{"location":"8-Labs/Newly Formatted/Lab23/#this-is-perhaps-the-best-known-problem-and-database-to-be-found-in-the-pattern-recognition-literature-fishers-paper-is-a-classic-in-the-field-and-is-referenced-frequently-to-this-day-the-iris-flower-dataset-involves-predicting-the-flower-species-given-measurements-of-iris-flowers-the-iris-data-set-contains-information-on-sepal-length-sepal-width-petal-length-petal-width-all-in-cm-and-class-of-iris-plants-the-data-set-contains-3-classes-of-50-instances-each-where-each-class-refers-to-a-type-of-iris-plant-hence-it-is-a-multiclass-classification-problem-and-the-number-of-observations-for-each-class-is-balanced","text":"","title":"This is perhaps the best known problem and database to be found in the pattern recognition literature.  Fisher's paper is a classic in the field and is referenced frequently to this day. The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers. The Iris Data Set contains information on sepal length, sepal width, petal length, petal width all in cm, and class of iris plants. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. Hence, it is a multiclass classification problem and the number of observations for each class is balanced."},{"location":"8-Labs/Newly Formatted/Lab23/#lets-use-a-knn-model-in-python-and-see-if-we-can-classifity-iris-plants-based-on-the-four-given-predictors","text":"Acknowledgements 1. Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). 2. Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. 3. Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. 4. Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. 5. See also: 1988 MLC Proceedings, 54-64. Cheeseman et al's AUTOCLASS II conceptual clustering system finds 3 classes in the data.","title":"Let's use a KNN model in Python and see if we can classifity iris plants based on the four given predictors."},{"location":"8-Labs/Newly Formatted/Lab23/#as-you-know-by-now-the-first-step-is-to-load-some-necessary-libraries","text":"import numpy as np import pandas as pd from matplotlib import pyplot as plt import sklearn.metrics as metrics import seaborn as sns %matplotlib inline","title":"As you know by now, the first step is to load some necessary libraries:"},{"location":"8-Labs/Newly Formatted/Lab23/#then-we-should-read-the-dataset-and-explore-it-using-tools-such-as-descriptive-statistics","text":"# This is something cool that you can do on your local machine (Jupyter): url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" # Assign colum names to the dataset names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class'] # Read dataset to pandas dataframe dataset = pd.read_csv(url, names=names) dataset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal-length sepal-width petal-length petal-width Class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa dataset = pd.read_csv('iris.csv') dataset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal-length sepal-width petal-length petal-width Class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa dataset.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal-length sepal-width petal-length petal-width count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.054000 3.758667 1.198667 std 0.828066 0.433594 1.764420 0.763161 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000","title":"Then, we should read the dataset and explore it using tools such as descriptive statistics:"},{"location":"8-Labs/Newly Formatted/Lab23/#we-should-seperate-the-predictors-and-target-similar-to-what-we-did-for-logisitc-regression","text":"X = dataset.iloc[:, :-1].values y = dataset.iloc[:, 4].values","title":"We should seperate the predictors and target - similar to what we did for logisitc regression:"},{"location":"8-Labs/Newly Formatted/Lab23/#then-the-dataset-should-be-split-into-training-and-testing-this-way-our-algorithm-is-tested-on-un-seen-data-as-it-would-be-in-a-real-world-application-lets-go-with-a-8020-split","text":"from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) #This means that out of total 150 records: #the training set will contain 120 records & #the test set contains 30 of those records.","title":"Then, the dataset should be split into training and testing. This way our algorithm is tested on un-seen data, as it would be in a real-world application. Let's go with a 80/20 split:"},{"location":"8-Labs/Newly Formatted/Lab23/#it-is-extremely-straight-forward-to-train-the-knn-algorithm-and-make-predictions-with-it-especially-when-using-scikit-learn-the-first-step-is-to-import-the-kneighborsclassifier-class-from-the-sklearnneighbors-library-in-the-second-line-this-class-is-initialized-with-one-parameter-ie-n_neigbours-this-is-basically-the-value-for-the-k-there-is-no-ideal-value-for-k-and-it-is-selected-after-testing-and-evaluation-however-to-start-out-5-seems-to-be-the-most-commonly-used-value-for-knn-algorithm","text":"from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors=5) classifier.fit(X_train, y_train) KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights='uniform')","title":"It is extremely straight forward to train the KNN algorithm and make predictions with it, especially when using Scikit-Learn. The first step is to import the \"KNeighborsClassifier\" class from the \"sklearn.neighbors\" library. In the second line, this class is initialized with one parameter, i.e. \"n_neigbours\". This is basically the value for the K. There is no ideal value for K and it is selected after testing and evaluation, however to start out, 5 seems to be the most commonly used value for KNN algorithm."},{"location":"8-Labs/Newly Formatted/Lab23/#the-final-step-is-to-make-predictions-on-our-test-data-to-do-so-execute-the-following-script","text":"y_pred = classifier.predict(X_test)","title":"The final step is to make predictions on our test data. To do so, execute the following script:"},{"location":"8-Labs/Newly Formatted/Lab23/#as-its-time-to-evaluate-our-model-we-will-go-to-our-rather-new-friends-confusion-matrix-precision-recall-and-f1-score-as-the-most-commonly-used-discrete-gof-metrics","text":"from sklearn.metrics import classification_report, confusion_matrix print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) [[11 0 0] [ 0 9 2] [ 0 0 8]] precision recall f1-score support Iris-setosa 1.00 1.00 1.00 11 Iris-versicolor 1.00 0.82 0.90 11 Iris-virginica 0.80 1.00 0.89 8 accuracy 0.93 30 macro avg 0.93 0.94 0.93 30 weighted avg 0.95 0.93 0.93 30","title":"As it's time to evaluate our model, we will go to our rather new friends, confusion matrix, precision, recall and f1 score as the most commonly used discrete GOF metrics."},{"location":"8-Labs/Newly Formatted/Lab23/#what-if-we-had-used-a-different-value-for-k-what-is-the-best-value-for-k","text":"","title":"What if we had used a different value for K? What is the best value for K?"},{"location":"8-Labs/Newly Formatted/Lab23/#one-way-to-help-you-find-the-best-value-of-k-is-to-plot-the-graph-of-k-value-and-the-corresponding-error-rate-for-the-dataset-in-this-section-we-will-plot-the-mean-error-for-the-predicted-values-of-test-set-for-all-the-k-values-between-1-and-50-to-do-so-lets-first-calculate-the-mean-of-error-for-all-the-predicted-values-where-k-ranges-from-1-and-50","text":"error = [] # Calculating error for K values between 1 and 50 # In each iteration the mean error for predicted values of test set is calculated and # the result is appended to the error list. for i in range(1, 50): knn = KNeighborsClassifier(n_neighbors=i) knn.fit(X_train, y_train) pred_i = knn.predict(X_test) error.append(np.mean(pred_i != y_test))","title":"One way to help you find the best value of K is to plot the graph of K value and the corresponding error rate for the dataset. In this section, we will plot the mean error for the predicted values of test set for all the K values between 1 and 50. To do so, let's first calculate the mean of error for all the predicted values where K ranges from 1 and 50:"},{"location":"8-Labs/Newly Formatted/Lab23/#the-next-step-is-to-plot-the-error-values-against-k-values","text":"plt.figure(figsize=(12, 6)) plt.plot(range(1, 50), error, color='red', linestyle='dashed', marker='o', markerfacecolor='blue', markersize=10) plt.title('Error Rate K Value') plt.xlabel('K Value') plt.ylabel('Mean Error') Text(0, 0.5, 'Mean Error')","title":"The next step is to plot the error values against K values:"},{"location":"8-Labs/Newly Formatted/Lab23/#final-remarks","text":"KNN is a simple yet powerful classification algorithm. It requires no training for making predictions, which is typically one of the most difficult parts of a machine learning algorithm. The KNN algorithm have been widely used to find document similarity and pattern recognition.","title":"Final remarks ..."},{"location":"8-Labs/Newly Formatted/Lab23/#decision-trees-and-random-forests","text":"","title":"Decision Trees and Random Forests"},{"location":"8-Labs/Newly Formatted/Lab23/#decision-trees-aka-classification-and-regression-tree-cart-models-are-extremely-intuitive-ways-to-classify-or-label-objects-you-simply-ask-a-series-of-questions-designed-to-zero-in-on-the-classification-for-example-if-you-wanted-to-build-a-decision-tree-to-classify-an-animal-you-come-across-while-on-a-hike-you-might-construct-the-one-shown-here","text":"","title":"Decision trees, AKA Classification And Regression Tree (CART) models, are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero-in on the classification. For example, if you wanted to build a decision tree to classify an animal you come across while on a hike, you might construct the one shown here: "},{"location":"8-Labs/Newly Formatted/Lab23/#the-binary-splitting-makes-this-extremely-efficient-in-a-well-constructed-tree-each-question-will-cut-the-number-of-options-by-approximately-half-very-quickly-narrowing-the-options-even-among-a-large-number-of-classes-the-trick-of-course-comes-in-deciding-which-questions-to-ask-at-each-step-in-machine-learning-implementations-of-decision-trees-the-questions-generally-take-the-form-of-axis-aligned-splits-in-the-data-that-is-each-node-in-the-tree-splits-the-data-into-two-groups-using-a-cutoff-value-within-one-of-the-features-lets-now-look-at-an-example-of-this","text":"","title":"The binary splitting makes this extremely efficient: in a well-constructed tree, each question will cut the number of options by approximately half, very quickly narrowing the options even among a large number of classes. The trick, of course, comes in deciding which questions to ask at each step. In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data: that is, each node in the tree splits the data into two groups using a cutoff value within one of the features. Let's now look at an example of this. "},{"location":"8-Labs/Newly Formatted/Lab23/#a-simple-decision-tree-built-on-this-data-will-iteratively-split-the-data-along-one-or-the-other-axis-according-to-some-quantitative-criterion-and-at-each-level-assign-the-label-of-the-new-region-according-to-a-majority-vote-of-points-within-it-this-figure-presents-a-visualization-of-the-first-four-levels-of-a-decision-tree-classifier-for-this-data","text":"","title":"A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it. This figure presents a visualization of the first four levels of a decision tree classifier for this data: "},{"location":"8-Labs/Newly Formatted/Lab23/#notice-that-after-the-first-split-every-point-in-the-upper-branch-remains-unchanged-so-there-is-no-need-to-further-subdivide-this-branch-except-for-nodes-that-contain-all-of-one-color-at-each-level-every-region-is-again-split-along-one-of-the-two-features","text":"","title":"Notice that after the first split, every point in the upper branch remains unchanged, so there is no need to further subdivide this branch. Except for nodes that contain all of one color, at each level every region is again split along one of the two features."},{"location":"8-Labs/Newly Formatted/Lab23/#notice-that-as-the-depth-increases-we-tend-to-get-very-strangely-shaped-classification-regions-for-example-at-a-depth-of-five-there-is-a-tall-and-skinny-purple-region-between-the-yellow-and-blue-regions-its-clear-that-this-is-less-a-result-of-the-true-intrinsic-data-distribution-and-more-a-result-of-the-particular-sampling-or-noise-properties-of-the-data-that-is-this-decision-tree-even-at-only-five-levels-deep-is-clearly-over-fitting-our-data","text":"","title":"Notice that as the depth increases, we tend to get very strangely shaped classification regions; for example, at a depth of five, there is a tall and skinny purple region between the yellow and blue regions. It's clear that this is less a result of the true, intrinsic data distribution, and more a result of the particular sampling or noise properties of the data. That is, this decision tree, even at only five levels deep, is clearly over-fitting our data. "},{"location":"8-Labs/Newly Formatted/Lab23/#such-over-fitting-turns-out-to-be-a-general-property-of-decision-trees-it-is-very-easy-to-go-too-deep-in-the-tree-and-thus-to-fit-details-of-the-particular-data-rather-than-the-overall-properties-of-the-distributions-they-are-drawn-from-another-way-to-see-this-over-fitting-is-to-look-at-models-trained-on-different-subsets-of-the-datafor-example-in-this-figure-we-train-two-different-trees-each-on-half-of-the-original-data","text":"","title":"Such over-fitting turns out to be a general property of decision trees: it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from. Another way to see this over-fitting is to look at models trained on different subsets of the data\u2014for example, in this figure we train two different trees, each on half of the original data:"},{"location":"8-Labs/Newly Formatted/Lab23/#it-is-clear-that-in-some-places-the-two-trees-produce-consistent-results-eg-in-the-four-corners-while-in-other-places-the-two-trees-give-very-different-classifications-eg-in-the-regions-between-any-two-clusters-the-key-observation-is-that-the-inconsistencies-tend-to-happen-where-the-classification-is-less-certain-and-thus-by-using-information-from-both-of-these-trees-we-might-come-up-with-a-better-result","text":"","title":"It is clear that in some places, the two trees produce consistent results (e.g., in the four corners), while in other places, the two trees give very different classifications (e.g., in the regions between any two clusters). The key observation is that the inconsistencies tend to happen where the classification is less certain, and thus by using information from both of these trees, we might come up with a better result!"},{"location":"8-Labs/Newly Formatted/Lab23/#just-as-using-information-from-two-trees-improves-our-results-we-might-expect-that-using-information-from-many-trees-would-improve-our-results-even-further-and-what-would-we-have-if-we-had-many-trees","text":"","title":"Just as using information from two trees improves our results, we might expect that using information from many trees would improve our results even further. AND WHAT WOULD WE HAVE IF WE HAD MANY TREES?"},{"location":"8-Labs/Newly Formatted/Lab23/#yes-a-forest-if-we-had-many-ents-smart-trees-we-could-have-fangorn-forest","text":"","title":"YES! A FOREST! (If we had many Ents (smart trees ;) ), we could have Fangorn Forest!)"},{"location":"8-Labs/Newly Formatted/Lab23/#this-notionthat-multiple-overfitting-estimators-can-be-combined-to-reduce-the-effect-of-this-overfittingis-what-underlies-an-ensemble-method-called-bagging-bagging-makes-use-of-an-ensemble-a-grab-bag-perhaps-of-parallel-estimators-each-of-which-over-fits-the-data-and-averages-the-results-to-find-a-better-classification-an-ensemble-of-randomized-decision-trees-is-known-as-a-random-forest","text":"","title":"This notion\u2014that multiple overfitting estimators can be combined to reduce the effect of this overfitting\u2014is what underlies an ensemble method called bagging. Bagging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which over-fits the data, and averages the results to find a better classification. An ensemble of randomized decision trees is known as a random forest."},{"location":"8-Labs/Newly Formatted/Lab23/#what-is-random-forest","text":"","title":"What is Random Forest?"},{"location":"8-Labs/Newly Formatted/Lab23/#random-forest-is-a-versatile-machine-learning-method-capable-of-performing-both-regression-and-classification-tasks-it-also-undertakes-dimensional-reduction-methods-treats-missing-values-outlier-values-and-other-essential-steps-of-data-exploration-and-does-a-fairly-good-job-it-is-a-type-of-ensemble-learning-method-where-a-group-of-weak-models-combine-to-form-a-powerful-model","text":"","title":"Random Forest is a versatile machine learning method capable of performing both regression and classification tasks. It also undertakes dimensional reduction methods, treats missing values, outlier values and other essential steps of data exploration, and does a fairly good job. It is a type of ensemble learning method, where a group of weak models combine to form a powerful model."},{"location":"8-Labs/Newly Formatted/Lab23/#in-random-forest-we-grow-multiple-trees-to-classify-a-new-object-based-on-attributes-each-tree-gives-a-classification-and-we-say-the-tree-votes-for-that-class-the-forest-chooses-the-classification-having-the-most-votes-over-all-the-trees-in-the-forest-and-in-case-of-regression-it-takes-the-average-of-outputs-by-different-trees","text":"","title":"In Random Forest, we grow multiple trees. To classify a new object based on attributes, each tree gives a classification and we say the tree \u201cvotes\u201d for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees."},{"location":"8-Labs/Newly Formatted/Lab23/#how-random-forest-algorithm-works","text":"","title":"How Random Forest algorithm works?"},{"location":"8-Labs/Newly Formatted/Lab23/#random-forest-is-like-bootstrapping-algorithm-with-decision-tree-cart-model-say-we-have-1000-observation-in-the-complete-population-with-10-variables-random-forest-tries-to-build-multiple-cart-models-with-different-samples-and-different-initial-variables-for-instance-it-will-take-a-random-sample-of-100-observation-and-5-randomly-chosen-initial-variables-to-build-a-cart-model-it-will-repeat-the-process-say-10-times-and-then-make-a-final-prediction-on-each-observation-final-prediction-is-a-function-of-each-prediction-this-final-prediction-can-simply-be-the-mean-of-each-prediction-lets-consider-an-imaginary-example","text":"","title":"Random forest is like bootstrapping algorithm with Decision tree (CART) model. Say, we have 1000 observation in the complete population with 10 variables. Random forest tries to build multiple CART models with different samples and different initial variables. For instance, it will take a random sample of 100 observation and 5 randomly chosen initial variables to build a CART model. It will repeat the process (say) 10 times and then make a final prediction on each observation. Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction. Let's consider an imaginary example:"},{"location":"8-Labs/Newly Formatted/Lab23/#out-of-a-large-population-say-the-algorithm-random-forest-picks-up-10k-observation-with-only-one-variable-for-simplicity-to-build-each-cart-model-in-total-we-are-looking-at-5-cart-model-being-built-with-different-variables-in-a-real-life-problem-you-will-have-more-number-of-population-sample-and-different-combinations-of-input-variables-the-target-variable-is-the-salary-bands","text":"Band1 : Below 40000 Band2 : 40000 - 150000 Band3 : Above 150000","title":"Out of a large population, Say, the algorithm Random forest picks up 10k observation with only one variable (for simplicity) to build each CART model. In total, we are looking at 5 CART model being built with different variables. In a real life problem, you will have more number of population sample and different combinations of  input variables. The target variable is the salary bands:"},{"location":"8-Labs/Newly Formatted/Lab23/#following-are-the-outputs-of-the-5-different-cart-model","text":"","title":"Following are the outputs of the 5 different CART model:"},{"location":"8-Labs/Newly Formatted/Lab23/#cart1-based-on-age-as-predictor","text":"","title":"CART1 : Based on \"Age\" as predictor:"},{"location":"8-Labs/Newly Formatted/Lab23/#cart2-based-on-gender-as-predictor","text":"","title":"CART2 : Based on \"Gender\" as predictor:"},{"location":"8-Labs/Newly Formatted/Lab23/#cart3-based-on-education-as-predictor","text":"","title":"CART3 : Based on \"Education\" as predictor:"},{"location":"8-Labs/Newly Formatted/Lab23/#cart4-based-on-residence-as-predictor","text":"","title":"CART4 : Based on \"Residence\" as predictor:"},{"location":"8-Labs/Newly Formatted/Lab23/#cart5-based-on-industry-as-predictor","text":"","title":"CART5 : Based on \"Industry\" as predictor:"},{"location":"8-Labs/Newly Formatted/Lab23/#using-these-5-cart-models-we-need-to-come-up-with-single-set-of-probability-to-belong-to-each-of-the-salary-classes-for-simplicity-we-will-just-take-a-mean-of-probabilities-in-this-case-study-other-than-simple-mean-we-also-consider-vote-method-to-come-up-with-the-final-prediction-to-come-up-with-the-final-prediction-lets-locate-the-following-profile-in-each-cart-model","text":"Age : 35 years Gender : Male Highest Educational Qualification : Diploma holder Industry : Manufacturing Residence : Metro","title":"Using these 5 CART models, we need to come up with single set of probability to belong to each of the salary classes. For simplicity, we will just take a mean of probabilities in this case study. Other than simple mean, we also consider vote method to come up with the final prediction. To come up with the final prediction let\u2019s locate the following profile in each CART model:"},{"location":"8-Labs/Newly Formatted/Lab23/#for-each-of-these-cart-model-following-is-the-distribution-across-salary-bands","text":"","title":"For each of these CART model, following is the distribution across salary bands :"},{"location":"8-Labs/Newly Formatted/Lab23/#the-final-probability-is-simply-the-average-of-the-probability-in-the-same-salary-bands-in-different-cart-models-as-you-can-see-from-this-analysis-that-there-is-70-chance-of-this-individual-falling-in-class-1-less-than-40000-and-around-24-chance-of-the-individual-falling-in-class-2","text":"","title":"The final probability is simply the average of the probability in the same salary bands in different CART models. As you can see from this analysis, that there is 70% chance of this individual falling in class 1 (less than 40,000) and around 24% chance of the individual falling in class 2."},{"location":"8-Labs/Newly Formatted/Lab23/#example-re-using-the-iris-plants-classification","text":"","title":"Example: Re-using the Iris Plants Classification "},{"location":"8-Labs/Newly Formatted/Lab23/#the-iris-flower-dataset-involves-predicting-the-flower-species-given-measurements-of-iris-flowers-the-iris-data-set-contains-information-on-sepal-length-sepal-width-petal-length-petal-width-all-in-cm-and-class-of-iris-plants-the-data-set-contains-3-classes-of-50-instances-each","text":"","title":"The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers. The Iris Data Set contains information on sepal length, sepal width, petal length, petal width all in cm, and class of iris plants. The data set contains 3 classes of 50 instances each."},{"location":"8-Labs/Newly Formatted/Lab23/#lets-use-random-forest-in-python-and-see-if-we-can-classifity-iris-plants-based-on-the-four-given-predictors","text":"Acknowledgements 1. Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). 2. Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. 3. Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. 4. Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. 5. See also: 1988 MLC Proceedings, 54-64. Cheeseman et al's AUTOCLASS II conceptual clustering system finds 3 classes in the data. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns dataset = pd.read_csv('iris.csv') #dataset.head() sns.pairplot(dataset, hue='Class') #A very cool plot to explore a dataset # Notice that iris-setosa is easily identifiable by petal length and petal width, # while the other two species are much more difficult to distinguish. <seaborn.axisgrid.PairGrid at 0x1998c254dc8> X = dataset.iloc[:, :-1].values y = dataset.iloc[:, 4].values from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75) from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456) rf.fit(X_train, y_train) predicted = rf.predict(X_test) from sklearn.metrics import classification_report, confusion_matrix print(confusion_matrix(y_test, predicted)) print(metrics.classification_report(y_test, y_pred=predicted)) [[40 0 0] [ 0 31 5] [ 0 1 36]] precision recall f1-score support Iris-setosa 1.00 1.00 1.00 40 Iris-versicolor 0.97 0.86 0.91 36 Iris-virginica 0.88 0.97 0.92 37 accuracy 0.95 113 macro avg 0.95 0.94 0.94 113 weighted avg 0.95 0.95 0.95 113 cm = pd.DataFrame(confusion_matrix(y_test, predicted), columns=iris.target_names, index=iris.target_names) sns.heatmap(cm, annot=True) #This lets us know that our model correctly separates the setosa examples, #but exhibits a small amount of confusion when attempting to distinguish between versicolor and virginica. <matplotlib.axes._subplots.AxesSubplot at 0x1998cd37708>","title":"Let's use Random Forest in Python and see if we can classifity iris plants based on the four given predictors."},{"location":"8-Labs/Newly Formatted/Lab23/#pros-and-cons-associated-with-random-forest","text":"","title":"Pros and Cons associated with Random Forest ..."},{"location":"8-Labs/Newly Formatted/Lab23/#pros","text":"this algorithm can solve both type of problems i.e. classification and regression and does a decent estimation at both fronts. It is effective in high dimensional spaces. One of the most essential benefits of Random forest is, the power to handle large data sets with higher dimensionality. It can handle thousands of input variables and identify most significant variables so it is considered as one of the dimensionality reduction methods. Further, the model outputs Importance of variable, which can be a very handy feature. It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing. It has methods for balancing errors in datasets where classes are imbalanced. The capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection. Both training and prediction are very fast, because of the simplicity of the underlying decision trees. In addition, both tasks can be straightforwardly parallelized, because the individual trees are entirely independent entities. The nonparametric model is extremely flexible, and can thus perform well on tasks that are under-fit by other estimators.","title":"Pros:"},{"location":"8-Labs/Newly Formatted/Lab23/#cons","text":"It surely does a good job at classification but not as good as for regression problem as it does not give continuous output. In case of regression, it doesn\u2019t predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy. Random Forest can feel like a black box approach for statistical modelers \u2013 you have very little control on what the model does. You can at best \u2013 try different parameters and random seeds! This notebook was inspired by several blogposts including: \"K-Nearest Neighbors Algorithm in Python and Scikit-Learn\" by Scott Robinson available at* https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/ \"Develop k-Nearest Neighbors in Python From Scratch\" by Jason Brownlee available at* https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/ \"4 Distance Measures for Machine Learning\" by Jason Brownlee available at* https://machinelearningmastery.com/distance-measures-for-machine-learning/ \"KNN Classification using Scikit-learn\" by Avinash Navlani available at* https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn \"In-Depth: Decision Trees and Random Forests\" by Jake VanderPlas available at *https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html \"Powerful Guide to learn Random Forest (with codes in R & Python)\" by SUNIL RAY available at *https://www.analyticsvidhya.com/blog/2015/09/random-forest-algorithm-multiple-challenges/?utm_source=blog \"Introduction to Random forest \u2013 Simplified\" by TAVISH SRIVASTAVA available at *https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/ Here are some great reads on these topics: - \"KNN in Python\" by Czako Zoltan available at https://towardsdatascience.com/knn-in-python-835643e2fb53 - \"K Nearest Neighbor Algorithm In Python\" by Cory Maklin available at https://towardsdatascience.com/k-nearest-neighbor-python-2fccc47d2a55 - \"k-nearest neighbor algorithm in Python\" available at https://www.geeksforgeeks.org/k-nearest-neighbor-algorithm-in-python/ - \"Using Random Forests in Python with Scikit-Learn\" available at https://www.blopig.com/blog/2017/07/using-random-forests-in-python-with-scikit-learn/ - \"Random Forest Regression in Python\" available at https://www.geeksforgeeks.org/random-forest-regression-in-python/ - \"Random Forest Algorithm with Python and Scikit-Learn\" by Usman Malik available at https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/ Here are some great videos on these topics: - \"StatQuest: K-nearest neighbors, Clearly Explained\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=HVXime0nQeI - \"How kNN algorithm works\" by Thales Sehn K\u00f6rting available at https://www.youtube.com/watch?v=UqYde-LULfs - \"KNN Algorithm Using Python | How KNN Algorithm Works | Data Science For Beginners | Simplilearn\" by Simplilearn available at https://www.youtube.com/watch?v=4HKqjENq9OU - \"Decision Tree (CART) - Machine Learning Fun and Easy\" by Augmented Startups available at https://www.youtube.com/watch?v=DCZ3tsQIoGU - \"StatQuest: Random Forests Part 1 - Building, Using and Evaluating\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=J4Wdy0Wc_xQ - \"StatQuest: Random Forests Part 2: Missing data and clustering\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=sQ870aTKqiM - \"Random Forest - Fun and Easy Machine Learning\" by Augmented Startups available at *https://www.youtube.com/watch?v=D_2LkhMJcfY","title":"Cons:"},{"location":"8-Labs/Newly Formatted/Lab23/#exercise-so-close-no-matter-how-far","text":"","title":"Exercise: So Close No Matter How Far  "},{"location":"8-Labs/Newly Formatted/Lab23/#what-are-some-issues-with-too-many-neighbors-high-k-value-how-about-too-few-neighbors-low-k-value","text":"","title":"What are some issues with too many neighbors (high K-value)? How about too few neighbors (low K-value)?"},{"location":"8-Labs/Newly Formatted/Lab23/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab24/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: (LINK NEEDS FIXING!) Lab24 Laboratory 24: \"A Tale of Supportive Machines!\" # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook Date: Support Vector Machines (SVM) \u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well (look at the below snapshot). Hyperplane: In geometry, a hyperplane is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines. (@Wikipedia: https://en.wikipedia.org/wiki/Hyperplane) Support Vectors are simply the co-ordinates of individual observation. The SVM classifier is a frontier which best segregates the two classes (hyper-plane/ line). How does it work? Identify the right hyper-plane (Scenario-1): Here, we have three hyper-planes (A, B and C). Now, identify the right hyper-plane to classify star and circle. Remember a thumb rule to identify the right hyper-plane: \u201cSelect the hyper-plane which segregates the two classes better\u201d. In this scenario, hyper-plane \u201cB\u201d has excellently performed this job. Identify the right hyper-plane (Scenario-2): Here, we have three hyper-planes (A, B and C) and all are segregating the classes well. Now, How can we identify the right hyper-plane? Here, maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. This distance is called as Margin. Let\u2019s look at the below snapshot: Below, you can see that the margin for hyper-plane C is high as compared to both A and B. Hence, we name the right hyper-plane as C. Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification. Identify the right hyper-plane (Scenario-3): Hint: Use the rules as discussed in previous section to identify the right hyper-plane Some of you may have selected the hyper-plane B as it has higher margin compared to A. But, here is the catch, SVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin. Here, hyper-plane B has a classification error and A has classified all correctly. Therefore, the right hyper-plane is A. Identify the right hyper-plane (Scenario-4): Below, I am unable to segregate the two classes using a straight line, as one of the stars lies in the territory of other(circle) class as an outlier. As I have already mentioned, one star at other end is like an outlier for star class. The SVM algorithm has a feature to ignore outliers and find the hyper-plane that has the maximum margin. Hence, we can say, SVM classification is robust to outliers. Identify the right hyper-plane (Scenario-5): In the scenario below, we can\u2019t have linear hyper-plane between the two classes, so how does SVM classify these two classes? Till now, we have only looked at the linear hyper-plane. SVM can solve this problem. Easily! It solves this problem by introducing additional feature. Here, we will add a new feature z=x^2+y^2. Now, let\u2019s plot the data points on axis x and z: In above plot, points to consider are: - All values for z would be positive always because z is the squared sum of both x and y - In the original plot, red circles appear close to the origin of x and y axes, leading to lower value of z and star relatively away from the origin result to higher value of z. In the SVM classifier, it is easy to have a linear hyper-plane between these two classes. But, another burning question which arises is, should we need to add this feature manually to have a hyper-plane. No, the SVM algorithm has a technique called the kernel trick. The SVM kernel is a function that takes low dimensional input space and transforms it to a higher dimensional space i.e. it converts not separable problem to separable problem. It is mostly useful in non-linear separation problem. Simply put, it does some extremely complex data transformations, then finds out the process to separate the data based on the labels or outputs you\u2019ve defined. When we look at the hyper-plane in original input space it looks like a circle: From another perspective, this is what's happening: There are different Kernel functions that can be used with SVMs: Depending on the nature of the problem, different Kernels may be advantagous: Let's see how to implement SVM in Python! Example: Re-using the Iris Plants Classification The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers. The Iris Data Set contains information on sepal length, sepal width, petal length, petal width all in cm, and class of iris plants. The data set contains 3 classes of 50 instances each. Let's use SVM in Python and see if we can classifity iris plants based on the four given predictors. Acknowledgements 1. Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). 2. Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. 3. Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. 4. Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. 5. See also: 1988 MLC Proceedings, 54-64. Cheeseman et al's AUTOCLASS II conceptual clustering system finds 3 classes in the data. As you know by now, the first step is to load some necessary libraries: import numpy as np import pandas as pd from matplotlib import pyplot as plt import sklearn.metrics as metrics import seaborn as sns %matplotlib inline from sklearn import datasets #There is a version of the iris database in the sklearn package from sklearn import svm #The function for applyin SVM # import some data to play with iris = datasets.load_iris() X = iris.data[:, :2] # Let's say we only take the first two features: Sepal Width and Sepal Length y = iris.target # we create an instance of SVM and fit our data. svc = svm.SVC(kernel='linear').fit(X, y) # create a mesh to plot in x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 h = (x_max / x_min)/100 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired) plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.xlim(xx.min(), xx.max()) plt.title('SVC with linear kernel') plt.show() plt.subplot(1, 1, 1) Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], cmap=plt.cm.Paired, c=y) plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.xlim(xx.min(), xx.max()) plt.title('SVC with linear kernel') plt.show() svc = svm.SVC(kernel='rbf').fit(X, y) C:\\Users\\Farha\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning. \"avoid this warning.\", FutureWarning) plt.subplot(1, 1, 1) Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired) plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.xlim(xx.min(), xx.max()) plt.title('SVC with linear kernel') plt.show() gamma: Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019. Higher the value of gamma, will try to exact fit the as per training data set i.e. generalization error and cause over-fitting problem. C: Penalty parameter C of the error term. It also controls the trade-off between smooth decision boundaries and classifying the training points correctly. Trying the route similar to what we did before, we should read the dataset and explore it using tools such as descriptive statistics: dataset = pd.read_csv('iris.csv') dataset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal-length sepal-width petal-length petal-width Class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa We should seperate the predictors and target - similar to what we did for logisitc regression: X = dataset.iloc[:, :-1].values y = dataset.iloc[:, 4].values Now we split the training and testing datasets with a 0.75/0.25 ratio: from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) Then, we create an instance of SVM and fit our data: from sklearn import svm #create a classifier cls = svm.SVC(kernel=\"linear\") #train the model cls.fit(X_train,y_train) #predict the response pred = cls.predict(X_test) And we can evaluate the performance of our SVM model: from sklearn.metrics import classification_report, confusion_matrix print(confusion_matrix(y_test, pred)) print(metrics.classification_report(y_test, y_pred=pred)) [[14 0 0] [ 0 12 1] [ 0 0 11]] precision recall f1-score support Iris-setosa 1.00 1.00 1.00 14 Iris-versicolor 1.00 0.92 0.96 13 Iris-virginica 0.92 1.00 0.96 11 accuracy 0.97 38 macro avg 0.97 0.97 0.97 38 weighted avg 0.98 0.97 0.97 38 Pros and Cons associated with SVM ... Pros: It works really well with a clear margin of separation. It is effective in high dimensional spaces. It is effective in cases where the number of dimensions is greater than the number of samples. It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient. It allows utilization of different kernel functions for the decision function which also makes it versatile. Cons: It doesn\u2019t perform well when we have large data set because the required training time is higher. It also doesn\u2019t perform very well, when the data set has more noise i.e. target classes are overlapping. This notebook was inspired by several blogposts including: \"Understanding Support Vector Machine(SVM) algorithm from examples (along with code)\" by SUNIL RAY available at *https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/ \"A Quick Guide To Learn Support Vector Machine In Python\" by Mohammad Waseem available at *https://www.edureka.co/blog/support-vector-machine-in-python/ Here are some great reads on these topics: - \"Support Vector Machine \u2013 Simplified\" by TAVISH SRIVASTAVA available at https://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/?utm_source=blog&utm_medium=understandingsupportvectormachinearticle - \"Creating a simple binary SVM classifier with Python and Scikit-learn\" by Chris available at https://www.machinecurve.com/index.php/2020/05/03/creating-a-simple-binary-svm-classifier-with-python-and-scikit-learn/#summary - \"Support Vector Machine introduction\" available at https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/ - \"Classifying data using Support Vector Machines(SVMs) in Python\" available at https://www.geeksforgeeks.org/classifying-data-using-support-vector-machinessvms-in-python/ - \"Support Vector Machines explained with Python examples\" by Carolina Bento available at *https://towardsdatascience.com/support-vector-machines-explained-with-python-examples-cb65e8172c85 Here are some great videos on these topics: - \"Support Vector Machines, Clearly Explained!!!\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=efR1C6CvhmE - \"Support Vector Machine (SVM) - Fun and Easy Machine Learning\" by Augmented Startups available at https://www.youtube.com/watch?v=Y6RRHw9uN9o - \"How SVM (Support Vector Machine) algorithm works\" by Thales Sehn K\u00f6rting available at *https://www.youtube.com/watch?v=1NxnPkZM9bc Exercise: Who would you trust? A tree or a machine? What are some advantages and disadvantages the Random Forest algorithm against the Support Vector Machines? Make sure to cite any resources that you may use.","title":"Lab24"},{"location":"8-Labs/Newly Formatted/Lab24/#laboratory-24-a-tale-of-supportive-machines","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 24: \"A Tale of Supportive Machines!\" "},{"location":"8-Labs/Newly Formatted/Lab24/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab24/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab24/#title-of-the-notebook","text":"","title":"Title of the notebook"},{"location":"8-Labs/Newly Formatted/Lab24/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab24/#support-vector-machines-svm","text":"","title":"Support Vector Machines (SVM)"},{"location":"8-Labs/Newly Formatted/Lab24/#support-vector-machine-svm-is-a-supervised-machine-learning-algorithm-which-can-be-used-for-both-classification-or-regression-challenges-however-it-is-mostly-used-in-classification-problems-in-the-svm-algorithm-we-plot-each-data-item-as-a-point-in-n-dimensional-space-where-n-is-number-of-features-you-have-with-the-value-of-each-feature-being-the-value-of-a-particular-coordinate-then-we-perform-classification-by-finding-the-hyper-plane-that-differentiates-the-two-classes-very-well-look-at-the-below-snapshot","text":"Hyperplane: In geometry, a hyperplane is a subspace whose dimension is one less than that of its ambient space. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines. (@Wikipedia: https://en.wikipedia.org/wiki/Hyperplane)","title":"\u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well (look at the below snapshot). "},{"location":"8-Labs/Newly Formatted/Lab24/#support-vectors-are-simply-the-co-ordinates-of-individual-observation-the-svm-classifier-is-a-frontier-which-best-segregates-the-two-classes-hyper-plane-line","text":"","title":"Support Vectors are simply the co-ordinates of individual observation. The SVM classifier is a frontier which best segregates the two classes (hyper-plane/ line). "},{"location":"8-Labs/Newly Formatted/Lab24/#how-does-it-work","text":"","title":"How does it work?"},{"location":"8-Labs/Newly Formatted/Lab24/#identify-the-right-hyper-plane-scenario-1","text":"Here, we have three hyper-planes (A, B and C). Now, identify the right hyper-plane to classify star and circle. Remember a thumb rule to identify the right hyper-plane: \u201cSelect the hyper-plane which segregates the two classes better\u201d. In this scenario, hyper-plane \u201cB\u201d has excellently performed this job.","title":"Identify the right hyper-plane (Scenario-1):"},{"location":"8-Labs/Newly Formatted/Lab24/#identify-the-right-hyper-plane-scenario-2","text":"Here, we have three hyper-planes (A, B and C) and all are segregating the classes well. Now, How can we identify the right hyper-plane? Here, maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. This distance is called as Margin. Let\u2019s look at the below snapshot: Below, you can see that the margin for hyper-plane C is high as compared to both A and B. Hence, we name the right hyper-plane as C. Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification.","title":"Identify the right hyper-plane (Scenario-2):"},{"location":"8-Labs/Newly Formatted/Lab24/#identify-the-right-hyper-plane-scenario-3","text":"Hint: Use the rules as discussed in previous section to identify the right hyper-plane Some of you may have selected the hyper-plane B as it has higher margin compared to A. But, here is the catch, SVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin. Here, hyper-plane B has a classification error and A has classified all correctly. Therefore, the right hyper-plane is A.","title":"Identify the right hyper-plane (Scenario-3):"},{"location":"8-Labs/Newly Formatted/Lab24/#identify-the-right-hyper-plane-scenario-4","text":"Below, I am unable to segregate the two classes using a straight line, as one of the stars lies in the territory of other(circle) class as an outlier. As I have already mentioned, one star at other end is like an outlier for star class. The SVM algorithm has a feature to ignore outliers and find the hyper-plane that has the maximum margin. Hence, we can say, SVM classification is robust to outliers.","title":"Identify the right hyper-plane (Scenario-4):"},{"location":"8-Labs/Newly Formatted/Lab24/#identify-the-right-hyper-plane-scenario-5","text":"In the scenario below, we can\u2019t have linear hyper-plane between the two classes, so how does SVM classify these two classes? Till now, we have only looked at the linear hyper-plane. SVM can solve this problem. Easily! It solves this problem by introducing additional feature. Here, we will add a new feature z=x^2+y^2. Now, let\u2019s plot the data points on axis x and z: In above plot, points to consider are: - All values for z would be positive always because z is the squared sum of both x and y - In the original plot, red circles appear close to the origin of x and y axes, leading to lower value of z and star relatively away from the origin result to higher value of z. In the SVM classifier, it is easy to have a linear hyper-plane between these two classes. But, another burning question which arises is, should we need to add this feature manually to have a hyper-plane. No, the SVM algorithm has a technique called the kernel trick. The SVM kernel is a function that takes low dimensional input space and transforms it to a higher dimensional space i.e. it converts not separable problem to separable problem. It is mostly useful in non-linear separation problem. Simply put, it does some extremely complex data transformations, then finds out the process to separate the data based on the labels or outputs you\u2019ve defined. When we look at the hyper-plane in original input space it looks like a circle: From another perspective, this is what's happening: There are different Kernel functions that can be used with SVMs: Depending on the nature of the problem, different Kernels may be advantagous:","title":"Identify the right hyper-plane (Scenario-5):"},{"location":"8-Labs/Newly Formatted/Lab24/#lets-see-how-to-implement-svm-in-python","text":"","title":"Let's see how to implement SVM in Python!"},{"location":"8-Labs/Newly Formatted/Lab24/#example-re-using-the-iris-plants-classification","text":"","title":"Example: Re-using the Iris Plants Classification "},{"location":"8-Labs/Newly Formatted/Lab24/#the-iris-flower-dataset-involves-predicting-the-flower-species-given-measurements-of-iris-flowers-the-iris-data-set-contains-information-on-sepal-length-sepal-width-petal-length-petal-width-all-in-cm-and-class-of-iris-plants-the-data-set-contains-3-classes-of-50-instances-each","text":"","title":"The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers. The Iris Data Set contains information on sepal length, sepal width, petal length, petal width all in cm, and class of iris plants. The data set contains 3 classes of 50 instances each."},{"location":"8-Labs/Newly Formatted/Lab24/#lets-use-svm-in-python-and-see-if-we-can-classifity-iris-plants-based-on-the-four-given-predictors","text":"Acknowledgements 1. Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950). 2. Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis. (Q327.D83) John Wiley & Sons. ISBN 0-471-22361-1. See page 218. 3. Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System Structure and Classification Rule for Recognition in Partially Exposed Environments\". IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-2, No. 1, 67-71. 4. Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\". IEEE Transactions on Information Theory, May 1972, 431-433. 5. See also: 1988 MLC Proceedings, 54-64. Cheeseman et al's AUTOCLASS II conceptual clustering system finds 3 classes in the data.","title":"Let's use SVM in Python and see if we can classifity iris plants based on the four given predictors."},{"location":"8-Labs/Newly Formatted/Lab24/#as-you-know-by-now-the-first-step-is-to-load-some-necessary-libraries","text":"import numpy as np import pandas as pd from matplotlib import pyplot as plt import sklearn.metrics as metrics import seaborn as sns %matplotlib inline from sklearn import datasets #There is a version of the iris database in the sklearn package from sklearn import svm #The function for applyin SVM # import some data to play with iris = datasets.load_iris() X = iris.data[:, :2] # Let's say we only take the first two features: Sepal Width and Sepal Length y = iris.target # we create an instance of SVM and fit our data. svc = svm.SVC(kernel='linear').fit(X, y) # create a mesh to plot in x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 h = (x_max / x_min)/100 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired) plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.xlim(xx.min(), xx.max()) plt.title('SVC with linear kernel') plt.show() plt.subplot(1, 1, 1) Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], cmap=plt.cm.Paired, c=y) plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.xlim(xx.min(), xx.max()) plt.title('SVC with linear kernel') plt.show() svc = svm.SVC(kernel='rbf').fit(X, y) C:\\Users\\Farha\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning. \"avoid this warning.\", FutureWarning) plt.subplot(1, 1, 1) Z = svc.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired) plt.xlabel('Sepal length') plt.ylabel('Sepal width') plt.xlim(xx.min(), xx.max()) plt.title('SVC with linear kernel') plt.show() gamma: Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019. Higher the value of gamma, will try to exact fit the as per training data set i.e. generalization error and cause over-fitting problem. C: Penalty parameter C of the error term. It also controls the trade-off between smooth decision boundaries and classifying the training points correctly.","title":"As you know by now, the first step is to load some necessary libraries:"},{"location":"8-Labs/Newly Formatted/Lab24/#trying-the-route-similar-to-what-we-did-before-we-should-read-the-dataset-and-explore-it-using-tools-such-as-descriptive-statistics","text":"dataset = pd.read_csv('iris.csv') dataset.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal-length sepal-width petal-length petal-width Class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa","title":"Trying the route similar to what we did before, we should read the dataset and explore it using tools such as descriptive statistics:"},{"location":"8-Labs/Newly Formatted/Lab24/#we-should-seperate-the-predictors-and-target-similar-to-what-we-did-for-logisitc-regression","text":"X = dataset.iloc[:, :-1].values y = dataset.iloc[:, 4].values","title":"We should seperate the predictors and target - similar to what we did for logisitc regression:"},{"location":"8-Labs/Newly Formatted/Lab24/#now-we-split-the-training-and-testing-datasets-with-a-075025-ratio","text":"from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","title":"Now we split the training and testing datasets with a 0.75/0.25 ratio:"},{"location":"8-Labs/Newly Formatted/Lab24/#then-we-create-an-instance-of-svm-and-fit-our-data","text":"from sklearn import svm #create a classifier cls = svm.SVC(kernel=\"linear\") #train the model cls.fit(X_train,y_train) #predict the response pred = cls.predict(X_test)","title":"Then, we create an instance of SVM and fit our data:"},{"location":"8-Labs/Newly Formatted/Lab24/#and-we-can-evaluate-the-performance-of-our-svm-model","text":"from sklearn.metrics import classification_report, confusion_matrix print(confusion_matrix(y_test, pred)) print(metrics.classification_report(y_test, y_pred=pred)) [[14 0 0] [ 0 12 1] [ 0 0 11]] precision recall f1-score support Iris-setosa 1.00 1.00 1.00 14 Iris-versicolor 1.00 0.92 0.96 13 Iris-virginica 0.92 1.00 0.96 11 accuracy 0.97 38 macro avg 0.97 0.97 0.97 38 weighted avg 0.98 0.97 0.97 38","title":"And we can evaluate the performance of our SVM model:"},{"location":"8-Labs/Newly Formatted/Lab24/#pros-and-cons-associated-with-svm","text":"","title":"Pros and Cons associated with SVM ..."},{"location":"8-Labs/Newly Formatted/Lab24/#pros","text":"It works really well with a clear margin of separation. It is effective in high dimensional spaces. It is effective in cases where the number of dimensions is greater than the number of samples. It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient. It allows utilization of different kernel functions for the decision function which also makes it versatile.","title":"Pros:"},{"location":"8-Labs/Newly Formatted/Lab24/#cons","text":"It doesn\u2019t perform well when we have large data set because the required training time is higher. It also doesn\u2019t perform very well, when the data set has more noise i.e. target classes are overlapping. This notebook was inspired by several blogposts including: \"Understanding Support Vector Machine(SVM) algorithm from examples (along with code)\" by SUNIL RAY available at *https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/ \"A Quick Guide To Learn Support Vector Machine In Python\" by Mohammad Waseem available at *https://www.edureka.co/blog/support-vector-machine-in-python/ Here are some great reads on these topics: - \"Support Vector Machine \u2013 Simplified\" by TAVISH SRIVASTAVA available at https://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/?utm_source=blog&utm_medium=understandingsupportvectormachinearticle - \"Creating a simple binary SVM classifier with Python and Scikit-learn\" by Chris available at https://www.machinecurve.com/index.php/2020/05/03/creating-a-simple-binary-svm-classifier-with-python-and-scikit-learn/#summary - \"Support Vector Machine introduction\" available at https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/ - \"Classifying data using Support Vector Machines(SVMs) in Python\" available at https://www.geeksforgeeks.org/classifying-data-using-support-vector-machinessvms-in-python/ - \"Support Vector Machines explained with Python examples\" by Carolina Bento available at *https://towardsdatascience.com/support-vector-machines-explained-with-python-examples-cb65e8172c85 Here are some great videos on these topics: - \"Support Vector Machines, Clearly Explained!!!\" by StatQuest with Josh Starmer available at https://www.youtube.com/watch?v=efR1C6CvhmE - \"Support Vector Machine (SVM) - Fun and Easy Machine Learning\" by Augmented Startups available at https://www.youtube.com/watch?v=Y6RRHw9uN9o - \"How SVM (Support Vector Machine) algorithm works\" by Thales Sehn K\u00f6rting available at *https://www.youtube.com/watch?v=1NxnPkZM9bc","title":"Cons:"},{"location":"8-Labs/Newly Formatted/Lab24/#exercise-who-would-you-trust-a-tree-or-a-machine","text":"","title":"Exercise: Who would you trust? A tree or a machine?  "},{"location":"8-Labs/Newly Formatted/Lab24/#what-are-some-advantages-and-disadvantages-the-random-forest-algorithm-against-the-support-vector-machines","text":"","title":"What are some advantages and disadvantages the Random Forest algorithm against the Support Vector Machines?"},{"location":"8-Labs/Newly Formatted/Lab24/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab3/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab3 Laboratory 3: Structures and Conditions. # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Data Structures: List (Array) A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\ x_1= 11 \\ x_2= 5 \\ x_3= 9 \\ x_4= 13 \\ ... \\ x_N= 223 \\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO. Alot of other lnguages start at ONE. Its just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9. MyOtherList = [] #Create an empty list MyOtherList.append(765) #Add one item to the list print(MyOtherList) MyList = [7,11,5,9,13,66,99,223] #Define a list print(MyList) sublist = MyList[3:6] #slice a sublist print(\"sublist is: \", sublist) mysum = sum(sublist) #sum the numbers in the sublist print(\"Sum: \", mysum) mylength = len(sublist) #get the length of the sublist print(\"Length: \", mylength) [765] [7, 11, 5, 9, 13, 66, 99, 223] sublist is: [9, 13, 66] Sum: 88 Length: 3 Data Structures: Special List | Tuple A tuple is a special kind of list where the values cannot be changed after the list is created. It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") Data Structures: Special List | Dictionary A dictionary is a special kind of list where the items are related data PAIRS. It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Some examples follow: MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") MyTupleName ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec') MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} print(MyPetsNamesAndMass) MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print(MyPetsNamesAndMassToo) {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} # Tuples MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") # Access a Tuple print (\"5th element of the tuple:\", MyTupleName[4]) # Dictionary MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} # Access the Dictionary print (\"Aspen's mass = \", MyPetsNamesAndMass[\"Aspen\"]) # Change a value in a dictionary print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"]) MyPetsNamesAndMass[\"Merrimee\"] = 0.01 print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"], \"She lost weight !\") # Alternate dictionary MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print (\"Merrimee's mass\" , MyPetsNamesAndMassToo[\"Merrimee\"]) # Attempt to change a Tuple #MyTupleName[3]=(\"Fred\") # Activate this line and see what happens! 5th element of the tuple: May Aspen's mass = 6.3 Merrimee's mass 0.03 Merrimee's mass 0.01 She lost weight ! Merrimee's mass 0.03 Example: Nested Dictionary From the dictionary below, print \"Pandemic\" and \"Tokyo\": FD = {\"Quentin\":\"Tarantino\",\"2020\":[2020,\"COVID\",19,\"Pandemic\"],\"Bond\":[\"James\",\"Gun\",(\"Paris\",\"Tokyo\",\"London\")]} #A nested dictionary print(FD) {'Quentin': 'Tarantino', '2020': [2020, 'COVID', 19, 'Pandemic'], 'Bond': ['James', 'Gun', ('Paris', 'Tokyo', 'London')]} FD['2020'][3] 'Pandemic' FD['Bond'][2][1] 'Tokyo' Conditional Execution Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, butmore recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs. Conditional Execution: Comparison The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal. # Compare x = 7 y = 10 print(\"x =: \",x,\"y =: \",y) print(\"x is equal to y : \",x==y) print(\"x is not equal to y : \",x!=y) print(\"x is greater than y : \",x>y) print(\"x is less than y : \",x<y) x =: 7 y =: 10 x is equal to y : False x is not equal to y : True x is greater than y : False x is less than y : True # Logical operators print(\"5 == 5 and 5 < 6 ? \",5 == 5 and 5 < 6) print(\"4 > 3 or 17 > 20 \",4 > 3 or 17 > 20) print(\"not 5 == 5\",not 5 == 5) 5 == 5 and 5 < 6 ? True 4 > 3 or 17 > 20 True not 5 == 5 False Conditional Execution: Block if statement The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter. The next code fragment illustrates illustrates how the if statements work. The program asks the user for input. The use of raw_input() will let the program read any input as a string so non-numeric results will not throw an error. The input is stored in the variable named userInput . Next the statement if userInput == \"1\": compares the value of userInput with the string \"1\" . If the value in the variable is indeed \\1\", then the program will execute the block of code in the indentation after the colon. In this case it will execute print \"Hello World\" print \"How do you do? \" Alternatively, if the value of userInput is the string '2' , then the program will execute print \"Snakes on a plane \" For all other values the program will execute print \"You did not enter a valid number\" # Block if example userInput = input('Enter the number 1 or 2') # Use block if structure if userInput == '1': print(\"Hello World\") print(\"How do you do? \") elif userInput == '2': print(\"Snakes on a plane \") else: print(\"You did not enter a valid number\") Enter the number 1 or 21 Hello World How do you do? Conditional Execution: Inline if statement An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops. myInt = 0 num1 = 12 if (myInt == 0) else 13 num1 12 Example: Pass or Fail? Take the following inputs from the user: 1. Grade for Lesson 1 (from 0 to 5) 2. Grade for Lesson 2 (from 0 to 5) 3. Grade for Lesson 3 (from 0 to 5) Compute the average of the three grades. Use the result to decide whether the student will pass or fail. Lesson1 = int(input('Enter the grade for Lesson 1')) Lesson2 = int(input('Enter the grade for Lesson 2')) Lesson3 = int(input('Enter the grade for Lesson 3')) Average = int(Lesson1+Lesson2+Lesson3)/3 print('Average Course Grade:',Average) if Average >= 5: print(\"Passed\") else: print(\"Failed\") Enter the grade for Lesson 12 Enter the grade for Lesson 25 Enter the grade for Lesson 31 Average Course Grade: 2.6666666666666665 Failed Here are some great reads on this topic: - \"Common Python Data Structures (Guide)\" by Dan Bader available at https://realpython.com/python-data-structures/ - \"Data Structures You Need To Learn In Python\" by Akash available at https://www.edureka.co/blog/data-structures-in-python/ - \"Data Structures in Python\u2014 A Brief Introduction\" by Sowmya Krishnan available at https://towardsdatascience.com/data-structures-in-python-a-brief-introduction-b4135d7a9b7d - \"Everything you Should Know About Data Structures in Python\" by ANIRUDDHA BHANDARI available at https://www.analyticsvidhya.com/blog/2020/06/data-structures-python/ - \"Conditional Statements in Python\" by John Sturtz available at https://realpython.com/python-conditional-statements/ - \"Python If Statement explained with examples\" by CHAITANYA SINGH available at https://beginnersbook.com/2018/01/python-if-statement-example/ Here are some great videos on these topics: - \"Python: Data Structures - Lists, Tuples, Sets & Dictionaries tutorial\" by Joe James available at https://www.youtube.com/watch?v=R-HLU9Fl5ug&t=92s - \"Python Tutorial for Beginners 5: Dictionaries - Working with Key-Value Pairs\" by Corey Schafer available at https://www.youtube.com/watch?v=daefaLgNkw0 - \"How to Use If Else Statements in Python (Python Tutorial #2)\" by CS Dojo available at https://www.youtube.com/watch?v=AWek49wXGzI - \"Python If Statements | Python Tutorial #10\" by Amigoscode available at https://www.youtube.com/watch?v=wKQRmXR3jhc Exercise: Why dictionaries? Why do we need to use dictionaries in python? * Make sure to cite any resources that you may use.","title":"Lab3"},{"location":"8-Labs/Newly Formatted/Lab3/#laboratory-3-structures-and-conditions","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 3: Structures and Conditions. "},{"location":"8-Labs/Newly Formatted/Lab3/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab3/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab3/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Newly Formatted/Lab3/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab3/#data-structures-list-array","text":"A list is a collection of data that are somehow related. It is a convenient way to refer to a collection of similar things by a single name, and using an index (like a subscript in math) to identify a particular item. Consider the \"math-like\" variable x below: \\begin{gather} x_0= 7 \\ x_1= 11 \\ x_2= 5 \\ x_3= 9 \\ x_4= 13 \\ ... \\ x_N= 223 \\ \\end{gather} The variable name is x and the subscripts correspond to different values. Thus the value of the variable named x associated with subscript 3 is the number 9 . The figure below is a visual representation of a the concept that treats a variable as a collection of cells. In the figure, the variable name is MyList , the subscripts are replaced by an index which identifies which cell is being referenced. The value is the cell content at the particular index. So in the figure the value of MyList at Index = 3 is the number 9.' In engineering and data science we use lists a lot - we often call then vectors, arrays, matrices and such, but they are ultimately just lists. To declare a list you can write the list name and assign it values. The square brackets are used to identify that the variable is a list. Like: MyList = [7,11,5,9,13,66,99,223] One can also declare a null list and use the append() method to fill it as needed. MyOtherList = [ ] Python indices start at ZERO. Alot of other lnguages start at ONE. Its just the convention. The first element in a list has an index of 0, the second an index of 1, and so on. We access the contents of a list by referring to its name and index. For example MyList[3] has a value of the number 9. MyOtherList = [] #Create an empty list MyOtherList.append(765) #Add one item to the list print(MyOtherList) MyList = [7,11,5,9,13,66,99,223] #Define a list print(MyList) sublist = MyList[3:6] #slice a sublist print(\"sublist is: \", sublist) mysum = sum(sublist) #sum the numbers in the sublist print(\"Sum: \", mysum) mylength = len(sublist) #get the length of the sublist print(\"Length: \", mylength) [765] [7, 11, 5, 9, 13, 66, 99, 223] sublist is: [9, 13, 66] Sum: 88 Length: 3","title":"Data Structures: List (Array)"},{"location":"8-Labs/Newly Formatted/Lab3/#data-structures-special-list-tuple","text":"A tuple is a special kind of list where the values cannot be changed after the list is created. It is useful for list-like things that are static - like days in a week, or months of a year. You declare a tuple like a list, except use round brackets instead of square brackets. MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")","title":"Data Structures: Special List | Tuple"},{"location":"8-Labs/Newly Formatted/Lab3/#data-structures-special-list-dictionary","text":"A dictionary is a special kind of list where the items are related data PAIRS. It is a lot like a relational database (it probably is one in fact) where the first item in the pair is called the key, and must be unique in a dictionary, and the second item in the pair is the data. The second item could itself be a list, so a dictionary would be a meaningful way to build a database in Python. To declare a dictionary using curly brackets MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} To declare a dictionary using the dict() method MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) Some examples follow: MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") MyTupleName ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec') MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} print(MyPetsNamesAndMass) MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print(MyPetsNamesAndMassToo) {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} {'Dusty': 7.8, 'Aspen': 6.3, 'Merrimee': 0.03} # Tuples MyTupleName = (\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\") # Access a Tuple print (\"5th element of the tuple:\", MyTupleName[4]) # Dictionary MyPetsNamesAndMass = { \"Dusty\":7.8 , \"Aspen\":6.3, \"Merrimee\":0.03} # Access the Dictionary print (\"Aspen's mass = \", MyPetsNamesAndMass[\"Aspen\"]) # Change a value in a dictionary print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"]) MyPetsNamesAndMass[\"Merrimee\"] = 0.01 print (\"Merrimee's mass\" , MyPetsNamesAndMass[\"Merrimee\"], \"She lost weight !\") # Alternate dictionary MyPetsNamesAndMassToo = dict(Dusty = 7.8 , Aspen = 6.3, Merrimee = 0.03) print (\"Merrimee's mass\" , MyPetsNamesAndMassToo[\"Merrimee\"]) # Attempt to change a Tuple #MyTupleName[3]=(\"Fred\") # Activate this line and see what happens! 5th element of the tuple: May Aspen's mass = 6.3 Merrimee's mass 0.03 Merrimee's mass 0.01 She lost weight ! Merrimee's mass 0.03","title":"Data Structures: Special List | Dictionary"},{"location":"8-Labs/Newly Formatted/Lab3/#example-nested-dictionary","text":"From the dictionary below, print \"Pandemic\" and \"Tokyo\": FD = {\"Quentin\":\"Tarantino\",\"2020\":[2020,\"COVID\",19,\"Pandemic\"],\"Bond\":[\"James\",\"Gun\",(\"Paris\",\"Tokyo\",\"London\")]} #A nested dictionary print(FD) {'Quentin': 'Tarantino', '2020': [2020, 'COVID', 19, 'Pandemic'], 'Bond': ['James', 'Gun', ('Paris', 'Tokyo', 'London')]} FD['2020'][3] 'Pandemic' FD['Bond'][2][1] 'Tokyo'","title":"Example: Nested Dictionary"},{"location":"8-Labs/Newly Formatted/Lab3/#conditional-execution","text":"Conditional statements are logical expressions that evaluate as TRUE or FALSE and using these results to perform further operations based on these conditions. All flow control in a program depends on evaluating conditions. The program will proceed diferently based on the outcome of one or more conditions - really sophisticated AI programs are a collection of conditions and correlations. Amazon knowing what you kind of want is based on correlations of your past behavior compared to other peoples similar, butmore recent behavior, and then it uses conditional statements to decide what item to offer you in your recommendation items. It's spooky, but ultimately just a program running in the background trying to make your money theirs.","title":"Conditional Execution"},{"location":"8-Labs/Newly Formatted/Lab3/#conditional-execution-comparison","text":"The most common conditional operation is comparison. If we wish to compare whether two variables are the same we use the == (double equal sign). For example x == y means the program will ask whether x and y have the same value. If they do, the result is TRUE if not then the result is FALSE. Other comparison signs are != does NOT equal, < smaller than, > larger than, <= less than or equal, and >= greater than or equal. There are also three logical operators when we want to build multiple compares (multiple conditioning); these are and , or , and not . The and operator returns TRUE if (and only if) all conditions are TRUE. For instance 5 == 5 and 5 < 6 will return a TRUE because both conditions are true. The or operator returns TRUE if at least one condition is true. If all conditions are FALSE, then it will return a FALSE. For instance 4 > 3 or 17 > 20 or 3 == 2 will return TRUE because the first condition is true. The not operator returns TRUE if the condition after the not keyword is false. Think of it as a way to do a logic reversal. # Compare x = 7 y = 10 print(\"x =: \",x,\"y =: \",y) print(\"x is equal to y : \",x==y) print(\"x is not equal to y : \",x!=y) print(\"x is greater than y : \",x>y) print(\"x is less than y : \",x<y) x =: 7 y =: 10 x is equal to y : False x is not equal to y : True x is greater than y : False x is less than y : True # Logical operators print(\"5 == 5 and 5 < 6 ? \",5 == 5 and 5 < 6) print(\"4 > 3 or 17 > 20 \",4 > 3 or 17 > 20) print(\"not 5 == 5\",not 5 == 5) 5 == 5 and 5 < 6 ? True 4 > 3 or 17 > 20 True not 5 == 5 False","title":"Conditional Execution: Comparison"},{"location":"8-Labs/Newly Formatted/Lab3/#conditional-execution-block-if-statement","text":"The if statement is a common flow control statement. It allows the program to evaluate if a certain condition is satisfied and to perform a designed action based on the result of the evaluation. The structure of an if statement is if condition1 is met: do A elif condition 2 is met: do b elif condition 3 is met: do c else: do e The elif means \"else if\". The : colon is an important part of the structure it tells where the action begins. Also there are no scope delimiters like (), or {} . Instead Python uses indentation to isolate blocks of code. This convention is hugely important - many other coding environments use delimiters (called scoping delimiters), but Python does not. The indentation itself is the scoping delimiter. The next code fragment illustrates illustrates how the if statements work. The program asks the user for input. The use of raw_input() will let the program read any input as a string so non-numeric results will not throw an error. The input is stored in the variable named userInput . Next the statement if userInput == \"1\": compares the value of userInput with the string \"1\" . If the value in the variable is indeed \\1\", then the program will execute the block of code in the indentation after the colon. In this case it will execute print \"Hello World\" print \"How do you do? \" Alternatively, if the value of userInput is the string '2' , then the program will execute print \"Snakes on a plane \" For all other values the program will execute print \"You did not enter a valid number\" # Block if example userInput = input('Enter the number 1 or 2') # Use block if structure if userInput == '1': print(\"Hello World\") print(\"How do you do? \") elif userInput == '2': print(\"Snakes on a plane \") else: print(\"You did not enter a valid number\") Enter the number 1 or 21 Hello World How do you do?","title":"Conditional Execution:  Block if statement"},{"location":"8-Labs/Newly Formatted/Lab3/#conditional-execution-inline-if-statement","text":"An inline if statement is a simpler form of an if statement and is more convenient if you only need to perform a simple conditional task. The syntax is: do TaskA `if` condition is true `else` do TaskB An example would be myInt = 3 num1 = 12 if myInt == 0 else 13 num1 An alternative way is to enclose the condition in brackets for some clarity like myInt = 3 num1 = 12 if (myInt == 0) else 13 num1 In either case the result is that num1 will have the value 13 (unless you set myInt to 0). One can also use if to construct extremely inefficient loops. myInt = 0 num1 = 12 if (myInt == 0) else 13 num1 12","title":"Conditional Execution:  Inline if statement"},{"location":"8-Labs/Newly Formatted/Lab3/#example-pass-or-fail","text":"Take the following inputs from the user: 1. Grade for Lesson 1 (from 0 to 5) 2. Grade for Lesson 2 (from 0 to 5) 3. Grade for Lesson 3 (from 0 to 5) Compute the average of the three grades. Use the result to decide whether the student will pass or fail. Lesson1 = int(input('Enter the grade for Lesson 1')) Lesson2 = int(input('Enter the grade for Lesson 2')) Lesson3 = int(input('Enter the grade for Lesson 3')) Average = int(Lesson1+Lesson2+Lesson3)/3 print('Average Course Grade:',Average) if Average >= 5: print(\"Passed\") else: print(\"Failed\") Enter the grade for Lesson 12 Enter the grade for Lesson 25 Enter the grade for Lesson 31 Average Course Grade: 2.6666666666666665 Failed Here are some great reads on this topic: - \"Common Python Data Structures (Guide)\" by Dan Bader available at https://realpython.com/python-data-structures/ - \"Data Structures You Need To Learn In Python\" by Akash available at https://www.edureka.co/blog/data-structures-in-python/ - \"Data Structures in Python\u2014 A Brief Introduction\" by Sowmya Krishnan available at https://towardsdatascience.com/data-structures-in-python-a-brief-introduction-b4135d7a9b7d - \"Everything you Should Know About Data Structures in Python\" by ANIRUDDHA BHANDARI available at https://www.analyticsvidhya.com/blog/2020/06/data-structures-python/ - \"Conditional Statements in Python\" by John Sturtz available at https://realpython.com/python-conditional-statements/ - \"Python If Statement explained with examples\" by CHAITANYA SINGH available at https://beginnersbook.com/2018/01/python-if-statement-example/ Here are some great videos on these topics: - \"Python: Data Structures - Lists, Tuples, Sets & Dictionaries tutorial\" by Joe James available at https://www.youtube.com/watch?v=R-HLU9Fl5ug&t=92s - \"Python Tutorial for Beginners 5: Dictionaries - Working with Key-Value Pairs\" by Corey Schafer available at https://www.youtube.com/watch?v=daefaLgNkw0 - \"How to Use If Else Statements in Python (Python Tutorial #2)\" by CS Dojo available at https://www.youtube.com/watch?v=AWek49wXGzI - \"Python If Statements | Python Tutorial #10\" by Amigoscode available at https://www.youtube.com/watch?v=wKQRmXR3jhc","title":"Example: Pass or Fail?"},{"location":"8-Labs/Newly Formatted/Lab3/#exercise-why-dictionaries","text":"","title":"Exercise: Why dictionaries? "},{"location":"8-Labs/Newly Formatted/Lab3/#why-do-we-need-to-use-dictionaries-in-python","text":"","title":"Why do we need to use dictionaries in python?"},{"location":"8-Labs/Newly Formatted/Lab3/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab4/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab4 Laboratory 4: Loops, Looops, Loooooops # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Program flow control (Loops) Controlled repetition Structured FOR Loop Structured WHILE Loop Count controlled repetition Count-controlled repetition is also called definite repetition because the number of repetitions is known before the loop begins executing. When we do not know in advance the number of times we want to execute a statement, we cannot use count-controlled repetition. In such an instance, we would use sentinel-controlled repetition. A count-controlled repetition will exit after running a certain number of times. The count is kept in a variable called an index or counter. When the index reaches a certain value (the loop bound) the loop will end. Count-controlled repetition requires control variable (or loop counter) initial value of the control variable increment (or decrement) by which the control variable is modified each iteration through the loop condition that tests for the final value of the control variable We can use both for and while loops, for count controlled repetition, but the for loop in combination with the range() function is more common. Structured FOR loop We have seen the for loop already, but we will formally introduce it here. The for loop executes a block of code repeatedly until the condition in the for statement is no longer true. Looping through an iterable An iterable is anything that can be looped over - typically a list, string, or tuple. The syntax for looping through an iterable is illustrated by an example. First a generic syntax for a in iterable: print(a) Notice the colon : and the indentation. Now a specific example: Example: A Loop to Begin With! Make a list with \"Walter\", \"Jesse\", \"Gus, \"Hank\". Then, write a loop that prints all the elements of your lisk. # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for AllStrings in BB: print(AllStrings) Walter Jesse Gus Hank The range() function to create an iterable The range(begin,end,increment) function will create an iterable starting at a value of begin, in steps defined by increment ( begin += increment ), ending at end . So a generic syntax becomes for a in range(begin,end,increment): print(a) The example that follows is count-controlled repetition (increment skip if greater) # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for i in range(0,4,1): # Change the numbers, what happens? print(BB[i]) Walter Jesse Gus Hank Example: That's odd! Write a loop to print all the odd numbers between 0 and 10. # For loop with range for x in range(1,10,2): # a sequence from 2 to 5 with steps of 1 print(x) 1 3 5 7 9 Sentinel-controlled repetition When loop control is based on the value of what we are processing, sentinel-controlled repetition is used. Sentinel-controlled repetition is also called indefinite repetition because it is not known in advance how many times the loop will be executed. It is a repetition procedure for solving a problem by using a sentinel value (also called a signal value, a dummy value or a flag value) to indicate \"end of process\". The sentinel value itself need not be a part of the processed data. One common example of using sentinel-controlled repetition is when we are processing data from a file and we do not know in advance when we would reach the end of the file. We can use both for and while loops, for Sentinel controlled repetition, but the while loop is more common. Structured WHILE loop The while loop repeats a block of instructions inside the loop while a condition remainsvtrue. First a generic syntax while condition is true: execute a execute b .... Notice our friend, the colon : and the indentation again. # set a counter counter = 5 # while loop while counter > 0: print(\"Counter = \",counter) counter = counter -1 Counter = 5 Counter = 4 Counter = 3 Counter = 2 Counter = 1 The while loop structure just depicted is a \"decrement, skip if equal\" in lower level languages. The next structure, also a while loop is an \"increment, skip if greater\" structure. # set a counter counter = 0 # while loop while counter <= 5: # change this line to: while counter <= 5: what happens? print (\"Counter = \",counter) counter = counter +1 # change this line to: counter +=1 what happens? Counter = 0 Counter = 1 Counter = 2 Counter = 3 Counter = 4 Counter = 5 Nested Repetition | Loops within Loops Round like a circle in a spiral, like a wheel within a wheel Never ending or beginning on an ever spinning reel Like a snowball down a mountain, or a carnival balloon Like a carousel that's turning running rings around the moon Like a clock whose hands are sweeping past the minutes of its face And the world is like an apple whirling silently in space Like the circles that you find in the windmills of your mind! Windmills of Your Mind lyrics \u00a9 Sony/ATV Music Publishing LLC, BMG Rights Management Songwriters: Marilyn Bergman / Michel Legrand / Alan Bergman Recommended versions: Neil Diamond | Dusty Springfield | Farhad Mehrad \"Like the circles that you find in the windmills of your mind\", Nested repetition is when a control structure is placed inside of the body or main part of another control structure. break to exit out of a loop Sometimes you may want to exit the loop when a certain condition different from the counting condition is met. Perhaps you are looping through a list and want to exit when you find the first element in the list that matches some criterion. The break keyword is useful for such an operation. For example run the following program: # j = 0 for i in range(0,5,1): j += 2 print (\"i = \",i,\"j = \",j) if j == 6: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 # One Small Change j = 0 for i in range(0,5,1): j += 2 print( \"i = \",i,\"j = \",j) if j == 7: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 i = 3 j = 8 i = 4 j = 10 In the first case, the for loop only executes 3 times before the condition j == 6 is TRUE and the loop is exited. In the second case, j == 7 never happens so the loop completes all its anticipated traverses. In both cases an if statement was used within a for loop. Such \"mixed\" control structures are quite common (and pretty necessary). A while loop contained within a for loop, with several if statements would be very common and such a structure is called nested control. There is typically an upper limit to nesting but the limit is pretty large - easily in the hundreds. It depends on the language and the system architecture ; suffice to say it is not a practical limit except possibly for general-domain AI applications. We can also do mundane activities and leverage loops, arithmetic, and format codes to make useful tables like Example: Cosines in the loop! Write a loop to print a table of the cosines of numbers between 0 and 0.01 with steps of 0.001. import math # package that contains cosine print(\" Cosines \") print(\" x \",\"|\",\" cos(x) \") print(\"--------|--------\") for i in range(0,100,1): x = float(i)*0.001 print(\"%.3f\" % x, \" |\", \" %.4f \" % math.cos(x)) # note the format code and the placeholder % and syntax of using package Example: Getting the hang of it! Write a Python script that takes a real input value (a float) for x and returns the y value according to the rules below \\begin{gather} y = x~for~0 <= x < 1 \\\\ y = x^2~for~1 <= x < 2 \\\\ y = x + 2~for~2 <= x < 3 \\\\ \\end{gather} Test the script with x values of 0.0, 1.0, 1.1, and 2.1. add functionality to automaticaly populate the table below: x y(x) 0.0 1.0 2.0 3.0 4.0 5.0 userInput = input('Enter enter a float') #ask for user's input x = float(userInput) print(\"x:\", x) if x >= 0 and x < 1: y = x print(\"y is equal to\",y) elif x >= 1 and x < 2: y = x*x print(\"y is equal to\",y) else: y = x+2 print(\"y is equal to\",y) # without pretty table print(\"---x---\",\"|\",\"---y---\") print(\"--------|--------\") for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) elif x >= 1 and x < 2: y = x*x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) else: y = x+2 print(\"%4.f\" % x, \" |\", \" %4.f \" % y) # with pretty table from prettytable import PrettyTable #Required to create tables t = PrettyTable(['x', 'y']) #Define an empty table for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) #will add a row to the table \"t\" elif x >= 1 and x < 2: y = x*x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) else: y = x+2 print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) print(t) The continue statement The continue instruction skips the block of code after it is executed for that iteration. It is best illustrated by an example. j = 0 for i in range(0,5,1): j += 2 print (\"\\n i = \", i , \", j = \", j) #here the \\n is a newline command if j == 6: continue print(\" this message will be skipped over if j = 6 \") # still within the loop, so the skip is implemented #When j ==6 the line after the continue keyword is not printed. #Other than that one difference the rest of the script runs normally. i = 0 , j = 2 this message will be skipped over if j = 6 i = 1 , j = 4 this message will be skipped over if j = 6 i = 2 , j = 6 i = 3 , j = 8 this message will be skipped over if j = 6 i = 4 , j = 10 this message will be skipped over if j = 6 The try , except structure An important control structure (and a pretty cool one for error trapping) is the try , except statement. The statement controls how the program proceeds when an error occurs in an instruction. The structure is really useful to trap likely errors (divide by zero, wrong kind of input) yet let the program keep running or at least issue a meaningful message to the user. The syntax is: try: do something except: do something else if ``do something'' returns an error Here is a really simple, but hugely important example: #MyErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition try: print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) except: print (\"error divide by zero\") y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 error divide by zero x = 12.0 y = -1.0 x/y = -12.0 x = 12.0 y = -2.0 x/y = -6.0 x = 12.0 y = -3.0 x/y = -4.0 x = 12.0 y = -4.0 x/y = -3.0 x = 12.0 y = -5.0 x/y = -2.4 x = 12.0 y = -6.0 x/y = -2.0 x = 12.0 y = -7.0 x/y = -1.7142857142857142 x = 12.0 y = -8.0 x/y = -1.5 x = 12.0 y = -9.0 x/y = -1.3333333333333333 x = 12.0 y = -10.0 x/y = -1.2 x = 12.0 y = -11.0 x/y = -1.0909090909090908 x = 12.0 y = -12.0 x/y = -1.0 So this silly code starts with x fixed at a value of 12, and y starting at 12 and decreasing by 1 until y equals -1. The code returns the ratio of x to y and at one point y is equal to zero and the division would be undefined. By trapping the error the code can issue us a measure and keep running. Modify the script as shown below,Run, and see what happens #NoErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) <ipython-input-31-82eeaceb9a12> in <module> 3 y = 12. 4 while y >= -12.: # sentinel controlled repetition ----> 5 print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) 6 y -= 1 ZeroDivisionError: float division by zero Here are some great reads on this topic: - \"Python for Loop\" available at https://www.programiz.com/python-programming/for-loop/ - \"Python \"for\" Loops (Definite Iteration)\" by John Sturtz available at https://realpython.com/python-for-loop/ - \"Python \"while\" Loops (Indefinite Iteration)\" by John Sturtz available at https://realpython.com/python-while-loop/ - \"loops in python\" available at https://www.geeksforgeeks.org/loops-in-python/ - \"Python Exceptions: An Introduction\" by Said van de Klundert available at *https://realpython.com/python-exceptions/ Here are some great videos on these topics: - \"Python For Loops - Python Tutorial for Absolute Beginners\" by Programming with Mosh available at https://www.youtube.com/watch?v=94UHCEmprCY - \"Python Tutorial for Beginners 7: Loops and Iterations - For/While Loops\" by Corey Schafer available at https://www.youtube.com/watch?v=6iF8Xb7Z3wQ - \"Python 3 Programming Tutorial - For loop\" by sentdex available at *https://www.youtube.com/watch?v=xtXexPSfcZg Exercise: FOR or WHILE? 1000 people have asked to be enlisted to take the first dose of COVID-19 vaccine. You are asked to write a loop and allow the ones who meet the requirements to take the shot. What kind of loop will you use? a FOR loop or a WHILE loop? Explain the logic behind your choice briefly. * Make sure to cite any resources that you may use.","title":"Lab4"},{"location":"8-Labs/Newly Formatted/Lab4/#laboratory-4-loops-looops-loooooops","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 4: Loops, Looops, Loooooops "},{"location":"8-Labs/Newly Formatted/Lab4/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab4/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab4/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Newly Formatted/Lab4/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab4/#program-flow-control-loops","text":"Controlled repetition Structured FOR Loop Structured WHILE Loop","title":"Program flow control (Loops)"},{"location":"8-Labs/Newly Formatted/Lab4/#count-controlled-repetition","text":"Count-controlled repetition is also called definite repetition because the number of repetitions is known before the loop begins executing. When we do not know in advance the number of times we want to execute a statement, we cannot use count-controlled repetition. In such an instance, we would use sentinel-controlled repetition. A count-controlled repetition will exit after running a certain number of times. The count is kept in a variable called an index or counter. When the index reaches a certain value (the loop bound) the loop will end. Count-controlled repetition requires control variable (or loop counter) initial value of the control variable increment (or decrement) by which the control variable is modified each iteration through the loop condition that tests for the final value of the control variable We can use both for and while loops, for count controlled repetition, but the for loop in combination with the range() function is more common.","title":"Count controlled repetition"},{"location":"8-Labs/Newly Formatted/Lab4/#structured-for-loop","text":"We have seen the for loop already, but we will formally introduce it here. The for loop executes a block of code repeatedly until the condition in the for statement is no longer true.","title":"Structured FOR loop"},{"location":"8-Labs/Newly Formatted/Lab4/#looping-through-an-iterable","text":"An iterable is anything that can be looped over - typically a list, string, or tuple. The syntax for looping through an iterable is illustrated by an example. First a generic syntax for a in iterable: print(a) Notice the colon : and the indentation. Now a specific example:","title":"Looping through an iterable"},{"location":"8-Labs/Newly Formatted/Lab4/#example-a-loop-to-begin-with","text":"Make a list with \"Walter\", \"Jesse\", \"Gus, \"Hank\". Then, write a loop that prints all the elements of your lisk. # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for AllStrings in BB: print(AllStrings) Walter Jesse Gus Hank","title":"Example: A Loop to Begin With!"},{"location":"8-Labs/Newly Formatted/Lab4/#the-range-function-to-create-an-iterable","text":"The range(begin,end,increment) function will create an iterable starting at a value of begin, in steps defined by increment ( begin += increment ), ending at end . So a generic syntax becomes for a in range(begin,end,increment): print(a) The example that follows is count-controlled repetition (increment skip if greater) # set a list BB = [\"Walter\",\"Jesse\",\"Gus\",\"Hank\"] # loop thru the list for i in range(0,4,1): # Change the numbers, what happens? print(BB[i]) Walter Jesse Gus Hank","title":"The range() function to create an iterable"},{"location":"8-Labs/Newly Formatted/Lab4/#example-thats-odd","text":"Write a loop to print all the odd numbers between 0 and 10. # For loop with range for x in range(1,10,2): # a sequence from 2 to 5 with steps of 1 print(x) 1 3 5 7 9","title":"Example: That's odd!"},{"location":"8-Labs/Newly Formatted/Lab4/#sentinel-controlled-repetition","text":"When loop control is based on the value of what we are processing, sentinel-controlled repetition is used. Sentinel-controlled repetition is also called indefinite repetition because it is not known in advance how many times the loop will be executed. It is a repetition procedure for solving a problem by using a sentinel value (also called a signal value, a dummy value or a flag value) to indicate \"end of process\". The sentinel value itself need not be a part of the processed data. One common example of using sentinel-controlled repetition is when we are processing data from a file and we do not know in advance when we would reach the end of the file. We can use both for and while loops, for Sentinel controlled repetition, but the while loop is more common.","title":"Sentinel-controlled repetition"},{"location":"8-Labs/Newly Formatted/Lab4/#structured-while-loop","text":"The while loop repeats a block of instructions inside the loop while a condition remainsvtrue. First a generic syntax while condition is true: execute a execute b .... Notice our friend, the colon : and the indentation again. # set a counter counter = 5 # while loop while counter > 0: print(\"Counter = \",counter) counter = counter -1 Counter = 5 Counter = 4 Counter = 3 Counter = 2 Counter = 1 The while loop structure just depicted is a \"decrement, skip if equal\" in lower level languages. The next structure, also a while loop is an \"increment, skip if greater\" structure. # set a counter counter = 0 # while loop while counter <= 5: # change this line to: while counter <= 5: what happens? print (\"Counter = \",counter) counter = counter +1 # change this line to: counter +=1 what happens? Counter = 0 Counter = 1 Counter = 2 Counter = 3 Counter = 4 Counter = 5","title":"Structured WHILE loop"},{"location":"8-Labs/Newly Formatted/Lab4/#nested-repetition-loops-within-loops","text":"Round like a circle in a spiral, like a wheel within a wheel Never ending or beginning on an ever spinning reel Like a snowball down a mountain, or a carnival balloon Like a carousel that's turning running rings around the moon Like a clock whose hands are sweeping past the minutes of its face And the world is like an apple whirling silently in space Like the circles that you find in the windmills of your mind! Windmills of Your Mind lyrics \u00a9 Sony/ATV Music Publishing LLC, BMG Rights Management Songwriters: Marilyn Bergman / Michel Legrand / Alan Bergman Recommended versions: Neil Diamond | Dusty Springfield | Farhad Mehrad \"Like the circles that you find in the windmills of your mind\", Nested repetition is when a control structure is placed inside of the body or main part of another control structure.","title":"Nested Repetition | Loops within Loops"},{"location":"8-Labs/Newly Formatted/Lab4/#break-to-exit-out-of-a-loop","text":"Sometimes you may want to exit the loop when a certain condition different from the counting condition is met. Perhaps you are looping through a list and want to exit when you find the first element in the list that matches some criterion. The break keyword is useful for such an operation. For example run the following program: # j = 0 for i in range(0,5,1): j += 2 print (\"i = \",i,\"j = \",j) if j == 6: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 # One Small Change j = 0 for i in range(0,5,1): j += 2 print( \"i = \",i,\"j = \",j) if j == 7: break i = 0 j = 2 i = 1 j = 4 i = 2 j = 6 i = 3 j = 8 i = 4 j = 10 In the first case, the for loop only executes 3 times before the condition j == 6 is TRUE and the loop is exited. In the second case, j == 7 never happens so the loop completes all its anticipated traverses. In both cases an if statement was used within a for loop. Such \"mixed\" control structures are quite common (and pretty necessary). A while loop contained within a for loop, with several if statements would be very common and such a structure is called nested control. There is typically an upper limit to nesting but the limit is pretty large - easily in the hundreds. It depends on the language and the system architecture ; suffice to say it is not a practical limit except possibly for general-domain AI applications. We can also do mundane activities and leverage loops, arithmetic, and format codes to make useful tables like","title":"break to exit out of a loop"},{"location":"8-Labs/Newly Formatted/Lab4/#example-cosines-in-the-loop","text":"Write a loop to print a table of the cosines of numbers between 0 and 0.01 with steps of 0.001. import math # package that contains cosine print(\" Cosines \") print(\" x \",\"|\",\" cos(x) \") print(\"--------|--------\") for i in range(0,100,1): x = float(i)*0.001 print(\"%.3f\" % x, \" |\", \" %.4f \" % math.cos(x)) # note the format code and the placeholder % and syntax of using package","title":"Example: Cosines in the loop!"},{"location":"8-Labs/Newly Formatted/Lab4/#example-getting-the-hang-of-it","text":"Write a Python script that takes a real input value (a float) for x and returns the y value according to the rules below \\begin{gather} y = x~for~0 <= x < 1 \\\\ y = x^2~for~1 <= x < 2 \\\\ y = x + 2~for~2 <= x < 3 \\\\ \\end{gather} Test the script with x values of 0.0, 1.0, 1.1, and 2.1. add functionality to automaticaly populate the table below: x y(x) 0.0 1.0 2.0 3.0 4.0 5.0 userInput = input('Enter enter a float') #ask for user's input x = float(userInput) print(\"x:\", x) if x >= 0 and x < 1: y = x print(\"y is equal to\",y) elif x >= 1 and x < 2: y = x*x print(\"y is equal to\",y) else: y = x+2 print(\"y is equal to\",y) # without pretty table print(\"---x---\",\"|\",\"---y---\") print(\"--------|--------\") for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) elif x >= 1 and x < 2: y = x*x print(\"%4.f\" % x, \" |\", \" %4.f \" % y) else: y = x+2 print(\"%4.f\" % x, \" |\", \" %4.f \" % y) # with pretty table from prettytable import PrettyTable #Required to create tables t = PrettyTable(['x', 'y']) #Define an empty table for x in range(0,6,1): if x >= 0 and x < 1: y = x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) #will add a row to the table \"t\" elif x >= 1 and x < 2: y = x*x print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) else: y = x+2 print(\"for x equal to\", x, \", y is equal to\",y) t.add_row([x, y]) print(t)","title":"Example: Getting the hang of it!"},{"location":"8-Labs/Newly Formatted/Lab4/#the-continue-statement","text":"The continue instruction skips the block of code after it is executed for that iteration. It is best illustrated by an example. j = 0 for i in range(0,5,1): j += 2 print (\"\\n i = \", i , \", j = \", j) #here the \\n is a newline command if j == 6: continue print(\" this message will be skipped over if j = 6 \") # still within the loop, so the skip is implemented #When j ==6 the line after the continue keyword is not printed. #Other than that one difference the rest of the script runs normally. i = 0 , j = 2 this message will be skipped over if j = 6 i = 1 , j = 4 this message will be skipped over if j = 6 i = 2 , j = 6 i = 3 , j = 8 this message will be skipped over if j = 6 i = 4 , j = 10 this message will be skipped over if j = 6","title":"The continue statement"},{"location":"8-Labs/Newly Formatted/Lab4/#the-try-except-structure","text":"An important control structure (and a pretty cool one for error trapping) is the try , except statement. The statement controls how the program proceeds when an error occurs in an instruction. The structure is really useful to trap likely errors (divide by zero, wrong kind of input) yet let the program keep running or at least issue a meaningful message to the user. The syntax is: try: do something except: do something else if ``do something'' returns an error Here is a really simple, but hugely important example: #MyErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition try: print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) except: print (\"error divide by zero\") y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 error divide by zero x = 12.0 y = -1.0 x/y = -12.0 x = 12.0 y = -2.0 x/y = -6.0 x = 12.0 y = -3.0 x/y = -4.0 x = 12.0 y = -4.0 x/y = -3.0 x = 12.0 y = -5.0 x/y = -2.4 x = 12.0 y = -6.0 x/y = -2.0 x = 12.0 y = -7.0 x/y = -1.7142857142857142 x = 12.0 y = -8.0 x/y = -1.5 x = 12.0 y = -9.0 x/y = -1.3333333333333333 x = 12.0 y = -10.0 x/y = -1.2 x = 12.0 y = -11.0 x/y = -1.0909090909090908 x = 12.0 y = -12.0 x/y = -1.0 So this silly code starts with x fixed at a value of 12, and y starting at 12 and decreasing by 1 until y equals -1. The code returns the ratio of x to y and at one point y is equal to zero and the division would be undefined. By trapping the error the code can issue us a measure and keep running. Modify the script as shown below,Run, and see what happens #NoErrorTrap.py x = 12. y = 12. while y >= -12.: # sentinel controlled repetition print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) y -= 1 x = 12.0 y = 12.0 x/y = 1.0 x = 12.0 y = 11.0 x/y = 1.0909090909090908 x = 12.0 y = 10.0 x/y = 1.2 x = 12.0 y = 9.0 x/y = 1.3333333333333333 x = 12.0 y = 8.0 x/y = 1.5 x = 12.0 y = 7.0 x/y = 1.7142857142857142 x = 12.0 y = 6.0 x/y = 2.0 x = 12.0 y = 5.0 x/y = 2.4 x = 12.0 y = 4.0 x/y = 3.0 x = 12.0 y = 3.0 x/y = 4.0 x = 12.0 y = 2.0 x/y = 6.0 x = 12.0 y = 1.0 x/y = 12.0 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) <ipython-input-31-82eeaceb9a12> in <module> 3 y = 12. 4 while y >= -12.: # sentinel controlled repetition ----> 5 print (\"x = \", x, \"y = \", y, \"x/y = \", x/y) 6 y -= 1 ZeroDivisionError: float division by zero Here are some great reads on this topic: - \"Python for Loop\" available at https://www.programiz.com/python-programming/for-loop/ - \"Python \"for\" Loops (Definite Iteration)\" by John Sturtz available at https://realpython.com/python-for-loop/ - \"Python \"while\" Loops (Indefinite Iteration)\" by John Sturtz available at https://realpython.com/python-while-loop/ - \"loops in python\" available at https://www.geeksforgeeks.org/loops-in-python/ - \"Python Exceptions: An Introduction\" by Said van de Klundert available at *https://realpython.com/python-exceptions/ Here are some great videos on these topics: - \"Python For Loops - Python Tutorial for Absolute Beginners\" by Programming with Mosh available at https://www.youtube.com/watch?v=94UHCEmprCY - \"Python Tutorial for Beginners 7: Loops and Iterations - For/While Loops\" by Corey Schafer available at https://www.youtube.com/watch?v=6iF8Xb7Z3wQ - \"Python 3 Programming Tutorial - For loop\" by sentdex available at *https://www.youtube.com/watch?v=xtXexPSfcZg","title":"The try, except structure"},{"location":"8-Labs/Newly Formatted/Lab4/#exercise-for-or-while","text":"","title":"Exercise: FOR or WHILE? "},{"location":"8-Labs/Newly Formatted/Lab4/#1000-people-have-asked-to-be-enlisted-to-take-the-first-dose-of-covid-19-vaccine-you-are-asked-to-write-a-loop-and-allow-the-ones-who-meet-the-requirements-to-take-the-shot-what-kind-of-loop-will-you-use-a-for-loop-or-a-while-loop-explain-the-logic-behind-your-choice-briefly","text":"","title":"1000 people have asked to be enlisted to take the first dose of COVID-19 vaccine. You are asked to write a loop and allow the ones who meet the requirements to take the shot. What kind of loop will you use? a FOR loop or a WHILE loop? Explain the logic behind your choice briefly."},{"location":"8-Labs/Newly Formatted/Lab4/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab5/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab5 Laboratory 5: Working with Files # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 sensei /opt/jupyterhub/bin/python3 3.8.5 (default, May 27 2021, 13:30:53) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Background A computer file is a computer resource for recording data discretely (not in the secretive context, but specifically somewhere on a piece of hardware) in a computer storage device. Just as words can be written to paper, so can information be written to a computer file. Files can be edited and transferred through the internet on that particular computer system. There are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once. By using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times. Typically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access. File system In computing, a file system or filesystem, controls how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stops and the next begins. By separating the data into pieces and giving each piece a name, the data is isolated and identified. Taking its name from the way paper-based data management system is named, each group of data is called a \u201cfile\u201d. The structure and logic rules used to manage the groups of data and their names is called a \u201cfile system\u201d. Path A path, the general form of the name of a file or directory, specifies a unique location in a file system. A path points to a file system location by following the directory tree hierarchy expressed in a string of characters in which path components, separated by a delimiting character, represent each directory. The delimiting character is most commonly the slash (\u201d/\u201d), the backslash character (\u201d\\\u201d), or colon (\u201d:\u201d), though some operating systems may use a different delimiter. Paths are used extensively in computer science to represent the directory/file relationships common in modern operating systems, and are essential in the construction of Uniform Resource Locators (URLs). Resources can be represented by either absolute or relative paths. As an example consider the following two files: /Users/theodore/MyGit/@atomickitty/hurri-sensors/.git/Guest.conf /etc/apache2/users/Guest.conf They both have the same file name, but are located on different paths. Failure to provide the path when addressing the file can be a problem. Another way to interpret is that the two unique files actually have different names, and only part of those names is common (Guest.conf) The two names above (including the path) are called fully qualified filenames (or absolute names), a relative path (usually relative to the file or program of interest depends on where in the directory structure the file lives. If we are currently in the .git directory (the first file) the path to the file is just the filename. We have experienced path issues with dependencies on .png files - in general your JupyterLab notebooks on CoCalc can only look at the local directory which is why we have to copy files into the directory for things to work. File Types Text Files. Text files are regular files that contain information readable by the user. This information is stored in ASCII. You can display and print these files. The lines of a text file must not contain NULL characters, and none can exceed a prescribed (by architecture) length, including the new-line character. The term text file does not prevent the inclusion of control or other nonprintable characters (other than NUL). Therefore, standard utilities that list text files as inputs or outputs are either able to process the special characters gracefully or they explicitly describe their limitations within their individual sections. Binary Files. Binary files are regular files that contain information readable by the computer. Binary files may be executable files that instruct the system to accomplish a job. Commands and programs are stored in executable, binary files. Special compiling programs translate ASCII text into binary code. The only difference between text and binary files is that text files have lines of less than some length, with no NULL characters, each terminated by a new-line character. Directory Files. Directory files contain information the system needs to access all types of files, but they do not contain the actual file data. As a result, directories occupy less space than a regular file and give the file system structure flexibility and depth. Each directory entry represents either a file or a subdirectory. Each entry contains the name of the file and the file's index node reference number (i-node). The i-node points to the unique index node assigned to the file. The i-node describes the location of the data associated with the file. Directories are created and controlled by a separate set of commands. File Manipulation For this laboratory we will learn just a handfull of file manipulations which are quite useful. Files can be \"created\",\"read\",\"updated\", or \"deleted\" (CRUD). Example: Create a file, write to it. Below is an example of creating a file that does not yet exist. The script is a bit pendandic on purpose. First will use some system commands to view the contents of the local directory import sys ! del myfirstfile.txt # delete file if it exists, Use rm -f on Mac %pwd # list name of working directory, note it includes path, so it is an absolute path 'C:\\\\Users\\\\Farha' # create file example externalfile = open(\"myfirstfile.txt\",'w') # create connection to file, set to write (w), file does not need to exist mymessage = 'message in a bottle' #some object to write, in this case a string externalfile.write(mymessage)# write the contents of mymessage to the file externalfile.close() # close the file connection At this point our new file should exist, lets list the directory and see if that is so Sure enough, its there, we will use a bash command cat to look at the contents of the file. ! type myfirstfile.txt message in a bottle Thats about it, use of system commands, of course depends on the system, the examples above should work OK on CoCalc or a Macintosh; on Winderz the shell commands are a little different. If you have the linux subsystem installed then these should work as is. Example: Read from an existing file. We will continue using the file we just made, and read from it the example is below # read file example externalfile = open(\"myfirstfile.txt\",'r') # create connection to file, set to read (r), file must exist silly_string = externalfile.read() # read the contents externalfile.close() # close the file connection print(silly_string) message in a bottle Example: Update a file. This example continues with our same file, but we will now add contents without destroying existing contents. The keyword is append externalfile = open(\"myfirstfile.txt\",'a') # create connection to file, set to append (a), file does not need to exist externalfile.write('\\n') # adds a newline character what_to_add = 'I love rock-and-roll, put another dime in the jukebox baby ... \\n' externalfile.write(what_to_add) # add a string including the linefeed what_to_add = '... the waiting is the hardest part \\n' externalfile.write(what_to_add) # add a string including the linefeed mylist = [1,2,3,4,5] # a list of numbers what_to_add = ','.join(map(repr, mylist)) + \"\\n\" # one way to write the list externalfile.write(what_to_add) what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" # another way to write the list externalfile.write(what_to_add) externalfile.close() As before we can examine the contents using a shell command sent from the notebook. ! type myfirstfile.txt message in a bottle I love rock-and-roll, put another dime in the jukebox baby ... ... the waiting is the hardest part 1,2,3,4,5 1,2,3,4,5 A little discussion on the part where we wrote numbers what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" Here are descriptions of the two functions map and repr map(function, iterable, ...) Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list. repr(object) Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse quotes). It is sometimes useful to be able to access this operation as an ordinary function. For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() , otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. A class can control what this function returns for its instances by defining a repr() method. What they do in this script is important. The statement: what_to_add = \u2019,\u2019.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" is building a string that will be comprised of elements of mylist[0:len(mylist)]. The repr() function gets these elements as they are represented in the computer, the delimiter a comma is added using the join method in Python, and because everything is now a string the ... + \"\\n\" puts a linefeed character at the end of the string so the output will start a new line the next time something is written. Example: Delete a file Delete can be done by a system call as we did above to clear the local directory In a JupyterLab notebook, we can either use import sys ! del myfirstfile.txt # delete file if it exists, Use rm -f on Mac or import os os.remove(\"myfirstfile.txt\") they both have same effect, both equally dangerous to your filesystem. Learn more about CRUD with text files at https://www.guru99.com/reading-and-writing-files-in-python.html Learn more about file delete at https://www.dummies.com/programming/python/how-to-delete-a-file-in-python/ # import os file2kill = \"myfirstfile.txt\" try: os.remove(file2kill) # file must exist or will generate an exception except: pass # example of using pass to improve readability print(file2kill, \" missing or deleted !\") myfirstfile.txt missing or deleted ! Here are some great reads on this topic: - \"Python Classes and Objects\" available at https://www.geeksforgeeks.org/python-classes-and-objects/ - \"Object-Oriented Programming (OOP) in Python 3\" by David Amos available at https://realpython.com/python3-object-oriented-programming/ - \"Python File Operations \u2013 Read and Write to files with Python\" available at *https://www.journaldev.com/14408/python-read-file-open-write-delete-copy Here are some great videos on these topics: - \"Python OOP Tutorial 1: Classes and Instances\" by Corey Schafer available at https://www.youtube.com/watch?v=ZDa-Z5JzLYM - \"Python Object Oriented Programming (OOP) - For Beginners\" by Tech With Tim available at https://www.youtube.com/watch?v=JeznW_7DlB0 - \"Python Classes and Objects || Python Tutorial || Learn Python Programming\" by Socratica available at https://www.youtube.com/watch?v=apACNr7DC_s - \"Python Tutorial: File Objects - Reading and Writing to Files\" by Corey Schafer available at https://www.youtube.com/watch?v=Uh2ebFW8OYM Exercise: Your Favorite Quotation! create a text file, name it \"MyFavoriteQuotation\" . Write your favorite quotation in the file. Read the file. Add this string to it in a new line : \"And that's something I wish I had said...\" Show the final outcome. # create the \"My Favorite Quotation\" file: externalfile = open(\"MyFavoriteQuotation.txt\",'w') # create connection to file, set to write (w) myquotation = 'The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.' #My choice: quotation from Pulp Fiction externalfile.write(myquotation)# write the contents of mymessage to the file externalfile.close() # close the file connection #Let's read the file #! type MyFavoriteQuotation.txt # Let's add the string externalfile = open(\"MyFavoriteQuotation.txt\",'a') #create connection to file, set to append (a) externalfile.write('\\n') # adds a newline character what_to_add = \"And that's something I wish I had said ... \\n\" externalfile.write(what_to_add) externalfile.close() #Let's read the file one last time ! type MyFavoriteQuotation.txt The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you. And that's something I wish I had said ...","title":"Lab5"},{"location":"8-Labs/Newly Formatted/Lab5/#laboratory-5-working-with-files","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) ip-172-26-4-2 sensei /opt/jupyterhub/bin/python3 3.8.5 (default, May 27 2021, 13:30:53) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Laboratory 5: Working with Files "},{"location":"8-Labs/Newly Formatted/Lab5/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab5/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab5/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Newly Formatted/Lab5/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab5/#background","text":"A computer file is a computer resource for recording data discretely (not in the secretive context, but specifically somewhere on a piece of hardware) in a computer storage device. Just as words can be written to paper, so can information be written to a computer file. Files can be edited and transferred through the internet on that particular computer system. There are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once. By using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times. Typically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access.","title":"Background"},{"location":"8-Labs/Newly Formatted/Lab5/#file-system","text":"In computing, a file system or filesystem, controls how data is stored and retrieved. Without a file system, data placed in a storage medium would be one large body of data with no way to tell where one piece of data stops and the next begins. By separating the data into pieces and giving each piece a name, the data is isolated and identified. Taking its name from the way paper-based data management system is named, each group of data is called a \u201cfile\u201d. The structure and logic rules used to manage the groups of data and their names is called a \u201cfile system\u201d.","title":"File system"},{"location":"8-Labs/Newly Formatted/Lab5/#path","text":"A path, the general form of the name of a file or directory, specifies a unique location in a file system. A path points to a file system location by following the directory tree hierarchy expressed in a string of characters in which path components, separated by a delimiting character, represent each directory. The delimiting character is most commonly the slash (\u201d/\u201d), the backslash character (\u201d\\\u201d), or colon (\u201d:\u201d), though some operating systems may use a different delimiter. Paths are used extensively in computer science to represent the directory/file relationships common in modern operating systems, and are essential in the construction of Uniform Resource Locators (URLs). Resources can be represented by either absolute or relative paths. As an example consider the following two files: /Users/theodore/MyGit/@atomickitty/hurri-sensors/.git/Guest.conf /etc/apache2/users/Guest.conf They both have the same file name, but are located on different paths. Failure to provide the path when addressing the file can be a problem. Another way to interpret is that the two unique files actually have different names, and only part of those names is common (Guest.conf) The two names above (including the path) are called fully qualified filenames (or absolute names), a relative path (usually relative to the file or program of interest depends on where in the directory structure the file lives. If we are currently in the .git directory (the first file) the path to the file is just the filename. We have experienced path issues with dependencies on .png files - in general your JupyterLab notebooks on CoCalc can only look at the local directory which is why we have to copy files into the directory for things to work.","title":"Path"},{"location":"8-Labs/Newly Formatted/Lab5/#file-types","text":"Text Files. Text files are regular files that contain information readable by the user. This information is stored in ASCII. You can display and print these files. The lines of a text file must not contain NULL characters, and none can exceed a prescribed (by architecture) length, including the new-line character. The term text file does not prevent the inclusion of control or other nonprintable characters (other than NUL). Therefore, standard utilities that list text files as inputs or outputs are either able to process the special characters gracefully or they explicitly describe their limitations within their individual sections. Binary Files. Binary files are regular files that contain information readable by the computer. Binary files may be executable files that instruct the system to accomplish a job. Commands and programs are stored in executable, binary files. Special compiling programs translate ASCII text into binary code. The only difference between text and binary files is that text files have lines of less than some length, with no NULL characters, each terminated by a new-line character. Directory Files. Directory files contain information the system needs to access all types of files, but they do not contain the actual file data. As a result, directories occupy less space than a regular file and give the file system structure flexibility and depth. Each directory entry represents either a file or a subdirectory. Each entry contains the name of the file and the file's index node reference number (i-node). The i-node points to the unique index node assigned to the file. The i-node describes the location of the data associated with the file. Directories are created and controlled by a separate set of commands.","title":"File Types"},{"location":"8-Labs/Newly Formatted/Lab5/#file-manipulation","text":"For this laboratory we will learn just a handfull of file manipulations which are quite useful. Files can be \"created\",\"read\",\"updated\", or \"deleted\" (CRUD).","title":"File Manipulation"},{"location":"8-Labs/Newly Formatted/Lab5/#example-create-a-file-write-to-it","text":"Below is an example of creating a file that does not yet exist. The script is a bit pendandic on purpose. First will use some system commands to view the contents of the local directory import sys ! del myfirstfile.txt # delete file if it exists, Use rm -f on Mac %pwd # list name of working directory, note it includes path, so it is an absolute path 'C:\\\\Users\\\\Farha' # create file example externalfile = open(\"myfirstfile.txt\",'w') # create connection to file, set to write (w), file does not need to exist mymessage = 'message in a bottle' #some object to write, in this case a string externalfile.write(mymessage)# write the contents of mymessage to the file externalfile.close() # close the file connection At this point our new file should exist, lets list the directory and see if that is so Sure enough, its there, we will use a bash command cat to look at the contents of the file. ! type myfirstfile.txt message in a bottle Thats about it, use of system commands, of course depends on the system, the examples above should work OK on CoCalc or a Macintosh; on Winderz the shell commands are a little different. If you have the linux subsystem installed then these should work as is.","title":"Example: Create a file, write to it."},{"location":"8-Labs/Newly Formatted/Lab5/#example-read-from-an-existing-file","text":"We will continue using the file we just made, and read from it the example is below # read file example externalfile = open(\"myfirstfile.txt\",'r') # create connection to file, set to read (r), file must exist silly_string = externalfile.read() # read the contents externalfile.close() # close the file connection print(silly_string) message in a bottle","title":"Example: Read from an existing file."},{"location":"8-Labs/Newly Formatted/Lab5/#example-update-a-file","text":"This example continues with our same file, but we will now add contents without destroying existing contents. The keyword is append externalfile = open(\"myfirstfile.txt\",'a') # create connection to file, set to append (a), file does not need to exist externalfile.write('\\n') # adds a newline character what_to_add = 'I love rock-and-roll, put another dime in the jukebox baby ... \\n' externalfile.write(what_to_add) # add a string including the linefeed what_to_add = '... the waiting is the hardest part \\n' externalfile.write(what_to_add) # add a string including the linefeed mylist = [1,2,3,4,5] # a list of numbers what_to_add = ','.join(map(repr, mylist)) + \"\\n\" # one way to write the list externalfile.write(what_to_add) what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" # another way to write the list externalfile.write(what_to_add) externalfile.close() As before we can examine the contents using a shell command sent from the notebook. ! type myfirstfile.txt message in a bottle I love rock-and-roll, put another dime in the jukebox baby ... ... the waiting is the hardest part 1,2,3,4,5 1,2,3,4,5 A little discussion on the part where we wrote numbers what_to_add = ','.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" Here are descriptions of the two functions map and repr map(function, iterable, ...) Apply function to every item of iterable and return a list of the results. If additional iterable arguments are passed, function must take that many arguments and is applied to the items from all iterables in parallel. If one iterable is shorter than another it is assumed to be extended with None items. If function is None, the identity function is assumed; if there are multiple arguments, map() returns a list consisting of tuples containing the corresponding items from all iterables (a kind of transpose operation). The iterable arguments may be a sequence or any iterable object; the result is always a list. repr(object) Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse quotes). It is sometimes useful to be able to access this operation as an ordinary function. For many types, this function makes an attempt to return a string that would yield an object with the same value when passed to eval() , otherwise the representation is a string enclosed in angle brackets that contains the name of the type of the object together with additional information often including the name and address of the object. A class can control what this function returns for its instances by defining a repr() method. What they do in this script is important. The statement: what_to_add = \u2019,\u2019.join(map(repr, mylist[0:len(mylist)])) + \"\\n\" is building a string that will be comprised of elements of mylist[0:len(mylist)]. The repr() function gets these elements as they are represented in the computer, the delimiter a comma is added using the join method in Python, and because everything is now a string the ... + \"\\n\" puts a linefeed character at the end of the string so the output will start a new line the next time something is written.","title":"Example: Update a file."},{"location":"8-Labs/Newly Formatted/Lab5/#example-delete-a-file","text":"Delete can be done by a system call as we did above to clear the local directory In a JupyterLab notebook, we can either use import sys ! del myfirstfile.txt # delete file if it exists, Use rm -f on Mac or import os os.remove(\"myfirstfile.txt\") they both have same effect, both equally dangerous to your filesystem. Learn more about CRUD with text files at https://www.guru99.com/reading-and-writing-files-in-python.html Learn more about file delete at https://www.dummies.com/programming/python/how-to-delete-a-file-in-python/ # import os file2kill = \"myfirstfile.txt\" try: os.remove(file2kill) # file must exist or will generate an exception except: pass # example of using pass to improve readability print(file2kill, \" missing or deleted !\") myfirstfile.txt missing or deleted ! Here are some great reads on this topic: - \"Python Classes and Objects\" available at https://www.geeksforgeeks.org/python-classes-and-objects/ - \"Object-Oriented Programming (OOP) in Python 3\" by David Amos available at https://realpython.com/python3-object-oriented-programming/ - \"Python File Operations \u2013 Read and Write to files with Python\" available at *https://www.journaldev.com/14408/python-read-file-open-write-delete-copy Here are some great videos on these topics: - \"Python OOP Tutorial 1: Classes and Instances\" by Corey Schafer available at https://www.youtube.com/watch?v=ZDa-Z5JzLYM - \"Python Object Oriented Programming (OOP) - For Beginners\" by Tech With Tim available at https://www.youtube.com/watch?v=JeznW_7DlB0 - \"Python Classes and Objects || Python Tutorial || Learn Python Programming\" by Socratica available at https://www.youtube.com/watch?v=apACNr7DC_s - \"Python Tutorial: File Objects - Reading and Writing to Files\" by Corey Schafer available at https://www.youtube.com/watch?v=Uh2ebFW8OYM","title":"Example: Delete a file"},{"location":"8-Labs/Newly Formatted/Lab5/#exercise-your-favorite-quotation","text":"create a text file, name it \"MyFavoriteQuotation\" . Write your favorite quotation in the file. Read the file. Add this string to it in a new line : \"And that's something I wish I had said...\" Show the final outcome. # create the \"My Favorite Quotation\" file: externalfile = open(\"MyFavoriteQuotation.txt\",'w') # create connection to file, set to write (w) myquotation = 'The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you.' #My choice: quotation from Pulp Fiction externalfile.write(myquotation)# write the contents of mymessage to the file externalfile.close() # close the file connection #Let's read the file #! type MyFavoriteQuotation.txt # Let's add the string externalfile = open(\"MyFavoriteQuotation.txt\",'a') #create connection to file, set to append (a) externalfile.write('\\n') # adds a newline character what_to_add = \"And that's something I wish I had said ... \\n\" externalfile.write(what_to_add) externalfile.close() #Let's read the file one last time ! type MyFavoriteQuotation.txt The path of the righteous man is beset on all sides by the inequities of the selfish and the tyranny of evil men. Blessed is he who, in the name of charity and good will, shepherds the weak through the valley of darkness. For he is truly his brother\u2019s keeper and the finder of lost children. And I will strike down upon thee with great vengeance and furious anger those who attempt to poison and destroy my brothers. And you will know my name is the Lord when I lay my vengeance upon you. And that's something I wish I had said ...","title":"Exercise: Your Favorite Quotation! "},{"location":"8-Labs/Newly Formatted/Lab6/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab6 Laboratory 6: FUNctions # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: What is a function in Python? Functions are simply pre-written code fragments that perform a certain task. In older procedural languages functions and subroutines are similar, but a function returns a value whereas a subroutine operates on data. The difference is subtle but important. More recent thinking has functions being able to operate on data (they always could) and the value returned may be simply an exit code. An analogy are the functions in MS Excel . To add numbers, we can use the sum(range) function and type =sum(A1:A5) instead of typing =A1+A2+A3+A4+A5 Calling the Function We call a function simply by typing the name of the function or by using the dot notation. Whether we can use the dot notation or not depends on how the function is written, whether it is part of a class, and how it is imported into a program. Some functions expect us to pass data to them to perform their tasks. These data are known as parameters( older terminology is arguments, or argument list) and we pass them to the function by enclosing their values in parenthesis ( ) separated by commas. For instance, the print() function for displaying text on the screen is \\called\" by typing print('Hello World') where print is the name of the function and the literal (a string) 'Hello World' is the argument. Program flow A function, whether built-in, or added must be defined before it is called, otherwise the script will fail. Certain built-in functions \"self define\" upon start (such as print() and type() and we need not worry about those funtions). The diagram below illustrates the requesite flow control for functions that need to be defined before use. An example below will illustrate, change the cell to code and run it, you should get an error. Then fix the indicated line (remove the leading \"#\" in the import math ... line) and rerun, should get a functioning script. # reset the notebook using a magic function in JupyterLab %reset -f # An example, run once as is then activate indicated line, run again - what happens? x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic ) #import math # import the math package ## activate and rerun sqrt_by_math = math.sqrt(x) # note the dot notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) # Here is an alternative way: We just load the function that we want: # reset the notebook using a magic function in JupyterLab %reset -f # An example, run once as is then activate indicated line, run again - what happens? x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic ) from math import sqrt # import sqrt from the math package ## activate and rerun sqrt_by_math = sqrt(x) # note the notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) Using arithmetic square root of 4.0 is 2.0 Using math package square root of 4.0 is 2.0 Built-In in Primitive Python (Base install) The base Python functions and types built into it that are always available, the figure below lists those functions. Notice all have the structure of function_name() , except __import__() which has a constructor type structure, and is not intended for routine use. We will learn about constructors later. Added-In using External Packages/Modules and Libaries (e.g. math) Python is also distributed with a large number of external functions. These functions are saved in files known as modules. To use the built-in codes in Python modules, we have to import them into our programs first. We do that by using the import keyword. There are three ways to import: 1. Import the entire module by writing import moduleName; For instance, to import the random module, we write import random. To use the randrange() function in the random module, we write random.randrange( 1, 10);28 2. Import and rename the module by writing import random as r (where r is any name of your choice). Now to use the randrange() function, you simply write r.randrange(1, 10); and 3. Import specific functions from the module by writing from moduleName import name1[,name2[, ... nameN]]. For instance, to import the randrange() function from the random module, we write from random import randrange. To import multiple functions, we separate them with a comma. To import the randrange() and randint() functions, we write from random import randrange, randint. To use the function now, we do not have to use the dot notation anymore. Just write randrange( 1, 10). # Example 1 of import %reset -f import random low = 1 ; high = 10 random.randrange(low,high) #generate random number in range low to high 4 # Example 2 of import %reset -f import random as r low = 1 ; high = 10 r.randrange(low,high) 7 # Example 3 of import %reset -f from random import randrange low = 1 ; high = 10 randrange(low,high) 5 The modules that come with Python are extensive and listed at https://docs.python.org/3/py-modindex.html. There are also other modules that can be downloaded and used (just like user defined modules below). In these labs we are building primitive codes to learn how to code and how to create algorithms. For many practical cases you will want to load a well-tested package to accomplish the tasks. That exercise is saved for the end of the document. User-Built We can define our own functions in Python and reuse them throughout the program. The syntax for defining a function is: def functionName( argument ): code detailing what the function should do note the colon above and indentation ... ... return [expression] The keyword def tells the program that the indented code from the next line onwards is part of the function. The keyword return tells the program to return an answer from the function. There can be multiple return statements in a function. Once the function executes a return statement, the program exits the function and continues with its next executable statement. If the function does not need to return any value, you can omit the return statement. Functions can be pretty elaborate; they can search for things in a list, determine variable types, open and close files, read and write to files. To get started we will build a few really simple mathematical functions; we will need this skill in the future anyway, especially in scientific programming contexts. User-built within a Code Block For our first function we will code f(x) = x\\sqrt{1 + x} into a function named dusty() . When you run the next cell, all it does is prototype the function (defines it), nothing happens until we use the function. def dusty(x) : temp = x * ((1.0+x)**(0.5)) # don't need the math package return temp # the function should make the evaluation # store in the local variable temp # return contents of temp # wrapper to run the dusty function yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = dusty(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589 Example: The Average Function Create the AVERAGE function for three values and test it for these values: - 3,4,5 - 10,100,1000 - -5,15,5 def AVERAGE3(x,y,z) : #define the function \"AVERAGE3\" Ave = (x+y+z)/3 #computes the average return Ave print(AVERAGE3(3,4,5)) print(AVERAGE3(10,100,1000)) print(AVERAGE3(-5,15,5)) 4.0 370.0 5.0 Example: The KATANA Function Create the Katana function for rounding off to the nearest hundredths (to 2 decimal places) and test it for these values: - 25.33694 - 15.753951 - 3.14159265359 def Katana(x) : #define the function \"Katana\" newX = round(x, 2) return newX print(Katana(25.33694)) print(Katana(15.753951)) print(Katana(3.14159265359)) 25.34 15.75 3.14 Variable Scope An important concept when defining a function is the concept of variable scope. Variables defined inside a function are treated differently from variables defined outside. Firstly, any variable declared within a function is only accessible within the function. These are known as local variables. In the dusty() function, the variables x and temp are local to the function. Any variable declared outside a function in a main program is known as a program variable and is accessible anywhere in the program. In the example, the variables xvalue and yvalue are program variables (global to the program; if they are addressed within a function, they could be operated on.) Generally we want to protect the program variables from the function unless the intent is to change their values. The way the function is written in the example, the function cannot damage xvalue or yvalue . If a local variable shares the same name as a program variable, any code inside the function is accessing the local variable. Any code outside is accessing the program variable As Separate Module/File In this section we will invent the neko() function, export it to a file, so we can reuse it in later notebooks without having to retype or cut-and-paste. The neko() function evaluates: f(x) = x\\sqrt{|(1 + x)|} Its the same as the dusty() function, except operates on the absolute value in the wadical. Create a text file named \"mylibrary.txt\" Copy the neko() function script below into that file. def neko(input_argument) : import math #ok to import into a function local_variable = input_argument * math.sqrt(abs(1.0+input_argument)) return local_variable rename mylibrary.txt to mylibrary.py modify the wrapper script to use the neko function as an external module # wrapper to run the neko function import mylibrary yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = mylibrary.neko(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589 In JupyterHub environments, you may discover that changes you make to your external python file are not reflected when you re-run your script; you need to restart the kernel to get the changes to actually update. The figure below depicts the notebook, external file relatonship Rudimentary Graphics Graphing values is part of the broader field of data visualization, which has two main goals: To explore data, and To communicate data. In this subsection we will concentrate on introducing skills to start exploring data and to produce meaningful visualizations we can use throughout the rest of this notebook. Data visualization is a rich field of study that fills entire books. The reason to start visualization here instead of elsewhere is that with functions plotting is a natural activity and we have to import the matplotlib module to make the plots. The example below is code adapted from Grus (2015) that illustrates simple generic plots. I added a single line (label the x-axis), and corrected some transcription errors (not the original author's mistake, just the consequence of how the API handled the cut-and-paste), but otherwise the code is unchanged. # python script to illustrate plotting # CODE BELOW IS ADAPTED FROM: # Grus, Joel (2015-04-14). Data Science from Scratch: First Principles with Python # (Kindle Locations 1190-1191). O'Reilly Media. Kindle Edition. # from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define one list for years gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] # and another one for Gross Domestic Product (GDP) plt.plot( years, gdp, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis # what if \"^\", \"P\", \"*\" for marker? # what if \"red\" for color? # what if \"dashdot\", '--' for linestyle? plt.title(\"Nominal GDP\")# add a title plt.ylabel(\"Billions of $\")# add a label to the x and y-axes plt.xlabel(\"Year\") plt.show() # display the plot # Now lets put the plotting script into a function so we can make line charts of any two numeric lists def plotAline(list1,list2,strx,stry,strtitle): # plot list1 on x, list2 on y, xlabel, ylabel, title from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() plt.plot( list1, list2, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis plt.title(strtitle)# add a title plt.ylabel(stry)# add a label to the x and y-axes plt.xlabel(strx) plt.show() # display the plot return #null return # wrapper years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define two lists years and gdp gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] print(type(years[0])) print(type(gdp[0])) plotAline(years,gdp,\"Year\",\"Billions of $\",\"Nominal GDP\") <class 'int'> <class 'float'> Example- The Hopeless Romantic! Copy the wrapper script for the plotAline() function, and modify the copy to create a plot of x = 16sin^3(t) y = 13cos(t) - 5cos(2t) - 2cos(3t) - cos(4t) for t raging from [0,2 \\Pi ] (inclusive). Label the plot and the plot axes. from matplotlib import pyplot as plt # import the plotting library import numpy as np # import NumPy: for large, multi-dimensional arrays and matrices, along with high-level mathematical functions to operate on these arrays. pi = np.pi #pi value from the np package t= np.linspace(0,2*pi,360)# the NumPy function np.linspace is similar to the range() x = 16*np.sin(t)**3 y = 13*np.cos(t) - 5*np.cos(2*t) - 2*np.cos(3*t) - np.cos(4*t) plt.plot( x, y, color ='purple', marker ='.', linestyle ='solid') plt.ylabel(\"Y-axis\")# add a label to the x and y-axes plt.xlabel(\"X-axis\") plt.axis('equal') #sets equal axis ratios plt.title(\"A Hopeless Romantic's Curve\")# add a title plt.show() # display the plot Here are some great reads on this topic: - \"Functions in Python\" available at https://www.geeksforgeeks.org/functions-in-python/ - \"Defining Your Own Python Function\" by John Sturtz available at https://realpython.com/defining-your-own-python-function/ - \"Graph Plotting in Python | Set 1\" available at https://www.geeksforgeeks.org/graph-plotting-in-python-set-1/ - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"How To Use Functions In Python (Python Tutorial #3)\" by CS Dojo available at https://www.youtube.com/watch?v=NSbOtYzIQI0 - \"Python Tutorial for Beginners 8: Functions\" by Corey Schafer available at https://www.youtube.com/watch?v=9Os0o3wzS_I - \"Python 3 Programming Tutorial - Functions\" by sentdex available at *https://www.youtube.com/watch?v=owglNL1KQf0 Exercise: A Function for Coffee. Because Coffee is life! Write a pseudo-code for a function that asks for user's preferences on their coffee (e.g., type of brew), follows certain steps to prepare that coffee, and delivers that coffee with an appropriate message. You can be as imaginative as you like, but make sure to provide logical justification for your script. * Make sure to cite any resources that you may use.","title":"Lab6"},{"location":"8-Labs/Newly Formatted/Lab6/#laboratory-6-functions","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 6: FUNctions "},{"location":"8-Labs/Newly Formatted/Lab6/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab6/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab6/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Newly Formatted/Lab6/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab6/#what-is-a-function-in-python","text":"Functions are simply pre-written code fragments that perform a certain task. In older procedural languages functions and subroutines are similar, but a function returns a value whereas a subroutine operates on data. The difference is subtle but important. More recent thinking has functions being able to operate on data (they always could) and the value returned may be simply an exit code. An analogy are the functions in MS Excel . To add numbers, we can use the sum(range) function and type =sum(A1:A5) instead of typing =A1+A2+A3+A4+A5","title":"What is a function in Python?"},{"location":"8-Labs/Newly Formatted/Lab6/#calling-the-function","text":"We call a function simply by typing the name of the function or by using the dot notation. Whether we can use the dot notation or not depends on how the function is written, whether it is part of a class, and how it is imported into a program. Some functions expect us to pass data to them to perform their tasks. These data are known as parameters( older terminology is arguments, or argument list) and we pass them to the function by enclosing their values in parenthesis ( ) separated by commas. For instance, the print() function for displaying text on the screen is \\called\" by typing print('Hello World') where print is the name of the function and the literal (a string) 'Hello World' is the argument.","title":"Calling the Function"},{"location":"8-Labs/Newly Formatted/Lab6/#program-flow","text":"A function, whether built-in, or added must be defined before it is called, otherwise the script will fail. Certain built-in functions \"self define\" upon start (such as print() and type() and we need not worry about those funtions). The diagram below illustrates the requesite flow control for functions that need to be defined before use. An example below will illustrate, change the cell to code and run it, you should get an error. Then fix the indicated line (remove the leading \"#\" in the import math ... line) and rerun, should get a functioning script. # reset the notebook using a magic function in JupyterLab %reset -f # An example, run once as is then activate indicated line, run again - what happens? x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic ) #import math # import the math package ## activate and rerun sqrt_by_math = math.sqrt(x) # note the dot notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) # Here is an alternative way: We just load the function that we want: # reset the notebook using a magic function in JupyterLab %reset -f # An example, run once as is then activate indicated line, run again - what happens? x= 4. sqrt_by_arithmetic = x**0.5 print('Using arithmetic square root of ', x, ' is ',sqrt_by_arithmetic ) from math import sqrt # import sqrt from the math package ## activate and rerun sqrt_by_math = sqrt(x) # note the notation print('Using math package square root of ', x,' is ',sqrt_by_arithmetic) Using arithmetic square root of 4.0 is 2.0 Using math package square root of 4.0 is 2.0","title":"Program flow"},{"location":"8-Labs/Newly Formatted/Lab6/#built-in-in-primitive-python-base-install","text":"The base Python functions and types built into it that are always available, the figure below lists those functions. Notice all have the structure of function_name() , except __import__() which has a constructor type structure, and is not intended for routine use. We will learn about constructors later.","title":"Built-In in Primitive Python (Base install)"},{"location":"8-Labs/Newly Formatted/Lab6/#added-in-using-external-packagesmodules-and-libaries-eg-math","text":"Python is also distributed with a large number of external functions. These functions are saved in files known as modules. To use the built-in codes in Python modules, we have to import them into our programs first. We do that by using the import keyword. There are three ways to import: 1. Import the entire module by writing import moduleName; For instance, to import the random module, we write import random. To use the randrange() function in the random module, we write random.randrange( 1, 10);28 2. Import and rename the module by writing import random as r (where r is any name of your choice). Now to use the randrange() function, you simply write r.randrange(1, 10); and 3. Import specific functions from the module by writing from moduleName import name1[,name2[, ... nameN]]. For instance, to import the randrange() function from the random module, we write from random import randrange. To import multiple functions, we separate them with a comma. To import the randrange() and randint() functions, we write from random import randrange, randint. To use the function now, we do not have to use the dot notation anymore. Just write randrange( 1, 10). # Example 1 of import %reset -f import random low = 1 ; high = 10 random.randrange(low,high) #generate random number in range low to high 4 # Example 2 of import %reset -f import random as r low = 1 ; high = 10 r.randrange(low,high) 7 # Example 3 of import %reset -f from random import randrange low = 1 ; high = 10 randrange(low,high) 5 The modules that come with Python are extensive and listed at https://docs.python.org/3/py-modindex.html. There are also other modules that can be downloaded and used (just like user defined modules below). In these labs we are building primitive codes to learn how to code and how to create algorithms. For many practical cases you will want to load a well-tested package to accomplish the tasks. That exercise is saved for the end of the document.","title":"Added-In using External Packages/Modules and Libaries (e.g. math)"},{"location":"8-Labs/Newly Formatted/Lab6/#user-built","text":"We can define our own functions in Python and reuse them throughout the program. The syntax for defining a function is: def functionName( argument ): code detailing what the function should do note the colon above and indentation ... ... return [expression] The keyword def tells the program that the indented code from the next line onwards is part of the function. The keyword return tells the program to return an answer from the function. There can be multiple return statements in a function. Once the function executes a return statement, the program exits the function and continues with its next executable statement. If the function does not need to return any value, you can omit the return statement. Functions can be pretty elaborate; they can search for things in a list, determine variable types, open and close files, read and write to files. To get started we will build a few really simple mathematical functions; we will need this skill in the future anyway, especially in scientific programming contexts.","title":"User-Built"},{"location":"8-Labs/Newly Formatted/Lab6/#user-built-within-a-code-block","text":"For our first function we will code f(x) = x\\sqrt{1 + x} into a function named dusty() . When you run the next cell, all it does is prototype the function (defines it), nothing happens until we use the function. def dusty(x) : temp = x * ((1.0+x)**(0.5)) # don't need the math package return temp # the function should make the evaluation # store in the local variable temp # return contents of temp # wrapper to run the dusty function yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = dusty(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589","title":"User-built within a Code Block"},{"location":"8-Labs/Newly Formatted/Lab6/#example-the-average-function","text":"Create the AVERAGE function for three values and test it for these values: - 3,4,5 - 10,100,1000 - -5,15,5 def AVERAGE3(x,y,z) : #define the function \"AVERAGE3\" Ave = (x+y+z)/3 #computes the average return Ave print(AVERAGE3(3,4,5)) print(AVERAGE3(10,100,1000)) print(AVERAGE3(-5,15,5)) 4.0 370.0 5.0","title":"Example: The Average Function"},{"location":"8-Labs/Newly Formatted/Lab6/#example-the-katana-function","text":"Create the Katana function for rounding off to the nearest hundredths (to 2 decimal places) and test it for these values: - 25.33694 - 15.753951 - 3.14159265359 def Katana(x) : #define the function \"Katana\" newX = round(x, 2) return newX print(Katana(25.33694)) print(Katana(15.753951)) print(Katana(3.14159265359)) 25.34 15.75 3.14","title":"Example: The KATANA Function"},{"location":"8-Labs/Newly Formatted/Lab6/#variable-scope","text":"An important concept when defining a function is the concept of variable scope. Variables defined inside a function are treated differently from variables defined outside. Firstly, any variable declared within a function is only accessible within the function. These are known as local variables. In the dusty() function, the variables x and temp are local to the function. Any variable declared outside a function in a main program is known as a program variable and is accessible anywhere in the program. In the example, the variables xvalue and yvalue are program variables (global to the program; if they are addressed within a function, they could be operated on.) Generally we want to protect the program variables from the function unless the intent is to change their values. The way the function is written in the example, the function cannot damage xvalue or yvalue . If a local variable shares the same name as a program variable, any code inside the function is accessing the local variable. Any code outside is accessing the program variable","title":"Variable Scope"},{"location":"8-Labs/Newly Formatted/Lab6/#as-separate-modulefile","text":"In this section we will invent the neko() function, export it to a file, so we can reuse it in later notebooks without having to retype or cut-and-paste. The neko() function evaluates: f(x) = x\\sqrt{|(1 + x)|} Its the same as the dusty() function, except operates on the absolute value in the wadical. Create a text file named \"mylibrary.txt\" Copy the neko() function script below into that file. def neko(input_argument) : import math #ok to import into a function local_variable = input_argument * math.sqrt(abs(1.0+input_argument)) return local_variable rename mylibrary.txt to mylibrary.py modify the wrapper script to use the neko function as an external module # wrapper to run the neko function import mylibrary yes = 0 while yes == 0: xvalue = input('enter a numeric value') try: xvalue = float(xvalue) yes = 1 except: print('enter a bloody number! Try again \\n') # call the function, get value , write output yvalue = mylibrary.neko(xvalue) print('f(',xvalue,') = ',yvalue) # and we are done enter a numeric value 5 f( 5.0 ) = 12.24744871391589 In JupyterHub environments, you may discover that changes you make to your external python file are not reflected when you re-run your script; you need to restart the kernel to get the changes to actually update. The figure below depicts the notebook, external file relatonship","title":"As Separate Module/File"},{"location":"8-Labs/Newly Formatted/Lab6/#rudimentary-graphics","text":"Graphing values is part of the broader field of data visualization, which has two main goals: To explore data, and To communicate data. In this subsection we will concentrate on introducing skills to start exploring data and to produce meaningful visualizations we can use throughout the rest of this notebook. Data visualization is a rich field of study that fills entire books. The reason to start visualization here instead of elsewhere is that with functions plotting is a natural activity and we have to import the matplotlib module to make the plots. The example below is code adapted from Grus (2015) that illustrates simple generic plots. I added a single line (label the x-axis), and corrected some transcription errors (not the original author's mistake, just the consequence of how the API handled the cut-and-paste), but otherwise the code is unchanged. # python script to illustrate plotting # CODE BELOW IS ADAPTED FROM: # Grus, Joel (2015-04-14). Data Science from Scratch: First Principles with Python # (Kindle Locations 1190-1191). O'Reilly Media. Kindle Edition. # from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define one list for years gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] # and another one for Gross Domestic Product (GDP) plt.plot( years, gdp, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis # what if \"^\", \"P\", \"*\" for marker? # what if \"red\" for color? # what if \"dashdot\", '--' for linestyle? plt.title(\"Nominal GDP\")# add a title plt.ylabel(\"Billions of $\")# add a label to the x and y-axes plt.xlabel(\"Year\") plt.show() # display the plot # Now lets put the plotting script into a function so we can make line charts of any two numeric lists def plotAline(list1,list2,strx,stry,strtitle): # plot list1 on x, list2 on y, xlabel, ylabel, title from matplotlib import pyplot as plt # import the plotting library from matplotlibplt.show() plt.plot( list1, list2, color ='green', marker ='o', linestyle ='solid') # create a line chart, years on x-axis, gdp on y-axis plt.title(strtitle)# add a title plt.ylabel(stry)# add a label to the x and y-axes plt.xlabel(strx) plt.show() # display the plot return #null return # wrapper years = [1950, 1960, 1970, 1980, 1990, 2000, 2010] # define two lists years and gdp gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3] print(type(years[0])) print(type(gdp[0])) plotAline(years,gdp,\"Year\",\"Billions of $\",\"Nominal GDP\") <class 'int'> <class 'float'>","title":"Rudimentary Graphics"},{"location":"8-Labs/Newly Formatted/Lab6/#example-the-hopeless-romantic","text":"Copy the wrapper script for the plotAline() function, and modify the copy to create a plot of x = 16sin^3(t) y = 13cos(t) - 5cos(2t) - 2cos(3t) - cos(4t) for t raging from [0,2 \\Pi ] (inclusive). Label the plot and the plot axes. from matplotlib import pyplot as plt # import the plotting library import numpy as np # import NumPy: for large, multi-dimensional arrays and matrices, along with high-level mathematical functions to operate on these arrays. pi = np.pi #pi value from the np package t= np.linspace(0,2*pi,360)# the NumPy function np.linspace is similar to the range() x = 16*np.sin(t)**3 y = 13*np.cos(t) - 5*np.cos(2*t) - 2*np.cos(3*t) - np.cos(4*t) plt.plot( x, y, color ='purple', marker ='.', linestyle ='solid') plt.ylabel(\"Y-axis\")# add a label to the x and y-axes plt.xlabel(\"X-axis\") plt.axis('equal') #sets equal axis ratios plt.title(\"A Hopeless Romantic's Curve\")# add a title plt.show() # display the plot Here are some great reads on this topic: - \"Functions in Python\" available at https://www.geeksforgeeks.org/functions-in-python/ - \"Defining Your Own Python Function\" by John Sturtz available at https://realpython.com/defining-your-own-python-function/ - \"Graph Plotting in Python | Set 1\" available at https://www.geeksforgeeks.org/graph-plotting-in-python-set-1/ - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"How To Use Functions In Python (Python Tutorial #3)\" by CS Dojo available at https://www.youtube.com/watch?v=NSbOtYzIQI0 - \"Python Tutorial for Beginners 8: Functions\" by Corey Schafer available at https://www.youtube.com/watch?v=9Os0o3wzS_I - \"Python 3 Programming Tutorial - Functions\" by sentdex available at *https://www.youtube.com/watch?v=owglNL1KQf0","title":"Example- The Hopeless Romantic!"},{"location":"8-Labs/Newly Formatted/Lab6/#exercise-a-function-for-coffee-because-coffee-is-life","text":"","title":"Exercise: A Function for Coffee. Because Coffee is life! "},{"location":"8-Labs/Newly Formatted/Lab6/#write-a-pseudo-code-for-a-function-that-asks-for-users-preferences-on-their-coffee-eg-type-of-brew-follows-certain-steps-to-prepare-that-coffee-and-delivers-that-coffee-with-an-appropriate-message-you-can-be-as-imaginative-as-you-like-but-make-sure-to-provide-logical-justification-for-your-script","text":"","title":"Write a pseudo-code for a function that asks for user's preferences on their coffee (e.g., type of brew), follows certain steps to prepare that coffee, and delivers that coffee with an appropriate message. You can be as imaginative as you like, but make sure to provide logical justification for your script."},{"location":"8-Labs/Newly Formatted/Lab6/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab7/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab7 Laboratory 7: Numpy for Bread! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Numpy Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is short for \u201cNumeric Python\u201d or \u201cNumerical Python\u201d. If you are curious about NumPy, this cheat sheet is recommended: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf Arrays A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension. In other words, an array contains information about the raw data, how to locate an element and how to interpret an element.To make a numpy array, you can just use the np.array() function. All you need to do is pass a list to it. Don\u2019t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. If you want to read more about the differences between a Python list and NumPy array, this link is recommended: https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference Example- 1D Arrays Let's create a 1D array from the 2000s (2000-2009): import numpy as np #First, we need to impoty \"numpy\" mylist = [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009] #Create a list of the years print(mylist) #Check how it looks np.array(mylist) #Define it as a numpy array [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009] array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009]) Example- n-Dimensional Arrays Let's create a 5x2 array from the 2000s (2000-2009): myotherlist = [[2000,2001],[2002,2003],[2004,2005],[2006,2007],[2008,2009]] #Since I want a 5x2 array, I should group the years two by two print(myotherlist) #See how it looks as a list np.array(myotherlist) #See how it looks as a numpy array [[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]] array([[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]]) Arrays Arithmetic Once you have created the arrays, you can do basic Numpy operations. Numpy offers a variety of operations applicable on arrays. From basic operations such as summation, subtraction, multiplication and division to more advanced and essential operations such as matrix multiplication and other elementwise operations. In the examples below, we will go over some of these: Example- 1D Array Arithmetic Define a 1D array with [0,12,24,36,48,60,72,84,96] Multiple all elements by 2 Take all elements to the power of 2 Find the maximum value of the array and its position Find the minimum value of the array and its position Define another 1D array with [-12,0,12,24,36,48,60,72,84] Find the summation and subtraction of these two arrays Find the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([0,12,24,36,48,60,72,84,96]) #Step1: Define Array1 print(Array1) print(Array1*2) #Step2: Multiple all elements by 2 print(Array1**2) #Step3: Take all elements to the power of 2 print(np.power(Array1,2)) #Another way to do the same thing, by using a function in numpy print(np.max(Array1)) #Step4: Find the maximum value of the array print(np.argmax(Array1)) ##Step4: Find the postition of the maximum value print(np.min(Array1)) #Step5: Find the minimum value of the array print(np.argmin(Array1)) ##Step5: Find the postition of the minimum value Array2 = np.array([-12,0,12,24,36,48,60,72,84]) #Step6: Define Array2 print(Array2) print(Array1+Array2) #Step7: Find the summation of these two arrays print(Array1-Array2) #Step7: Find the subtraction of these two arrays print(Array1*Array2) #Step8: Find the multiplication of these two arrays [ 0 12 24 36 48 60 72 84 96] [ 0 24 48 72 96 120 144 168 192] [ 0 144 576 1296 2304 3600 5184 7056 9216] [ 0 144 576 1296 2304 3600 5184 7056 9216] 96 8 0 0 [-12 0 12 24 36 48 60 72 84] [-12 12 36 60 84 108 132 156 180] [12 12 12 12 12 12 12 12 12] [ 0 0 288 864 1728 2880 4320 6048 8064] Example- n-Dimensional Array Arithmetic Define a 2x2 array with [5,10,15,20] Define another 2x2 array with [3,6,9,12] Find the summation and subtraction of these two arrays Find the minimum number in the multiplication of these two arrays Find the position of the maximum in the multiplication of these two arrays Find the mean of the multiplication of these two arrays Find the mean of the first row of the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([[5,10],[15,20]]) #Step1: Define Array1 print(Array1) Array2 = np.array([[3,6],[9,12]]) #Step2: Define Array2 print(Array2) print(Array1+Array2) #Step3: Find the summation print(Array1-Array2) #Step3: Find the subtraction MultArray = Array1@Array2 #Step4: To perform a typical matrix multiplication (or matrix product) MultArray1 = Array1.dot(Array2) #Step4: Another way To perform a matrix multiplication print(MultArray) print(MultArray1) print(np.min(MultArray)) #Step4: Find the minimum value of the multiplication print(np.argmax(MultArray)) ##Step5: Find the postition of the maximum value print(np.mean(MultArray)) ##Step6: Find the mean of the multiplication of these two arrays print(np.mean(MultArray[0,:])) ##Step7: Find the mean of the first row of the multiplication of these two arrays [[ 5 10] [15 20]] [[ 3 6] [ 9 12]] [[ 8 16] [24 32]] [[2 4] [6 8]] [[105 150] [225 330]] [[105 150] [225 330]] 105 3 202.5 127.5 Arrays Comparison Comparing two NumPy arrays determines whether they are equivalent by checking if every element at each corresponding index are the same. Example- 1D Array Comparison Define a 1D array with [1.0,2.5,3.4,7,7] Define another 1D array with [5.0/5.0,5.0/2,6.8/2,21/3,14/2] Compare and see if the two arrays are equal Define another 1D array with [6,1.4,2.2,7.5,7] Compare and see if the first array is greater than or equal to the third array import numpy as np #import numpy Array1 = np.array([1.0,2.5,3.4,7,7]) #Step1: Define Array1 print(Array1) Array2 = np.array([5.0/5.0,5.0/2,6.8/2,21/3,14/2]) #Step2: Define Array1 print(Array2) print(np.equal(Array1, Array2)) #Step3: Compare and see if the two arrays are equal Array3 = np.array([6,1.4,2.2,7.5,7]) #Step4: Define Array3 print(Array3) print(np.greater_equal(Array1, Array3)) #Step3: Compare and see if the two arrays are equal [1. 2.5 3.4 7. 7. ] [1. 2.5 3.4 7. 7. ] [ True True True True True] [6. 1.4 2.2 7.5 7. ] [False True True False True] Arrays Manipulation numpy.copy() allows us to create a copy of an array. This is particularly useful when we need to manipulate an array while keeping an original copy in memory. The numpy.delete() function returns a new array with sub-arrays along an axis deleted. Let's have a look at the examples. Example- Copying and Deleting Arrays and Elements Define a 1D array, named \"x\" with [1,2,3] Define \"y\" so that \"y=x\" Define \"z\" as a copy of \"x\" Discuss the difference between y and z Delete the second element of x import numpy as np #import numpy x = np.array([1,2,3]) #Step1: Define x print(x) y = x #Step2: Define y as y=x print(y) z = np.copy(x) #Step3: Define z as a copy of x print(z) # For Step4: They look similar but check this out: x[1] = 8 # If we change x ... print(x) print(y) print(z) # By modifying x, y changes but z remains as a copy of the initial version of x. x = np.delete(x, 1) #Step5: Delete the second element of x print(x) [1 2 3] [1 2 3] [1 2 3] [1 8 3] [1 8 3] [1 2 3] [1 3] Sorting Arrays Sorting means putting elements in an ordered sequence. Ordered sequence is any sequence that has an order corresponding to elements, like numeric or alphabetical, ascending or descending. If you use the sort() method on a 2-D array, both arrays will be sorted. Example- Sorting 1D Arrays Define a 1D array as ['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed'] and print it out. Then, sort the array alphabetically. import numpy as np #import numpy games = np.array(['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed']) print(games) print(np.sort(games)) ['FIFA 2020' 'Red Dead Redemption' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed'] ['FIFA 2020' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed' 'Red Dead Redemption'] Example- Sorting n-Dimensional Arrays Define a 3x3 array with 17,-6,2,86,-12,0,0,23,12 and print it out. Then, sort the array. import numpy as np #import numpy a = np.array([[17,-6,2],[86,-12,0],[0,23,12]]) print(a) print (\"Along columns : \\n\", np.sort(a,axis = 0) ) #This will be sorting in each column print (\"Along rows : \\n\", np.sort(a,axis = 1) ) #This will be sorting in each row print (\"Sorting by default : \\n\", np.sort(a) ) #Same as above print (\"Along None Axis : \\n\", np.sort(a,axis = None) ) #This will be sorted like a 1D array [[ 17 -6 2] [ 86 -12 0] [ 0 23 12]] Along columns : [[ 0 -12 0] [ 17 -6 2] [ 86 23 12]] Along rows : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Sorting by default : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Along None Axis : [-12 -6 0 0 2 12 17 23 86] Partitioning (Slice) Arrays Slicing in python means taking elements from one given index to another given index. We can do slicing like this: [start:end]. We can also define the step, like this: [start:end:step]. If we don't pass start its considered 0 If we don't pass end its considered length of array in that dimension If we don't pass step its considered 1 Example- Slicing 1D Arrays Define a 1D array as [1,3,5,7,9], slice out the [3,5,7] and print it out. import numpy as np #import numpy a = np.array([1,3,5,7,9]) #Define the array print(a) aslice = a[1:4] #slice the [3,5,7] print(aslice) #print it out [1 3 5 7 9] [3 5 7] Example- Slicing n-Dimensional Arrays Define a 5x5 array with \"Superman, Batman, Jim Hammond, Captain America, Green Arrow, Aquaman, Wonder Woman, Martian Manhunter, Barry Allen, Hal Jordan, Hawkman, Ray Palmer, Spider Man, Thor, Hank Pym, Solar, Iron Man, Dr. Strange, Daredevil, Ted Kord, Captian Marvel, Black Panther, Wolverine, Booster Gold, Spawn \" and print it out. Then: - Slice the first column and print it out - Slice the third row and print it out - Slice 'Wolverine' and print it out - Slice a 3x3 array with 'Wonder Woman, Ray Palmer, Iron Man, Martian Manhunter, Spider Man, Dr. Strange, Barry Allen, Thor, Daredevil' import numpy as np #import numpy Superheroes = np.array([['Superman', 'Batman', 'Jim Hammond', 'Captain America', 'Green Arrow'], ['Aquaman', 'Wonder Woman', 'Martian Manhunter', 'Barry Allen', 'Hal Jordan'], ['Hawkman', 'Ray Palmer', 'Spider Man', 'Thor', 'Hank Pym'], ['Solar', 'Iron Man', 'Dr. Strange', 'Daredevil', 'Ted Kord'], ['Captian Marvel', 'Black Panther', 'Wolverine', 'Booster Gold', 'Spawn']]) print(Superheroes) #Step1 print(Superheroes[:,0]) print(Superheroes[2,:]) print(Superheroes[4,2]) print(Superheroes[1:4,1:4]) [['Superman' 'Batman' 'Jim Hammond' 'Captain America' 'Green Arrow'] ['Aquaman' 'Wonder Woman' 'Martian Manhunter' 'Barry Allen' 'Hal Jordan'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] ['Solar' 'Iron Man' 'Dr. Strange' 'Daredevil' 'Ted Kord'] ['Captian Marvel' 'Black Panther' 'Wolverine' 'Booster Gold' 'Spawn']] ['Superman' 'Aquaman' 'Hawkman' 'Solar' 'Captian Marvel'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] Wolverine [['Wonder Woman' 'Martian Manhunter' 'Barry Allen'] ['Ray Palmer' 'Spider Man' 'Thor'] ['Iron Man' 'Dr. Strange' 'Daredevil']] This is a Numpy Cheat Sheet- similar to the one you had on top of this notebook! Check out this link for more: https://blog.finxter.com/collection-10-best-numpy-cheat-sheets-every-python-coder-must-own/ Here are some of the resources used for creating this notebook: - Johnson, J. (2020). Python Numpy Tutorial (with Jupyter and Colab). Retrieved September 15, 2020, from https://cs231n.github.io/python-numpy-tutorial/ - Willems, K. (2019). (Tutorial) Python NUMPY Array TUTORIAL. Retrieved September 15, 2020, from https://www.datacamp.com/community/tutorials/python-numpy-tutorial?utm_source=adwords_ppc - Willems, K. (2017). NumPy Cheat Sheet: Data Analysis in Python. Retrieved September 15, 2020, from https://www.datacamp.com/community/blog/python-numpy-cheat-sheet - W3resource. (2020). NumPy: Compare two given arrays. Retrieved September 15, 2020, from https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-28.php Here are some great reads on this topic: - \"Python NumPy Tutorial\" available at https://www.geeksforgeeks.org/python-numpy-tutorial/ - \"What Is NumPy?\" a collection of blogs, available at https://realpython.com/tutorials/numpy/ - \"Look Ma, No For-Loops: Array Programming With NumPy\" by Brad Solomon available at https://realpython.com/numpy-array-programming/ - \"The Ultimate Beginner\u2019s Guide to NumPy\" by Anne Bonner available at https://towardsdatascience.com/the-ultimate-beginners-guide-to-numpy-f5a2f99aef54 Here are some great videos on these topics: - \"Learn NUMPY in 5 minutes - BEST Python Library!\" by Python Programmer available at https://www.youtube.com/watch?v=xECXZ3tyONo - \"Python NumPy Tutorial for Beginners\" by freeCodeCamp.org available at https://www.youtube.com/watch?v=QUT1VHiLmmI - \"Complete Python NumPy Tutorial (Creating Arrays, Indexing, Math, Statistics, Reshaping)\" by Keith Galli available at https://www.youtube.com/watch?v=GB9ByFAIAH4 - \"Python NumPy Tutorial | NumPy Array | Python Tutorial For Beginners | Python Training | Edureka\" by edureka! available at https://www.youtube.com/watch?v=8JfDAm9y_7s Exercise: Python List vs. Numpy Arrays? What are some differences between Python lists and Numpy arrays? * Make sure to cite any resources that you may use.","title":"Lab7"},{"location":"8-Labs/Newly Formatted/Lab7/#laboratory-7-numpy-for-bread","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) DESKTOP-EH6HD63 desktop-eh6hd63\\farha C:\\Users\\Farha\\Anaconda3\\python.exe 3.7.4 (default, Aug 9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)] sys.version_info(major=3, minor=7, micro=4, releaselevel='final', serial=0)","title":"Laboratory 7: Numpy for Bread! "},{"location":"8-Labs/Newly Formatted/Lab7/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab7/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab7/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Newly Formatted/Lab7/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab7/#numpy","text":"Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. The library\u2019s name is short for \u201cNumeric Python\u201d or \u201cNumerical Python\u201d. If you are curious about NumPy, this cheat sheet is recommended: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf","title":"Numpy"},{"location":"8-Labs/Newly Formatted/Lab7/#arrays","text":"A numpy array is a grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the rank of the array; the shape of an array is a tuple of integers giving the size of the array along each dimension. In other words, an array contains information about the raw data, how to locate an element and how to interpret an element.To make a numpy array, you can just use the np.array() function. All you need to do is pass a list to it. Don\u2019t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. If you want to read more about the differences between a Python list and NumPy array, this link is recommended: https://webcourses.ucf.edu/courses/1249560/pages/python-lists-vs-numpy-arrays-what-is-the-difference","title":"Arrays"},{"location":"8-Labs/Newly Formatted/Lab7/#example-1d-arrays","text":"Let's create a 1D array from the 2000s (2000-2009): import numpy as np #First, we need to impoty \"numpy\" mylist = [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009] #Create a list of the years print(mylist) #Check how it looks np.array(mylist) #Define it as a numpy array [2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009] array([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009])","title":"Example- 1D Arrays"},{"location":"8-Labs/Newly Formatted/Lab7/#example-n-dimensional-arrays","text":"Let's create a 5x2 array from the 2000s (2000-2009): myotherlist = [[2000,2001],[2002,2003],[2004,2005],[2006,2007],[2008,2009]] #Since I want a 5x2 array, I should group the years two by two print(myotherlist) #See how it looks as a list np.array(myotherlist) #See how it looks as a numpy array [[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]] array([[2000, 2001], [2002, 2003], [2004, 2005], [2006, 2007], [2008, 2009]])","title":"Example- n-Dimensional Arrays"},{"location":"8-Labs/Newly Formatted/Lab7/#arrays-arithmetic","text":"Once you have created the arrays, you can do basic Numpy operations. Numpy offers a variety of operations applicable on arrays. From basic operations such as summation, subtraction, multiplication and division to more advanced and essential operations such as matrix multiplication and other elementwise operations. In the examples below, we will go over some of these:","title":"Arrays Arithmetic"},{"location":"8-Labs/Newly Formatted/Lab7/#example-1d-array-arithmetic","text":"Define a 1D array with [0,12,24,36,48,60,72,84,96] Multiple all elements by 2 Take all elements to the power of 2 Find the maximum value of the array and its position Find the minimum value of the array and its position Define another 1D array with [-12,0,12,24,36,48,60,72,84] Find the summation and subtraction of these two arrays Find the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([0,12,24,36,48,60,72,84,96]) #Step1: Define Array1 print(Array1) print(Array1*2) #Step2: Multiple all elements by 2 print(Array1**2) #Step3: Take all elements to the power of 2 print(np.power(Array1,2)) #Another way to do the same thing, by using a function in numpy print(np.max(Array1)) #Step4: Find the maximum value of the array print(np.argmax(Array1)) ##Step4: Find the postition of the maximum value print(np.min(Array1)) #Step5: Find the minimum value of the array print(np.argmin(Array1)) ##Step5: Find the postition of the minimum value Array2 = np.array([-12,0,12,24,36,48,60,72,84]) #Step6: Define Array2 print(Array2) print(Array1+Array2) #Step7: Find the summation of these two arrays print(Array1-Array2) #Step7: Find the subtraction of these two arrays print(Array1*Array2) #Step8: Find the multiplication of these two arrays [ 0 12 24 36 48 60 72 84 96] [ 0 24 48 72 96 120 144 168 192] [ 0 144 576 1296 2304 3600 5184 7056 9216] [ 0 144 576 1296 2304 3600 5184 7056 9216] 96 8 0 0 [-12 0 12 24 36 48 60 72 84] [-12 12 36 60 84 108 132 156 180] [12 12 12 12 12 12 12 12 12] [ 0 0 288 864 1728 2880 4320 6048 8064]","title":"Example- 1D Array Arithmetic"},{"location":"8-Labs/Newly Formatted/Lab7/#example-n-dimensional-array-arithmetic","text":"Define a 2x2 array with [5,10,15,20] Define another 2x2 array with [3,6,9,12] Find the summation and subtraction of these two arrays Find the minimum number in the multiplication of these two arrays Find the position of the maximum in the multiplication of these two arrays Find the mean of the multiplication of these two arrays Find the mean of the first row of the multiplication of these two arrays import numpy as np #import numpy Array1 = np.array([[5,10],[15,20]]) #Step1: Define Array1 print(Array1) Array2 = np.array([[3,6],[9,12]]) #Step2: Define Array2 print(Array2) print(Array1+Array2) #Step3: Find the summation print(Array1-Array2) #Step3: Find the subtraction MultArray = Array1@Array2 #Step4: To perform a typical matrix multiplication (or matrix product) MultArray1 = Array1.dot(Array2) #Step4: Another way To perform a matrix multiplication print(MultArray) print(MultArray1) print(np.min(MultArray)) #Step4: Find the minimum value of the multiplication print(np.argmax(MultArray)) ##Step5: Find the postition of the maximum value print(np.mean(MultArray)) ##Step6: Find the mean of the multiplication of these two arrays print(np.mean(MultArray[0,:])) ##Step7: Find the mean of the first row of the multiplication of these two arrays [[ 5 10] [15 20]] [[ 3 6] [ 9 12]] [[ 8 16] [24 32]] [[2 4] [6 8]] [[105 150] [225 330]] [[105 150] [225 330]] 105 3 202.5 127.5","title":"Example- n-Dimensional Array Arithmetic"},{"location":"8-Labs/Newly Formatted/Lab7/#arrays-comparison","text":"Comparing two NumPy arrays determines whether they are equivalent by checking if every element at each corresponding index are the same.","title":"Arrays Comparison"},{"location":"8-Labs/Newly Formatted/Lab7/#example-1d-array-comparison","text":"Define a 1D array with [1.0,2.5,3.4,7,7] Define another 1D array with [5.0/5.0,5.0/2,6.8/2,21/3,14/2] Compare and see if the two arrays are equal Define another 1D array with [6,1.4,2.2,7.5,7] Compare and see if the first array is greater than or equal to the third array import numpy as np #import numpy Array1 = np.array([1.0,2.5,3.4,7,7]) #Step1: Define Array1 print(Array1) Array2 = np.array([5.0/5.0,5.0/2,6.8/2,21/3,14/2]) #Step2: Define Array1 print(Array2) print(np.equal(Array1, Array2)) #Step3: Compare and see if the two arrays are equal Array3 = np.array([6,1.4,2.2,7.5,7]) #Step4: Define Array3 print(Array3) print(np.greater_equal(Array1, Array3)) #Step3: Compare and see if the two arrays are equal [1. 2.5 3.4 7. 7. ] [1. 2.5 3.4 7. 7. ] [ True True True True True] [6. 1.4 2.2 7.5 7. ] [False True True False True]","title":"Example- 1D Array Comparison"},{"location":"8-Labs/Newly Formatted/Lab7/#arrays-manipulation","text":"numpy.copy() allows us to create a copy of an array. This is particularly useful when we need to manipulate an array while keeping an original copy in memory. The numpy.delete() function returns a new array with sub-arrays along an axis deleted. Let's have a look at the examples.","title":"Arrays Manipulation"},{"location":"8-Labs/Newly Formatted/Lab7/#example-copying-and-deleting-arrays-and-elements","text":"Define a 1D array, named \"x\" with [1,2,3] Define \"y\" so that \"y=x\" Define \"z\" as a copy of \"x\" Discuss the difference between y and z Delete the second element of x import numpy as np #import numpy x = np.array([1,2,3]) #Step1: Define x print(x) y = x #Step2: Define y as y=x print(y) z = np.copy(x) #Step3: Define z as a copy of x print(z) # For Step4: They look similar but check this out: x[1] = 8 # If we change x ... print(x) print(y) print(z) # By modifying x, y changes but z remains as a copy of the initial version of x. x = np.delete(x, 1) #Step5: Delete the second element of x print(x) [1 2 3] [1 2 3] [1 2 3] [1 8 3] [1 8 3] [1 2 3] [1 3]","title":"Example- Copying and Deleting Arrays and Elements"},{"location":"8-Labs/Newly Formatted/Lab7/#sorting-arrays","text":"Sorting means putting elements in an ordered sequence. Ordered sequence is any sequence that has an order corresponding to elements, like numeric or alphabetical, ascending or descending. If you use the sort() method on a 2-D array, both arrays will be sorted.","title":"Sorting Arrays"},{"location":"8-Labs/Newly Formatted/Lab7/#example-sorting-1d-arrays","text":"Define a 1D array as ['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed'] and print it out. Then, sort the array alphabetically. import numpy as np #import numpy games = np.array(['FIFA 2020','Red Dead Redemption','Fallout','GTA','NBA 2018','Need For Speed']) print(games) print(np.sort(games)) ['FIFA 2020' 'Red Dead Redemption' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed'] ['FIFA 2020' 'Fallout' 'GTA' 'NBA 2018' 'Need For Speed' 'Red Dead Redemption']","title":"Example- Sorting 1D Arrays"},{"location":"8-Labs/Newly Formatted/Lab7/#example-sorting-n-dimensional-arrays","text":"Define a 3x3 array with 17,-6,2,86,-12,0,0,23,12 and print it out. Then, sort the array. import numpy as np #import numpy a = np.array([[17,-6,2],[86,-12,0],[0,23,12]]) print(a) print (\"Along columns : \\n\", np.sort(a,axis = 0) ) #This will be sorting in each column print (\"Along rows : \\n\", np.sort(a,axis = 1) ) #This will be sorting in each row print (\"Sorting by default : \\n\", np.sort(a) ) #Same as above print (\"Along None Axis : \\n\", np.sort(a,axis = None) ) #This will be sorted like a 1D array [[ 17 -6 2] [ 86 -12 0] [ 0 23 12]] Along columns : [[ 0 -12 0] [ 17 -6 2] [ 86 23 12]] Along rows : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Sorting by default : [[ -6 2 17] [-12 0 86] [ 0 12 23]] Along None Axis : [-12 -6 0 0 2 12 17 23 86]","title":"Example- Sorting n-Dimensional Arrays"},{"location":"8-Labs/Newly Formatted/Lab7/#partitioning-slice-arrays","text":"Slicing in python means taking elements from one given index to another given index. We can do slicing like this: [start:end]. We can also define the step, like this: [start:end:step]. If we don't pass start its considered 0 If we don't pass end its considered length of array in that dimension If we don't pass step its considered 1","title":"Partitioning (Slice) Arrays"},{"location":"8-Labs/Newly Formatted/Lab7/#example-slicing-1d-arrays","text":"Define a 1D array as [1,3,5,7,9], slice out the [3,5,7] and print it out. import numpy as np #import numpy a = np.array([1,3,5,7,9]) #Define the array print(a) aslice = a[1:4] #slice the [3,5,7] print(aslice) #print it out [1 3 5 7 9] [3 5 7]","title":"Example- Slicing 1D Arrays"},{"location":"8-Labs/Newly Formatted/Lab7/#example-slicing-n-dimensional-arrays","text":"Define a 5x5 array with \"Superman, Batman, Jim Hammond, Captain America, Green Arrow, Aquaman, Wonder Woman, Martian Manhunter, Barry Allen, Hal Jordan, Hawkman, Ray Palmer, Spider Man, Thor, Hank Pym, Solar, Iron Man, Dr. Strange, Daredevil, Ted Kord, Captian Marvel, Black Panther, Wolverine, Booster Gold, Spawn \" and print it out. Then: - Slice the first column and print it out - Slice the third row and print it out - Slice 'Wolverine' and print it out - Slice a 3x3 array with 'Wonder Woman, Ray Palmer, Iron Man, Martian Manhunter, Spider Man, Dr. Strange, Barry Allen, Thor, Daredevil' import numpy as np #import numpy Superheroes = np.array([['Superman', 'Batman', 'Jim Hammond', 'Captain America', 'Green Arrow'], ['Aquaman', 'Wonder Woman', 'Martian Manhunter', 'Barry Allen', 'Hal Jordan'], ['Hawkman', 'Ray Palmer', 'Spider Man', 'Thor', 'Hank Pym'], ['Solar', 'Iron Man', 'Dr. Strange', 'Daredevil', 'Ted Kord'], ['Captian Marvel', 'Black Panther', 'Wolverine', 'Booster Gold', 'Spawn']]) print(Superheroes) #Step1 print(Superheroes[:,0]) print(Superheroes[2,:]) print(Superheroes[4,2]) print(Superheroes[1:4,1:4]) [['Superman' 'Batman' 'Jim Hammond' 'Captain America' 'Green Arrow'] ['Aquaman' 'Wonder Woman' 'Martian Manhunter' 'Barry Allen' 'Hal Jordan'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] ['Solar' 'Iron Man' 'Dr. Strange' 'Daredevil' 'Ted Kord'] ['Captian Marvel' 'Black Panther' 'Wolverine' 'Booster Gold' 'Spawn']] ['Superman' 'Aquaman' 'Hawkman' 'Solar' 'Captian Marvel'] ['Hawkman' 'Ray Palmer' 'Spider Man' 'Thor' 'Hank Pym'] Wolverine [['Wonder Woman' 'Martian Manhunter' 'Barry Allen'] ['Ray Palmer' 'Spider Man' 'Thor'] ['Iron Man' 'Dr. Strange' 'Daredevil']]","title":"Example- Slicing n-Dimensional Arrays"},{"location":"8-Labs/Newly Formatted/Lab7/#this-is-a-numpy-cheat-sheet-similar-to-the-one-you-had-on-top-of-this-notebook","text":"","title":"This is a Numpy Cheat Sheet- similar to the one you had on top of this notebook!"},{"location":"8-Labs/Newly Formatted/Lab7/#check-out-this-link-for-more","text":"https://blog.finxter.com/collection-10-best-numpy-cheat-sheets-every-python-coder-must-own/ Here are some of the resources used for creating this notebook: - Johnson, J. (2020). Python Numpy Tutorial (with Jupyter and Colab). Retrieved September 15, 2020, from https://cs231n.github.io/python-numpy-tutorial/ - Willems, K. (2019). (Tutorial) Python NUMPY Array TUTORIAL. Retrieved September 15, 2020, from https://www.datacamp.com/community/tutorials/python-numpy-tutorial?utm_source=adwords_ppc - Willems, K. (2017). NumPy Cheat Sheet: Data Analysis in Python. Retrieved September 15, 2020, from https://www.datacamp.com/community/blog/python-numpy-cheat-sheet - W3resource. (2020). NumPy: Compare two given arrays. Retrieved September 15, 2020, from https://www.w3resource.com/python-exercises/numpy/python-numpy-exercise-28.php Here are some great reads on this topic: - \"Python NumPy Tutorial\" available at https://www.geeksforgeeks.org/python-numpy-tutorial/ - \"What Is NumPy?\" a collection of blogs, available at https://realpython.com/tutorials/numpy/ - \"Look Ma, No For-Loops: Array Programming With NumPy\" by Brad Solomon available at https://realpython.com/numpy-array-programming/ - \"The Ultimate Beginner\u2019s Guide to NumPy\" by Anne Bonner available at https://towardsdatascience.com/the-ultimate-beginners-guide-to-numpy-f5a2f99aef54 Here are some great videos on these topics: - \"Learn NUMPY in 5 minutes - BEST Python Library!\" by Python Programmer available at https://www.youtube.com/watch?v=xECXZ3tyONo - \"Python NumPy Tutorial for Beginners\" by freeCodeCamp.org available at https://www.youtube.com/watch?v=QUT1VHiLmmI - \"Complete Python NumPy Tutorial (Creating Arrays, Indexing, Math, Statistics, Reshaping)\" by Keith Galli available at https://www.youtube.com/watch?v=GB9ByFAIAH4 - \"Python NumPy Tutorial | NumPy Array | Python Tutorial For Beginners | Python Training | Edureka\" by edureka! available at https://www.youtube.com/watch?v=8JfDAm9y_7s","title":"Check out this link for more: "},{"location":"8-Labs/Newly Formatted/Lab7/#exercise-python-list-vs-numpy-arrays","text":"","title":"Exercise: Python List vs. Numpy Arrays? "},{"location":"8-Labs/Newly Formatted/Lab7/#what-are-some-differences-between-python-lists-and-numpy-arrays","text":"","title":"What are some differences between Python lists and Numpy arrays?"},{"location":"8-Labs/Newly Formatted/Lab7/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab8/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab8 Laboratory 8: Pandas for Butter! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Pandas A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. To use pandas, we need to import the module, generally pandas has numpy as a dependency so it also must be imported import numpy as np #Importing NumPy library as \"np\" import pandas as pd #Importing Pandas library as \"pd\" Dataframe-structure using primative python First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. mytabular = np.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction print(mytabular) [[61 82 48 85] [45 36 97 72] [91 3 22 35] [18 65 30 63] [79 71 8 45]] The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 61, 82, 48, 85] ['B', 45, 36, 97, 72] ['C', 91, 3, 22, 35] ['D', 18, 65, 30, 63] ['E', 79, 71, 8, 45] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 91, 3, 22, 35] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 82 36 3 65 71 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 22 Create a proper dataframe We will now do the same using pandas df = pd.DataFrame(np.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 We can also turn our table into a dataframe, notice how the constructor adds header row and index column df1 = pd.DataFrame(mytable) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 61 82 48 85 2 B 45 36 97 72 3 C 91 3 22 35 4 D 18 65 30 63 5 E 79 71 8 45 To get proper behavior, we can just reuse our original objects df2 = pd.DataFrame(mytabular,myrowname,mycolname) df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 61 82 48 85 B 45 36 97 72 C 91 3 22 35 D 18 65 30 63 E 79 71 8 45 Getting the shape of dataframes The shape method will return the row and column rank (count) of a dataframe. df.shape (5, 4) df1.shape (6, 5) df2.shape (5, 4) Appending new columns To append a column simply assign a value to a new column name to the dataframe df['new']= 'NA' df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA Appending new rows A bit trickier but we can create a copy of a row and concatenate it back into the dataframe. newrow = df.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pd.concat([df,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA X 9 46 81 84 NA Removing Rows and Columns To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 Indexing We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 34 B 40 D 51 E 46 X 46 Name: X, dtype: int64 newtable[['X','W']] #Selecing multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 34 52 B 40 11 D 51 96 E 46 9 X 46 9 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 9 X 46 Y 81 Z 84 Name: E, dtype: int64 newtable.loc[['E','X','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 9 46 81 84 X 9 46 81 84 B 11 40 9 69 newtable.loc[['B','E','D'],['X','Y']] #Selecting elemens via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 40 9 E 46 81 D 51 89 Conditional Selection df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? df[df['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? df[df['col2']==df['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object Descriptor Functions #Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach head method Returns the first few rows, useful to infer structure #Returns only the first five rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit info method Returns the data model (data column count, names, data types) #Info about the dataframe df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes describe method Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000 Counting and Sum methods There are also methods for counts and sums by specific columns df['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) df['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values df['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) df['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64 Using functions in dataframes - symbolic apply The power of pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. Its pretty complicated but quite handy, best shown by an example def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(df) print('Apply the times2 function to col2') df['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64 Sorts df.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit Aggregating (Grouping Values) dataframe contents #Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } df1 = pd.DataFrame(data) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' df1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' df1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27 Filtering out missing values #Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach Reading a File into a Dataframe Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. import pandas as pd readfilecsv = pd.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 Writing a dataframe to file #Creating and writing to a .csv file readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pd.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pd.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') readfileexcel = pd.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 Unnamed: 0.1 a b c d 0 0 0 0 1 2 3 1 1 1 4 5 6 7 2 2 2 8 9 10 11 3 3 3 12 13 14 15 This is a Pandas Cheat Sheet Here are some of the resources used for creating this notebook: Pandas foundations. Retrieved February 15, 2021, from https://www.datacamp.com/courses/pandas-foundations Pandas tutorial. Retrieved February 15, 2021, from https://www.w3schools.com/python/pandas/default.asp Pandas tutorial: Dataframes in Python. Retrieved February 15, 2021, from https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python Here are some great reads on this topic: - \"Introduction to Pandas in Python\" available at https://www.geeksforgeeks.org/introduction-to-pandas-in-python/ - \"Pandas Introduction & Tutorials for Beginners\" by Walker Rowe , available at https://www.bmc.com/blogs/pandas-basics/ - \"Using Pandas and Python to Explore Your Dataset\" by Reka Horvath available at https://realpython.com/pandas-python-explore-dataset/ - \"Python Pandas Tutorial: A Complete Introduction for Beginners\" by George McIntire, Lauren Washington, and Brendan Martin available at https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/ Here are some great videos on these topics: - \"Python: Pandas Tutorial | Intro to DataFrames\" by Joe James available at https://www.youtube.com/watch?v=e60ItwlZTKM - \"Complete Python Pandas Data Science Tutorial! (Reading CSV/Excel files, Sorting, Filtering, Groupby)\" by Keith Galli available at https://www.youtube.com/watch?v=vmEHCJofslg - \"What is Pandas? Why and How to Use Pandas in Python\" by Python Programmer available at *https://www.youtube.com/watch?v=dcqPhpY7tWk Exercise: Pandas of Data Pandas library supports three major types of data structures: Series, DataFrames, and Panels. What are some differences between the three structures? * Make sure to cite any resources that you may use.","title":"Lab8"},{"location":"8-Labs/Newly Formatted/Lab8/#laboratory-8-pandas-for-butter","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Laboratory 8: Pandas for Butter! "},{"location":"8-Labs/Newly Formatted/Lab8/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab8/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab8/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Newly Formatted/Lab8/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab8/#pandas","text":"A data table is called a DataFrame in pandas (and other programming environments too). The figure below from https://pandas.pydata.org/docs/getting_started/index.html illustrates a dataframe model: Each column and each row in a dataframe is called a series, the header row, and index column are special. To use pandas, we need to import the module, generally pandas has numpy as a dependency so it also must be imported import numpy as np #Importing NumPy library as \"np\" import pandas as pd #Importing Pandas library as \"pd\"","title":"Pandas"},{"location":"8-Labs/Newly Formatted/Lab8/#dataframe-structure-using-primative-python","text":"First lets construct a dataframe like object using python primatives. We will construct 3 lists, one for row names, one for column names, and one for the content. mytabular = np.random.randint(1,100,(5,4)) myrowname = ['A','B','C','D','E'] mycolname = ['W','X','Y','Z'] mytable = [['' for jcol in range(len(mycolname)+1)] for irow in range(len(myrowname)+1)] #non-null destination matrix, note the implied loop construction print(mytabular) [[61 82 48 85] [45 36 97 72] [91 3 22 35] [18 65 30 63] [79 71 8 45]] The above builds a placeholder named mytable for the psuedo-dataframe. Next we populate the table, using a for loop to write the column names in the first row, row names in the first column, and the table fill for the rest of the table. for irow in range(1,len(myrowname)+1): # write the row names mytable[irow][0]=myrowname[irow-1] for jcol in range(1,len(mycolname)+1): # write the column names mytable[0][jcol]=mycolname[jcol-1] for irow in range(1,len(myrowname)+1): # fill the table (note the nested loop) for jcol in range(1,len(mycolname)+1): mytable[irow][jcol]=mytabular[irow-1][jcol-1] Now lets print the table out by row and we see we have a very dataframe-like structure for irow in range(0,len(myrowname)+1): print(mytable[irow][0:len(mycolname)+1]) ['', 'W', 'X', 'Y', 'Z'] ['A', 61, 82, 48, 85] ['B', 45, 36, 97, 72] ['C', 91, 3, 22, 35] ['D', 18, 65, 30, 63] ['E', 79, 71, 8, 45] We can also query by row print(mytable[3][0:len(mycolname)+1]) ['C', 91, 3, 22, 35] Or by column for irow in range(0,len(myrowname)+1): #cannot use implied loop in a column slice print(mytable[irow][2]) X 82 36 3 65 71 Or by row+column index; sort of looks like a spreadsheet syntax. print(' ',mytable[0][3]) print(mytable[3][0],mytable[3][3]) Y C 22","title":"Dataframe-structure using primative python"},{"location":"8-Labs/Newly Formatted/Lab8/#create-a-proper-dataframe","text":"We will now do the same using pandas df = pd.DataFrame(np.random.randint(1,100,(5,4)), ['A','B','C','D','E'], ['W','X','Y','Z']) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 We can also turn our table into a dataframe, notice how the constructor adds header row and index column df1 = pd.DataFrame(mytable) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 0 W X Y Z 1 A 61 82 48 85 2 B 45 36 97 72 3 C 91 3 22 35 4 D 18 65 30 63 5 E 79 71 8 45 To get proper behavior, we can just reuse our original objects df2 = pd.DataFrame(mytabular,myrowname,mycolname) df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 61 82 48 85 B 45 36 97 72 C 91 3 22 35 D 18 65 30 63 E 79 71 8 45","title":"Create a proper dataframe"},{"location":"8-Labs/Newly Formatted/Lab8/#getting-the-shape-of-dataframes","text":"The shape method will return the row and column rank (count) of a dataframe. df.shape (5, 4) df1.shape (6, 5) df2.shape (5, 4)","title":"Getting the shape of dataframes"},{"location":"8-Labs/Newly Formatted/Lab8/#appending-new-columns","text":"To append a column simply assign a value to a new column name to the dataframe df['new']= 'NA' df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA","title":"Appending new columns"},{"location":"8-Labs/Newly Formatted/Lab8/#appending-new-rows","text":"A bit trickier but we can create a copy of a row and concatenate it back into the dataframe. newrow = df.loc[['E']].rename(index={\"E\": \"X\"}) # create a single row, rename the index newtable = pd.concat([df,newrow]) # concatenate the row to bottom of df - note the syntax newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z new A 52 34 33 4 NA B 11 40 9 69 NA C 59 60 71 32 NA D 96 51 89 63 NA E 9 46 81 84 NA X 9 46 81 84 NA","title":"Appending new rows"},{"location":"8-Labs/Newly Formatted/Lab8/#removing-rows-and-columns","text":"To remove a column is straightforward, we use the drop method newtable.drop('new', axis=1, inplace = True) newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 C 59 60 71 32 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84 To remove a row, you really got to want to, easiest is probablty to create a new dataframe with the row removed newtable = newtable.loc[['A','B','D','E','X']] # select all rows except C newtable .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z A 52 34 33 4 B 11 40 9 69 D 96 51 89 63 E 9 46 81 84 X 9 46 81 84","title":"Removing Rows and Columns"},{"location":"8-Labs/Newly Formatted/Lab8/#indexing","text":"We have already been indexing, but a few examples follow: newtable['X'] #Selecing a single column A 34 B 40 D 51 E 46 X 46 Name: X, dtype: int64 newtable[['X','W']] #Selecing multiple columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X W A 34 52 B 40 11 D 51 96 E 46 9 X 46 9 newtable.loc['E'] #Selecing rows based on label via loc[ ] indexer W 9 X 46 Y 81 Z 84 Name: E, dtype: int64 newtable.loc[['E','X','B']] #Selecing multiple rows based on label via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } W X Y Z E 9 46 81 84 X 9 46 81 84 B 11 40 9 69 newtable.loc[['B','E','D'],['X','Y']] #Selecting elemens via both rows and columns via loc[ ] indexer .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y B 40 9 E 46 81 D 51 89","title":"Indexing"},{"location":"8-Labs/Newly Formatted/Lab8/#conditional-selection","text":"df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach #What fruit corresponds to the number 555 in \u2018col2\u2019? df[df['col2']==555]['col3'] 1 apple Name: col3, dtype: object #What fruit corresponds to the minimum number in \u2018col2\u2019? df[df['col2']==df['col2'].min()]['col3'] 5 watermelon Name: col3, dtype: object","title":"Conditional Selection"},{"location":"8-Labs/Newly Formatted/Lab8/#descriptor-functions","text":"#Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,5,6,7,8], 'col2':[444,555,666,444,666,111,222,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach","title":"Descriptor Functions"},{"location":"8-Labs/Newly Formatted/Lab8/#head-method","text":"Returns the first few rows, useful to infer structure #Returns only the first five rows df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit","title":"head method"},{"location":"8-Labs/Newly Formatted/Lab8/#info-method","text":"Returns the data model (data column count, names, data types) #Info about the dataframe df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 8 entries, 0 to 7 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 col1 8 non-null int64 1 col2 8 non-null int64 2 col3 8 non-null object dtypes: int64(2), object(1) memory usage: 320.0+ bytes","title":"info method"},{"location":"8-Labs/Newly Formatted/Lab8/#describe-method","text":"Returns summary statistics of each numeric column. Also returns the minimum and maximum value in each column, and the IQR (Interquartile Range). Again useful to understand structure of the columns. #Statistics of the dataframe df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 count 8.00000 8.0000 mean 4.50000 416.2500 std 2.44949 211.8576 min 1.00000 111.0000 25% 2.75000 222.0000 50% 4.50000 444.0000 75% 6.25000 582.7500 max 8.00000 666.0000","title":"describe method"},{"location":"8-Labs/Newly Formatted/Lab8/#counting-and-sum-methods","text":"There are also methods for counts and sums by specific columns df['col2'].sum() #Sum of a specified column 3330 The unique method returns a list of unique values (filters out duplicates in the list, underlying dataframe is preserved) df['col2'].unique() #Returns the list of unique values along the indexed column array([444, 555, 666, 111, 222]) The nunique method returns a count of unique values df['col2'].nunique() #Returns the total number of unique values along the indexed column 5 The value_counts() method returns the count of each unique value (kind of like a histogram, but each value is the bin) df['col2'].value_counts() #Returns the number of occurences of each unique value 222 2 444 2 666 2 111 1 555 1 Name: col2, dtype: int64","title":"Counting and Sum methods"},{"location":"8-Labs/Newly Formatted/Lab8/#using-functions-in-dataframes-symbolic-apply","text":"The power of pandas is an ability to apply a function to each element of a dataframe series (or a whole frame) by a technique called symbolic (or synthetic programming) application of the function. Its pretty complicated but quite handy, best shown by an example def times2(x): # A prototype function to scalar multiply an object x by 2 return(x*2) print(df) print('Apply the times2 function to col2') df['col2'].apply(times2) #Symbolic apply the function to each element of column col2, result is another dataframe col1 col2 col3 0 1 444 orange 1 2 555 apple 2 3 666 grape 3 4 444 mango 4 5 666 jackfruit 5 6 111 watermelon 6 7 222 banana 7 8 222 peach Apply the times2 function to col2 0 888 1 1110 2 1332 3 888 4 1332 5 222 6 444 7 444 Name: col2, dtype: int64","title":"Using functions in dataframes - symbolic apply"},{"location":"8-Labs/Newly Formatted/Lab8/#sorts","text":"df.sort_values('col2', ascending = True) #Sorting based on columns .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 5 6 111 watermelon 6 7 222 banana 7 8 222 peach 0 1 444 orange 3 4 444 mango 1 2 555 apple 2 3 666 grape 4 5 666 jackfruit","title":"Sorts"},{"location":"8-Labs/Newly Formatted/Lab8/#aggregating-grouping-values-dataframe-contents","text":"#Creating a dataframe from a dictionary data = { 'key' : ['A', 'B', 'C', 'A', 'B', 'C'], 'data1' : [1, 2, 3, 4, 5, 6], 'data2' : [10, 11, 12, 13, 14, 15], 'data3' : [20, 21, 22, 13, 24, 25] } df1 = pd.DataFrame(data) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key data1 data2 data3 0 A 1 10 20 1 B 2 11 21 2 C 3 12 22 3 A 4 13 13 4 B 5 14 24 5 C 6 15 25 # Grouping and summing values in all the columns based on the column 'key' df1.groupby('key').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 data3 key A 5 23 33 B 7 25 45 C 9 27 47 # Grouping and summing values in the selected columns based on the column 'key' df1.groupby('key')[['data1', 'data2']].sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } data1 data2 key A 5 23 B 7 25 C 9 27","title":"Aggregating (Grouping Values) dataframe contents"},{"location":"8-Labs/Newly Formatted/Lab8/#filtering-out-missing-values","text":"#Creating a dataframe from a dictionary df = pd.DataFrame({'col1':[1,2,3,4,None,6,7,None], 'col2':[444,555,None,444,666,111,None,222], 'col3':['orange','apple','grape','mango','jackfruit','watermelon','banana','peach']}) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 NaN grape 3 4.0 444.0 mango 4 NaN 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 NaN banana 7 NaN 222.0 peach Below we drop any row that contains a NaN code. df_dropped = df.dropna() df_dropped .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 3 4.0 444.0 mango 5 6.0 111.0 watermelon Below we replace NaN codes with some value, in this case 0 df_filled1 = df.fillna(0) df_filled1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.0 444.0 orange 1 2.0 555.0 apple 2 3.0 0.0 grape 3 4.0 444.0 mango 4 0.0 666.0 jackfruit 5 6.0 111.0 watermelon 6 7.0 0.0 banana 7 0.0 222.0 peach Below we replace NaN codes with some value, in this case the mean value of of the column in which the missing value code resides. df_filled2 = df.fillna(df.mean()) df_filled2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } col1 col2 col3 0 1.000000 444.0 orange 1 2.000000 555.0 apple 2 3.000000 407.0 grape 3 4.000000 444.0 mango 4 3.833333 666.0 jackfruit 5 6.000000 111.0 watermelon 6 7.000000 407.0 banana 7 3.833333 222.0 peach","title":"Filtering out missing values"},{"location":"8-Labs/Newly Formatted/Lab8/#reading-a-file-into-a-dataframe","text":"Pandas has methods to read common file types, such as csv , xlsx , and json . Ordinary text files are also quite manageable. On a machine you control you can write script to retrieve files from the internet and process them. import pandas as pd readfilecsv = pd.read_csv('CSV_ReadingFile.csv') #Reading a .csv file print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 Similar to reading and writing .csv files, you can also read and write .xslx files as below (useful to know this) readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') #Reading a .xlsx file print(readfileexcel) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15","title":"Reading a File into a Dataframe"},{"location":"8-Labs/Newly Formatted/Lab8/#writing-a-dataframe-to-file","text":"#Creating and writing to a .csv file readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile1.csv') readfilecsv = pd.read_csv('CSV_WritingFile1.csv') print(readfilecsv) Unnamed: 0 a b c d 0 0 0 1 2 3 1 1 4 5 6 7 2 2 8 9 10 11 3 3 12 13 14 15 #Creating and writing to a .csv file by excluding row labels readfilecsv = pd.read_csv('CSV_ReadingFile.csv') readfilecsv.to_csv('CSV_WritingFile2.csv', index = False) readfilecsv = pd.read_csv('CSV_WritingFile2.csv') print(readfilecsv) a b c d 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 3 12 13 14 15 #Creating and writing to a .xlsx file readfileexcel = pd.read_excel('Excel_ReadingFile.xlsx', sheet_name='Sheet1', engine='openpyxl') readfileexcel.to_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') readfileexcel = pd.read_excel('Excel_WritingFile.xlsx', sheet_name='MySheet', engine='openpyxl') print(readfileexcel) Unnamed: 0 Unnamed: 0.1 a b c d 0 0 0 0 1 2 3 1 1 1 4 5 6 7 2 2 2 8 9 10 11 3 3 3 12 13 14 15","title":"Writing a dataframe to file"},{"location":"8-Labs/Newly Formatted/Lab8/#this-is-a-pandas-cheat-sheet","text":"Here are some of the resources used for creating this notebook: Pandas foundations. Retrieved February 15, 2021, from https://www.datacamp.com/courses/pandas-foundations Pandas tutorial. Retrieved February 15, 2021, from https://www.w3schools.com/python/pandas/default.asp Pandas tutorial: Dataframes in Python. Retrieved February 15, 2021, from https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python Here are some great reads on this topic: - \"Introduction to Pandas in Python\" available at https://www.geeksforgeeks.org/introduction-to-pandas-in-python/ - \"Pandas Introduction & Tutorials for Beginners\" by Walker Rowe , available at https://www.bmc.com/blogs/pandas-basics/ - \"Using Pandas and Python to Explore Your Dataset\" by Reka Horvath available at https://realpython.com/pandas-python-explore-dataset/ - \"Python Pandas Tutorial: A Complete Introduction for Beginners\" by George McIntire, Lauren Washington, and Brendan Martin available at https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/ Here are some great videos on these topics: - \"Python: Pandas Tutorial | Intro to DataFrames\" by Joe James available at https://www.youtube.com/watch?v=e60ItwlZTKM - \"Complete Python Pandas Data Science Tutorial! (Reading CSV/Excel files, Sorting, Filtering, Groupby)\" by Keith Galli available at https://www.youtube.com/watch?v=vmEHCJofslg - \"What is Pandas? Why and How to Use Pandas in Python\" by Python Programmer available at *https://www.youtube.com/watch?v=dcqPhpY7tWk","title":"This is a Pandas Cheat Sheet"},{"location":"8-Labs/Newly Formatted/Lab8/#exercise-pandas-of-data","text":"","title":"Exercise: Pandas of Data  "},{"location":"8-Labs/Newly Formatted/Lab8/#pandas-library-supports-three-major-types-of-data-structures-series-dataframes-and-panels-what-are-some-differences-between-the-three-structures","text":"","title":"Pandas library supports three major types of data structures: Series, DataFrames, and Panels. What are some differences between the three structures?"},{"location":"8-Labs/Newly Formatted/Lab8/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"8-Labs/Newly Formatted/Lab9/","text":"Download (right-click, save target as ...) this page as a jupyterlab notebook from: Lab9 Laboratory 9: Matplotlib for Jam! # Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0) Full name: R#: Title of the notebook: Date: Matplotlip and Visual Display of Data This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib About matplotlib Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis). Background Data are not always numerical. Data can music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) They can also be categorical into which you can place individuals: - The individuals are cartons of ice-cream, and the category is the flavor in the carton - The individuals are professional basketball players, and the category is the player's team. Bar Graphs Bar charts (graphs) are good display tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='maroon', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='orange', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.bar(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:xlabel='Flavor'> Example- Language Bars! Consider the data set \"data\" defined as data = {'C':20, 'C++':15, 'Java':30, 'Python':35} which lists student count by programming language in some school. Produce a bar chart of number of students in each language, where language is the classification, and student count is the variable. # Code and run your solution here import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show() Plot it as a horizontal bar chart: # Code and run your solution here # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.barh(courses, values, color ='maroon', height = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show() Line Charts A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application. Example- Speed vs Time Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() From examination of the plot, estimate the speed at time t = 5.0 (eyeball estimate) Example- Add a linear fit Using the same series from Exercise 1, Plot the speed vs time (speed on y-axis, time on x-axis) using a line plot. Plot a second line based on the linear model y = mx + b , where b=0~\\text{and}~m=7.6 . # Code and run your solution here: def ymodel(xmodel,slope,intercept): ymodel = slope*xmodel+intercept return(ymodel) yseries = [] slope = 7.6 intercept = 0.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0.5) # basic line plot plt.plot(time, yseries, c='blue') plt.show() Example- Find a better fit Using trial and error try to improve the 'fit' of the model, by adjusting values of m~\\text{and}~b . # Code and run your solution here: yseries = [] slope = 7.6 intercept = -8.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0) # basic scatter plot plt.plot(time, yseries, c='blue') plt.show() Scatter Plots A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot Example- Examine the dataset with heights of fathers, mothers and sons df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists dad = df['father'] ; mom = df['mother'] ; son = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red' , label='Father') # one plot series plt.scatter(son, mom, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\") Histograms Quoting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson.[1] To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\" Example- Explore the \"top_movies\" dataset and draw histograms for Gross and Year. import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Year\"]].hist() array([[<AxesSubplot:title={'center':'Year'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) This is a Matplotlib Cheat Sheet Here are some of the resources used for creating this notebook: \"Discrete distribution as horizontal bar chart\" available at *https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html \"Bar Plot in Matplotlib\" available at *https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ Here are some great reads on this topic: - \"Python | Introduction to Matplotlib\" available at https://www.geeksforgeeks.org/python-introduction-matplotlib/ - \"Visualization with Matplotlib\" available at https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html - \"Introduction to Matplotlib \u2014 Data Visualization in Python\" by Ehi Aigiomawu available at https://heartbeat.fritz.ai/introduction-to-matplotlib-data-visualization-in-python-d9143287ae39 - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"Matplotlib Tutorial (Part 1): Creating and Customizing Our First Plots\" by Corey Schafer available at https://www.youtube.com/watch?v=UO98lJQ3QGI - \"Intro to Data Analysis / Visualization with Python, Matplotlib and Pandas | Matplotlib Tutorial\" by CS Dojo available at https://www.youtube.com/watch?v=a9UrKTVEeZA - \"Intro to Data Visualization in Python with Matplotlib! (line graph, bar chart, title, labels, size)\" by Keith Galli available at *https://www.youtube.com/watch?v=DAQNHzOcO5A Exercise: Bins, Bins, Bins! Selecting the number of bins is an important decision when working with histograms. Are there any rules or recommendations for choosing the number or width of bins? What happens if we use too many or too few bins? * Make sure to cite any resources that you may use.","title":"Lab9"},{"location":"8-Labs/Newly Formatted/Lab9/#laboratory-9-matplotlib-for-jam","text":"# Preamble script block to identify host, user, and kernel import sys ! hostname ! whoami print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Laboratory 9: Matplotlib for Jam! "},{"location":"8-Labs/Newly Formatted/Lab9/#full-name","text":"","title":"Full name:"},{"location":"8-Labs/Newly Formatted/Lab9/#r","text":"","title":"R#:"},{"location":"8-Labs/Newly Formatted/Lab9/#title-of-the-notebook","text":"","title":"Title of the notebook:"},{"location":"8-Labs/Newly Formatted/Lab9/#date","text":"","title":"Date:"},{"location":"8-Labs/Newly Formatted/Lab9/#matplotlip-and-visual-display-of-data","text":"This lesson will introduce the matplotlib external module package, and examine how to construct line charts, scatter plots, bar charts, and histograms using methods in matplotlib and pandas The theory of histograms will appear in later lessons, here we only show how to construct one using matplotlib","title":"Matplotlip and Visual Display of Data"},{"location":"8-Labs/Newly Formatted/Lab9/#about-matplotlib","text":"Quoting from: https://matplotlib.org/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py matplotlib.pyplot is a collection of functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc. In matplotlib.pyplot various states are preserved across function calls, so that it keeps track of things like the current figure and plotting area, and the plotting functions are directed to the current axes (please note that \"axes\" here and in most places in the documentation refers to the axes part of a figure and not the strict mathematical term for more than one axis).","title":"About matplotlib"},{"location":"8-Labs/Newly Formatted/Lab9/#background","text":"Data are not always numerical. Data can music (audio files), or places on a map (georeferenced attributes files), images (various imge files, e.g. .png, jpeg) They can also be categorical into which you can place individuals: - The individuals are cartons of ice-cream, and the category is the flavor in the carton - The individuals are professional basketball players, and the category is the player's team.","title":"Background"},{"location":"8-Labs/Newly Formatted/Lab9/#bar-graphs","text":"Bar charts (graphs) are good display tools to graphically represent categorical information. The bars are evenly spaced and of constant width. The height/length of each bar is proportional to the relative frequency of the corresponding category. Relative frequency is the ratio of how many things in the category to how many things in the whole collection. The example below uses matplotlib to create a box plot for the ice cream analogy, the example is adapted from an example at https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = matplotlib.pyplot.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot matplotlib.pyplot.bar(flavors, cartons, color ='maroon', width = 0.4) matplotlib.pyplot.xlabel(\"Flavors\") matplotlib.pyplot.ylabel(\"No. of Cartons in Stock\") matplotlib.pyplot.title(\"Current Ice Cream in Storage\") matplotlib.pyplot.show() Lets tidy up the script so it is more understandable, a small change in the import statement makes a simpler to read (for humans) script - also changed the bar colors just 'cause! ice_cream = {'Chocolate':16, 'Strawberry':5, 'Vanilla':9} # build a data model import matplotlib.pyplot as plt # the python plotting library flavors = list(ice_cream.keys()) # make a list object based on flavors cartons = list(ice_cream.values()) # make a list object based on carton count -- assumes 1:1 association! myfigure = plt.figure(figsize = (10,5)) # generate a object from the figure class, set aspect ratio # Built the plot plt.bar(flavors, cartons, color ='orange', width = 0.4) plt.xlabel(\"Flavors\") plt.ylabel(\"No. of Cartons in Stock\") plt.title(\"Current Ice Cream in Storage\") plt.show() Using pandas, we can build bar charts a bit easier. import pandas as pd my_data = { \"Flavor\": ['Chocolate', 'Strawberry', 'Vanilla'], \"Number of Cartons\": [16, 5, 9] } df = pd.DataFrame(my_data) df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Flavor Number of Cartons 0 Chocolate 16 1 Strawberry 5 2 Vanilla 9 df.plot.bar(x='Flavor', y='Number of Cartons', color='magenta' ) <AxesSubplot:xlabel='Flavor'> df.plot.bar(x='Flavor', y='Number of Cartons', color=\"red\") # rotate the category labels <AxesSubplot:xlabel='Flavor'>","title":"Bar Graphs"},{"location":"8-Labs/Newly Formatted/Lab9/#example-language-bars","text":"Consider the data set \"data\" defined as data = {'C':20, 'C++':15, 'Java':30, 'Python':35} which lists student count by programming language in some school. Produce a bar chart of number of students in each language, where language is the classification, and student count is the variable. # Code and run your solution here import numpy as np import matplotlib.pyplot as plt # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.bar(courses, values, color ='maroon', width = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show()","title":"Example- Language Bars!"},{"location":"8-Labs/Newly Formatted/Lab9/#plot-it-as-a-horizontal-bar-chart","text":"# Code and run your solution here # creating the dataset data = {'C':20, 'C++':15, 'Java':30, 'Python':35} courses = list(data.keys()) values = list(data.values()) fig = plt.figure(figsize = (10, 5)) # creating the bar plot plt.barh(courses, values, color ='maroon', height = 0.4) plt.xlabel(\"Courses offered\") plt.ylabel(\"No. of students enrolled\") plt.title(\"Students enrolled in different courses\") plt.show()","title":"Plot it as a horizontal bar chart:"},{"location":"8-Labs/Newly Formatted/Lab9/#line-charts","text":"A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments. It is a basic type of chart common in many fields. It is similar to a scatter plot (below) except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments. A line chart is often used to visualize a trend in data over intervals of time \u2013 a time series \u2013 thus the line is often drawn chronologically. The x-axis spacing is sometimes tricky, hence line charts can unintentionally decieve - so be careful that it is the appropriate chart for your application.","title":"Line Charts"},{"location":"8-Labs/Newly Formatted/Lab9/#example-speed-vs-time","text":"Consider the experimental data below Elapsed Time (s) Speed (m/s) 0 0 1.0 3 2.0 7 3.0 12 4.0 20 5.0 30 6.0 45.6 Show the relationship between time and speed. Is the relationship indicating acceleration? How much? # Create two lists; time and speed. time = [0,1.0,2.0,3.0,4.0,5.0,6.0] speed = [0,3,7,12,20,30,45.6] # Create a line chart of speed on y axis and time on x axis mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='v',linewidth=1) # basic line plot plt.show() From examination of the plot, estimate the speed at time t = 5.0 (eyeball estimate)","title":"Example- Speed vs Time"},{"location":"8-Labs/Newly Formatted/Lab9/#example-add-a-linear-fit","text":"Using the same series from Exercise 1, Plot the speed vs time (speed on y-axis, time on x-axis) using a line plot. Plot a second line based on the linear model y = mx + b , where b=0~\\text{and}~m=7.6 . # Code and run your solution here: def ymodel(xmodel,slope,intercept): ymodel = slope*xmodel+intercept return(ymodel) yseries = [] slope = 7.6 intercept = 0.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0.5) # basic line plot plt.plot(time, yseries, c='blue') plt.show()","title":"Example- Add a linear fit"},{"location":"8-Labs/Newly Formatted/Lab9/#example-find-a-better-fit","text":"Using trial and error try to improve the 'fit' of the model, by adjusting values of m~\\text{and}~b . # Code and run your solution here: yseries = [] slope = 7.6 intercept = -8.0 for i in range(0,len(time)): yseries.append(ymodel(time[i],slope,intercept)) # Create a markers only line chart mydata = plt.figure(figsize = (10,5)) # build a square drawing canvass from figure class plt.plot(time, speed, c='red', marker='^',linewidth=0) # basic scatter plot plt.plot(time, yseries, c='blue') plt.show()","title":"Example- Find a better fit"},{"location":"8-Labs/Newly Formatted/Lab9/#scatter-plots","text":"A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis. A scatter plot can be used either when one continuous variable that is under the control of the experimenter and the other depends on it or when both continuous variables are independent. If a parameter exists that is systematically incremented and/or decremented by the other, it is called the control parameter or independent variable and is customarily plotted along the horizontal axis. The measured or dependent variable is customarily plotted along the vertical axis. If no dependent variable exists, either type of variable can be plotted on either axis and a scatter plot will illustrate only the degree of correlation (not causation) between two variables. A scatter plot can suggest various kinds of correlations between variables with a certain confidence interval. For example, weight and height, weight would be on y axis and height would be on the x axis. Correlations may be positive (rising), negative (falling), or null (uncorrelated). If the pattern of dots slopes from lower left to upper right, it indicates a positive correlation between the variables being studied. If the pattern of dots slopes from upper left to lower right, it indicates a negative correlation. A line of best fit (alternatively called 'trendline') can be drawn in order to study the relationship between the variables. An equation for the correlation between the variables can be determined by established best-fit procedures. For a linear correlation, the best-fit procedure is known as linear regression and is guaranteed to generate a correct solution in a finite time. No universal best-fit procedure is guaranteed to generate a solution for arbitrary relationships. A scatter plot is also very useful when we wish to see how two comparable data sets agree and to show nonlinear relationships between variables. Furthermore, if the data are represented by a mixture model of simple relationships, these relationships will be visually evident as superimposed patterns. Scatter charts can be built in the form of bubble, marker, or/and line charts. Much of the above is verbatim/adapted from: https://en.wikipedia.org/wiki/Scatter_plot","title":"Scatter Plots"},{"location":"8-Labs/Newly Formatted/Lab9/#example-examine-the-dataset-with-heights-of-fathers-mothers-and-sons","text":"df = pd.read_csv('galton_subset.csv') df['child']= df['son'] ; df.drop('son', axis=1, inplace = True) # rename son to child - got to imagine there are some daughters df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } father mother child 0 78.5 67.0 73.2 1 75.5 66.5 73.5 2 75.0 64.0 71.0 3 75.0 64.0 70.5 4 75.0 58.5 72.0 # build some lists dad = df['father'] ; mom = df['mother'] ; son = df['child'] myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red') # basic scatter plot plt.show() # Looks lousy, needs some labels myfamily = plt.figure(figsize = (10, 10)) # build a square drawing canvass from figure class plt.scatter(son, dad, c='red' , label='Father') # one plot series plt.scatter(son, mom, c='blue', label='Mother') # two plot series plt.xlabel(\"Child's height\") plt.ylabel(\"Parents' height\") plt.legend() plt.show() # render the two plots # Repeat in pandas - The dataframe already is built df.plot.scatter(x=\"child\", y=\"father\") <AxesSubplot:xlabel='child', ylabel='father'> ax = df.plot.scatter(x=\"child\", y=\"father\", c=\"red\", label='Father') df.plot.scatter(x=\"child\", y=\"mother\", c=\"blue\", label='Mother', ax=ax) ax.set_xlabel(\"Child's height\") ax.set_ylabel(\"Parents' Height\") Text(0, 0.5, \"Parents' Height\")","title":"Example- Examine the dataset with heights of fathers, mothers and sons"},{"location":"8-Labs/Newly Formatted/Lab9/#histograms","text":"Quoting from https://en.wikipedia.org/wiki/Histogram \"A histogram is an approximate representation of the distribution of numerical data. It was first introduced by Karl Pearson.[1] To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014that is, divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval. The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent, and are often (but not required to be) of equal size. If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency\u2014the number of cases in each bin. A histogram may also be normalized to display \"relative\" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1. However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density\u2014the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below. As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous. Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot. A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently. An alternative to kernel density estimation is the average shifted histogram, which is fast to compute and gives a smooth curve estimate of the density without using kernels. The histogram is one of the seven basic tools of quality control. Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.\"","title":"Histograms"},{"location":"8-Labs/Newly Formatted/Lab9/#example-explore-the-top_movies-dataset-and-draw-histograms-for-gross-and-year","text":"import pandas as pd df = pd.read_csv('top_movies.csv') df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Title Studio Gross Gross (Adjusted) Year 0 Star Wars: The Force Awakens Buena Vista (Disney) 906723418 906723400 2015 1 Avatar Fox 760507625 846120800 2009 2 Titanic Paramount 658672302 1178627900 1997 3 Jurassic World Universal 652270625 687728000 2015 4 Marvel's The Avengers Buena Vista (Disney) 623357910 668866600 2012 df[[\"Gross\"]].hist() array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object) df[[\"Year\"]].hist() array([[<AxesSubplot:title={'center':'Year'}>]], dtype=object) df[[\"Gross\"]].hist(bins=100) array([[<AxesSubplot:title={'center':'Gross'}>]], dtype=object)","title":"Example- Explore the \"top_movies\" dataset and draw histograms for Gross and Year."},{"location":"8-Labs/Newly Formatted/Lab9/#this-is-a-matplotlib-cheat-sheet","text":"Here are some of the resources used for creating this notebook: \"Discrete distribution as horizontal bar chart\" available at *https://matplotlib.org/stable/gallery/lines_bars_and_markers/horizontal_barchart_distribution.html \"Bar Plot in Matplotlib\" available at *https://www.geeksforgeeks.org/bar-plot-in-matplotlib/ Here are some great reads on this topic: - \"Python | Introduction to Matplotlib\" available at https://www.geeksforgeeks.org/python-introduction-matplotlib/ - \"Visualization with Matplotlib\" available at https://jakevdp.github.io/PythonDataScienceHandbook/04.00-introduction-to-matplotlib.html - \"Introduction to Matplotlib \u2014 Data Visualization in Python\" by Ehi Aigiomawu available at https://heartbeat.fritz.ai/introduction-to-matplotlib-data-visualization-in-python-d9143287ae39 - \"Python Plotting With Matplotlib (Guide)\" by Brad Solomon available at https://realpython.com/python-matplotlib-guide/ Here are some great videos on these topics: - \"Matplotlib Tutorial (Part 1): Creating and Customizing Our First Plots\" by Corey Schafer available at https://www.youtube.com/watch?v=UO98lJQ3QGI - \"Intro to Data Analysis / Visualization with Python, Matplotlib and Pandas | Matplotlib Tutorial\" by CS Dojo available at https://www.youtube.com/watch?v=a9UrKTVEeZA - \"Intro to Data Visualization in Python with Matplotlib! (line graph, bar chart, title, labels, size)\" by Keith Galli available at *https://www.youtube.com/watch?v=DAQNHzOcO5A","title":"This is a Matplotlib Cheat Sheet"},{"location":"8-Labs/Newly Formatted/Lab9/#exercise-bins-bins-bins","text":"","title":"Exercise: Bins, Bins, Bins!  "},{"location":"8-Labs/Newly Formatted/Lab9/#selecting-the-number-of-bins-is-an-important-decision-when-working-with-histograms-are-there-any-rules-or-recommendations-for-choosing-the-number-or-width-of-bins-what-happens-if-we-use-too-many-or-too-few-bins","text":"","title":"Selecting the number of bins is an important decision when working with histograms. Are there any rules or recommendations for choosing the number or width of bins? What happens if we use too many or too few bins?"},{"location":"8-Labs/Newly Formatted/Lab9/#make-sure-to-cite-any-resources-that-you-may-use","text":"","title":"* Make sure to cite any resources that you may use."},{"location":"lesson0/lesson0/","text":"table {margin-left: 0 !important;} ENGR 1330 Computational Thinking with Data Science Last GitHub Commit Date: 13 January 2021 Lesson 0 Introduction to Computational Thinking with Data Science: Computational thinking concepts Data science and practices JupyterLab (iPython) as a programming environment Programming as a problem solving process CCMR Approach Special Script Blocks In the lesson notebooks there will usually be two script blocks, identical to the ones below. The first block identifies the particular computer, the user, and the python kernel in use. The second block sets markdown tables to left edge when rendering. I usually put both blocks at the top of the notebook, just after some kind of title block, as done here. Computational Thinking Concepts Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. Much of what follows is borrowed from (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2696102/). Computational thinking is taking an approach to solving problems, designing systems and understanding human behaviour that draws on concepts fundamental to computing (http://www.cs.cmu.edu/~15110-s13/Wing06-ct.pdf). Computational thinking is a kind of analytical thinking: It shares with mathematical thinking in the general ways in which we might approach solving a problem. It shares with engineering thinking in the general ways in which we might approach designing and evaluating a large, complex system that operates within the constraints of the real world. - It shares with scientific thinking in the general ways in which we might approach understanding computability, intelligence, the mind and human behaviour. The essence of computational thinking is abstraction and automation . In computing, we abstract notions beyond the physical dimensions of time and space. Our abstractions are extremely general because they are symbolic, where numeric abstractions are just a special case. CT Foundations CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation) Decomposition Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Examples include: - Writing a paper: - Introduction - Body - Conclusion Wide-viewed (Panorama) image: Taking multiple overlapped photos Stitch them Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution. Pattern Recognition Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it also will require assembly (system integration) to produce a desired solution. Abstraction Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve. Books in an online bookstore Important NOT important title Cover color ISBN Author\u2019s hometown Authors ... ... ... Algorithms Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. Image from https://www.newyorker.com/magazine/2021/01/18/whats-wrong-with-the-way-we-work?utm_source=pocket-newtab An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation. Algorithms are unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, can incorporate random input. System Integration (implementation) System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand. Data Science and Practice Data science is leveraging existing data sources, to create new ones as needed in order to extract meaningful information and actionable insights through business domain expertise, effective communication and results interpretation. Data science uses relevant statistical techniques, programming languages, software packages and libraries, and data infrastructure; The insights are used to drive business decisions and take actions intended to achieve business goals. Why is this important for engineers? Because engineering is a business! A list of typical skills (https://elitedatascience.com/data-science-resources): Foundational Skills Programming and Data Manipulation Statistics and Probability Technical Skills Data Collection SQL Data Visualization Applied Machine Learning Business Skills Communication Creativity and Innovation Operations and Strategy Business Analytics Supplementary Skills Natural Language Processing Recommendation Systems Time Series Analysis Practice Projects Competitions Problem Solving Challenges JupyterLab (iPython) Environment The tools: JupyterLab (https://jupyter.org/) is a web-based interactive development environment for Jupyter notebooks, code, and data. Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data organizing and transformation, numerical simulation, statistical modeling, visualization, machine learning, and other similar types of uses. JupyterHub (https://github.com/jupyterhub/jupyterhub) is a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server. All these tools allow use of various coding languages; Python is the choice for ENGR 1330. Installing JupyterLab on your own computer is relatively straightforward if it is an Intel-based Linux, Macintosh, or Windows machine - simply use Anaconda (https://www.anaconda.com/) as the installer. Installing onto an ARM-based machine is more difficult, but possible (this notebook was created on a Raspberry Pi). With both Apple and Microsoft abandoning Intel you can expect Anaconda builds for aarch64 (ARM) in the future. This course: You will create and use Jupyter Notebooks that use the ipython kernel, the notebook files will look like filename.ipynb ; these are ASCII files that the JupyterLab interprets and runs. Python The programming language we will use is Python (actually iPython). Python is an example of a high-level language; other high-level languages include C, C++, PHP, FORTRAN, ADA, Pascal, Go, Java, etc (there are a lot). As you might infer from the name high-level language, there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous. First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with few or no modifications. Low-level programs can run on only one kind of computer and have to be rewritten to run on another. Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications, and for device drivers. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. Interpreted Program. Image from (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) A compiler reads the program and translates it completely before the program starts running. In this case, the high-level program is called the source code, and the translated program is called the object code or the executable. Once a program is compiled, you can execute it repeatedly without further translation. Compiled Prorgam. Image from: (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) Many modern languages use both processes. They are first compiled into a lower level language, called byte code, and then interpreted by a program called a virtual machine. Python uses both processes, but because of the way programmers interact with it, it is usually considered an interpreted language. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Formal languages are languages that are designed by people for specific applications. For example, the notation that mathematicians use is a formal language that is particularly good at denoting relationships among numbers and symbols. Chemists use a formal language to represent the chemical structure of molecules. Programming languages are formal languages that have been designed to express computations. Formal languages have strict rules about syntax. For example, 3+3=6 is a syntactically correct mathematical statement, but 3=+6& is not. Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. One of the problems with 3=+6& is that & is not a legal token in mathematics (at least as far as we know). The second type of syntax rule pertains to the structure of a statement\u2014 that is, the way the tokens are arranged. The statement 3=+6& is structurally illegal (in mathematics) because you don\u2019t place a plus sign immediately after an equal sign (of course we will in python!). When you read a sentence in English or a statement in a formal language, you have to figure out what the structure of the sentence is; This process is called parsing . For example, when you hear the sentence, \u201cThe other shoe fell\u201d, you understand that the other shoe is the subject and fell is the verb. Once you have parsed a sentence, you can figure out what it means, or the semantics of the sentence. Assuming that you know what a shoe is and what it means to fall, you will understand the general implication of this sentence. Good Resources: Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science) Programming as a problem solving process The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method (https://en.wikipedia.org/wiki/Scientific_method) is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Generate and evaluate potential solutions Refine and implement a solution Verify and test the solution. For actual computational methods the protocol becomes: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4). Example 1 Problem Solving Process Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background. CCMR Approach A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity. Readings Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /home/sensei/1330-textbook-webroot/docs/lesson0 /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Engine Update"},{"location":"lesson0/lesson0/#engr-1330-computational-thinking-with-data-science","text":"Last GitHub Commit Date: 13 January 2021","title":"ENGR 1330 Computational Thinking with Data Science"},{"location":"lesson0/lesson0/#lesson-0-introduction-to-computational-thinking-with-data-science","text":"Computational thinking concepts Data science and practices JupyterLab (iPython) as a programming environment Programming as a problem solving process CCMR Approach","title":"Lesson 0 Introduction to Computational Thinking with Data Science:"},{"location":"lesson0/lesson0/#special-script-blocks","text":"In the lesson notebooks there will usually be two script blocks, identical to the ones below. The first block identifies the particular computer, the user, and the python kernel in use. The second block sets markdown tables to left edge when rendering. I usually put both blocks at the top of the notebook, just after some kind of title block, as done here.","title":"Special Script Blocks"},{"location":"lesson0/lesson0/#computational-thinking-concepts","text":"Computational thinking (CT) refers to the thought processes involved in expressing solutions as computational steps or algorithms that can be carried out by a computer. Much of what follows is borrowed from (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2696102/). Computational thinking is taking an approach to solving problems, designing systems and understanding human behaviour that draws on concepts fundamental to computing (http://www.cs.cmu.edu/~15110-s13/Wing06-ct.pdf). Computational thinking is a kind of analytical thinking: It shares with mathematical thinking in the general ways in which we might approach solving a problem. It shares with engineering thinking in the general ways in which we might approach designing and evaluating a large, complex system that operates within the constraints of the real world. - It shares with scientific thinking in the general ways in which we might approach understanding computability, intelligence, the mind and human behaviour. The essence of computational thinking is abstraction and automation . In computing, we abstract notions beyond the physical dimensions of time and space. Our abstractions are extremely general because they are symbolic, where numeric abstractions are just a special case.","title":"Computational Thinking Concepts"},{"location":"lesson0/lesson0/#ct-foundations","text":"CT is literally a process for breaking down a problem into smaller parts, looking for patterns in the problems, identifying what kind of information is needed, developing a step-by-step solution, and implementing that solution. Decomposition Pattern Recognition Abstraction Algorithms System Integration (implementation)","title":"CT Foundations"},{"location":"lesson0/lesson0/#decomposition","text":"Decomposition is the process of taking a complex problem and breaking it into more manageable sub-problems. Examples include: - Writing a paper: - Introduction - Body - Conclusion Wide-viewed (Panorama) image: Taking multiple overlapped photos Stitch them Decomposition often leaves a framework of sub-problems that later have to be assembled (system integration) to produce a desired solution.","title":"Decomposition"},{"location":"lesson0/lesson0/#pattern-recognition","text":"Refers to finding similarities, or shared characteristics of problems. Allows a complex problem to become easier to solve. Allows use of same solution method for each occurrence of the pattern. Pattern recognition allows use of automation to process things - its a fundamental drilled shaft of CT. It also provides a way to use analogs from old problems to address new situations; it also will require assembly (system integration) to produce a desired solution.","title":"Pattern Recognition"},{"location":"lesson0/lesson0/#abstraction","text":"Determine important characteristics of the problem and ignore characteristics that are not important. Use these characteristics to create a representation of what we are trying to solve. Books in an online bookstore Important NOT important title Cover color ISBN Author\u2019s hometown Authors ... ... ...","title":"Abstraction"},{"location":"lesson0/lesson0/#algorithms","text":"Step-by-step instructions of how to solve a problem (https://en.wikipedia.org/wiki/Algorithm). Identifies what is to be done, and the order in which they should be done. Image from https://www.newyorker.com/magazine/2021/01/18/whats-wrong-with-the-way-we-work?utm_source=pocket-newtab An algorithm is a finite sequence of defined, instructions, typically to solve a class of problems or to perform a computation. Algorithms are unambiguous and are used as specifications for performing calculations, data processing, automated reasoning, and other tasks. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, can incorporate random input.","title":"Algorithms"},{"location":"lesson0/lesson0/#system-integration-implementation","text":"System integration is the assembly of the parts above into the complete (integrated) solution. Integration combines parts into a program which is the realization of an algorithm using a syntax that the computer can understand.","title":"System Integration (implementation)"},{"location":"lesson0/lesson0/#data-science-and-practice","text":"Data science is leveraging existing data sources, to create new ones as needed in order to extract meaningful information and actionable insights through business domain expertise, effective communication and results interpretation. Data science uses relevant statistical techniques, programming languages, software packages and libraries, and data infrastructure; The insights are used to drive business decisions and take actions intended to achieve business goals. Why is this important for engineers? Because engineering is a business! A list of typical skills (https://elitedatascience.com/data-science-resources): Foundational Skills Programming and Data Manipulation Statistics and Probability Technical Skills Data Collection SQL Data Visualization Applied Machine Learning Business Skills Communication Creativity and Innovation Operations and Strategy Business Analytics Supplementary Skills Natural Language Processing Recommendation Systems Time Series Analysis Practice Projects Competitions Problem Solving Challenges","title":"Data Science and Practice"},{"location":"lesson0/lesson0/#jupyterlab-ipython-environment","text":"","title":"JupyterLab (iPython) Environment"},{"location":"lesson0/lesson0/#the-tools","text":"JupyterLab (https://jupyter.org/) is a web-based interactive development environment for Jupyter notebooks, code, and data. Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text. Uses include: data organizing and transformation, numerical simulation, statistical modeling, visualization, machine learning, and other similar types of uses. JupyterHub (https://github.com/jupyterhub/jupyterhub) is a multi-user Hub that spawns, manages, and proxies multiple instances of the single-user Jupyter notebook server. All these tools allow use of various coding languages; Python is the choice for ENGR 1330. Installing JupyterLab on your own computer is relatively straightforward if it is an Intel-based Linux, Macintosh, or Windows machine - simply use Anaconda (https://www.anaconda.com/) as the installer. Installing onto an ARM-based machine is more difficult, but possible (this notebook was created on a Raspberry Pi). With both Apple and Microsoft abandoning Intel you can expect Anaconda builds for aarch64 (ARM) in the future.","title":"The tools:"},{"location":"lesson0/lesson0/#this-course","text":"You will create and use Jupyter Notebooks that use the ipython kernel, the notebook files will look like filename.ipynb ; these are ASCII files that the JupyterLab interprets and runs.","title":"This course:"},{"location":"lesson0/lesson0/#python","text":"The programming language we will use is Python (actually iPython). Python is an example of a high-level language; other high-level languages include C, C++, PHP, FORTRAN, ADA, Pascal, Go, Java, etc (there are a lot). As you might infer from the name high-level language, there are also low-level languages, sometimes referred to as machine languages or assembly languages. Machine language is the encoding of instructions in binary so that they can be directly executed by the computer. Assembly language uses a slightly easier format to refer to the low level instructions. Loosely speaking, computers can only execute programs written in low-level languages. To be exact, computers can actually only execute programs written in machine language. Thus, programs written in a high-level language (and even those in assembly language) have to be processed before they can run. This extra processing takes some time, which is a small disadvantage of high-level languages. However, the advantages to high-level languages are enormous. First, it is much easier to program in a high-level language. Programs written in a high-level language take less time to write, they are shorter and easier to read, and they are more likely to be correct. Second, high-level languages are portable, meaning that they can run on different kinds of computers with few or no modifications. Low-level programs can run on only one kind of computer and have to be rewritten to run on another. Due to these advantages, almost all programs are written in high-level languages. Low-level languages are used only for a few specialized applications, and for device drivers. Two kinds of programs process high-level languages into low-level languages: interpreters and compilers. An interpreter reads a high-level program and executes it, meaning that it does what the program says. It processes the program a little at a time, alternately reading lines and performing computations. Interpreted Program. Image from (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) A compiler reads the program and translates it completely before the program starts running. In this case, the high-level program is called the source code, and the translated program is called the object code or the executable. Once a program is compiled, you can execute it repeatedly without further translation. Compiled Prorgam. Image from: (https://runestone.academy/runestone/books/published/thinkcspy/GeneralIntro/ThePythonProgrammingLanguage.html) Many modern languages use both processes. They are first compiled into a lower level language, called byte code, and then interpreted by a program called a virtual machine. Python uses both processes, but because of the way programmers interact with it, it is usually considered an interpreted language. As a language, python is a formal language that has certain requirements and structure called \"syntax.\" Formal languages are languages that are designed by people for specific applications. For example, the notation that mathematicians use is a formal language that is particularly good at denoting relationships among numbers and symbols. Chemists use a formal language to represent the chemical structure of molecules. Programming languages are formal languages that have been designed to express computations. Formal languages have strict rules about syntax. For example, 3+3=6 is a syntactically correct mathematical statement, but 3=+6& is not. Syntax rules come in two flavors, pertaining to tokens and structure . Tokens are the basic elements of the language, such as words, numbers, and chemical elements. One of the problems with 3=+6& is that & is not a legal token in mathematics (at least as far as we know). The second type of syntax rule pertains to the structure of a statement\u2014 that is, the way the tokens are arranged. The statement 3=+6& is structurally illegal (in mathematics) because you don\u2019t place a plus sign immediately after an equal sign (of course we will in python!). When you read a sentence in English or a statement in a formal language, you have to figure out what the structure of the sentence is; This process is called parsing . For example, when you hear the sentence, \u201cThe other shoe fell\u201d, you understand that the other shoe is the subject and fell is the verb. Once you have parsed a sentence, you can figure out what it means, or the semantics of the sentence. Assuming that you know what a shoe is and what it means to fall, you will understand the general implication of this sentence.","title":"Python"},{"location":"lesson0/lesson0/#good-resources","text":"Learn Python the Hard Way (Online Book) (https://learnpythonthehardway.org/book/) Recommended for beginners who want a complete course in programming with Python. LearnPython.org (Interactive Tutorial) (https://www.learnpython.org/) Short, interactive tutorial for those who just need a quick way to pick up Python syntax. How to Think Like a Computer Scientist (Interactive Book) (https://runestone.academy/runestone/books/published/thinkcspy/index.html) Interactive \"CS 101\" course taught in Python that really focuses on the art of problem solving. How to Learn Python for Data Science, The Self-Starter Way (https://elitedatascience.com/learn-python-for-data-science)","title":"Good Resources:"},{"location":"lesson0/lesson0/#programming-as-a-problem-solving-process","text":"The entire point of this course is to develop problem solving skills and begin using some tools (Statistics, Numerical Methods, Data Science, implemented as JupyterLab/Python programs). The scientific method (https://en.wikipedia.org/wiki/Scientific_method) is one example of an effective problem solving strategy. Stated as a protocol it goes something like: Observation: Formulation of a question Hypothesis: A conjecture that may explain observed behavior. Falsifiable by an experiment whose outcome conflicts with predictions deduced from the hypothesis Prediction: How the experiment should conclude if hypothesis is correct Testing: Experimental design, and conduct of the experiment. Analysis: Interpretation of experimental results This protocol can be directly adapted to CT/DS problems as: Define the problem (problem statement) Gather information (identify known and unknown values, and governing equations) Generate and evaluate potential solutions Refine and implement a solution Verify and test the solution. For actual computational methods the protocol becomes: Explicitly state the problem State: Input information Governing equations or principles, and The required output information. Work a sample problem by-hand for testing the general solution. Develop a general solution method (coding). Test the general solution against the by-hand example, then apply to the real problem. Oddly enough the first step is the most important and sometimes the most difficult. In a practical problem, step 2 is sometimes difficult because a skilled programmer is needed to translate the governing principles into an algorithm for the general solution (step 4).","title":"Programming as a problem solving process"},{"location":"lesson0/lesson0/#example-1-problem-solving-process","text":"Consider a need to compute an arithmetic mean, what would the process look like? Step 1. Develop script to compute the arithmetic mean of a stream of data of unknown length. Step 2. - Inputs: The data stream - Governing equation: \\bar x = \\frac{1}{N} \\sum_{i=1}^{N} x_i where N is the number of items in the data stream, and x_i is the value of the i-th element. - Outputs: The arithmetic mean \\bar x Step 3. Work a sample problem by-hand for testing the general solution. Data 23.43 37.43 34.91 28.37 30.62 The arithmetic mean requires us to count how many elements are in the data stream (in this case there are 5) and compute their sum (in this case 154.76), and finally divide the sum by the count and report this result as the arithmetic mean. \\bar x = \\frac{1}{5}(23.43+37.43+34.91+28.37+30.62)=\\frac{154.76}{5}=30.95 Step 4. Develop a general solution (code) The by-hand exercise helps identify the required steps in an \u201calgorithm\u201d or recipe to compute mean values. First we essentially capture or read the values then count how many there are (either as we go or as a separate step), then sum the values, then divide the values by the count, and finally report the result. In a flow-chart it would look like: Flowchart for Artihmetic Mean Algorithm Step 5. This step we would code the algorithm expressed in the figure and test it with the by-hand data and other small datasets until we are convinced it works correctly. In a simple JupyterLab script # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",(accumulator/howlong)) arithmetic mean = 30.951999999999998 Step 6. This step we would refine the code to generalize the algorithm. In the example we want a way to supply the xlist from a file perhaps, and tidy the output by rounding to only two decimal places - rounding is relatively simple: # Arithmetic Mean in Very Elementary and Primative Python xlist = [23.43,37.43,34.91,28.37,30.62] # list is a type of data structure howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + xlist[i] print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Reading from a file, is a bit more complicated. We need to create a connection to the file, then read the contents into our script, then put the contents into the xlist xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection howlong = len(xlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(xlist[i]) print(\"arithmetic mean = \",round((accumulator/howlong),2)) arithmetic mean = 30.95 Finally, if we want to reuse the code a lot, it is convienent to make it into a function def average(inputlist): # inputlist should be a list of values howlong = len(inputlist) # len is a built-in function that returns how many items in a list accumulator = 0 # a variable to accumulate the sum for i in range(howlong): accumulator = accumulator + float(inputlist[i]) result = (accumulator/howlong) return(result) Put our file reading and compute mean code here xlist=[] # list (null) is a type of data structure externalfile = open(\"data.txt\",'r') # create connection to file, set to read (r), file must exist how_many_lines = 0 for line in externalfile: # parse each line, append to xlist xlist.append(line) how_many_lines += 1 externalfile.close() # close the file connection print(\"arithmetic mean = \",round(average(xlist),2)) arithmetic mean = 30.95 So the simple task of computing the mean of a collection of values, is a bit more complex when decomposed that it first appears, but illustrates a five step process (with a refinement step). Throughout the course this process is always in the background.","title":"Example 1 Problem Solving Process"},{"location":"lesson0/lesson0/#ccmr-approach","text":"A lot of the problems we will encounter from a CT/DS perspective have already been solved, or at least analogs have been solved. It is perfectly acceptable to use prior work for a new set of conditions as long as proper attribution is made. We call this process CCMR: Copy: Find a solution to your problem from some online example: SourceForge, StackOverflow, GeeksForGeeks, DigitalOcean, etc. Cite: Cite the original source. In general a citation will look like one of the references below, but a URL to the source is sufficient at first. Modify: Modify the original cited work for your specific needs. Note the changes in the code using comment statements. Run: Apply the modified code to the problem of interest. In cases where we use CCMR we are not so much programming and developing our own work as scaffolding parts (https://en.wikipedia.org/wiki/Scaffold_(programming)) - a legitimate and valuable engineering activity.","title":"CCMR Approach"},{"location":"lesson0/lesson0/#readings","text":"Computational and Inferential Thinking Ani Adhikari and John DeNero, Computational and Inferential Thinking, The Foundations of Data Science, Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND) Chapter 1 https://www.inferentialthinking.com/chapters/01/what-is-data-science.html # Script block to identify host, user, and kernel import sys ! hostname ! whoami ! pwd print(sys.executable) print(sys.version) print(sys.version_info) atomickitty sensei /home/sensei/1330-textbook-webroot/docs/lesson0 /opt/jupyterhub/bin/python3 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)","title":"Readings"}]}