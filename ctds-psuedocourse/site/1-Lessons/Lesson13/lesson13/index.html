<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Non-Linear Equation Systems - Engr 1330 - Web Book</title>
        <link href="../../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../../..">Engr 1330 - Web Book</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../../..">Home</a>
                            </li>
                            <li >
                                <a href="../../../about/">About</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Introduction <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../Lesson00/lesson0/">Computational Thinking and Data Science</a>
</li>
                                    
<li >
    <a href="../../Lesson01/lesson1/">Problem Solving with Computational Thinking</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Scripting Fundamentals <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../Lesson02/lesson2/">Simple Computation</a>
</li>
                                    
<li >
    <a href="../../Lesson03/lesson3/">Data Structures and the MATH package</a>
</li>
                                    
<li >
    <a href="../../Lesson04/lesson4/">Program Control Structures</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">External Functions and Modules</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">Engineering and Scientific Data Files</a>
</li>
                                    
<li >
    <a href="../../Lesson07/lesson6/">Computational Linear Algebra using NUMPY</a>
</li>
                                    
<li >
    <a href="../../Lesson08/lesson7/">Database Query and Manipulation using PANDAS</a>
</li>
                                    
<li >
    <a href="../../Lesson09/lesson8/">Visual Display of Data using MATPLOTLIB</a>
</li>
                                    
<li >
    <a href="../../Lesson10/lesson10/">Implicit Equations</a>
</li>
                                    
<li >
    <a href="../../Lesson11/lesson11/">Interpolation and Integration</a>
</li>
                                    
<li >
    <a href="../../Lesson12/lesson12/">Linear Equation Systems</a>
</li>
                                    
<li class="active">
    <a href="./">Non-Linear Equation Systems</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Models and Decisions <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../../lesson0/lesson0/">Statistical Data Modeling</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">Randomness and Probability</a>
</li>
                                    
<li >
    <a href="../../Lesson15/lesson15/">Descriptive Statistics</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">Distribution Models</a>
</li>
                                    
<li >
    <a href="../../Lesson17/lesson17/">Probability Estimation Modeling</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">Hypothesis Testing</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">Experimental Design (A/B Testing)</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">Interval Estimates</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Prediction <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../../lesson0/lesson0/">Fitting Models to Observations</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">Least Squares (Regression) Model Fitting</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">Model Quality</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">Estimating Probability/Quantile Regression</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Classification <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../../lesson0/lesson0/">Introduction</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">Types</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">K Nearest Neighbor</a>
</li>
                                    
<li >
    <a href="../../../lesson0/lesson0/">Engine Update</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../../Lesson12/lesson12/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../../../lesson0/lesson0/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#engr-1330-computational-thinking-with-data-science">ENGR 1330 Computational Thinking with Data Science</a></li>
            <li><a href="#lesson-13-nonlinear-systems-of-equations">Lesson 13 Nonlinear Systems of Equations</a></li>
            <li><a href="#objectives">Objectives</a></li>
        <li class="main "><a href="#nonlinear-systems-of-equations">Nonlinear Systems of Equations</a></li>
            <li><a href="#multiple-variable-extension-of-newtons-method">Multiple-variable extension of Newton’s Method</a></li>
            <li><a href="#example-using-analytical-derivatives">Example using Analytical Derivatives</a></li>
            <li><a href="#quasi-newton-method-using-finite-difference-approximations-for-the-derivative">Quasi-Newton Method using Finite Difference Approximations for the Derivative</a></li>
            <li><a href="#exercises">Exercises</a></li>
        <li class="main "><a href="#branched-system-3-reservoir-example">Branched System 3-Reservoir Example</a></li>
            <li><a href="#solution-approach">Solution Approach</a></li>
            <li><a href="#apply-the-multiple-variable-extension-of-newtons-method">Apply the multiple-variable Extension of Newton’s Method</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p>Copyright © DATE Author, <em>all rights reserved</em></p>
<h1 id="engr-1330-computational-thinking-with-data-science">ENGR 1330 Computational Thinking with Data Science</h1>
<p>Last GitHub Commit Date: </p>
<h2 id="lesson-13-nonlinear-systems-of-equations">Lesson 13 Nonlinear Systems of Equations</h2>
<p>This lesson will </p>
<hr />
<h2 id="objectives">Objectives</h2>
<ul>
<li>Construct multivariate non-linear equation systems</li>
<li>Construct the Jacobian using analytical partial derivatives</li>
<li>Solve using a Newton-Raphson method</li>
<li>Construct the Jacobian using finite-difference approximations of the partial derivatives</li>
<li>Demonstrate practical application with a pipeline network</li>
</ul>
<h1 id="nonlinear-systems-of-equations">Nonlinear Systems of Equations</h1>
<p>Non-linear systems are extensions of the linear systems cases except the systems involve products and powers of the unknown variables. Non-linear problems are often quite difficult to manage, especially when the systems are large (many rows and many variables).
The solution to non-linear systems, if non-trivial or even possible, are usually iterative. Within the iterative steps is a linearization component – these linear systems which are intermediate computations within the overall solution process are treated by an appropriate linear system method (direct or iterative).
Consider the system below:</p>
<p>
<script type="math/tex; mode=display">\begin{gather}
\begin{matrix}
x^2 & +~y^2  \\
 e^x & +~y  \\
\end{matrix}
\begin{matrix}
= 4\\
= 1\\
\end{matrix}
\end{gather}</script>
</p>
<p>Suppose we have a solution guess <script type="math/tex">x_{k},y_{k}</script>, which of course could be wrong, but we could linearize about that guess as</p>
<p>
<script type="math/tex; mode=display">\begin{gather}
\mathbf{A} =
\begin{pmatrix}
x_k & + ~y_k  \\
0 & + ~1  \\
\end{pmatrix}
~\mathbf{x} = 
\begin{pmatrix}
x_{k+1} \\
y_{k+1} \\
\end{pmatrix}
~ \mathbf{b} = 
\begin{pmatrix}
4\\
1 - e^{x_k}\\
\end{pmatrix}
\end{gather}</script>
</p>
<p>Now if we assemble the system in the usual fashion, <script type="math/tex">\mathbf{A} \cdot \mathbf{x_{k+1}} = ~ \mathbf{b}~</script> we have a system of linear equations\footnote{Linear in <script type="math/tex">\mathbf{x_{k+1}}</script>}, which expanded look like:</p>
<p>
<script type="math/tex; mode=display">\begin{gather}
\begin{pmatrix}
x_k & + ~y_k  \\
0 & + ~1  \\
\end{pmatrix}
\cdot
\begin{pmatrix}
x_{k+1} \\
y_{k+1} \\
\end{pmatrix}
~ = 
\begin{pmatrix}
4\\
1 - e^{x_k}\\
\end{pmatrix}
\end{gather}</script>
</p>
<p>Now that the system is linear, and we can solve for <script type="math/tex">\mathbf{x_{k+1}}</script> using our linear system solver for the new guess.
If the system is convergent (not all are) then if we update, and repeat  we will eventually find a result.</p>
<p>What one really needs is a way to construct the linear system that has a systematic update method, that is discussed below</p>
<h2 id="multiple-variable-extension-of-newtons-method">Multiple-variable extension of Newton’s Method</h2>
<p>This section presents the Newton-Raphson method as a way to sometimes solve systems of non-linear equations.</p>
<p>Consider an example where the function \textbf{f} is a vector-valued function of a vector argument.</p>
<p>
<script type="math/tex; mode=display">\begin{gather}
\mathbf{f(x)} = 
\begin{matrix}
f_1 = & x^2 & +~y^2 & - 4\\
f_2 = & e^x & +~y  & - 1\\
\end{matrix}
\end{gather}</script>
</p>
<p>Let's also recall Newtons method for scalar valued function of a single variable.</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
x_{k+1}=x_{k} - \frac{  f(x_{k})  }{   \frac{df}{dx}\rvert_{x_k} } 
\label{eqn:NewtonFormula}
\end{equation}</script>
</p>
<p>When extending to higher dimensions, the analog for <script type="math/tex">x</script> is the vector \textbf{x} and the analog for the function <script type="math/tex">f()</script> is the vector function \textbf{f()}.
What remains is an analog for the first derivative in the denominator (and the concept of division of a matrix).</p>
<p>The analog to the first derivative is a matrix called the Jacobian which is comprised of the first derivatives of the function \textbf{f} with respect to the arguments \textbf{x}. <br />
For example for a 2-value function of 2 arguments (as our example above)</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\frac{df}{dx}\rvert_{x_k} =>
\begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} \\
~ & ~ \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} \\
\end{pmatrix}
\label{eqn:Jacobian}
\end{equation}</script>
</p>
<p>Next recall that division is replaced by matrix multiplication with the multiplicative inverse, so the analogy continues as</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\frac{1}{\frac{df}{dx}\rvert_{x_k}} =>
{\begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} \\
~ & ~ \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} \\
\end{pmatrix}}^{-1}
\label{eqn:JacobianInverse}
\end{equation}</script>
</p>
<p>Let's name the Jacobian \textbf{J(x)}.</p>
<p>So the multi-variate Newton's method can be written as</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\mathbf{x}_{k+1}=\mathbf{x}_{k} - \mathbf{J(x)}^{-1}\rvert_{x_k} \cdot \mathbf{f(x)}\rvert_{x_k}
\label{eqn:VectorNewtonFormula}
\end{equation}</script>
</p>
<p>In the linear systems lessons we did find a way to solve for an inverse, but it's not necessary, and is computationally expensive to invert in these examples -- a series of rearrangement of the system above yields a nice scheme that does not require inversion of a matrix.</p>
<p>First, move the <script type="math/tex">\mathbf{x}_{k}</script> to the left-hand side.</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\mathbf{x}_{k+1}-\mathbf{x}_{k} = - \mathbf{J(x)}^{-1}\rvert_{x_k} \cdot \mathbf{f(x)}\rvert_{x_k}
\end{equation}</script>
</p>
<p>Next multiply both sides by the Jacobian (The Jacobian must be non-singular otherwise we are dividing by zero)</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\mathbf{J(x)}\rvert_{x_k} \cdot (\mathbf{x}_{k+1}-\mathbf{x}_{k}) = - \mathbf{J(x)}\rvert_{x_k} \cdot \mathbf{J(x)}^{-1}\rvert_{x_k} \cdot \mathbf{f(x)}\rvert_{x_k}
\end{equation}</script>
</p>
<p>Recall a matrix multiplied by its inverse returns the identity matrix (the matrix equivalent of unity)</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
-\mathbf{J(x)}\rvert_{x_k} \cdot (\mathbf{x}_{k+1}-\mathbf{x}_{k}) = \mathbf{f(x)}\rvert_{x_k}
\end{equation}</script>
</p>
<p>So we now have an algorithm:</p>
<p>1) Start with an initial guess <script type="math/tex">\mathbf{x}_{k}</script>, compute <script type="math/tex">\mathbf{f(x)}\rvert_{x_k}</script>, and <script type="math/tex">\mathbf{J(x)}\rvert_{x_k}</script>.</p>
<p>2) Test for stopping.  Is <script type="math/tex">\mathbf{f(x)}\rvert_{x_k}</script> close to zero?  If yes, exit and report results, otherwise continue.</p>
<p>3) Solve the linear system <script type="math/tex">\mathbf{J(x)}\rvert_{x_k} \cdot (\mathbf{x}_{k+1}-\mathbf{x}_{k}) = \mathbf{f(x)}\rvert_{x_k}</script>.</p>
<p>4) Test for stopping.  Is <script type="math/tex"> (\mathbf{x}_{k+1}-\mathbf{x}_{k})</script> close to zero?  If yes, exit and report results, otherwise continue.</p>
<p>5) Compute the update <script type="math/tex">\mathbf{x}_{k+1} = \mathbf{x}_{k} - (\mathbf{x}_{k+1}-\mathbf{x}_{k}) </script>, then</p>
<p>6) Move the update into the guess vector <script type="math/tex">\mathbf{x}_{k} <=\mathbf{x}_{k+1}</script> =and repeat step 1. Stop after too many steps.</p>
<h2 id="example-using-analytical-derivatives">Example using Analytical Derivatives</h2>
<p>Now to complete the example we will employ this algorithm.</p>
<p>The function (repeated)</p>
<p>
<script type="math/tex; mode=display">\begin{gather}
\mathbf{f(x)} = 
\begin{matrix}
f_1 = & x^2 & +~y^2 & - 4\\
f_2 = & e^x & +~y  & - 1\\
\end{matrix}
\end{gather}</script>
</p>
<p>Then the Jacobian, here we will compute it analytically because we can</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\mathbf{J(x)}=>
{\begin{pmatrix}
2x & 2y \\
~ & ~ \\
e^x & 1 \\
\end{pmatrix}}
\end{equation}</script>
</p>
<p>Now for the scripts.</p>
<p>We will start by defining the two equations, and their derivatives, as well a a vector valued function <code>func</code> and its Jacobian <code>jacob</code> as below.  Here the two modules <code>LinearSolverPivot</code> and <code>vector_matrix_lib</code> are just python source code files containing prototype functions.</p>
<pre><code class="python">#################################################################
# Newton Solver Example -- Analytical Derivatives               #
#################################################################
import math   # This will import math module from python distribution
from LinearSolverPivot import linearsolver # This will import our solver module
from vector_matrix_lib import writeM,writeV,vdotv,vvsub # This will import our vector functions

def eq1(x,y):
    eq1 = x**2 + y**2 - 4.0
    return(eq1)

def eq2(x,y):
    eq2 = math.exp(x) + y - 1.0
    return(eq2)

def ddxeq1(x,y):
    ddxeq1 = 2.0*x
    return(ddxeq1)

def ddyeq1(x,y):
    ddyeq1 = 2.0*y
    return(ddyeq1)

def ddxeq2(x,y):
    ddxeq2 = math.exp(x)
    return(ddxeq2)

def ddyeq2(x,y):
    ddyeq2 = 1.0
    return(ddyeq2)

def func(x,y):
    func  = [0.0 for i in range(2)] # null list
    # build the function
    func[0] = eq1(x,y)
    func[1] = eq2(x,y)
    return(func)

def jacob(x,y):
    jacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list  
    #build the  jacobian
    jacob[0][0]=ddxeq1(x,y)
    jacob[0][1]=ddyeq1(x,y)
    jacob[1][0]=ddxeq2(x,y)
    jacob[1][1]=ddyeq2(x,y)
    return(jacob)
</code></pre>

<p>Next we create vectors to store values, and supply initial guesses to the system, and echo the inputs.</p>
<pre><code class="python">deltax  = [0.0 for i in range(2)] # null list
xguess  = [0.0 for i in range(2)] # null list
myfunc  = [0.0 for i in range(2)] # null list
myjacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list  
# supply initial guess
xguess[0] = float(input(&quot;Value for x : &quot;))
xguess[1] = float(input(&quot;Value for y : &quot;))
# build the initial function
myfunc = func(xguess[0],xguess[1])
#build the initial jacobian
myjacob=jacob(xguess[0],xguess[1])
#write initial results
writeV(xguess,2,&quot;Initial X vector &quot;)
writeV(myfunc,2,&quot;Initial FUNC vector &quot;)
writeM(myjacob,2,2,&quot;Initial Jacobian &quot;)
# solver parameters
tolerancef = 1.0e-9
tolerancex = 1.0e-9
</code></pre>

<pre><code>Value for x :  1
Value for y :  2


------ Initial X vector  ------
1.0
2.0
-----------------------------
------ Initial FUNC vector  ------
1.0
3.7182818284590446
-----------------------------
------ Initial Jacobian  ------
[2.0, 4.0]
[2.718281828459045, 1.0]
-----------------------------
</code></pre>
<p>Now we apply the algorithm a few times, here the count is set to 10. So eneter the loop, test for stopping, then update.</p>
<pre><code class="python"># Newton-Raphson
for iteration in range(10):
    myfunc = func(xguess[0],xguess[1])
    testf = vdotv(myfunc,myfunc,2)
    if testf &lt;= tolerancef :
        print(&quot;f(x) close to zero\n test value : &quot;, testf)
        break
    myjacob=jacob(xguess[0],xguess[1])
    deltax=linearsolver(myjacob,myfunc)
    testx = vdotv(deltax,deltax,2)
    if testx &lt;= tolerancex :
        print(&quot;solution change small\n test value : &quot;, testx)
        break
    xguess=vvsub(xguess,deltax,2)
##    print(&quot;iteration : &quot;,iteration)
##    writeV(xguess,2,&quot;Current X vector &quot;)
##    writeV(myfunc,2,&quot;Current FUNC vector &quot;)
print(&quot;Exiting Iteration : &quot;,iteration)
writeV(xguess,2,&quot;Exiting X vector &quot;)
writeV(myfunc,2,&quot;Exiting FUNC vector &quot;)
</code></pre>

<pre><code>solution change small
 test value :  2.1803484657072266e-10
Exiting Iteration :  5
------ Exiting X vector  ------
-1.8162775103511606
0.8373739123240562
-----------------------------
------ Exiting FUNC vector  ------
5.90636483064344e-05
3.926422552424924e-06
-----------------------------
</code></pre>
<h2 id="quasi-newton-method-using-finite-difference-approximations-for-the-derivative">Quasi-Newton Method using Finite Difference Approximations for the Derivative</h2>
<p>The next variant is to approximate the derivatives -- usually a Finite-Difference approximation is used, either forward, backward, or centered differences -- generally determined based on the actual behavior of the functions themselves or by trial and error. </p>
<p>For really huge systems, we usually make the program itself make the adaptions as it proceeds.</p>
<p>The coding for a finite-difference representation of a Jacobian is shown in Listing that follows 
In constructing the Jacobian, we observe that each column of the Jacobian is simply the directional derivative of the function with respect to the variable associated with the column.<br />
For instance, the first column of the Jacobian in the example is first derivative of the first function (all rows) with respect to the first variable, in this case <script type="math/tex">x</script>.  The second column is the first derivative of the second function with respect to the second variable, <script type="math/tex">y</script>.<br />
This structure is useful to generalize the Jacobian construction method because we could write (yet another) prototype function that can take the directional derivatives for us, and just insert the returns as columns; in the example we simply modified the <code>ddx</code> and <code>ddy</code> functions from analytical to simple finite differences.</p>
<p>The example listing is specific to the 2X2 function in the example, but the extension to more general cases is evident.</p>
<pre><code class="python">#################################################################
# Newton Solver Example -- Numerical Derivatives               #
#################################################################
import math   # This will import math module from python distribution
from LinearSolverPivot import linearsolver # This will import our solver module
from vector_matrix_lib import writeM,writeV,vdotv,vvsub # This will import our vector functions

def eq1(x,y):
    eq1 = x**2 + y**2 - 4.0
    return(eq1)

def eq2(x,y):
    eq2 = math.exp(x) + y - 1.0
    return(eq2)
##############################################################
# This portion is changed for finite-difference method to evaluate derivatives #
##############################################################
def ddxeq1(x,y):
    delta = 1.0e-6
    ddxeq1 = (eq1(x+delta,y)-eq1(x,y))/delta
    return(ddxeq1)

def ddyeq1(x,y):
    delta = 1.0e-6
    ddyeq1 = (eq1(x,y+delta)-eq1(x,y))/delta
    return(ddyeq1)

def ddxeq2(x,y):
    delta = 1.0e-6
    ddxeq2 = (eq2(x+delta,y)-eq2(x,y))/delta
    return(ddxeq2)

def ddyeq2(x,y):
    delta = 1.0e-6
    ddyeq2 = (eq2(x,y+delta)-eq2(x,y))/delta
    return(ddyeq2)
##############################################################
def func(x,y):
    func  = [0.0 for i in range(2)] # null list
    # build the function
    func[0] = eq1(x,y)
    func[1] = eq2(x,y)
    return(func)

def jacob(x,y):
    jacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list  
    #build the  jacobian
    jacob[0][0]=ddxeq1(x,y)
    jacob[0][1]=ddyeq1(x,y)
    jacob[1][0]=ddxeq2(x,y)
    jacob[1][1]=ddyeq2(x,y)
    return(jacob)
deltax  = [0.0 for i in range(2)] # null list
xguess  = [0.0 for i in range(2)] # null list
myfunc  = [0.0 for i in range(2)] # null list
myjacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list  
# supply initial guess
xguess[0] = float(input(&quot;Value for x : &quot;))
xguess[1] = float(input(&quot;Value for y : &quot;))
# build the initial function
myfunc = func(xguess[0],xguess[1])
#build the initial jacobian
myjacob=jacob(xguess[0],xguess[1])
#write initial results
writeV(xguess,2,&quot;Initial X vector &quot;)
writeV(myfunc,2,&quot;Initial FUNC vector &quot;)
writeM(myjacob,2,2,&quot;Initial Jacobian &quot;)
# solver parameters
tolerancef = 1.0e-9
tolerancex = 1.0e-9
# Newton-Raphson
for iteration in range(10):
    myfunc = func(xguess[0],xguess[1])
    testf = vdotv(myfunc,myfunc,2)
    if testf &lt;= tolerancef :
        print(&quot;f(x) close to zero\n test value : &quot;, testf)
        break
    myjacob=jacob(xguess[0],xguess[1])
    deltax=linearsolver(myjacob,myfunc)
    testx = vdotv(deltax,deltax,2)
    if testx &lt;= tolerancex :
        print(&quot;solution change small\n test value : &quot;, testx)
        break
    xguess=vvsub(xguess,deltax,2)
##    print(&quot;iteration : &quot;,iteration)
##    writeV(xguess,2,&quot;Current X vector &quot;)
##    writeV(myfunc,2,&quot;Current FUNC vector &quot;)
print(&quot;Exiting Iteration : &quot;,iteration)
writeV(xguess,2,&quot;Exiting X vector &quot;)
writeV(myfunc,2,&quot;Exiting FUNC vector using Finite-Differences&quot;)
</code></pre>

<pre><code>Value for x :  1
Value for y :  2


------ Initial X vector  ------
1.0
2.0
-----------------------------
------ Initial FUNC vector  ------
1.0
3.7182818284590446
-----------------------------
------ Initial Jacobian  ------
[2.0000010003684565, 4.0000010006480125]
[2.718283187874704, 1.0000000010279564]
-----------------------------
solution change small
 test value :  2.1800662368653786e-10
Exiting Iteration :  5
------ Exiting X vector  ------
-1.8162775096127992
0.8373739116345714
-----------------------------
------ Exiting FUNC vector using Finite-Differences ------
5.905981145470918e-05
3.925853147235259e-06
-----------------------------
</code></pre>
<pre><code class="python">
</code></pre>

<h2 id="exercises">Exercises</h2>
<p>Write a script that forward defines the multi-variate functions and implements the Newton-Raphson technique.
Implement the method, using analytical derivatives, and find a solution to:
<script type="math/tex; mode=display">\begin{gather}
\begin{matrix}
 x^3 & +~3y^2 & = 21\\
x^2& +~2y  & = -2 \\
\end{matrix}
\end{gather}</script>
</p>
<p>Repeat the exercise, except use finite-differences to approximate the derivatives.</p>
<pre><code class="python">
</code></pre>

<p>Write a script that forward defines the multi-variate functions and implements the Newton-Raphson technique.
Implement the method, using analytical derivatives, and find a solution to:
<script type="math/tex; mode=display">\begin{gather}
\begin{matrix}
x^2 & +~ y^2 & +~z^2 & =~ 9\\
~ & ~ & xyz & =~ 1\\
x & +~ y & -z^2 & =~ 0\\
\end{matrix}
\end{gather}</script>
</p>
<p>Repeat the exercise, except use finite-differences to approximate the derivatives.</p>
<pre><code class="python">
</code></pre>

<p>Write a script that forward defines the multi-variate functions and implements the Newton-Raphson technique.
Implement the method, using analytical derivatives, and find a solution to:
<script type="math/tex; mode=display">\begin{gather}
\begin{matrix}
xyz & -~ x^2 & +~y^2 & =~ 1.34\\
~ & xy &-~z^2 & =~ 0.09\\
e^x & -~ e^y & +z & =~ 0.41\\
\end{matrix}
\end{gather}</script>
</p>
<p>Repeat the exercise, except use finite-differences to approximate the derivatives.</p>
<pre><code class="python">
</code></pre>

<h1 id="branched-system-3-reservoir-example">Branched System 3-Reservoir Example</h1>
<p>Consider the branched system shown in below.  In this example the friction factor is assumed constant for simplicity, but in practice would vary during the solution computations.</p>
<p><img alt="name when missing" src="../3-reservoir-example.png" /></p>
<p>The hydraulics question is what is the discharge in each pipe and what is the total head at the junction (notice we don't know the junction elevation in this example --- if the elevation were specified, we could also find the pressure head).</p>
<h2 id="solution-approach">Solution Approach</h2>
<p>First populate the four equations with the appropriate numerical values.</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
70 = h_D + (0.015)\frac{5000}{0.6}\frac{V_{AD}|V_{AD}|}{2(9.8)}
\end{equation}</script>
</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
100 = h_D + (0.015)\frac{3000}{0.8}\frac{V_{BD}|V_{BD}|}{2(9.8)}
\end{equation}</script>
</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
-80 = - h_D  + (0.015)\frac{4000}{1.2}\frac{V_{DC}|V_{DC}|}{2(9.8)}
\end{equation}</script>
</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\frac{\pi(0.6)^2}{4} V_{AD}+\frac{\pi(0.8)^2}{4} V_{BD} = \frac{\pi(1.2)^2}{4} V_{DC}
\end{equation}</script>
</p>
<p>Next, compute all the constants, and organize the 4 equations into a system of simultaneous equations</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\begin{matrix}
h_D & 6.377 V_{AD}|V_{AD}| & 0 & 0  & = 70\\
h_D & 0 & 2.869 V_{BD}|V_{BD}| & 0  & = 100\\
h_D & 0 & 0 & - 2.551 V_{DC}|V_{DC}|  & = 80\\
0 & 0.2827 V_{AD} & 0.5026 V_{BD}& -1.1309 V_{DC}  & = 0\\
\end{matrix}
\end{equation}</script>
</p>
<p>Because the system is non-linear we need to employ some appropriate method, herein we will use a Quasi-Newton method with numerical approximations to derivatives.</p>
<p>First a more conventional vector-matrix structure</p>
<p>
<script type="math/tex; mode=display">\begin{gather}
\begin{pmatrix}
1 & 6.377 |V_{AD}| & 0 & 0 \\      
1 & 0 & 2.869 |V_{BD}| & 0 \\    
1 & 0 & 0 & - 2.551 |V_{DC}| \\    
0 & 0.2827  & 0.5026 & -1.1309 \\ 
\end{pmatrix}
\bullet
\begin{pmatrix}
 h_D \\
V_{AD} \\
V_{BD} \\
V_{DC} \\
\end{pmatrix}
-
\begin{pmatrix}
70 \\
100 \\
 80 \\
0 \\
\end{pmatrix}
=
\begin{pmatrix}
0 \\
0 \\
0 \\
0 \\
\end{pmatrix}
\end{gather}</script>
</p>
<h2 id="apply-the-multiple-variable-extension-of-newtons-method">Apply the multiple-variable Extension of Newton’s Method</h2>
<p>The solution can use the methods directly from ENGR-1330 and similar courses, here we just implement the methods.  A very brief explaination is in the readings, but multi-variable Newton's method is usually taught in calculus.</p>
<p>Here we will define some support modules <code>linearsolver()</code> and <code>vector_matrix_lib</code> which are just python source code containing prototype functions which are embedded into the notebook, but ordinarily would save them in separate file and use <code>import</code>.</p>
<pre><code class="python"># SolveLinearSystem.py
# Code to read A and b
# Then solve Ax = b for x by Gaussian elimination with back substitution
##########
def linearsolver(A,b):
    n = len(A)
# M = A  #this is object to object equivalence
# copy A into M element by element
    M=[[0.0 for jcol in range(n)]for irow in range(n)]
    for irow in range(n):
        for jcol in range(n):
            M[irow][jcol]=A[irow][jcol]
    i = 0
    for x in M:
     x.append(b[i])
     i += 1
    for k in range(n):
     for i in range(k,n):
       if abs(M[i][k]) &gt; abs(M[k][k]):
          M[k], M[i] = M[i],M[k]
       else:
          pass
     for j in range(k+1,n):
         q = float(M[j][k]) / M[k][k]
         for m in range(k, n+1):
            M[j][m] -=  q * M[k][m]
    x = [0 for i in range(n)]
    x[n-1] =float(M[n-1][n])/M[n-1][n-1]
    for i in range (n-1,-1,-1):
      z = 0
      for j in range(i+1,n):
          z = z  + float(M[i][j])*x[j]
      x[i] = float(M[i][n] - z)/M[i][i]
#    print (x)
    return(x)
</code></pre>

<pre><code class="python"># vector_matrix_lib.py
# useful linear algebra tools
import math   # This will import math module

def writeM(M,ir,jc,label):
    print (&quot;------&quot;,label,&quot;------&quot;)
    for i in range(0,ir,1):
        print (M[i][0:jc])
    print (&quot;-----------------------------&quot;)
    #return

def writeV(V,ir,label):
    print (&quot;------&quot;,label,&quot;------&quot;)
    for i in range(0,ir,1):
        print (V[i])
    print (&quot;-----------------------------&quot;)
    #return

def mmmult(amatrix,bmatrix,rowNumA,colNumA,rowNumB,colNumB):
    AB =[[0.0 for j in range(colNumB)] for i in range(rowNumA)]
    for i in range(0,rowNumA):
        for j in range(0,colNumB):
            for k in range(0,colNumA):
                AB[i][j]=AB[i][j]+amatrix[i][k]*bmatrix[k][j]
    return(AB)

def mvmult(amatrix,xvector,rowNumA,colNumA):
    bvector=[0.0 for i in range(rowNumA)]
    for i in range(0,rowNumA):
        for j in range(0,1):
            for k in range(0,colNumA):
                bvector[i]=bvector[i]+amatrix[i][k]*xvector[k]
    return(bvector)

def vvadd(avector,bvector,length):
    aplusb=[0.0 for i in range(length)]
    for i in range(length):
        aplusb[i] = avector[i] + bvector[i]
    return(aplusb)

def vvsub(avector,bvector,length):
    aminusb=[0.0 for i in range(length)]
    for i in range(length):
        aminusb[i] = avector[i] - bvector[i]
    return(aminusb)

def vdotv(avector,bvector,length):
    adotb=0.0
    for i in range(length):
        adotb=adotb+avector[i]*bvector[i]
    return(adotb)
</code></pre>

<p>Next we define the four equations, as well a a vector valued function <code>func</code> and its Jacobian <code>jacob</code> as below.  </p>
<pre><code class="python">#################################################################
# Newton Solver Example -- Numerical Derivatives                #
#################################################################
import math   # This will import math module from python distribution
#from LinearSolverPivot import linearsolver # This will import our solver module
#from vector_matrix_lib import writeM,writeV,vdotv,vvsub # This will import our vector functions

def eq1(x1,x2,x3,x4):
    eq1 = 1*x1 + 6.377*x2 +0*x3 + 0*x4 - 70
    return(eq1)
def eq2(x1,x2,x3,x4):
    eq2 = 1*x1 + 0*x2 +2.869*x3 + 0*x4 - 100
    return(eq2)
def eq3(x1,x2,x3,x4):
    eq3 = 1*x1 + 0*x2 +0*x3 + -2.551*x4 -80
    return(eq3)
def eq4(x1,x2,x3,x4):
    eq4 = 0*x1 + 0.2827*x2 +0.5026*x3 + -1.1309*x4 - 0
    return(eq4)
##############################################################
def func(x1,x2,x3,x4):
    func  = [0.0 for i in range(4)] # null list
    # build the function
    func[0] = eq1(x1,x2,x3,x4)
    func[1] = eq2(x1,x2,x3,x4)
    func[2] = eq3(x1,x2,x3,x4)
    func[3] = eq4(x1,x2,x3,x4)
    return(func)

def jacob(x1,x2,x3,x4):
    jacob = [[0.0 for j in range(4)] for i in range(4)] # constructed list  
    #build the  jacobian
    jacob[0][0]=1
    jacob[0][1]=6.377*x2
    jacob[0][2]=0
    jacob[0][3]=0
    jacob[1][0]=1
    jacob[1][1]=0
    jacob[1][2]=2.869*x3
    jacob[1][3]=0
    jacob[2][0]=1
    jacob[2][1]=0
    jacob[2][2]=0
    jacob[2][3]=-2.551*x4
    jacob[3][0]=0
    jacob[3][1]=0.2827
    jacob[3][2]=0.5026
    jacob[3][3]=-1.1309
    return(jacob)
</code></pre>

<p>Next we create vectors to store values, and supply initial guesses to the system, and echo the inputs.</p>
<pre><code class="python">deltax  = [0.0 for i in range(4)] # null list
xguess  = [0.0 for i in range(4)] # null list
myfunc  = [0.0 for i in range(4)] # null list
myjacob = [[0.0 for j in range(4)] for i in range(4)] # constructed list  
# supply initial guess
xguess[0] = float(input(&quot;Value for x1 : &quot;))
xguess[1] = float(input(&quot;Value for x2 : &quot;))
xguess[2] = float(input(&quot;Value for x3 : &quot;))
xguess[3] = float(input(&quot;Value for x4 : &quot;))
# build the initial function
myfunc = func(xguess[0],xguess[1],xguess[2],xguess[3])
#build the initial jacobian
myjacob=jacob(xguess[0],xguess[1],xguess[2],xguess[3])
#write initial results
writeV(xguess,4,&quot;Initial X vector &quot;)
writeV(myfunc,4,&quot;Initial FUNC vector &quot;)
writeM(myjacob,4,4,&quot;Initial Jacobian &quot;)
# solver parameters
tolerancef = 1.0e-9
tolerancex = 1.0e-9
# Newton-Raphson
</code></pre>

<pre><code>Value for x1 :  1
Value for x2 :  1
Value for x3 :  1
Value for x4 :  1


------ Initial X vector  ------
1.0
1.0
1.0
1.0
-----------------------------
------ Initial FUNC vector  ------
-62.623
-96.131
-81.551
-0.3455999999999999
-----------------------------
------ Initial Jacobian  ------
[1, 6.377, 0, 0]
[1, 0, 2.869, 0]
[1, 0, 0, -2.551]
[0, 0.2827, 0.5026, -1.1309]
-----------------------------
</code></pre>
<p>Now we apply the algorithm a few times, here the count is set to 24. So eneter the loop, test for stopping, then update.</p>
<pre><code class="python">for iteration in range(24):
    myfunc = func(xguess[0],xguess[1],xguess[2],xguess[3])
    testf = vdotv(myfunc,myfunc,4)
    if testf &lt;= tolerancef :
        print(&quot;f(x) close to zero\n test value : &quot;, testf)
        break
    myjacob=jacob(xguess[0],xguess[1],xguess[2],xguess[3])
    deltax=linearsolver(myjacob,myfunc)
    testx = vdotv(deltax,deltax,4)
    if testx &lt;= tolerancex :
        print(&quot;solution change small\n test value : &quot;, testx)
        break
    xguess=vvsub(xguess,deltax,4)
##    print(&quot;iteration : &quot;,iteration)
##    writeV(xguess,2,&quot;Current X vector &quot;)
##    writeV(myfunc,2,&quot;Current FUNC vector &quot;)
print(&quot;Exiting Iteration : &quot;,iteration)
writeV(xguess,4,&quot;Exiting X vector &quot;)
writeV(myfunc,4,&quot;Exiting FUNC vector using Finite-Differences&quot;)
</code></pre>

<pre><code>f(x) close to zero
 test value :  2.0273725264180003e-28
Exiting Iteration :  1
------ Exiting X vector  ------
84.61708956275575
-2.292157685864162
5.361767318663032
1.8099135879089594
-----------------------------
------ Exiting FUNC vector using Finite-Differences ------
-1.4210854715202004e-14
0.0
0.0
-8.881784197001252e-16
-----------------------------
</code></pre>
<p>Now tidy up the output, convert back to discharges (from velocities) and report results.</p>
<pre><code class="python">print(&quot;Head at Junction : %.3f&quot; % xguess[0], &quot; feet &quot;)
print(&quot;Discharge in Pipe AD: %.3f&quot; % (0.2827 * xguess[1]), &quot; cubic feet per second&quot;)
print(&quot;Discharge in Pipe BD: %.3f&quot; % (0.5026 * xguess[2]), &quot; cubic feet per second &quot;)
print(&quot;Discharge in Pipe DC: %.3f&quot; % (1.1309 * xguess[3]), &quot; cubic feet per second &quot;)
</code></pre>

<pre><code>Head at Junction : 84.617  feet 
Discharge in Pipe AD: -0.648  cubic feet per second
Discharge in Pipe BD: 2.695  cubic feet per second 
Discharge in Pipe DC: 2.047  cubic feet per second
</code></pre>
<pre><code class="python">
</code></pre></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../../mathjaxhelper.js" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
