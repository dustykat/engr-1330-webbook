<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>Nonlinear Systems of Equations - CTDS Databases</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../..">CTDS Databases</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../..">Home</a>
                            </li>
                            <li >
                                <a href="../../untitled/">Databases</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Sample Code Library <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../repetition-for-loop/repetition-for-loop/">FOR ... loop</a>
</li>
                                    
<li >
    <a href="../../repetition-while-loop/repetition-while-loop/">WHILE ... loop</a>
</li>
                                    
<li >
    <a href="../../conditional-block-if/conditional-block-if/">IF ... (block structure)</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Solved Examples <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../solved-integration/NumericalIntegration/">Numerical Integration of Functional Data</a>
</li>
                                    
<li >
    <a href="../../solved-integration/NumericalIntegrationTabular/">Numerical Integration of Tabular Data</a>
</li>
                                    
<li >
    <a href="../../solved-newton/NewtonsMethod/">Single Variable Newtons Method</a>
</li>
                                    
<li class="active">
    <a href="./">Nonlinear Systems of Equations</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../../solved-newton/NewtonsMethod/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="disabled">
                                <a rel="prev" >
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#nonlinear-systems-of-equations">Nonlinear Systems of Equations</a></li>
            <li><a href="#multiple-variable-extension-of-newtons-method">Multiple-variable extension of Newton’s Method</a></li>
            <li><a href="#example-using-analytical-derivatives">Example using Analytical Derivatives</a></li>
            <li><a href="#quasi-newton-method-using-finite-difference-approximations-for-the-derivative">Quasi-Newton Method using Finite Difference Approximations for the Derivative</a></li>
            <li><a href="#vector_matrix_libpy">vector_matrix_lib.py</a></li>
            <li><a href="#linearsolverpivotpy">LinearSolverPivot.py</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="nonlinear-systems-of-equations">Nonlinear Systems of Equations</h1>
<p>Non-linear systems are extensions of the linear systems cases except the systems involve products and powers of the unknown variables. Non-linear problems are often quite difficult to manage, especially when the systems are large (many rows and many variables).
The solution to non-linear systems, if non-trivial or even possible, are usually iterative. Within the iterative steps is a linearization component – these linear systems which are intermediate computations within the overall solution process are treated by an appropriate linear system method (direct or iterative).
Consider the system below:</p>
<p>
<script type="math/tex; mode=display">\begin{gather}
\begin{matrix}
x^2 & +~y^2  \\
 e^x & +~y  \\
\end{matrix}
\begin{matrix}
= 4\\
= 1\\
\end{matrix}
\end{gather}</script>
</p>
<p>Suppose we have a solution guess <script type="math/tex">x_{k},y_{k}</script>, which of course could be wrong, but we could linearize about that guess as</p>
<p>
<script type="math/tex; mode=display">\begin{gather}
\mathbf{A} =
\begin{pmatrix}
x_k & + ~y_k  \\
0 & + ~1  \\
\end{pmatrix}
~\mathbf{x} = 
\begin{pmatrix}
x_{k+1} \\
y_{k+1} \\
\end{pmatrix}
~ \mathbf{b} = 
\begin{pmatrix}
4\\
1 - e^{x_k}\\
\end{pmatrix}
\end{gather}</script>
</p>
<p>Now if we assemble the system in the usual fashion, <script type="math/tex">\mathbf{A} \cdot \mathbf{x_{k+1}} = ~ \mathbf{b}~</script> we have a system of linear equations\footnote{Linear in <script type="math/tex">\mathbf{x_{k+1}}</script>}, which expanded look like:</p>
<p>
<script type="math/tex; mode=display">\begin{gather}
\begin{pmatrix}
x_k & + ~y_k  \\
0 & + ~1  \\
\end{pmatrix}
\cdot
\begin{pmatrix}
x_{k+1} \\
y_{k+1} \\
\end{pmatrix}
~ = 
\begin{pmatrix}
4\\
1 - e^{x_k}\\
\end{pmatrix}
\end{gather}</script>
</p>
<p>Now that the system is linear, and we can solve for <script type="math/tex">\mathbf{x_{k+1}}</script> using our linear system solver for the new guess.
If the system is convergent (not all are) then if we update, and repeat  we will eventually find a result.</p>
<p>What one really needs is a way to construct the linear system that has a systematic update method, that is discussed below</p>
<h2 id="multiple-variable-extension-of-newtons-method">Multiple-variable extension of Newton’s Method</h2>
<p>This section presents the Newton-Raphson method as a way to sometimes solve systems of non-linear equations.</p>
<p>Consider an example where the function \textbf{f} is a vector-valued function of a vector argument.</p>
<p>
<script type="math/tex; mode=display">\begin{gather}
\mathbf{f(x)} = 
\begin{matrix}
f_1 = & x^2 & +~y^2 & - 4\\
f_2 = & e^x & +~y  & - 1\\
\end{matrix}
\end{gather}</script>
</p>
<p>Let's also recall Newtons method for scalar valued function of a single variable.</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
x_{k+1}=x_{k} - \frac{  f(x_{k})  }{   \frac{df}{dx}\rvert_{x_k} } 
\label{eqn:NewtonFormula}
\end{equation}</script>
</p>
<p>When extending to higher dimensions, the analog for <script type="math/tex">x</script> is the vector \textbf{x} and the analog for the function <script type="math/tex">f()</script> is the vector function \textbf{f()}.
What remains is an analog for the first derivative in the denominator (and the concept of division of a matrix).</p>
<p>The analog to the first derivative is a matrix called the Jacobian which is comprised of the first derivatives of the function \textbf{f} with respect to the arguments \textbf{x}. <br />
For example for a 2-value function of 2 arguments (as our example above)</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\frac{df}{dx}\rvert_{x_k} =>
\begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} \\
~ & ~ \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} \\
\end{pmatrix}
\label{eqn:Jacobian}
\end{equation}</script>
</p>
<p>Next recall that division is replaced by matrix multiplication with the multiplicative inverse, so the analogy continues as</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\frac{1}{\frac{df}{dx}\rvert_{x_k}} =>
{\begin{pmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} \\
~ & ~ \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} \\
\end{pmatrix}}^{-1}
\label{eqn:JacobianInverse}
\end{equation}</script>
</p>
<p>Let's name the Jacobian \textbf{J(x)}.</p>
<p>So the multi-variate Newton's method can be written as</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\mathbf{x}_{k+1}=\mathbf{x}_{k} - \mathbf{J(x)}^{-1}\rvert_{x_k} \cdot \mathbf{f(x)}\rvert_{x_k}
\label{eqn:VectorNewtonFormula}
\end{equation}</script>
</p>
<p>In the linear systems lessons we did find a way to solve for an inverse, but it's not necessary, and is computationally expensive to invert in these examples -- a series of rearrangement of the system above yields a nice scheme that does not require inversion of a matrix.</p>
<p>First, move the <script type="math/tex">\mathbf{x}_{k}</script> to the left-hand side.</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\mathbf{x}_{k+1}-\mathbf{x}_{k} = - \mathbf{J(x)}^{-1}\rvert_{x_k} \cdot \mathbf{f(x)}\rvert_{x_k}
\end{equation}</script>
</p>
<p>Next multiply both sides by the Jacobian (The Jacobian must be non-singular otherwise we are dividing by zero)</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\mathbf{J(x)}\rvert_{x_k} \cdot (\mathbf{x}_{k+1}-\mathbf{x}_{k}) = - \mathbf{J(x)}\rvert_{x_k} \cdot \mathbf{J(x)}^{-1}\rvert_{x_k} \cdot \mathbf{f(x)}\rvert_{x_k}
\end{equation}</script>
</p>
<p>Recall a matrix multiplied by its inverse returns the identity matrix (the matrix equivalent of unity)</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
-\mathbf{J(x)}\rvert_{x_k} \cdot (\mathbf{x}_{k+1}-\mathbf{x}_{k}) = \mathbf{f(x)}\rvert_{x_k}
\end{equation}</script>
</p>
<p>So we now have an algorithm:</p>
<p>1) Start with an initial guess <script type="math/tex">\mathbf{x}_{k}</script>, compute <script type="math/tex">\mathbf{f(x)}\rvert_{x_k}</script>, and <script type="math/tex">\mathbf{J(x)}\rvert_{x_k}</script>.</p>
<p>2) Test for stopping.  Is <script type="math/tex">\mathbf{f(x)}\rvert_{x_k}</script> close to zero?  If yes, exit and report results, otherwise continue.</p>
<p>3) Solve the linear system <script type="math/tex">\mathbf{J(x)}\rvert_{x_k} \cdot (\mathbf{x}_{k+1}-\mathbf{x}_{k}) = \mathbf{f(x)}\rvert_{x_k}</script>.</p>
<p>4) Test for stopping.  Is <script type="math/tex"> (\mathbf{x}_{k+1}-\mathbf{x}_{k})</script> close to zero?  If yes, exit and report results, otherwise continue.</p>
<p>5) Compute the update <script type="math/tex">\mathbf{x}_{k+1} = \mathbf{x}_{k} - (\mathbf{x}_{k+1}-\mathbf{x}_{k}) </script>, then</p>
<p>6) Move the update into the guess vector <script type="math/tex">\mathbf{x}_{k} <=\mathbf{x}_{k+1}</script> =and repeat step 1. Stop after too many steps.</p>
<h2 id="example-using-analytical-derivatives">Example using Analytical Derivatives</h2>
<p>Now to complete the example we will employ this algorithm.</p>
<p>The function (repeated)</p>
<p>
<script type="math/tex; mode=display">\begin{gather}
\mathbf{f(x)} = 
\begin{matrix}
f_1 = & x^2 & +~y^2 & - 4\\
f_2 = & e^x & +~y  & - 1\\
\end{matrix}
\end{gather}</script>
</p>
<p>Then the Jacobian, here we will compute it analytically because we can</p>
<p>
<script type="math/tex; mode=display">\begin{equation}
\mathbf{J(x)}=>
{\begin{pmatrix}
2x & 2y \\
~ & ~ \\
e^x & 1 \\
\end{pmatrix}}
\end{equation}</script>
</p>
<p>Now for the scripts.</p>
<p>We will start by defining the two equations, and their derivatives, as well a a vector valued function <code>func</code> and its Jacobian <code>jacob</code> as below.  Here the two modules <code>LinearSolverPivot</code> and <code>vector_matrix_lib</code> are just python source code files containing prototype functions.</p>
<pre><code class="python">#################################################################
# Newton Solver Example -- Analytical Derivatives               #
#################################################################
import math   # This will import math module from python distribution
from LinearSolverPivot import linearsolver # This will import our solver module
from vector_matrix_lib import writeM,writeV,vdotv,vvsub # This will import our vector functions

def eq1(x,y):
    eq1 = x**2 + y**2 - 4.0
    return(eq1)

def eq2(x,y):
    eq2 = math.exp(x) + y - 1.0
    return(eq2)

def ddxeq1(x,y):
    ddxeq1 = 2.0*x
    return(ddxeq1)

def ddyeq1(x,y):
    ddyeq1 = 2.0*y
    return(ddyeq1)

def ddxeq2(x,y):
    ddxeq2 = math.exp(x)
    return(ddxeq2)

def ddyeq2(x,y):
    ddyeq2 = 1.0
    return(ddyeq2)

def func(x,y):
    func  = [0.0 for i in range(2)] # null list
    # build the function
    func[0] = eq1(x,y)
    func[1] = eq2(x,y)
    return(func)

def jacob(x,y):
    jacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list  
    #build the  jacobian
    jacob[0][0]=ddxeq1(x,y)
    jacob[0][1]=ddyeq1(x,y)
    jacob[1][0]=ddxeq2(x,y)
    jacob[1][1]=ddyeq2(x,y)
    return(jacob)
</code></pre>

<p>Next we create vectors to store values, and supply initial guesses to the system, and echo the inputs.</p>
<pre><code class="python">deltax  = [0.0 for i in range(2)] # null list
xguess  = [0.0 for i in range(2)] # null list
myfunc  = [0.0 for i in range(2)] # null list
myjacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list  
# supply initial guess
xguess[0] = float(input(&quot;Value for x : &quot;))
xguess[1] = float(input(&quot;Value for y : &quot;))
# build the initial function
myfunc = func(xguess[0],xguess[1])
#build the initial jacobian
myjacob=jacob(xguess[0],xguess[1])
#write initial results
writeV(xguess,2,&quot;Initial X vector &quot;)
writeV(myfunc,2,&quot;Initial FUNC vector &quot;)
writeM(myjacob,2,2,&quot;Initial Jacobian &quot;)
# solver parameters
tolerancef = 1.0e-9
tolerancex = 1.0e-9
</code></pre>

<pre><code>Value for x :  2
Value for y :  3


------ Initial X vector  ------
2.0
3.0
-----------------------------
------ Initial FUNC vector  ------
9.0
9.38905609893065
-----------------------------
------ Initial Jacobian  ------
[4.0, 6.0]
[7.38905609893065, 1.0]
-----------------------------
</code></pre>
<p>Now we apply the algorithm a few times, here the count is set to 10. So eneter the loop, test for stopping, then update.</p>
<pre><code class="python"># Newton-Raphson
for iteration in range(10):
    myfunc = func(xguess[0],xguess[1])
    testf = vdotv(myfunc,myfunc,2)
    if testf &lt;= tolerancef :
        print(&quot;f(x) close to zero\n test value : &quot;, testf)
        break
    myjacob=jacob(xguess[0],xguess[1])
    deltax=linearsolver(myjacob,myfunc)
    testx = vdotv(deltax,deltax,2)
    if testx &lt;= tolerancex :
        print(&quot;solution change small\n test value : &quot;, testx)
        break
    xguess=vvsub(xguess,deltax,2)
##    print(&quot;iteration : &quot;,iteration)
##    writeV(xguess,2,&quot;Current X vector &quot;)
##    writeV(myfunc,2,&quot;Current FUNC vector &quot;)
print(&quot;Exiting Iteration : &quot;,iteration)
writeV(xguess,2,&quot;Exiting X vector &quot;)
writeV(myfunc,2,&quot;Exiting FUNC vector &quot;)
</code></pre>

<pre><code>f(x) close to zero
 test value :  4.741631714784361e-10
Exiting Iteration :  6
------ Exiting X vector  ------
-1.8162690125838175
0.8373700502918618
-----------------------------
------ Exiting FUNC vector  ------
2.1727197990095704e-05
1.446388252723807e-06
-----------------------------
</code></pre>
<h2 id="quasi-newton-method-using-finite-difference-approximations-for-the-derivative">Quasi-Newton Method using Finite Difference Approximations for the Derivative</h2>
<p>The next variant is to approximate the derivatives -- usually a Finite-Difference approximation is used, either forward, backward, or centered differences -- generally determined based on the actual behavior of the functions themselves or by trial and error. </p>
<p>For really huge systems, we usually make the program itself make the adaptions as it proceeds.</p>
<p>The coding for a finite-difference representation of a Jacobian is shown in Listing that follows 
In constructing the Jacobian, we observe that each column of the Jacobian is simply the directional derivative of the function with respect to the variable associated with the column.<br />
For instance, the first column of the Jacobian in the example is first derivative of the first function (all rows) with respect to the first variable, in this case <script type="math/tex">x</script>.  The second column is the first derivative of the second function with respect to the second variable, <script type="math/tex">y</script>.<br />
This structure is useful to generalize the Jacobian construction method because we could write (yet another) prototype function that can take the directional derivatives for us, and just insert the returns as columns; in the example we simply modified the <code>ddx</code> and <code>ddy</code> functions from analytical to simple finite differences.</p>
<p>The example listing is specific to the 2X2 function in the example, but the extension to more general cases is evident.</p>
<pre><code class="python">#################################################################
# Newton Solver Example -- Numerical Derivatives               #
#################################################################
import math   # This will import math module from python distribution
from LinearSolverPivot import linearsolver # This will import our solver module
from vector_matrix_lib import writeM,writeV,vdotv,vvsub # This will import our vector functions

def eq1(x,y):
    eq1 = x**2 + y**2 - 4.0
    return(eq1)

def eq2(x,y):
    eq2 = math.exp(x) + y - 1.0
    return(eq2)
##############################################################
# This portion is changed for finite-difference method to evaluate derivatives #
##############################################################
def ddxeq1(x,y):
    delta = 1.0e-6
    ddxeq1 = (eq1(x+delta,y)-eq1(x,y))/delta
    return(ddxeq1)

def ddyeq1(x,y):
    delta = 1.0e-6
    ddyeq1 = (eq1(x,y+delta)-eq1(x,y))/delta
    return(ddyeq1)

def ddxeq2(x,y):
    delta = 1.0e-6
    ddxeq2 = (eq2(x+delta,y)-eq2(x,y))/delta
    return(ddxeq2)

def ddyeq2(x,y):
    delta = 1.0e-6
    ddyeq2 = (eq2(x,y+delta)-eq2(x,y))/delta
    return(ddyeq2)
##############################################################
def func(x,y):
    func  = [0.0 for i in range(2)] # null list
    # build the function
    func[0] = eq1(x,y)
    func[1] = eq2(x,y)
    return(func)

def jacob(x,y):
    jacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list  
    #build the  jacobian
    jacob[0][0]=ddxeq1(x,y)
    jacob[0][1]=ddyeq1(x,y)
    jacob[1][0]=ddxeq2(x,y)
    jacob[1][1]=ddyeq2(x,y)
    return(jacob)
deltax  = [0.0 for i in range(2)] # null list
xguess  = [0.0 for i in range(2)] # null list
myfunc  = [0.0 for i in range(2)] # null list
myjacob = [[0.0 for j in range(2)] for i in range(2)] # constructed list  
# supply initial guess
xguess[0] = float(input(&quot;Value for x : &quot;))
xguess[1] = float(input(&quot;Value for y : &quot;))
# build the initial function
myfunc = func(xguess[0],xguess[1])
#build the initial jacobian
myjacob=jacob(xguess[0],xguess[1])
#write initial results
writeV(xguess,2,&quot;Initial X vector &quot;)
writeV(myfunc,2,&quot;Initial FUNC vector &quot;)
writeM(myjacob,2,2,&quot;Initial Jacobian &quot;)
# solver parameters
tolerancef = 1.0e-9
tolerancex = 1.0e-9
# Newton-Raphson
for iteration in range(10):
    myfunc = func(xguess[0],xguess[1])
    testf = vdotv(myfunc,myfunc,2)
    if testf &lt;= tolerancef :
        print(&quot;f(x) close to zero\n test value : &quot;, testf)
        break
    myjacob=jacob(xguess[0],xguess[1])
    deltax=linearsolver(myjacob,myfunc)
    testx = vdotv(deltax,deltax,2)
    if testx &lt;= tolerancex :
        print(&quot;solution change small\n test value : &quot;, testx)
        break
    xguess=vvsub(xguess,deltax,2)
##    print(&quot;iteration : &quot;,iteration)
##    writeV(xguess,2,&quot;Current X vector &quot;)
##    writeV(myfunc,2,&quot;Current FUNC vector &quot;)
print(&quot;Exiting Iteration : &quot;,iteration)
writeV(xguess,2,&quot;Exiting X vector &quot;)
writeV(myfunc,2,&quot;Exiting FUNC vector using Finite-Differences&quot;)
</code></pre>

<p>When we run the script:</p>
<pre><code class="python">    Value for x :  0
    Value for y :  2


    ------ Initial X vector  ------
    0.0
    2.0
    -----------------------------
    ------ Initial FUNC vector  ------
    0.0
    2.0
    -----------------------------
    ------ Initial Jacobian  ------
    [1.000088900582341e-06, 4.0000010006480125]
    [1.0000005001842283, 1.000000000139778]
    -----------------------------
    f(x) close to zero
     test value :  1.124762923231742e-16
    Exiting Iteration :  5
    ------ Exiting X vector  ------
    -1.816264071231508
    0.8373678009903361
    -----------------------------
    ------ Exiting FUNC vector using Finite-Differences ------
    1.0581842957435583e-08
    7.077372021768724e-10
    -----------------------------
</code></pre>

<h2 id="vector_matrix_libpy">vector_matrix_lib.py</h2>
<pre><code class="python"># vector_matrix_lib.py
# suggested citation goes here
# useful linear algebra tools
import math   # This will import math module

def writeM(M,ir,jc,label):
    print (&quot;------&quot;,label,&quot;------&quot;)
    for i in range(0,ir,1):
        print (M[i][0:jc])
    print (&quot;-----------------------------&quot;)
    #return

def writeV(V,ir,label):
    print (&quot;------&quot;,label,&quot;------&quot;)
    for i in range(0,ir,1):
        print (V[i])
    print (&quot;-----------------------------&quot;)
    #return

def mmmult(amatrix,bmatrix,rowNumA,colNumA,rowNumB,colNumB):
    AB =[[0.0 for j in range(colNumB)] for i in range(rowNumA)]
    for i in range(0,rowNumA):
        for j in range(0,colNumB):
            for k in range(0,colNumA):
                AB[i][j]=AB[i][j]+amatrix[i][k]*bmatrix[k][j]
    return(AB)

def mvmult(amatrix,xvector,rowNumA,colNumA):
    bvector=[0.0 for i in range(rowNumA)]
    for i in range(0,rowNumA):
        for j in range(0,1):
            for k in range(0,colNumA):
                bvector[i]=bvector[i]+amatrix[i][k]*xvector[k]
    return(bvector)

def vvadd(avector,bvector,length):
    aplusb=[0.0 for i in range(length)]
    for i in range(length):
        aplusb[i] = avector[i] + bvector[i]
    return(aplusb)

def vvsub(avector,bvector,length):
    aminusb=[0.0 for i in range(length)]
    for i in range(length):
        aminusb[i] = avector[i] - bvector[i]
    return(aminusb)

def vdotv(avector,bvector,length):
    adotb=0.0
    for i in range(length):
        adotb=adotb+avector[i]*bvector[i]
    return(adotb)
</code></pre>

<h2 id="linearsolverpivotpy">LinearSolverPivot.py</h2>
<pre><code class="python"># SolveLinearSystem.py
# linearsolver with pivoting adapted from 
# https://stackoverflow.com/questions/31957096/gaussian-elimination-with-pivoting-in-python/31959226
# Supply wrapper code to read A and b
# Then solve Ax = b for x by Gaussian elimination with back substitution
# Script preserves A matrix, and b vector
##########
def linearsolver(A,b):
    n = len(A)
#    M = A  #this is object to object equivalence
# copy A into M element by element
    M=[[0.0 for jcol in range(n)]for irow in range(n)]
    for irow in range(n):
        for jcol in range(n):
            M[irow][jcol]=A[irow][jcol]


    i = 0
    for x in M:
     x.append(b[i])
     i += 1

    for k in range(n):
     for i in range(k,n):
       if abs(M[i][k]) &gt; abs(M[k][k]):
          M[k], M[i] = M[i],M[k]
       else:
          pass

     for j in range(k+1,n):
         q = float(M[j][k]) / M[k][k]
         for m in range(k, n+1):
            M[j][m] -=  q * M[k][m]

    x = [0 for i in range(n)]

    x[n-1] =float(M[n-1][n])/M[n-1][n-1]
    for i in range (n-1,-1,-1):
      z = 0
      for j in range(i+1,n):
          z = z  + float(M[i][j])*x[j]
      x[i] = float(M[i][n] - z)/M[i][i]
#    print (x)
    return(x)
#
</code></pre></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" defer></script>
        <script src="../../mathjaxhelper.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
